title,context
Westminster_Abbey,"The chapter house has an original mid-13th-century tiled pavement. A door within the vestibule dates from around 1050 and is believed to be the oldest in England.[citation needed] The exterior includes flying buttresses added in the 14th century and a leaded tent-lantern roof on an iron frame designed by Scott. The Chapter house was originally used in the 13th century by Benedictine monks for daily meetings. It later became a meeting place of the King's Great Council and the Commons, predecessors of Parliament. Westminster School and Westminster Abbey Choir School are also in the precincts of the abbey. It was natural for the learned and literate monks to be entrusted with education, and Benedictine monks were required by the Pope to maintain a charity school in 1179. The Choir School educates and trains the choirboys who sing for services in the Abbey. Since the coronations in 1066 of both King Harold and William the Conqueror, coronations of English and British monarchs were held in the abbey. In 1216, Henry III was unable to be crowned in London when he first came to the throne, because the French prince Louis had taken control of the city, and so the king was crowned in Gloucester Cathedral. This coronation was deemed by the Pope to be improper, and a further coronation was held in the abbey on 17 May 1220. The Archbishop of Canterbury is the traditional cleric in the coronation ceremony.[citation needed] Later wax effigies include a likeness of Horatio, Viscount Nelson, wearing some of his own clothes and another of Prime Minister William Pitt, Earl of Chatham, modelled by the American-born sculptor Patience Wright.[citation needed] During recent conservation of Elizabeth I's effigy, a unique corset dating from 1603 was found on the figure and is now displayed separately.[citation needed] The chapter house and Pyx Chamber at Westminster Abbey are in the guardianship of English Heritage, but under the care and management of the Dean and Chapter of Westminster. English Heritage have funded a major programme of work on the chapter house, comprising repairs to the roof, gutters, stonework on the elevations and flying buttresses as well as repairs to the lead light. Since 1066, when Harold Godwinson and William the Conqueror were crowned, the coronations of English and British monarchs have been held there. There have been at least 16 royal weddings at the abbey since 1100. Two were of reigning monarchs (Henry I and Richard II), although, before 1919, there had been none for some 500 years. In addition to the Dean and canons, there are at present two full-time minor canons, one is precentor, and the other is sacrist. The office of Priest Vicar was created in the 1970s for those who assist the minor canons. Together with the clergy and Receiver General and Chapter Clerk, various lay officers constitute the college, including the Organist and Master of the Choristers, the Registrar, the Auditor, the Legal Secretary, the Surveyor of the Fabric, the Head Master of the choir school, the Keeper of the Muniments and the Clerk of the Works, as well as 12 lay vicars, 10 choristers and the High Steward and High Bailiff. In June 2009 the first major building work at the abbey for 250 years was proposed. A corona—a crown-like architectural feature—was intended to be built around the lantern over the central crossing, replacing an existing pyramidal structure dating from the 1950s. This was part of a wider £23m development of the abbey expected to be completed in 2013. On 4 August 2010 the Dean and Chapter announced that, ""[a]fter a considerable amount of preliminary and exploratory work"", efforts toward the construction of a corona would not be continued. In 2012, architects Panter Hudspith completed refurbishment of the 14th-century food-store originally used by the abbey's monks, converting it into a restaurant with English Oak furniture by Covent Garden-based furniture makers Luke Hughes and Company. A project that is proceeding is the creation of The Queen's Diamond Jubilee Galleries in the medieval triforium of the abbey. The aim is to create a new display area for the abbey's treasures in the galleries high up around the abbey's nave. To this end a new Gothic access tower with lift has been designed by the abbey architect and Surveyor of the Fabric, Ptolemy Dean. It is planned that the new galleries will open in 2018. The abbey became the coronation site of Norman kings. None were buried there until Henry III, intensely devoted to the cult of the Confessor, rebuilt the abbey in Anglo-French Gothic style as a shrine to venerate King Edward the Confessor and as a suitably regal setting for Henry's own tomb, under the highest Gothic nave in England. The Confessor's shrine subsequently played a great part in his canonisation. The work continued between 1245 and 1517 and was largely finished by the architect Henry Yevele in the reign of Richard II. Henry III also commissioned unique Cosmati pavement in front of the High Altar (the pavement has recently undergone a major cleaning and conservation programme and was re-dedicated by the Dean at a service on 21 May 2010). Henry VII added a Perpendicular style chapel dedicated to the Blessed Virgin Mary in 1503 (known as the Henry VII Chapel or the ""Lady Chapel""). Much of the stone came from Caen, in France (Caen stone), the Isle of Portland (Portland stone) and the Loire Valley region of France (tuffeau limestone).[citation needed] Until the 19th century, Westminster was the third seat of learning in England, after Oxford and Cambridge. It was here that the first third of the King James Bible Old Testament and the last half of the New Testament were translated. The New English Bible was also put together here in the 20th century. Westminster suffered minor damage during the Blitz on 15 November 1940. On Saturday September 6, 1997 the formal, though not ""state"" Funeral of Diana, Princess of Wales, was held. It was a royal ceremonial funeral including royal pageantry and Anglican funeral liturgy. A Second Public service was held on Sunday at the demand of the people. The burial occurred privately later the same day. Diana's former husband, sons, mother, siblings, a close friend, and a clergyman were present. Diana's body was clothed in a black long-sleeved dress designed by Catherine Walker, which she had chosen some weeks before. A set of rosary beads was placed in her hands, a gift she had received from Mother Teresa. Her grave is on the grounds of her family estate, Althorp, on a private island.[citation needed] In the 1990s two icons by the Russian icon painter Sergei Fyodorov were hung in the abbey. On 6 September 1997 the funeral of Diana, Princess of Wales, was held at the Abbey. On 17 September 2010 Pope Benedict XVI became the first pope to set foot in the abbey. According to a tradition first reported by Sulcard in about 1080, a church was founded at the site (then known as Thorn Ey (Thorn Island)) in the 7th century, at the time of Mellitus, a Bishop of London. Construction of the present church began in 1245, on the orders of King Henry III. The organ was built by Harrison & Harrison in 1937, then with four manuals and 84 speaking stops, and was used for the first time at the coronation of King George VI. Some pipework from the previous Hill organ of 1848 was revoiced and incorporated in the new scheme. The two organ cases, designed in the late 19th century by John Loughborough Pearson, were re-instated and coloured in 1959. In 1982 and 1987, Harrison and Harrison enlarged the organ under the direction of the then abbey organist Simon Preston to include an additional Lower Choir Organ and a Bombarde Organ: the current instrument now has five manuals and 109 speaking stops. In 2006, the console of the organ was refurbished by Harrison and Harrison, and space was prepared for two additional 16 ft stops on the Lower Choir Organ and the Bombarde Organ. One part of the instrument, the Celestial Organ, is currently not connected or playable. The abbot and monks, in proximity to the royal Palace of Westminster, the seat of government from the later 12th century, became a powerful force in the centuries after the Norman Conquest. The abbot often was employed on royal service and in due course took his place in the House of Lords as of right. Released from the burdens of spiritual leadership, which passed to the reformed Cluniac movement after the mid-10th century, and occupied with the administration of great landed properties, some of which lay far from Westminster, ""the Benedictines achieved a remarkable degree of identification with the secular life of their times, and particularly with upper-class life"", Barbara Harvey concludes, to the extent that her depiction of daily life provides a wider view of the concerns of the English gentry in the High and Late Middle Ages.[citation needed] The exhibits include a collection of royal and other funeral effigies (funeral saddle, helm and shield of Henry V), together with other treasures, including some panels of mediaeval glass, 12th-century sculpture fragments, Mary II's coronation chair and replicas of the coronation regalia, and historic effigies of Edward III, Henry VII and his queen, Elizabeth of York, Charles II, William III, Mary II and Queen Anne. A narthex (a portico or entrance hall) for the west front was designed by Sir Edwin Lutyens in the mid-20th century but was not built. Images of the abbey prior to the construction of the towers are scarce, though the abbey's official website states that the building was without towers following Yevele's renovation, with just the lower segments beneath the roof level of the Nave completed. King Edward's Chair (or St Edward's Chair), the throne on which English and British sovereigns have been seated at the moment of coronation, is housed within the abbey and has been used at every coronation since 1308. From 1301 to 1996 (except for a short time in 1950 when it was temporarily stolen by Scottish nationalists), the chair also housed the Stone of Scone upon which the kings of Scots are crowned. Although the Stone is now kept in Scotland, in Edinburgh Castle, at future coronations it is intended that the Stone will be returned to St Edward's Chair for use during the coronation ceremony.[citation needed] Westminster Abbey is a collegiate church governed by the Dean and Chapter of Westminster, as established by Royal charter of Queen Elizabeth I in 1560, which created it as the Collegiate Church of St Peter Westminster and a Royal Peculiar under the personal jurisdiction of the Sovereign. The members of the Chapter are the Dean and four canons residentiary, assisted by the Receiver General and Chapter Clerk. One of the canons is also Rector of St Margaret's Church, Westminster, and often holds also the post of Chaplain to the Speaker of the House of Commons. It suffered damage during the turbulent 1640s, when it was attacked by Puritan iconoclasts, but was again protected by its close ties to the state during the Commonwealth period. Oliver Cromwell was given an elaborate funeral there in 1658, only to be disinterred in January 1661 and posthumously hanged from a gibbet at Tyburn. A recent addition to the exhibition is the late 13th-century Westminster Retable, England's oldest altarpiece, which was most probably designed for the high altar of the abbey. Although it has been damaged in past centuries, the panel has been expertly cleaned and conserved. The Pyx Chamber formed the undercroft of the monks' dormitory. It dates to the late 11th century and was used as a monastic and royal treasury. The outer walls and circular piers are of 11th-century date, several of the capitals were enriched in the 12th century and the stone altar added in the 13th century. The term pyx refers to the boxwood chest in which coins were held and presented to a jury during the Trial of the Pyx, in which newly minted coins were presented to ensure they conformed to the required standards. In addition there are two service bells, cast by Robert Mot, in 1585 and 1598 respectively, a Sanctus bell cast in 1738 by Richard Phelps and Thomas Lester and two unused bells—one cast about 1320, by the successor to R de Wymbish, and a second cast in 1742, by Thomas Lester. The two service bells and the 1320 bell, along with a fourth small silver ""dish bell"", kept in the refectory, have been noted as being of historical importance by the Church Buildings Council of the Church of England. The proximity of the Palace of Westminster did not extend to providing monks or abbots with high royal connections; in social origin the Benedictines of Westminster were as modest as most of the order. The abbot remained Lord of the Manor of Westminster as a town of two to three thousand persons grew around it: as a consumer and employer on a grand scale the monastery helped fuel the town economy, and relations with the town remained unusually cordial, but no enfranchising charter was issued during the Middle Ages. The abbey built shops and dwellings on the west side, encroaching upon the sanctuary.[citation needed] In 1535, the abbey's annual income of £2400–2800[citation needed] (£1,310,000 to £1,530,000 as of 2016), during the assessment attendant on the Dissolution of the Monasteries rendered it second in wealth only to Glastonbury Abbey. From the Middle Ages, aristocrats were buried inside chapels, while monks and other people associated with the abbey were buried in the cloisters and other areas. One of these was Geoffrey Chaucer, who was buried here as he had apartments in the abbey where he was employed as master of the King's Works. Other poets, writers and musicians were buried or memorialised around Chaucer in what became known as Poets' Corner. Abbey musicians such as Henry Purcell were also buried in their place of work.[citation needed] Henry III rebuilt the abbey in honour of a royal saint, Edward the Confessor, whose relics were placed in a shrine in the sanctuary. Henry III himself was interred nearby, as were many of the Plantagenet kings of England, their wives and other relatives. Until the death of George II of Great Britain in 1760, most kings and queens were buried in the abbey, some notable exceptions being Henry VI, Edward IV, Henry VIII and Charles I who are buried in St George's Chapel at Windsor Castle. Other exceptions include Richard III, now buried at Leicester Cathedral, and the de facto queen Lady Jane Grey, buried in the chapel of St Peter ad Vincula in the Tower of London. Most monarchs and royals who died after 1760 are buried either in St George's Chapel or at Frogmore to the east of Windsor Castle.[citation needed] Inner and outer vestibules lead to the octagonal chapter house, which is of exceptional architectural purity. It is built in a Geometrical Gothic style with an octagonal crypt below. A pier of eight shafts carries the vaulted ceiling. To the sides are blind arcading, remains of 14th-century paintings and numerous stone benches above which are innovatory large 4-light quatre-foiled windows. These are virtually contemporary with the Sainte-Chapelle, Paris. Westminster Abbey, formally titled the Collegiate Church of St Peter at Westminster, is a large, mainly Gothic abbey church in the City of Westminster, London, located just to the west of the Palace of Westminster. It is one of the most notable religious buildings in the United Kingdom and has been the traditional place of coronation and burial site for English and, later, British monarchs. Between 1540 and 1556 the abbey had the status of a cathedral. Since 1560, however, the building is no longer an abbey nor a cathedral, having instead the status of a Church of England ""Royal Peculiar""—a church responsible directly to the sovereign. The building itself is the original abbey church. The only extant depiction of Edward's abbey, together with the adjacent Palace of Westminster, is in the Bayeux Tapestry. Some of the lower parts of the monastic dormitory, an extension of the South Transept, survive in the Norman undercroft of the Great School, including a door said to come from the previous Saxon abbey. Increased endowments supported a community increased from a dozen monks in Dunstan's original foundation, up to a maximum about eighty monks, although there was also a large community of lay brothers who supported the monastery's extensive property and activities. The Westminster Abbey Museum is located in the 11th-century vaulted undercroft beneath the former monks' dormitory in Westminster Abbey. This is one of the oldest areas of the abbey, dating back almost to the foundation of the church by Edward the Confessor in 1065. This space has been used as a museum since 1908. The first reports of the abbey are based on a late tradition claiming that a young fisherman called Aldrich on the River Thames saw a vision of Saint Peter near the site. This seems to be quoted to justify the gifts of salmon from Thames fishermen that the abbey received in later years. In the present era, the Fishmonger's Company still gives a salmon every year. The proven origins are that in the 960s or early 970s, Saint Dunstan, assisted by King Edgar, installed a community of Benedictine monks here. At the east end of the Lady Chapel is a memorial chapel to the airmen of the RAF who were killed in the Second World War. It incorporates a memorial window to the Battle of Britain, which replaces an earlier Tudor stained glass window destroyed in the war. Subsequently, it became one of Britain's most significant honours to be buried or commemorated in the abbey. The practice of burying national figures in the abbey began under Oliver Cromwell with the burial of Admiral Robert Blake in 1657. The practice spread to include generals, admirals, politicians, doctors and scientists such as Isaac Newton, buried on 4 April 1727, and Charles Darwin, buried 26 April 1882. Another was William Wilberforce who led the movement to abolish slavery in the United Kingdom and the Plantations, buried on 3 August 1833. Wilberforce was buried in the north transept, close to his friend, the former Prime Minister, William Pitt.[citation needed] The abbey was restored to the Benedictines under the Catholic Mary I of England, but they were again ejected under Elizabeth I in 1559. In 1560, Elizabeth re-established Westminster as a ""Royal Peculiar"" – a church of the Church of England responsible directly to the Sovereign, rather than to a diocesan bishop – and made it the Collegiate Church of St Peter (that is, a non-cathedral church with an attached chapter of canons, headed by a dean.) The last of Mary's abbots was made the first dean. During the early 20th century it became increasingly common to bury cremated remains rather than coffins in the abbey. In 1905 the actor Sir Henry Irving was cremated and his ashes buried in Westminster Abbey, thereby becoming the first person ever to be cremated prior to interment at the abbey. The majority of interments at the Abbey are of cremated remains, but some burials still take place - Frances Challen, wife of the Rev Sebastian Charles, Canon of Westminster, was buried alongside her husband in the south choir aisle in 2014. Members of the Percy Family have a family vault, The Northumberland Vault, in St Nicholas's chapel within the abbey. In the floor, just inside the great west door, in the centre of the nave, is the tomb of The Unknown Warrior, an unidentified British soldier killed on a European battlefield during the First World War. He was buried in the abbey on 11 November 1920. This grave is the only one in the abbey on which it is forbidden to walk.[citation needed] The chapter house was built concurrently with the east parts of the abbey under Henry III, between about 1245 and 1253. It was restored by Sir George Gilbert Scott in 1872. The entrance is approached from the east cloister walk and includes a double doorway with a large tympanum above. The abbey's two western towers were built between 1722 and 1745 by Nicholas Hawksmoor, constructed from Portland stone to an early example of a Gothic Revival design. Purbeck marble was used for the walls and the floors of Westminster Abbey, even though the various tombstones are made of different types of marble. Further rebuilding and restoration occurred in the 19th century under Sir George Gilbert Scott. Between 1042 and 1052 King Edward the Confessor began rebuilding St Peter's Abbey to provide himself with a royal burial church. It was the first church in England built in the Romanesque style. The building was not completed until around 1090 but was consecrated on 28 December 1065, only a week before Edward's death on 5 January 1066. A week later he was buried in the church, and nine years later his wife Edith was buried alongside him. His successor, Harold II, was probably crowned in the abbey, although the first documented coronation is that of William the Conqueror later the same year. The bells at the abbey were overhauled in 1971. The ring is now made up of ten bells, hung for change ringing, cast in 1971, by the Whitechapel Bell Foundry, tuned to the notes: F#, E, D, C#, B, A, G, F#, E and D. The Tenor bell in D (588.5 Hz) has a weight of 30 cwt, 1 qtr, 15 lb (3403 lb or 1544 kg). Henry VIII assumed direct royal control in 1539 and granted the abbey the status of a cathedral by charter in 1540, simultaneously issuing letters patent establishing the Diocese of Westminster. By granting the abbey cathedral status Henry VIII gained an excuse to spare it from the destruction or dissolution which he inflicted on most English abbeys during this period. Westminster diocese was dissolved in 1550, but the abbey was recognised (in 1552, retroactively to 1550) as a second cathedral of the Diocese of London until 1556. The already-old expression ""robbing Peter to pay Paul"" may have been given a new lease of life when money meant for the abbey, which is dedicated to Saint Peter, was diverted to the treasury of St Paul's Cathedral."
Armenians,"Following the breakup of the Russian Empire in the aftermath of World War I for a brief period, from 1918 to 1920, Armenia was an independent republic. In late 1920, the communists came to power following an invasion of Armenia by the Red Army, and in 1922, Armenia became part of the Transcaucasian SFSR of the Soviet Union, later forming the Armenian Soviet Socialist Republic (1936 to September 21, 1991). In 1991, Armenia declared independence from the USSR and established the second Republic of Armenia. Small Armenian trading and religious communities have existed outside of Armenia for centuries. For example, a community has existed for over a millennium in the Holy Land, and one of the four quarters of the walled Old City of Jerusalem has been called the Armenian Quarter. An Armenian Catholic monastic community of 35 founded in 1717 exists on an island near Venice, Italy. There are also remnants of formerly populous communities in India, Myanmar, Thailand, Belgium, Portugal, Italy, Poland, Austria, Hungary, Bulgaria, Romania, Serbia, Ethiopia, Sudan and Egypt.[citation needed] Eric P. Hamp in his 2012 Indo-European family tree, groups the Armenian language along with Greek and Ancient Macedonian (""Helleno-Macedonian"") in the Pontic Indo-European (also called Helleno-Armenian) subgroup. In Hamp's view the homeland of this subgroup is the northeast coast of the Black Sea and its hinterlands. He assumes that they migrated from there southeast through the Caucasus with the Armenians remaining after Batumi while the pre-Greeks proceeded westwards along the southern coast of the Black Sea. The first geographical entity that was called Armenia by neighboring peoples (such as by Hecataeus of Miletus and on the Achaemenid Behistun Inscription) was established in the late 6th century BC under the Orontid dynasty within the Achaemenid Persian Empire as part of the latters' territories, and which later became a kingdom. At its zenith (95–65 BC), the state extended from the Caucasus all the way to what is now central Turkey, Lebanon, and northern Iran. The imperial reign of Tigranes the Great is thus the span of time during which Armenia itself conquered areas populated by other peoples. The Armenian Highland lies in the highlands surrounding Mount Ararat, the highest peak of the region. In the Bronze Age, several states flourished in the area of Greater Armenia, including the Hittite Empire (at the height of its power), Mitanni (South-Western historical Armenia), and Hayasa-Azzi (1600–1200 BC). Soon after Hayasa-Azzi were Arme-Shupria (1300s–1190 BC), the Nairi (1400–1000 BC) and the Kingdom of Urartu (860–590 BC), who successively established their sovereignty over the Armenian Highland. Each of the aforementioned nations and tribes participated in the ethnogenesis of the Armenian people. Under Ashurbanipal (669–627 BC), the Assyrian empire reached the Caucasus Mountains (modern Armenia, Georgia and Azerbaijan). The first Armenian churches were built between the 4th and 7th century, beginning when Armenia converted to Christianity, and ending with the Arab invasion of Armenia. The early churches were mostly simple basilicas, but some with side apses. By the fifth century the typical cupola cone in the center had become widely used. By the seventh century, centrally planned churches had been built and a more complicated niched buttress and radiating Hrip'simé style had formed. By the time of the Arab invasion, most of what we now know as classical Armenian architecture had formed. Armenians have had a presence in the Armenian Highland for over four thousand years, since the time when Hayk, the legendary patriarch and founder of the first Armenian nation, led them to victory over Bel of Babylon. Today, with a population of 3.5 million, they not only constitute an overwhelming majority in Armenia, but also in the disputed region of Nagorno-Karabakh. Armenians in the diaspora informally refer to them as Hayastantsis (Հայաստանցի), meaning those that are from Armenia (that is, those born and raised in Armenia). They, as well as the Armenians of Iran and Russia speak the Eastern dialect of the Armenian language. The country itself is secular as a result of Soviet domination, but most of its citizens identify themselves as Apostolic Armenian Christian. While the Armenian Apostolic Church remains the most prominent church in the Armenian community throughout the world, Armenians (especially in the diaspora) subscribe to any number of other Christian denominations. These include the Armenian Catholic Church (which follows its own liturgy but recognizes the Roman Catholic Pope), the Armenian Evangelical Church, which started as a reformation in the Mother church but later broke away, and the Armenian Brotherhood Church, which was born in the Armenian Evangelical Church, but later broke apart from it. There are other numerous Armenian churches belonging to Protestant denominations of all kinds. During Soviet rule, Armenian athletes rose to prominence winning plenty of medals and helping the USSR win the medal standings at the Olympics on numerous occasions. The first medal won by an Armenian in modern Olympic history was by Hrant Shahinyan, who won two golds and two silvers in gymnastics at the 1952 Summer Olympics in Helsinki. In football, their most successful team was Yerevan's FC Ararat, which had claimed most of the Soviet championships in the 70s and had also gone to post victories against professional clubs like FC Bayern Munich in the Euro cup. Armenian literature dates back to 400 AD, when Mesrop Mashtots first invented the Armenian alphabet. This period of time is often viewed as the Golden Age of Armenian literature. Early Armenian literature was written by the ""father of Armenian history"", Moses of Chorene, who authored The History of Armenia. The book covers the time-frame from the formation of the Armenian people to the fifth century AD. The nineteenth century beheld a great literary movement that was to give rise to modern Armenian literature. This period of time, during which Armenian culture flourished, is known as the Revival period (Zartonki sherchan). The Revivalist authors of Constantinople and Tiflis, almost identical to the Romanticists of Europe, were interested in encouraging Armenian nationalism. Most of them adopted the newly created Eastern or Western variants of the Armenian language depending on the targeted audience, and preferred them over classical Armenian (grabar). This period ended after the Hamidian massacres, when Armenians experienced turbulent times. As Armenian history of the 1920s and of the Genocide came to be more openly discussed, writers like Paruyr Sevak, Gevork Emin, Silva Kaputikyan and Hovhannes Shiraz began a new era of literature. Instruments like the duduk, the dhol, the zurna and the kanun are commonly found in Armenian folk music. Artists such as Sayat Nova are famous due to their influence in the development of Armenian folk music. One of the oldest types of Armenian music is the Armenian chant which is the most common kind of religious music in Armenia. Many of these chants are ancient in origin, extending to pre-Christian times, while others are relatively modern, including several composed by Saint Mesrop Mashtots, the inventor of the Armenian alphabet. Whilst under Soviet rule, Armenian classical music composer Aram Khatchaturian became internationally well known for his music, for various ballets and the Sabre Dance from his composition for the ballet Gayane. Art historian Hravard Hakobyan notes that ""Artsakh carpets occupy a special place in the history of Armenian carpet-making."" Common themes and patterns found on Armenian carpets were the depiction of dragons and eagles. They were diverse in style, rich in color and ornamental motifs, and were even separated in categories depending on what sort of animals were depicted on them, such as artsvagorgs (eagle-carpets), vishapagorgs (dragon-carpets) and otsagorgs (serpent-carpets). The rug mentioned in the Kaptavan inscriptions is composed of three arches, ""covered with vegatative ornaments"", and bears an artistic resemblance to the illuminated manuscripts produced in Artsakh. Armenians enjoy many different native and foreign foods. Arguably the favorite food is khorovats an Armenian-styled barbecue. Lavash is a very popular Armenian flat bread, and Armenian paklava is a popular dessert made from filo dough. Other famous Armenian foods include the kabob (a skewer of marinated roasted meat and vegetables), various dolmas (minced lamb, or beef meat and rice wrapped in grape leaves, cabbage leaves, or stuffed into hollowed vegetables), and pilaf, a rice dish. Also, ghapama, a rice-stuffed pumpkin dish, and many different salads are popular in Armenian culture. Fruits play a large part in the Armenian diet. Apricots (Prunus armeniaca, also known as Armenian Plum) have been grown in Armenia for centuries and have a reputation for having an especially good flavor. Peaches are popular as well, as are grapes, figs, pomegranates, and melons. Preserves are made from many fruits, including cornelian cherries, young walnuts, sea buckthorn, mulberries, sour cherries, and many others. Historically, the name Armenian has come to internationally designate this group of people. It was first used by neighbouring countries of ancient Armenia. The earliest attestations of the exonym Armenia date around the 6th century BC. In his trilingual Behistun Inscription dated to 517 BC, Darius I the Great of Persia refers to Urashtu (in Babylonian) as Armina (in Old Persian; Armina (    ) and Harminuya (in Elamite). In Greek, Αρμένιοι ""Armenians"" is attested from about the same time, perhaps the earliest reference being a fragment attributed to Hecataeus of Miletus (476 BC). Xenophon, a Greek general serving in some of the Persian expeditions, describes many aspects of Armenian village life and hospitality in around 401 BC. He relates that the people spoke a language that to his ear sounded like the language of the Persians. From the early 16th century, both Western Armenia and Eastern Armenia fell under Iranian Safavid rule. Owing to the century long Turco-Iranian geo-political rivalry that would last in Western Asia, significant parts of the region were frequently fought over between the two rivalling empires. From the mid 16th century with the Peace of Amasya, and decisively from the first half of the 17th century with the Treaty of Zuhab until the first half of the 19th century, Eastern Armenia was ruled by the successive Iranian Safavid, Afsharid and Qajar empires, while Western Armenia remained under Ottoman rule. In the late 1820s, the parts of historic Armenia under Iranian control centering on Yerevan and Lake Sevan (all of Eastern Armenia) were incorporated into the Russian Empire following Iran's forced ceding of the territories after its loss in the Russo-Persian War (1826-1828) and the outcoming Treaty of Turkmenchay. Western Armenia however, remained in Ottoman hands. Governments of Republic of Turkey since that time have consistently rejected charges of genocide, typically arguing either that those Armenians who died were simply in the way of a war or that killings of Armenians were justified by their individual or collective support for the enemies of the Ottoman Empire. Passage of legislation in various foreign countries condemning the persecution of the Armenians as genocide has often provoked diplomatic conflict. (See Recognition of the Armenian Genocide) The Arsacid Kingdom of Armenia, itself a branch of the Arsacid dynasty of Parthia, was the first state to adopt Christianity as its religion (it had formerly been adherent to Armenian paganism, which was influenced by Zoroastrianism, while later on adopting a few elements regarding identification of its pantheon with Greco-Roman deities). in the early years of the 4th century, likely AD 301, partly in defiance of the Sassanids it seems. In the late Parthian period, Armenia was a predominantly Zoroastrian-adhering land, but by the Christianisation, previously predominant Zoroastrianism and paganism in Armenia gradually declined. Later on, in order to further strengthen Armenian national identity, Mesrop Mashtots invented the Armenian alphabet, in 405 AD. This event ushered the Golden Age of Armenia, during which many foreign books and manuscripts were translated to Armenian by Mesrop's pupils. Armenia lost its sovereignty again in 428 AD to the rivalling Byzantine and Sassanid Persian empires, until the Muslim conquest of Persia overran also the regions in which Armenians lived. The Armenian Genocide caused widespread emigration that led to the settlement of Armenians in various countries in the world. Armenians kept to their traditions and certain diasporans rose to fame with their music. In the post-Genocide Armenian community of the United States, the so-called ""kef"" style Armenian dance music, using Armenian and Middle Eastern folk instruments (often electrified/amplified) and some western instruments, was popular. This style preserved the folk songs and dances of Western Armenia, and many artists also played the contemporary popular songs of Turkey and other Middle Eastern countries from which the Armenians emigrated. Richard Hagopian is perhaps the most famous artist of the traditional ""kef"" style and the Vosbikian Band was notable in the 40s and 50s for developing their own style of ""kef music"" heavily influenced by the popular American Big Band Jazz of the time. Later, stemming from the Middle Eastern Armenian diaspora and influenced by Continental European (especially French) pop music, the Armenian pop music genre grew to fame in the 60s and 70s with artists such as Adiss Harmandian and Harout Pamboukjian performing to the Armenian diaspora and Armenia. Also with artists such as Sirusho, performing pop music combined with Armenian folk music in today's entertainment industry. Other Armenian diasporans that rose to fame in classical or international music circles are world-renowned French-Armenian singer and composer Charles Aznavour, pianist Sahan Arzruni, prominent opera sopranos such as Hasmik Papian and more recently Isabel Bayrakdarian and Anna Kasyan. Certain Armenians settled to sing non-Armenian tunes such as the heavy metal band System of a Down (which nonetheless often incorporates traditional Armenian instrumentals and styling into their songs) or pop star Cher. Ruben Hakobyan (Ruben Sasuntsi) is a well recognized Armenian ethnographic and patriotic folk singer who has achieved widespread national recognition due to his devotion to Armenian folk music and exceptional talent. In the Armenian diaspora, Armenian revolutionary songs are popular with the youth.[citation needed] These songs encourage Armenian patriotism and are generally about Armenian history and national heroes. Armenians constitute the main population of Armenia and the de facto independent Nagorno-Karabakh Republic. There is a wide-ranging diaspora of around 5 million people of full or partial Armenian ancestry living outside of modern Armenia. The largest Armenian populations today exist in Russia, the United States, France, Georgia, Iran, Ukraine, Lebanon, and Syria. With the exceptions of Iran and the former Soviet states, the present-day Armenian diaspora was formed mainly as a result of the Armenian Genocide. The Armenians collective has, at times, constituted a Christian ""island"" in a mostly Muslim region. There is, however, a minority of ethnic Armenian Muslims, known as Hamshenis but many Armenians view them as a separate race, while the history of the Jews in Armenia dates back 2,000 years. The Armenian Kingdom of Cilicia had close ties to European Crusader States. Later on, the deteriorating situation in the region led the bishops of Armenia to elect a Catholicos in Etchmiadzin, the original seat of the Catholicosate. In 1441, a new Catholicos was elected in Etchmiadzin in the person of Kirakos Virapetsi, while Krikor Moussapegiants preserved his title as Catholicos of Cilicia. Therefore, since 1441, there have been two Catholicosates in the Armenian Church with equal rights and privileges, and with their respective jurisdictions. The primacy of honor of the Catholicosate of Etchmiadzin has always been recognized by the Catholicosate of Cilicia. In 885 AD the Armenians reestablished themselves as a sovereign kingdom under the leadership of Ashot I of the Bagratid Dynasty. A considerable portion of the Armenian nobility and peasantry fled the Byzantine occupation of Bagratid Armenia in 1045, and the subsequent invasion of the region by Seljuk Turks in 1064. They settled in large numbers in Cilicia, an Anatolian region where Armenians were already established as a minority since Roman times. In 1080, they founded an independent Armenian Principality then Kingdom of Cilicia, which became the focus of Armenian nationalism. The Armenians developed close social, cultural, military, and religious ties with nearby Crusader States, but eventually succumbed to Mamluk invasions. In the next few centuries, Djenghis Khan, Timurids, and the tribal Turkic federations of the Ak Koyunlu and the Kara Koyunlu ruled over the Armenians. Armenia established a Church that still exists independently of both the Catholic and the Eastern Orthodox churches, having become so in 451 AD as a result of its stance regarding the Council of Chalcedon. Today this church is known as the Armenian Apostolic Church, which is a part of the Oriental Orthodox communion, not to be confused with the Eastern Orthodox communion. During its later political eclipses, Armenia depended on the church to preserve and protect its unique identity. The original location of the Armenian Catholicosate is Echmiadzin. However, the continuous upheavals, which characterized the political scenes of Armenia, made the political power move to safer places. The Church center moved as well to different locations together with the political authority. Therefore, it eventually moved to Cilicia as the Holy See of Cilicia. From the 9th to 11th century, Armenian architecture underwent a revival under the patronage of the Bagratid Dynasty with a great deal of building done in the area of Lake Van, this included both traditional styles and new innovations. Ornately carved Armenian Khachkars were developed during this time. Many new cities and churches were built during this time, including a new capital at Lake Van and a new Cathedral on Akdamar Island to match. The Cathedral of Ani was also completed during this dynasty. It was during this time that the first major monasteries, such as Haghpat and Haritchavank were built. This period was ended by the Seljuk invasion. Within the diasporan Armenian community, there is an unofficial classification of the different kinds of Armenians. For example, Armenians who originate from Iran are referred to as Parskahay (Պարսկահայ), while Armenians from Lebanon are usually referred to as Lipananahay (Լիբանանահայ). Armenians of the Diaspora are the primary speakers of the Western dialect of the Armenian language. This dialect has considerable differences with Eastern Armenian, but speakers of either of the two variations can usually understand each other. Eastern Armenian in the diaspora is primarily spoken in Iran and European countries such as Ukraine, Russia, and Georgia (where they form a majority in the Samtskhe-Javakheti province). In diverse communities (such as in Canada and the U.S.) where many different kinds of Armenians live together, there is a tendency for the different groups to cluster together. Carpet-weaving is historically a major traditional profession for the majority of Armenian women, including many Armenian families. Prominent Karabakh carpet weavers there were men too. The oldest extant Armenian carpet from the region, referred to as Artsakh (see also Karabakh carpet) during the medieval era, is from the village of Banants (near Gandzak) and dates to the early 13th century. The first time that the Armenian word for carpet, gorg, was used in historical sources was in a 1242–1243 Armenian inscription on the wall of the Kaptavan Church in Artsakh."
Egypt,"The Suez Canal is an artificial sea-level waterway in Egypt considered the most important centre of the maritime transport in the Middle East, connecting the Mediterranean Sea and the Red Sea. Opened in November 1869 after 10 years of construction work, it allows ship transport between Europe and Asia without navigation around Africa. The northern terminus is Port Said and the southern terminus is Port Tawfiq at the city of Suez. Ismailia lies on its west bank, 3 km (1.9 mi) from the half-way point. With over 90 million inhabitants, Egypt is the most populous country in North Africa and the Arab World, the third-most populous in Africa (after Nigeria and Ethiopia), and the fifteenth-most populous in the world. The great majority of its people live near the banks of the Nile River, an area of about 40,000 square kilometres (15,000 sq mi), where the only arable land is found. The large regions of the Sahara desert, which constitute most of Egypt's territory, are sparsely inhabited. About half of Egypt's residents live in urban areas, with most spread across the densely populated centres of greater Cairo, Alexandria and other major cities in the Nile Delta. Egypt's economy depends mainly on agriculture, media, petroleum imports, natural gas, and tourism; there are also more than three million Egyptians working abroad, mainly in Saudi Arabia, the Persian Gulf and Europe. The completion of the Aswan High Dam in 1970 and the resultant Lake Nasser have altered the time-honored place of the Nile River in the agriculture and ecology of Egypt. A rapidly growing population, limited arable land, and dependence on the Nile all continue to overtax resources and stress the economy. Egypt actively practices capital punishment. Egypt's authorities do not release figures on death sentences and executions, despite repeated requests over the years by human rights organisations. The United Nations human rights office and various NGOs expressed ""deep alarm"" after an Egyptian Minya Criminal Court sentenced 529 people to death in a single hearing on 25 March 2014. Sentenced supporters of former President Mohamed Morsi will be executed for their alleged role in violence following his ousting in July 2013. The judgment was condemned as a violation of international law. By May 2014, approximately 16,000 people (and as high as more than 40,000 by one independent count), mostly Brotherhood members or supporters, have been imprisoned after the coup  after the Muslim Brotherhood was labelled as terrorist organisation by the post-coup interim Egyptian government. The permanent headquarters of the Arab League are located in Cairo and the body's secretary general has traditionally been Egyptian. This position is currently held by former foreign minister Nabil el-Araby. The Arab League briefly moved from Egypt to Tunis in 1978 to protest the Egypt-Israel Peace Treaty, but it later returned to Cairo in 1989. Gulf monarchies, including the United Arab Emirates and Saudi Arabia, have pledged billions of dollars to help Egypt overcome its economic difficulties since the July 2013 coup. Of the Christian minority in Egypt over 90% belong to the native Coptic Orthodox Church of Alexandria, an Oriental Orthodox Christian Church. Other native Egyptian Christians are adherents of the Coptic Catholic Church, the Evangelical Church of Egypt and various other Protestant denominations. Non-native Christian communities are largely found in the urban regions of Cairo and Alexandria, such as the Syro-Lebanese, who belong to Greek Catholic, Greek Orthodox, and Maronite Catholic denominations. The New Kingdom c. 1550–1070 BC began with the Eighteenth Dynasty, marking the rise of Egypt as an international power that expanded during its greatest extension to an empire as far south as Tombos in Nubia, and included parts of the Levant in the east. This period is noted for some of the most well known Pharaohs, including Hatshepsut, Thutmose III, Akhenaten and his wife Nefertiti, Tutankhamun and Ramesses II. The first historically attested expression of monotheism came during this period as Atenism. Frequent contacts with other nations brought new ideas to the New Kingdom. The country was later invaded and conquered by Libyans, Nubians and Assyrians, but native Egyptians eventually drove them out and regained control of their country. Although Egypt was a majority Christian country before the 7th Century, after Islam arrived, the country was slowly Islamified to become a majority Muslim country. Egypt emerged as a centre of politics and culture in the Muslim world. Under Anwar Sadat, Islam became the official state religion and Sharia the main source of law. It is estimated that 15 million Egyptians follow Native Sufi orders, with the Sufi leadership asserting that the numbers are much greater as many Egyptian Sufis are not officially registered with a Sufi order. Miṣr (IPA: [mi̠sˤr] or Egyptian Arabic pronunciation: [mesˤɾ]; Arabic: مِصر‎) is the Classical Quranic Arabic and modern official name of Egypt, while Maṣr (IPA: [mɑsˤɾ]; Egyptian Arabic: مَصر) is the local pronunciation in Egyptian Arabic. The name is of Semitic origin, directly cognate with other Semitic words for Egypt such as the Hebrew מִצְרַיִם (Mitzráyim). The oldest attestation of this name for Egypt is the Akkadian 𒆳 𒈪 𒄑 𒊒 KURmi-iṣ-ru miṣru, related to miṣru/miṣirru/miṣaru, meaning ""border"" or ""frontier"". At the time of the fall of the Egyptian monarchy in the early 1950s, less than half a million Egyptians were considered upper class and rich, four million middle class and 17 million lower class and poor. Fewer than half of all primary-school-age children attended school, most of them being boys. Nasser's policies changed this. Land reform and distribution, the dramatic growth in university education, and government support to national industries greatly improved social mobility and flattened the social curve. From academic year 1953-54 through 1965-66, overall public school enrolments more than doubled. Millions of previously poor Egyptians, through education and jobs in the public sector, joined the middle class. Doctors, engineers, teachers, lawyers, journalists, constituted the bulk of the swelling middle class in Egypt under Nasser. During the 1960s, the Egyptian economy went from sluggish to the verge of collapse, the society became less free, and Nasser's appeal waned considerably. Following the 1973 war and the subsequent peace treaty, Egypt became the first Arab nation to establish diplomatic relations with Israel. Despite that, Israel is still widely considered as a hostile state by the majority of Egyptians. Egypt has played a historical role as a mediator in resolving various disputes in the Middle East, most notably its handling of the Israeli-Palestinian conflict and the peace process. Egypt's ceasefire and truce brokering efforts in Gaza have hardly been challenged following Israel's evacuation of its settlements from the strip in 2005, despite increasing animosity towards the Hamas government in Gaza following the ouster of Mohamed Morsi, and despite recent attempts by countries like Turkey and Qatar to take over this role. As a result of modernisation efforts over the years, Egypt's healthcare system has made great strides forward. Access to healthcare in both urban and rural areas greatly improved and immunisation programs are now able to cover 98% of the population. Life expectancy increased from 44.8 years during the 1960s to 72.12 years in 2009. There was a noticeable decline of the infant mortality rate (during the 1970s to the 1980s the infant mortality rate was 101-132/1000 live births, in 2000 the rate was 50-60/1000, and in 2008 it was 28-30/1000). Coptic Christians face discrimination at multiple levels of the government, ranging from disproportionate representation in government ministries to laws that limit their ability to build or repair churches. Intolerance of Bahá'ís and non-orthodox Muslim sects, such as Sufis, Shi'a and Ahmadis, also remains a problem. When the government moved to computerise identification cards, members of religious minorities, such as Bahá'ís, could not obtain identification documents. An Egyptian court ruled in early 2008 that members of other faiths may obtain identity cards without listing their faiths, and without becoming officially recognised. Drinking water supply and sanitation in Egypt is characterised by both achievements and challenges. Among the achievements are an increase of piped water supply between 1990 and 2010 from 89% to 100% in urban areas and from 39% to 93% in rural areas despite rapid population growth, the elimination of open defecation in rural areas during the same period, and in general a relatively high level of investment in infrastructure. Access to an improved water source in Egypt is now practically universal with a rate of 99%. About one half of the population is connected to sanitary sewers. Egypt's most prominent multinational companies are the Orascom Group and Raya Contact Center. The information technology (IT) sector has expanded rapidly in the past few years, with many start-ups selling outsourcing services to North America and Europe, operating with companies such as Microsoft, Oracle and other major corporations, as well as many small and medium size enterprises. Some of these companies are the Xceed Contact Center, Raya, E Group Connections and C3. The IT sector has been stimulated by new Egyptian entrepreneurs with government encouragement.[citation needed] The canal is 193.30 km (120.11 mi) long, 24 m (79 ft) deep and 205 metres (673 ft) wide as of 2010. It consists of the northern access channel of 22 km (14 mi), the canal itself of 162.25 km (100.82 mi) and the southern access channel of 9 km (5.6 mi). The canal is a single lane with passing places in the ""Ballah By-Pass"" and the Great Bitter Lake. It contains no locks; seawater flows freely through the canal. In general, the canal north of the Bitter Lakes flows north in winter and south in summer. The current south of the lakes changes with the tide at Suez. Ethnic Egyptians are by far the largest ethnic group in the country, constituting 91% of the total population. Ethnic minorities include the Abazas, Turks, Greeks, Bedouin Arab tribes living in the eastern deserts and the Sinai Peninsula, the Berber-speaking Siwis (Amazigh) of the Siwa Oasis, and the Nubian communities clustered along the Nile. There are also tribal Beja communities concentrated in the south-eastern-most corner of the country, and a number of Dom clans mostly in the Nile Delta and Faiyum who are progressively becoming assimilated as urbanisation increases. In mid May 1967, the Soviet Union issued warnings to Nasser of an impending Israeli attack on Syria. Although the chief of staff Mohamed Fawzi verified them as ""baseless"", Nasser took three successive steps that made the war virtually inevitable: On 14 May he deployed his troops in Sinai near the border with Israel, on 19 May he expelled the UN peacekeepers stationed in the Sinai Peninsula border with Israel, and on 23 May he closed the Straits of Tiran to Israeli shipping. On 26 May Nasser declared, ""The battle will be a general one and our basic objective will be to destroy Israel"". The Byzantines were able to regain control of the country after a brief Sasanian Persian invasion early in the 7th century amidst the Byzantine–Sasanian War of 602–628 during which they established a new short-lived province for ten years known as Sasanian Egypt, until 639–42, when Egypt was invaded and conquered by the Islamic Empire by the Muslim Arabs. When they defeated the Byzantine Armies in Egypt, the Arabs brought Sunni Islam to the country. Early in this period, Egyptians began to blend their new faith with indigenous beliefs and practices, leading to various Sufi orders that have flourished to this day. These earlier rites had survived the period of Coptic Christianity. On 18 January 2014, the interim government instituted a new constitution following a referendum in which 98.1% of voters were supportive. Participation was low with only 38.6% of registered voters participating although this was higher than the 33% who voted in a referendum during Morsi's tenure. On 26 March 2014 Abdel Fattah el-Sisi the head of the Egyptian Armed Forces, who at this time was in control of the country, resigned from the military, announcing he would stand as a candidate in the 2014 presidential election. The poll, held between 26 and 28 May 2014, resulted in a landslide victory for el-Sisi. Sisi was sworn into office as President of Egypt on 8 June 2014. The Muslim Brotherhood and some liberal and secular activist groups boycotted the vote. Even though the military-backed authorities extended voting to a third day, the 46% turnout was lower than the 52% turnout in the 2012 election. Egypt also hosts an unknown number of refugees and asylum seekers, estimated to be between 500,000 and 3 million. There are some 70,000 Palestinian refugees, and about 150,000 recently arrived Iraqi refugees, but the number of the largest group, the Sudanese, is contested.[nb 1] The once-vibrant and ancient Greek and Jewish communities in Egypt have almost disappeared, with only a small number remaining in the country, but many Egyptian Jews visit on religious or other occasions and tourism. Several important Jewish archaeological and historical sites are found in Cairo, Alexandria and other cities. Egyptian literature traces its beginnings to ancient Egypt and is some of the earliest known literature. Indeed, the Egyptians were the first culture to develop literature as we know it today, that is, the book. It is an important cultural element in the life of Egypt. Egyptian novelists and poets were among the first to experiment with modern styles of Arabic literature, and the forms they developed have been widely imitated throughout the Middle East. The first modern Egyptian novel Zaynab by Muhammad Husayn Haykal was published in 1913 in the Egyptian vernacular. Egyptian novelist Naguib Mahfouz was the first Arabic-language writer to win the Nobel Prize in Literature. Egyptian women writers include Nawal El Saadawi, well known for her feminist activism, and Alifa Rifaat who also writes about women and tradition. In 1975, Sadat shifted Nasser's economic policies and sought to use his popularity to reduce government regulations and encourage foreign investment through his program of Infitah. Through this policy, incentives such as reduced taxes and import tariffs attracted some investors, but investments were mainly directed at low risk and profitable ventures like tourism and construction, abandoning Egypt's infant industries. Even though Sadat's policy was intended to modernise Egypt and assist the middle class, it mainly benefited the higher class, and, because of the elimination of subsidies on basic foodstuffs, led to the 1977 Egyptian Bread Riots. In the 1980s, 1990s, and 2000s, terrorist attacks in Egypt became numerous and severe, and began to target Christian Copts, foreign tourists and government officials. In the 1990s an Islamist group, Al-Gama'a al-Islamiyya, engaged in an extended campaign of violence, from the murders and attempted murders of prominent writers and intellectuals, to the repeated targeting of tourists and foreigners. Serious damage was done to the largest sector of Egypt's economy—tourism—and in turn to the government, but it also devastated the livelihoods of many of the people on whom the group depended for support. Economic conditions have started to improve considerably, after a period of stagnation, due to the adoption of more liberal economic policies by the government as well as increased revenues from tourism and a booming stock market. In its annual report, the International Monetary Fund (IMF) has rated Egypt as one of the top countries in the world undertaking economic reforms. Some major economic reforms undertaken by the government since 2003 include a dramatic slashing of customs and tariffs. A new taxation law implemented in 2005 decreased corporate taxes from 40% to the current 20%, resulting in a stated 100% increase in tax revenue by the year 2006. Egypt has a wide range of beaches situated on the Mediterranean and the Red Sea that extend to over 3,000 km. The Red Sea has serene waters, coloured coral reefs, rare fish and beautiful mountains. The Akba Gulf beaches also provide facilities for practising sea sports. Safaga tops the Red Sea zone with its beautiful location on the Suez Gulf. Last but not least, Sharm el-Sheikh (or City of Peace), Hurghada, Luxor (known as world's greatest open-air museum/ or City of the ⅓ of world monuments), Dahab, Ras Sidr, Marsa Alam, Safaga and the northern coast of the Mediterranean are major tourist's destinations of the recreational tourism. On 18 January 2014, the interim government successfully institutionalised a more secular constitution. The president is elected to a four-year term and may serve 2 terms. The parliament may impeach the president. Under the constitution, there is a guarantee of gender equality and absolute freedom of thought. The military retains the ability to appoint the national Minister of Defence for the next 8 years. Under the constitution, political parties may not be based on ""religion, race, gender or geography"". Muhammad Ali Pasha evolved the military from one that convened under the tradition of the corvée to a great modernised army. He introduced conscription of the male peasantry in 19th century Egypt, and took a novel approach to create his great army, strengthening it with numbers and in skill. Education and training of the new soldiers was not an option; the new concepts were furthermore enforced by isolation. The men were held in barracks to avoid distraction of their growth as a military unit to be reckoned with. The resentment for the military way of life eventually faded from the men and a new ideology took hold, one of nationalism and pride. It was with the help of this newly reborn martial unit that Muhammad Ali imposed his rule over Egypt. The Ptolemaic Kingdom was a powerful Hellenistic state, extending from southern Syria in the east, to Cyrene to the west, and south to the frontier with Nubia. Alexandria became the capital city and a centre of Greek culture and trade. To gain recognition by the native Egyptian populace, they named themselves as the successors to the Pharaohs. The later Ptolemies took on Egyptian traditions, had themselves portrayed on public monuments in Egyptian style and dress, and participated in Egyptian religious life. The official language of the Republic is Modern Standard Arabic. Arabic was adopted by the Egyptians after the Arab invasion of Egypt. The spoken languages are: Egyptian Arabic (68%), Sa'idi Arabic (29%), Eastern Egyptian Bedawi Arabic (1.6%), Sudanese Arabic (0.6%), Domari (0.3%), Nobiin (0.3%), Beja (0.1%), Siwi and others. Additionally, Greek, Armenian and Italian are the main languages of immigrants. In Alexandria in the 19th century there was a large community of Italian Egyptians and Italian was the ""lingua franca"" of the city. The United States provides Egypt with annual military assistance, which in 2015 amounted to US$1.3 billion. In 1989, Egypt was designated as a major non-NATO ally of the United States. Nevertheless, ties between the two countries have partially soured since the July 2013 military coup that deposed Islamist president Mohamed Morsi, with the Obama administration condemning Egypt's violent crackdown on the Muslim Brotherhood and its supporters, and cancelling future military exercises involving the two countries. There have been recent attempts, however, to normalise relations between the two, with both governments frequently calling for mutual support in the fight against regional and international terrorism. In 1970, President Nasser died and was succeeded by Anwar Sadat. Sadat switched Egypt's Cold War allegiance from the Soviet Union to the United States, expelling Soviet advisors in 1972. He launched the Infitah economic reform policy, while clamping down on religious and secular opposition. In 1973, Egypt, along with Syria, launched the October War, a surprise attack to regain part of the Sinai territory Israel had captured 6 years earlier. it presented Sadat with a victory that allowed him to regain the Sinai later in return for peace with Israel. Football is the most popular national sport of Egypt. The Cairo Derby is one of the fiercest derbies in Africa, and the BBC picked it as one of the 7 toughest derbies in the world. Al Ahly is the most successful club of the 20th century in the African continent according to CAF, closely followed by their rivals Zamalek SC. Al Ahly was named in 2000 by the Confederation of African Football as the ""African Club of the Century"". With twenty titles, Al Ahly is currently the world's most successful club in terms of international trophies, surpassing Italy's A.C. Milan and Argentina's Boca Juniors, both having eighteen. Modern Egypt is considered to be a regional and middle power, with significant cultural, political, and military influence in North Africa, the Middle East and the Muslim world. Its economy is one of the largest and most diversified in the Middle East, with sectors such as tourism, agriculture, industry and services at almost equal production levels. In 2011, longtime President Hosni Mubarak stepped down amid mass protests. Later elections saw the rise of the Muslim Brotherhood, which was ousted by the army a year later amid mass protests. Although one of the main obstacles still facing the Egyptian economy is the limited trickle down of wealth to the average population, many Egyptians criticise their government for higher prices of basic goods while their standards of living or purchasing power remains relatively stagnant. Corruption is often cited by Egyptians as the main impediment to further economic growth. The government promised major reconstruction of the country's infrastructure, using money paid for the newly acquired third mobile license ($3 billion) by Etisalat in 2006. In the Corruption Perceptions Index 2013, Egypt was ranked 114 out of 177. Egypt has a developed energy market based on coal, oil, natural gas, and hydro power. Substantial coal deposits in the northeast Sinai are mined at the rate of about 600,000 tonnes (590,000 long tons; 660,000 short tons) per year. Oil and gas are produced in the western desert regions, the Gulf of Suez, and the Nile Delta. Egypt has huge reserves of gas, estimated at 2,180 cubic kilometres (520 cu mi), and LNG up to 2012 exported to many countries. In 2013, the Egyptian General Petroleum Co (EGPC) said the country will cut exports of natural gas and tell major industries to slow output this summer to avoid an energy crisis and stave off political unrest, Reuters has reported. Egypt is counting on top liquid natural gas (LNG) exporter Qatar to obtain additional gas volumes in summer, while encouraging factories to plan their annual maintenance for those months of peak demand, said EGPC chairman, Tarek El Barkatawy. Egypt produces its own energy, but has been a net oil importer since 2008 and is rapidly becoming a net importer of natural gas. Egypt has one of the longest histories of any modern country, arising in the tenth millennium BC as one of the world's first nation states. Considered a cradle of civilisation, Ancient Egypt experienced some of the earliest developments of writing, agriculture, urbanisation, organised religion and central government. Iconic monuments such as the Giza Necropolis and its Great Sphinx, as well the ruins of Memphis, Thebes, Karnak, and the Valley of the Kings, reflect this legacy and remain a significant focus of archaeological study and popular interest worldwide. Egypt's rich cultural heritage is an integral part of its national identity, having endured, and at times assimilated, various foreign influences, including Greek, Persian, Roman, Arab, Ottoman, and European. Although Christianised in the first century of the Common Era, it was subsequently Islamised due to the Islamic conquests of the seventh century. By about 6000 BC, a Neolithic culture rooted in the Nile Valley. During the Neolithic era, several predynastic cultures developed independently in Upper and Lower Egypt. The Badarian culture and the successor Naqada series are generally regarded as precursors to dynastic Egypt. The earliest known Lower Egyptian site, Merimda, predates the Badarian by about seven hundred years. Contemporaneous Lower Egyptian communities coexisted with their southern counterparts for more than two thousand years, remaining culturally distinct, but maintaining frequent contact through trade. The earliest known evidence of Egyptian hieroglyphic inscriptions appeared during the predynastic period on Naqada III pottery vessels, dated to about 3200 BC. Most of Egypt's rain falls in the winter months. South of Cairo, rainfall averages only around 2 to 5 mm (0.1 to 0.2 in) per year and at intervals of many years. On a very thin strip of the northern coast the rainfall can be as high as 410 mm (16.1 in), mostly between October and March. Snow falls on Sinai's mountains and some of the north coastal cities such as Damietta, Baltim, Sidi Barrany, etc. and rarely in Alexandria. A very small amount of snow fell on Cairo on 13 December 2013, the first time Cairo received snowfall in many decades. Frost is also known in mid-Sinai and mid-Egypt. Egypt is the driest and the sunniest country in the world, and most of its land surface is desert. The First Intermediate Period ushered in a time of political upheaval for about 150 years. Stronger Nile floods and stabilisation of government, however, brought back renewed prosperity for the country in the Middle Kingdom c. 2040 BC, reaching a peak during the reign of Pharaoh Amenemhat III. A second period of disunity heralded the arrival of the first foreign ruling dynasty in Egypt, that of the Semitic Hyksos. The Hyksos invaders took over much of Lower Egypt around 1650 BC and founded a new capital at Avaris. They were driven out by an Upper Egyptian force led by Ahmose I, who founded the Eighteenth Dynasty and relocated the capital from Memphis to Thebes. Foreign direct investment (FDI) in Egypt increased considerably before the removal of Hosni Mubarak, exceeding $6 billion in 2006, due to economic liberalisation and privatisation measures taken by minister of investment Mahmoud Mohieddin.[citation needed] Since the fall of Hosni Mubarak in 2011, Egypt has experienced a drastic fall in both foreign investment and tourism revenues, followed by a 60% drop in foreign exchange reserves, a 3% drop in growth, and a rapid devaluation of the Egyptian pound. Egyptian music is a rich mixture of indigenous, Mediterranean, African and Western elements. It has been an integral part of Egyptian culture since antiquity. The ancient Egyptians credited one of their gods Hathor with the invention of music, which Osiris in turn used as part of his effort to civilise the world. Egyptians used music instruments since then. Contemporary Egyptian music traces its beginnings to the creative work of people such as Abdu El Hamouli, Almaz and Mahmoud Osman, who influenced the later work of Sayed Darwish, Umm Kulthum, Mohammed Abdel Wahab and Abdel Halim Hafez whose age is considered the golden age of music in Egypt and the whole Middle East and North-Africa. Prominent contemporary Egyptian pop singers include Amr Diab and Mohamed Mounir. The Egyptian military has dozens of factories manufacturing weapons as well as consumer goods. The Armed Forces' inventory includes equipment from different countries around the world. Equipment from the former Soviet Union is being progressively replaced by more modern US, French, and British equipment, a significant portion of which is built under license in Egypt, such as the M1 Abrams tank.[citation needed] Relations with Russia have improved significantly following Mohamed Morsi's removal and both countries have worked since then to strengthen military and trade ties among other aspects of bilateral co-operation. Relations with China have also improved considerably. In 2014, Egypt and China have established a bilateral ""comprehensive strategic partnership"". The Pew Forum on Religion & Public Life ranks Egypt as the fifth worst country in the world for religious freedom. The United States Commission on International Religious Freedom, a bipartisan independent agency of the US government, has placed Egypt on its watch list of countries that require close monitoring due to the nature and extent of violations of religious freedom engaged in or tolerated by the government. According to a 2010 Pew Global Attitudes survey, 84% of Egyptians polled supported the death penalty for those who leave Islam; 77% supported whippings and cutting off of hands for theft and robbery; and 82% support stoning a person who commits adultery. The plan stated that the following numbers of species of different groups had been recorded from Egypt: algae (1483 species), animals (about 15,000 species of which more than 10,000 were insects), fungi (more than 627 species), monera (319 species), plants (2426 species), protozoans (371 species). For some major groups, for example lichen-forming fungi and nematode worms, the number was not known. Apart from small and well-studied groups like amphibians, birds, fish, mammals and reptiles, the many of those numbers are likely to increase as further species are recorded from Egypt. For the fungi, including lichen-forming species, for example, subsequent work has shown that over 2200 species have been recorded from Egypt, and the final figure of all fungi actually occurring in the country is expected to be much higher. The Suez Canal, built in partnership with the French, was completed in 1869. Its construction led to enormous debt to European banks, and caused popular discontent because of the onerous taxation it required. In 1875 Ismail was forced to sell Egypt's share in the canal to the British Government. Within three years this led to the imposition of British and French controllers who sat in the Egyptian cabinet, and, ""with the financial power of the bondholders behind them, were the real power in the Government."" Partly because of low sanitation coverage about 17,000 children die each year because of diarrhoea. Another challenge is low cost recovery due to water tariffs that are among the lowest in the world. This in turn requires government subsidies even for operating costs, a situation that has been aggravated by salary increases without tariff increases after the Arab Spring. Poor operation of facilities, such as water and wastewater treatment plants, as well as limited government accountability and transparency, are also issues. The last ruler from the Ptolemaic line was Cleopatra VII, who committed suicide following the burial of her lover Mark Antony who had died in her arms (from a self-inflicted stab wound), after Octavian had captured Alexandria and her mercenary forces had fled. The Ptolemies faced rebellions of native Egyptians often caused by an unwanted regime and were involved in foreign and civil wars that led to the decline of the kingdom and its annexation by Rome. Nevertheless, Hellenistic culture continued to thrive in Egypt well after the Muslim conquest. Egypt has hosted several international competitions. the last one was 2009 FIFA U-20 World Cup which took place between 24 September - 16 October 2009. On Friday 19 September of the year 2014, Guinness World Records has announced that Egyptian scuba diver Ahmed Gabr is the new title holder for deepest salt water scuba dive, at 332.35 metres. Ahmed set a new world record Friday when he reached a depth of more than 1,000 feet. The 14-hour feat took Gabr 1,066 feet down into the abyss near the Egyptian town of Dahab in ther Red Sea, where he works as a diving instructor. The House of Representatives, whose members are elected to serve five-year terms, specialises in legislation. Elections were last held between November 2011 and January 2012 which was later dissolved. The next parliamentary election will be held within 6 months of the constitution's ratification on 18 January 2014. Originally, the parliament was to be formed before the president was elected, but interim president Adly Mansour pushed the date. The Egyptian presidential election, 2014, took place on 26–28 May 2014. Official figures showed a turnout of 25,578,233 or 47.5%, with Abdel Fattah el-Sisi winning with 23.78 million votes, or 96.91% compared to 757,511 (3.09%) for Hamdeen Sabahi. After Morsi was ousted by the military, the judiciary system aligned itself with the new government, actively suopporting the repression of Muslim Brotherhood members. This resulted in a sharp increase in mass death sentences that arose criticism from the US president Barack Obama and the General Secretary of the UN, Ban Ki Moon. In April 2013, one judge of the Minya governatorate of Upper Egypt, sentenced 1,212 people to death. In December 2014 the judge Mohammed Nagi Shahata, notorious for his fierceness in passing on death sentences, condemened to the capital penalty 188 members of the Muslim Brotherhood, for assaulting a police station. Various Egyptian and international human rights organisations have already pointed out the lack of fair trials, that often last only a few minutes and do not take into consideration the procedural standards of fair trials. In 1958, Egypt and Syria formed a sovereign union known as the United Arab Republic. The union was short-lived, ending in 1961 when Syria seceded, thus ending the union. During most of its existence, the United Arab Republic was also in a loose confederation with North Yemen (or the Mutawakkilite Kingdom of Yemen), known as the United Arab States. In 1959, the All-Palestine Government of the Gaza Strip, an Egyptian client state, was absorbed into the United Arab Republic under the pretext of Arab union, and was never restored. The Egyptians were one of the first major civilisations to codify design elements in art and architecture. Egyptian blue, also known as calcium copper silicate is a pigment used by Egyptians for thousands of years. It is considered to be the first synthetic pigment. The wall paintings done in the service of the Pharaohs followed a rigid code of visual rules and meanings. Egyptian civilisation is renowned for its colossal pyramids, temples and monumental tombs. Well-known examples are the Pyramid of Djoser designed by ancient architect and engineer Imhotep, the Sphinx, and the temple of Abu Simbel. Modern and contemporary Egyptian art can be as diverse as any works in the world art scene, from the vernacular architecture of Hassan Fathy and Ramses Wissa Wassef, to Mahmoud Mokhtar's sculptures, to the distinctive Coptic iconography of Isaac Fanous. The Cairo Opera House serves as the main performing arts venue in the Egyptian capital. The new government drafted and implemented a constitution in 1923 based on a parliamentary system. Saad Zaghlul was popularly elected as Prime Minister of Egypt in 1924. In 1936, the Anglo-Egyptian Treaty was concluded. Continued instability due to remaining British influence and increasing political involvement by the king led to the dissolution of the parliament in a military coup d'état known as the 1952 Revolution. The Free Officers Movement forced King Farouk to abdicate in support of his son Fuad. British military presence in Egypt lasted until 1954. During Mubarak's reign, the political scene was dominated by the National Democratic Party, which was created by Sadat in 1978. It passed the 1993 Syndicates Law, 1995 Press Law, and 1999 Nongovernmental Associations Law which hampered freedoms of association and expression by imposing new regulations and draconian penalties on violations.[citation needed] As a result, by the late 1990s parliamentary politics had become virtually irrelevant and alternative avenues for political expression were curtailed as well. Egypt (i/ˈiːdʒɪpt/; Arabic: مِصر‎ Miṣr, Egyptian Arabic: مَصر Maṣr, Coptic: Ⲭⲏⲙⲓ Khemi), officially the Arab Republic of Egypt, is a transcontinental country spanning the northeast corner of Africa and southwest corner of Asia, via a land bridge formed by the Sinai Peninsula. It is the world's only contiguous Eurafrasian nation. Most of Egypt's territory of 1,010,408 square kilometres (390,000 sq mi) lies within the Nile Valley. Egypt is a Mediterranean country. It is bordered by the Gaza Strip and Israel to the northeast, the Gulf of Aqaba to the east, the Red Sea to the east and south, Sudan to the south and Libya to the west. Egypt was producing 691,000 bbl/d of oil and 2,141.05 Tcf of natural gas (in 2013), which makes Egypt as the largest oil producer not member of the Organization of the Petroleum Exporting Countries (OPEC) and the second-largest dry natural gas producer in Africa. In 2013, Egypt was the largest consumer of oil and natural gas in Africa, as more than 20% of total oil consumption and more than 40% of total dry natural gas consumption in Africa. Also, Egypt possesses the largest oil refinery capacity in Africa 726,000 bbl/d (in 2012). Egypt is currently planning to build its first nuclear power plant in El Dabaa city, northern Egypt. Some consider koshari (a mixture of rice, lentils, and macaroni) to be the national dish. Fried onions can be also added to koshari. In addition, ful medames (mashed fava beans) is one of the most popular dishes. Fava bean is also used in making falafel (also known as ""ta'meyya""), which may have originated in Egypt and spread to other parts of the Middle East. Garlic fried with coriander is added to mulukhiyya, a popular green soup made from finely chopped jute leaves, sometimes with chicken or rabbit. Constitutional changes voted on 19 March 2007 prohibited parties from using religion as a basis for political activity, allowed the drafting of a new anti-terrorism law, authorised broad police powers of arrest and surveillance, and gave the president power to dissolve parliament and end judicial election monitoring. In 2009, Dr. Ali El Deen Hilal Dessouki, Media Secretary of the National Democratic Party (NDP), described Egypt as a ""pharaonic"" political system, and democracy as a ""long-term goal"". Dessouki also stated that ""the real center of power in Egypt is the military"". In 525 BC, the powerful Achaemenid Persians, led by Cambyses II, began their conquest of Egypt, eventually capturing the pharaoh Psamtik III at the battle of Pelusium. Cambyses II then assumed the formal title of pharaoh, but ruled Egypt from his home of Susa in Persia (modern Iran), leaving Egypt under the control of a satrapy. The entire Twenty-seventh Dynasty of Egypt, from 525 BC to 402 BC, save for Petubastis III, was an entirely Persian ruled period, with the Achaemenid kings all being granted the title of pharaoh. A few temporarily successful revolts against the Persians marked the fifth century BC, but Egypt was never able to permanently overthrow the Persians. Egypt recognises only three religions: Islam, Christianity, and Judaism. Other faiths and minority Muslim sects practised by Egyptians, such as the small Bahá'í and Ahmadi community, are not recognised by the state and face persecution since they are labelled as far right groups that threaten Egypt's national security. Individuals, particularly Baha'is and atheists, wishing to include their religion (or lack thereof) on their mandatory state issued identification cards are denied this ability (see Egyptian identification card controversy), and are put in the position of either not obtaining required identification or lying about their faith. A 2008 court ruling allowed members of unrecognised faiths to obtain identification and leave the religion field blank. Egyptian cinema became a regional force with the coming of sound. In 1936, Studio Misr, financed by industrialist Talaat Harb, emerged as the leading Egyptian studio, a role the company retained for three decades. For over 100 years, more than 4000 films have been produced in Egypt, three quarters of the total Arab production.[citation needed] Egypt is considered the leading country in the field of cinema in the Middle East. Actors from all over the Arab World seek to appear in the Egyptian cinema for the sake of fame. The Cairo International Film Festival has been rated as one of 11 festivals with a top class rating worldwide by the International Federation of Film Producers' Associations. Egypt has one of the oldest civilisations in the world. It has been in contact with many other civilisations and nations and has been through so many eras, starting from prehistoric age to the modern age, passing through so many ages such as; Pharonic, Roman, Greek, Islamic and many other ages. Because of this wide variation of ages, the continuous contact with other nations and the big number of conflicts Egypt had been through, at least 60 museums may be found in Egypt, mainly covering a wide area of these ages and conflicts. Cairo University is ranked as 401-500 according to the Academic Ranking of World Universities (Shanghai Ranking) and 551-600 according to QS World University Rankings. American University in Cairo is ranked as 360 according to QS World University Rankings and Al-Azhar University, Alexandria University and Ain Shams University fall in the 701+ range. Egypt is currently opening new research institutes for the aim of modernising research in the nation, the most recent example of which is Zewail City of Science and Technology."
Capital_punishment_in_the_United_States,"In a five-to-four decision, the Supreme Court struck down the impositions of the death penalty in each of the consolidated cases as unconstitutional. The five justices in the majority did not produce a common opinion or rationale for their decision, however, and agreed only on a short statement announcing the result. The narrowest opinions, those of Byron White and Potter Stewart, expressed generalized concerns about the inconsistent application of the death penalty across a variety of cases but did not exclude the possibility of a constitutional death penalty law. Stewart and William O. Douglas worried explicitly about racial discrimination in enforcement of the death penalty. Thurgood Marshall and William J. Brennan, Jr. expressed the opinion that the death penalty was proscribed absolutely by the Eighth Amendment as ""cruel and unusual"" punishment. Other states with long histories of no death penalty include Wisconsin (the only state with only one execution), Rhode Island (although later reintroduced, it was unused and abolished again), Maine, North Dakota, Minnesota, West Virginia, Iowa, and Vermont. The District of Columbia has also abolished the death penalty; it was last used in 1957. Oregon abolished the death penalty through an overwhelming majority in a 1964 public referendum but reinstated it in a 1984 joint death penalty/life imprisonment referendum by an even higher margin after a similar 1978 referendum succeeded but was not implemented due to judicial rulings. In 2010, bills to abolish the death penalty in Kansas and in South Dakota (which had a de facto moratorium at the time) were rejected. Idaho ended its de facto moratorium, during which only one volunteer had been executed, on November 18, 2011 by executing Paul Ezra Rhoades; South Dakota executed Donald Moeller on October 30, 2012, ending a de facto moratorium during which only two volunteers had been executed. Of the 12 prisoners whom Nevada has executed since 1976, 11 waived their rights to appeal. Kentucky and Montana have executed two prisoners against their will (KY: 1997 and 1999, MT: 1995 and 1998) and one volunteer, respectively (KY: 2008, MT: 2006). Colorado (in 1997) and Wyoming (in 1992) have executed only one prisoner, respectively. Puerto Rico's constitution expressly forbids capital punishment, stating ""The death penalty shall not exist"", setting it apart from all U.S. states and territories other than Michigan, which also has a constitutional prohibition (eleven other states and the District of Columbia have abolished capital punishment through statutory law). However, capital punishment is still applicable to offenses committed in Puerto Rico, if they fall under the jurisdiction of the federal government, though federal death penalty prosecutions there have generated significant controversy. Other capital crimes include: the use of a weapon of mass destruction resulting in death, espionage, terrorism, certain violations of the Geneva Conventions that result in the death of one or more persons, and treason at the federal level; aggravated rape in Louisiana, Florida, and Oklahoma; extortionate kidnapping in Oklahoma; aggravated kidnapping in Georgia, Idaho, Kentucky and South Carolina; aircraft hijacking in Alabama and Mississippi; assault by an escaping capital felon in Colorado; armed robbery in Georgia; drug trafficking resulting in a person's death in Florida; train wrecking which leads to a person's death, and perjury which leads to a person's death in California, Colorado, Idaho and Nebraska. The death penalty is sought and applied more often in some jurisdictions, not only between states but within states. A 2004 Cornell University study showed that while 2.5 percent of murderers convicted nationwide were sentenced to the death penalty, in Nevada 6 percent were given the death penalty. Texas gave 2 percent of murderers a death sentence, less than the national average. Texas, however, executed 40 percent of those sentenced, which was about four times higher than the national average. California had executed only 1 percent of those sentenced. In May 2014, Oklahoma Director of Corrections, Robert Patton, recommended an indefinite hold on executions in the state after the botched execution of African-American Clayton Lockett. The prisoner had to be tasered to restrain him prior to the execution, and the lethal injection missed a vein in his groin, resulting in Lockett regaining consciousness, trying to get up, and to speak, before dying of a heart attack 43 minutes later, after the attempted execution had been called off. In 2015, the state approved nitrogen asphyxiation as a method of execution. Around 1890, a political movement developed in the United States to mandate private executions. Several states enacted laws which required executions to be conducted within a ""wall"" or ""enclosure"" to ""exclude public view."" For example, in 1919, the Missouri legislature adopted a statute (L.1919, p. 781) which required, ""the sentence of death should be executed within the county jail, if convenient, and otherwise within an enclosure near the jail."" The Missouri law permitted the local sheriff to distribute passes to individuals (usually local citizens) whom he believed should witness the hanging, but the sheriffs – for various reasons – sometimes denied passes to individuals who wanted to watch. Missouri executions conducted after 1919 were not ""public"" because they were conducted behind closed walls, and the general public was not permitted to attend. After the September 2011 execution of Troy Davis, believed by many to be innocent, Richard Dieter, the director of the Death Penalty Information Center, said this case was a clear wake-up call to politicians across the United States. He said: ""They weren't expecting such passion from people in opposition to the death penalty. There's a widely held perception that all Americans are united in favor of executions, but this message came across loud and clear that many people are not happy with it."" Brian Evans of Amnesty International, which led the campaign to spare Davis's life, said that there was a groundswell in America of people ""who are tired of a justice system that is inhumane and inflexible and allows executions where there is clear doubts about guilt"". He predicted the debate would now be conducted with renewed energy. The moratorium ended on January 17, 1977 with the shooting of Gary Gilmore by firing squad in Utah. The first use of the electric chair after the moratorium was the electrocution of John Spenkelink in Florida on May 25, 1979. The first use of the gas chamber after the moratorium was the gassing of Jesse Bishop in Nevada on October 22, 1979. The first use of the gallows after the moratorium was the hanging of Westley Allan Dodd in Washington on January 5, 1993. The first use of lethal injection was on December 7, 1982, when Charles Brooks, Jr., was executed in Texas. In October 2009, the American Law Institute voted to disavow the framework for capital punishment that it had created in 1962, as part of the Model Penal Code, ""in light of the current intractable institutional and structural obstacles to ensuring a minimally adequate system for administering capital punishment."" A study commissioned by the institute had said that experience had proved that the goal of individualized decisions about who should be executed and the goal of systemic fairness for minorities and others could not be reconciled. Four states in the modern era, Nebraska in 2008, New York and Kansas in 2004, and Massachusetts in 1984, had their statutes ruled unconstitutional by state courts. The death rows of New York and Massachusetts were disestablished, and attempts to restore the death penalty were unsuccessful. Kansas successfully appealed State v. Kleypas, the Kansas Supreme Court decision that declared the state's death penalty statute unconstitutional, to the United States Supreme Court. Nebraska's death penalty statute was rendered ineffective on February 8, 2008 when the required method, electrocution, was ruled unconstitutional by the Nebraska Supreme Court. In 2009, Nebraska enacted a bill that changed its method of execution to lethal injection. In total, 156 prisoners have been either acquitted, or received pardons or commutations on the basis of possible innocence, between 1973 to 2015. Death penalty opponents often argue that this statistic shows how perilously close states have come to undertaking wrongful executions; proponents point out that the statistic refers only to those exonerated in law, and that the truly innocent may be a smaller number. Statistics likely understate the actual problem of wrongful convictions because once an execution has occurred there is often insufficient motivation and finance to keep a case open, and it becomes unlikely at that point that the miscarriage of justice will ever be exposed. Electrocution was the preferred method of execution during the 20th century. Electric chairs have commonly been nicknamed Old Sparky; however, Alabama's electric chair became known as the ""Yellow Mama"" due to its unique color. Some, particularly in Florida, were noted for malfunctions, which caused discussion of their cruelty and resulted in a shift to lethal injection as the preferred method of execution. Although lethal injection dominates as a method of execution, some states allow prisoners on death row to choose the method used to execute them. As noted in the introduction to this article, the American public has maintained its position of support for capital punishment for murder. However, when given a choice between the death penalty and life imprisonment without parole, support has traditionally been significantly lower than polling which has only mentioned the death penalty as a punishment. In 2010, for instance, one poll showed 49 percent favoring the death penalty and 46 percent favoring life imprisonment while in another 61% said they preferred another punishment to the death penalty. The highest level of support for the death penalty recorded overall was 80 percent in 1994 (16 percent opposed), and the lowest recorded was 42 percent in 1966 (47 percent opposed). On the question of the death penalty vs. life without parole, the strongest preference for the death penalty was 61 percent in 1997 (29 percent favoring life), and the lowest preference for the death penalty was 47 percent in 2006 (48 percent favoring life). Present-day statutes from across the nation use the same words and phrases, requiring modern executions to take place within a wall or enclosure to exclude public view. Connecticut General Statute § 54–100 requires death sentences to be conducted in an ""enclosure"" which ""shall be so constructed as to exclude public view."" Kentucky Revised Statute 431.220 and Missouri Revised Statute § 546.730 contain substantially identical language. New Mexico's former death penalty, since repealed, see N.M. Stat. § 31-14-12, required executions be conducted in a ""room or place enclosed from public view."" Similarly, a dormant Massachusetts law, see Mass. Gen. Law ch. 279 § 60, required executions to take place ""within an enclosure or building."" North Carolina General Statute § 15-188 requires death sentences to be executed ""within the walls"" of the penitentiary, as do Oklahoma Statute Title 22 § 1015 and Montana Code § 46-19-103. Ohio Revised Code § 2949.22 requires that ""[t]he enclosure shall exclude public view."" Similarly, Tennessee Code § 40-23-116 requires ""an enclosure"" for ""strict seclusion and privacy."" United States Code Title 18 § 3596 and the Code of Federal Regulations 28 CFR 26.4 limit the witnesses permitted at federal executions. At times when a death sentence is affirmed on direct review, it is considered final. Yet, supplemental methods to attack the judgment, though less familiar than a typical appeal, do remain. These supplemental remedies are considered collateral review, that is, an avenue for upsetting judgments that have become otherwise final. Where the prisoner received his death sentence in a state-level trial, as is usually the case, the first step in collateral review is state collateral review. (If the case is a federal death penalty case, it proceeds immediately from direct review to federal habeas corpus.) Although all states have some type of collateral review, the process varies widely from state to state. Generally, the purpose of these collateral proceedings is to permit the prisoner to challenge his sentence on grounds that could not have been raised reasonably at trial or on direct review. Most often these are claims, such as ineffective assistance of counsel, which requires the court to consider new evidence outside the original trial record, something courts may not do in an ordinary appeal. State collateral review, though an important step in that it helps define the scope of subsequent review through federal habeas corpus, is rarely successful in and of itself. Only around 6 percent of death sentences are overturned on state collateral review. In 2010, the death sentences of 53 inmates were overturned as a result of legal appeals or high court reversals. Traditionally, Section 1983 was of limited use for a state prisoner under sentence of death because the Supreme Court has held that habeas corpus, not Section 1983, is the only vehicle by which a state prisoner can challenge his judgment of death. In the 2006 Hill v. McDonough case, however, the United States Supreme Court approved the use of Section 1983 as a vehicle for challenging a state's method of execution as cruel and unusual punishment in violation of the Eighth Amendment. The theory is that a prisoner bringing such a challenge is not attacking directly his judgment of death, but rather the means by which that the judgment will be carried out. Therefore, the Supreme Court held in the Hill case that a prisoner can use Section 1983 rather than habeas corpus to bring the lawsuit. Yet, as Clarence Hill's own case shows, lower federal courts have often refused to hear suits challenging methods of execution on the ground that the prisoner brought the claim too late and only for the purposes of delay. Further, the Court's decision in Baze v. Rees, upholding a lethal injection method used by many states, has drastically narrowed the opportunity for relief through Section 1983. The United States Supreme Court in Penry v. Lynaugh and the United States Court of Appeals for the Fifth Circuit in Bigby v. Dretke have been clear in their decisions that jury instructions in death penalty cases that do not ask about mitigating factors regarding the defendant's mental health violate the defendant's Eighth Amendment rights, saying that the jury is to be instructed to consider mitigating factors when answering unrelated questions. This ruling suggests that specific explanations to the jury are necessary to weigh mitigating factors. All of the executions which have taken place since the 1936 hanging of Bethea in Owensboro have been conducted within a wall or enclosure. For example, Fred Adams was legally hanged in Kennett, Missouri, on April 2, 1937, within a 10-foot (3 m) wooden stockade. Roscoe ""Red"" Jackson was hanged within a stockade in Galena, Missouri, on May 26, 1937. Two Kentucky hangings were conducted after Galena in which numerous persons were present within a wooden stockade, that of John ""Peter"" Montjoy in Covington, Kentucky on December 17, 1937, and that of Harold Van Venison in Covington on June 3, 1938. An estimated 400 witnesses were present for the hanging of Lee Simpson in Ryegate, Montana, on December 30, 1939. The execution of Timothy McVeigh on June 11, 2001 was witnessed by some 300 people, some by closed-circuit television. Pharmaceutical companies whose products are used in the three-drug cocktails for lethal injections are predominantly European, and they have strenuously objected to the use of their drugs for executions and taken steps to prevent their use. For example, Hospira, the sole American manufacturer of sodium thiopental, the critical anesthetic in the three-drug cocktail, announced in 2011 that it would no longer manufacture the drug for the American market, in part for ethical reasons and in part because its transfer of sodium thiopental manufacturing to Italy would subject it to the European Union's Torture Regulation, which forbids the use of any product manufactured within the Union for torture (as execution by lethal injection is considered by the Regulation). Since the drug manufacturers began taking these steps and the EU regulation ended the importation of drugs produced in Europe, the resulting shortage of execution drugs has led to or influenced decisions to impose moratoria in Arkansas, California, Kentucky, Louisiana, Mississippi, Montana, Nevada, North Carolina, and Tennessee. African Americans made up 41 percent of death row inmates while making up only 12.6 percent of the general population. (They have made up 34 percent of those actually executed since 1976.) However, that number is lower than that of prison inmates, which is 47 percent. According to the US Department of Justice, African Americans accounted for 52.5% of homicide offenders from 1980 to 2008, with whites 45.3% and Native Americans and Asians 2.2%. This means African Americans are less likely to be executed on a per capita basis. However, according to a 2003 Amnesty International report, blacks and whites were the victims of murder in almost equal numbers, yet 80 percent of the people executed since 1977 were convicted of murders involving white victims. 13.5% of death row inmates are of Hispanic or Latino descent, while they make up 17.4% of the general population. Since 1642 (in the 13 colonies, the United States under the Articles of Confederation, and the current United States) an estimated 364 juvenile offenders have been put to death by the states and the federal government. The earliest known execution of a prisoner for crimes committed as a juvenile was Thomas Graunger in 1642. Twenty-two of the executions occurred after 1976, in seven states. Due to the slow process of appeals, it was highly unusual for a condemned person to be under 18 at the time of execution. The youngest person to be executed in the 20th century was George Stinney, who was electrocuted in South Carolina at the age of 14 on June 16, 1944. The last execution of a juvenile may have been Leonard Shockley, who died in the Maryland gas chamber on April 10, 1959, at the age of 17. No one has been under age 19 at time of execution since at least 1964. Since the reinstatement of the death penalty in 1976, 22 people have been executed for crimes committed under the age of 18. Twenty-one were 17 at the time of the crime. The last person to be executed for a crime committed as a juvenile was Scott Hain on April 3, 2003 in Oklahoma. The last use of the firing squad between 1608 and the moratorium on judicial executions between 1967 and 1977 was when Utah shot James W. Rodgers on March 30, 1960. The last use of the gallows between 1608 and the moratorium was when Kansas hanged George York on June 22, 1965. The last use of the electric chair between the first electrocution on August 6, 1890 and the moratorium was when Oklahoma electrocuted James French on August 10, 1966. The last use of the gas chamber between the first gassing on February 8, 1924 and the moratorium was when Colorado gassed Luis Monge on June 2, 1967. Sixteen was held to be the minimum permissible age in the 1988 Supreme Court decision of Thompson v. Oklahoma. The Court, considering the case Roper v. Simmons in March 2005, found the execution of juvenile offenders unconstitutional by a 5–4 margin, effectively raising the minimum permissible age to 18. State laws have not been updated to conform with this decision. In the American legal system, unconstitutional laws do not need to be repealed; instead, they are held to be unenforceable. (See also List of juvenile offenders executed in the United States) Several states have never had capital punishment, the first being Michigan, which abolished it shortly after entering the Union. (However, the United States government executed Tony Chebatoris at the Federal Correctional Institution in Milan, Michigan in 1938.) Article 4, Section 46 of Michigan's fourth Constitution (ratified in 1963; effective in 1964) prohibits any law providing for the penalty of death. Attempts to change the provision have failed. In 2004, a constitutional amendment proposed to allow capital punishment in some circumstances failed to make it on the November ballot after a resolution failed in the legislature and a public initiative failed to gather enough signatures. Congress acted defiantly toward the Supreme Court by passing the Drug Kingpin Act of 1988 and the Federal Death Penalty Act of 1994 that made roughly fifty crimes punishable by death, including crimes that do not always involve the death of someone. Such non-death capital offenses include treason, espionage (spying for another country), and high-level drug trafficking. Since no one has yet been sentenced to death for such non-death capital offenses, the Supreme Court has not ruled on their constitutionality. James Liebman, a professor of law at Columbia Law School, stated in 1996 that his study found that when habeas corpus petitions in death penalty cases were traced from conviction to completion of the case that there was ""a 40 percent success rate in all capital cases from 1978 to 1995."" Similarly, a study by Ronald Tabak in a law review article puts the success rate in habeas corpus cases involving death row inmates even higher, finding that between ""1976 and 1991, approximately 47 percent of the habeas petitions filed by death row inmates were granted."" The different numbers are largely definitional, rather than substantive. Freedam's statistics looks at the percentage of all death penalty cases reversed, while the others look only at cases not reversed prior to habeas corpus review. Capital punishment was suspended in the United States from 1972 through 1976 primarily as a result of the Supreme Court's decision in Furman v. Georgia. The last pre-Furman execution was that of Luis Monge on June 2, 1967. In this case, the court found that the death penalty was being imposed in an unconstitutional manner, on the grounds of cruel and unusual punishment in violation of the Eighth Amendment to the United States Constitution. The Supreme Court has never ruled the death penalty to be per se unconstitutional. In 1976, contemporaneously with Woodson and Roberts, the Court decided Gregg v. Georgia and upheld a procedure in which the trial of capital crimes was bifurcated into guilt-innocence and sentencing phases. At the first proceeding, the jury decides the defendant's guilt; if the defendant is innocent or otherwise not convicted of first-degree murder, the death penalty will not be imposed. At the second hearing, the jury determines whether certain statutory aggravating factors exist, whether any mitigating factors exist, and, in many jurisdictions, weigh the aggravating and mitigating factors in assessing the ultimate penalty – either death or life in prison, either with or without parole. Under the Antiterrorism and Effective Death Penalty Act of 1996, a state prisoner is ordinarily only allowed one suit for habeas corpus in federal court. If the federal courts refuse to issue a writ of habeas corpus, an execution date may be set. In recent times, however, prisoners have postponed execution through a final round of federal litigation using the Civil Rights Act of 1871 — codified at 42 U.S.C. § 1983 — which allows people to bring lawsuits against state actors to protect their federal constitutional and statutory rights. The legal administration of the death penalty in the United States is complex. Typically, it involves four critical steps: (1) sentencing, (2) direct review, (3) state collateral review, and (4) federal habeas corpus. Recently, a narrow and final fifth level of process – (5) the Section 1983 challenge – has become increasingly important. (Clemency or pardon, through which the Governor or President of the jurisdiction can unilaterally reduce or abrogate a death sentence, is an executive rather than judicial process.) The number of new death sentences handed down peaked in 1995–1996 (309). There were 73 new death sentences handed down in 2014, the lowest number since 1973 (44). The method of execution of federal prisoners for offenses under the Violent Crime Control and Law Enforcement Act of 1994 is that of the state in which the conviction took place. If the state has no death penalty, the judge must choose a state with the death penalty for carrying out the execution. For offenses under the Drug Kingpin Act of 1988, the method of execution is lethal injection. The Federal Correctional Complex in Terre Haute, Indiana is currently the home of the only death chamber for federal death penalty recipients in the United States, where inmates are put to death by lethal injection. The complex has so far been the only location used for federal executions post-Gregg. Timothy McVeigh and Juan Garza were put to death in June 2001, and Louis Jones, Jr. was put to death on March 18, 2003. In the decades since Furman, new questions have emerged about whether or not prosecutorial arbitrariness has replaced sentencing arbitrariness. A study by Pepperdine University School of Law published in Temple Law Review, ""Unpredictable Doom and Lethal Injustice: An Argument for Greater Transparency in Death Penalty Decisions,"" surveyed the decision-making process among prosecutors in various states. The authors found that prosecutors' capital punishment filing decisions remain marked by local ""idiosyncrasies,"" suggesting they are not in keeping with the spirit of the Supreme Court's directive. This means that ""the very types of unfairness that the Supreme Court sought to eliminate"" may still ""infect capital cases."" Wide prosecutorial discretion remains because of overly broad criteria. California law, for example, has 22 ""special circumstances,"" making nearly all premeditated murders potential capital cases. The 32 death penalty states have varying numbers and types of ""death qualifiers"" – circumstances that allow for capital charges. The number varies from a high of 34 in California to 22 in Colorado and Delaware to 12 in Texas, Nebraska, Georgia and Montana. The study's authors call for reform of state procedures along the lines of reforms in the federal system, which the U.S. Department of Justice initiated with a 1995 protocol. Crimes subject to the death penalty vary by jurisdiction. All jurisdictions that use capital punishment designate the highest grade of murder a capital crime, although most jurisdictions require aggravating circumstances. Treason against the United States, as well as treason against the states of Arkansas, California, Georgia, Louisiana, Mississippi, and Missouri are capital offenses. In the 2010s, American jurisdictions have experienced a shortage of lethal injection drugs, due to anti-death penalty advocacy and low production volume. Hospira, the only U.S. manufacturer of sodium thiopental, stopped making the drug in 2011. The European Union has outlawed the export of any product that could be used in an execution; this has prevented executioners from using EU-manufactured anesthetics like propofol which are needed for general medical purposes. Another alternative, pentobarbital, is also only manufactured in the European Union, which has caused the Danish producer to restrict distribution to U.S. government customers. If a defendant is sentenced to death at the trial level, the case then goes into a direct review. The direct review process is a typical legal appeal. An appellate court examines the record of evidence presented in the trial court and the law that the lower court applied and decides whether the decision was legally sound or not. Direct review of a capital sentencing hearing will result in one of three outcomes. If the appellate court finds that no significant legal errors occurred in the capital sentencing hearing, the appellate court will affirm the judgment, or let the sentence stand. If the appellate court finds that significant legal errors did occur, then it will reverse the judgment, or nullify the sentence and order a new capital sentencing hearing. Lastly, if the appellate court finds that no reasonable juror could find the defendant eligible for the death penalty, a rarity, then it will order the defendant acquitted, or not guilty, of the crime for which he/she was given the death penalty, and order him sentenced to the next most severe punishment for which the offense is eligible. About 60 percent survive the process of direct review intact. Opponents argue that the death penalty is not an effective means of deterring crime, risks the execution of the innocent, is unnecessarily barbaric in nature, cheapens human life, and puts a government on the same base moral level as those criminals involved in murder. Furthermore, some opponents argue that the arbitrariness with which it is administered and the systemic influence of racial, socio-economic, geographic, and gender bias on determinations of desert make the current practice of capital punishment immoral and illegitimate. Executions resumed on January 17, 1977, when Gary Gilmore went before a firing squad in Utah. But the pace was quite slow due to the use of litigation tactics which involved filing repeated writs of habeas corpus, which succeeded for many in delaying their actual execution for many years. Although hundreds of individuals were sentenced to death in the United States during the 1970s and early 1980s, only ten people besides Gilmore (who had waived all of his appeal rights) were actually executed prior to 1984. After a death sentence is affirmed in state collateral review, the prisoner may file for federal habeas corpus, which is a unique type of lawsuit that can be brought in federal courts. Federal habeas corpus is a species of collateral review, and it is the only way that state prisoners may attack a death sentence in federal court (other than petitions for certiorari to the United States Supreme Court after both direct review and state collateral review). The scope of federal habeas corpus is governed by the Antiterrorism and Effective Death Penalty Act of 1996, which restricted significantly its previous scope. The purpose of federal habeas corpus is to ensure that state courts, through the process of direct review and state collateral review, have done at least a reasonable job in protecting the prisoner's federal constitutional rights. Prisoners may also use federal habeas corpus suits to bring forth new evidence that they are innocent of the crime, though to be a valid defense at this late stage in the process, evidence of innocence must be truly compelling. Various methods have been used in the history of the American colonies and the United States but only five methods are currently used. Historically, burning, crushing, breaking on wheel, and bludgeoning were used for a small number of executions, while hanging was the most common method. The last person burned at the stake was a black slave in South Carolina in August 1825. The last person to be hanged in chains was a murderer named John Marshall in West Virginia on April 4, 1913. Although beheading was a legal method in Utah from 1851 to 1888, it was never used. Previous post-Furman mass clemencies took place in 1986 in New Mexico, when Governor Toney Anaya commuted all death sentences because of his personal opposition to the death penalty. In 1991, outgoing Ohio Governor Dick Celeste commuted the sentences of eight prisoners, among them all four women on the state's death row. And during his two terms (1979–1987) as Florida's Governor, Bob Graham, although a strong death penalty supporter who had overseen the first post-Furman involuntary execution as well as 15 others, agreed to commute the sentences of six people on the grounds of ""possible innocence"" or ""disproportionality."" The largest single execution in United States history was the hanging of 38 American Indians convicted of murder and rape during the Dakota War of 1862. They were executed simultaneously on December 26, 1862, in Mankato, Minnesota. A single blow from an axe cut the rope that held the large four-sided platform, and the prisoners (except for one whose rope had broken and who had to be re-hanged) fell to their deaths. The second-largest mass execution was also a hanging: the execution of 13 African-American soldiers for taking part in the Houston Riot of 1917. The largest non-military mass execution occurred in one of the original thirteen colonies in 1723, when 26 convicted pirates were hanged in Newport, Rhode Island by order of the Admiralty Court. Within the context of the overall murder rate, the death penalty cannot be said to be widely or routinely used in the United States; in recent years the average has been about one execution for about every 700 murders committed, or 1 execution for about every 325 murder convictions. However, 32 of the 50 states still execute people. Among them, Alabama has the highest per capita rate of death sentences. This is due to judges overriding life imprisonment sentences and imposing the death penalty. No other states allow this. As of November 2008, there is only one person on death row facing capital punishment who has not been convicted of murder. Demarcus Sears remains under a death sentence in Georgia for the crime of ""kidnapping with bodily injury."" Sears was convicted in 1986 for the kidnapping and bodily injury of victim Gloria Ann Wilbur. Wilbur was kidnapped and beaten in Georgia, raped in Tennessee, and murdered in Kentucky. Sears was never charged with the murder of Wilbur in Kentucky, but was sentenced to death by a jury in Georgia for ""kidnapping with bodily injury."" In 1977, the Supreme Court's Coker v. Georgia decision barred the death penalty for rape of an adult woman, and implied that the death penalty was inappropriate for any offense against another person other than murder. Prior to the decision, the death penalty for rape of an adult had been gradually phased out in the United States, and at the time of the decision, the State of Georgia and the U.S. Federal government were the only two jurisdictions to still retain the death penalty for that offense. However, three states maintained the death penalty for child rape, as the Coker decision only imposed a ban on executions for the rape of an adult woman. In 2008, the Kennedy v. Louisiana decision barred the death penalty for child rape. The result of these two decisions means that the death penalty in the United States is largely restricted to cases where the defendant took the life of another human being. The current federal kidnapping statute, however, may be exempt because the death penalty applies if the victim dies in the perpetrator's custody, not necessarily by his hand, thus stipulating a resulting death, which was the wording of the objection. In addition, the Federal government retains the death penalty for non-murder offenses that are considered crimes against the state, including treason, espionage, and crimes under military jurisdiction. Possibly in part due to expedited federal habeas corpus procedures embodied in the Antiterrorism and Effective Death Penalty Act of 1996, the pace of executions picked up, reaching a peak of 98 in 1999 and then they declined gradually to 28 in 2015. Since the death penalty was reauthorized in 1976, 1,411 people have been executed, almost exclusively by the states, with most occurring after 1990. Texas has accounted for over one-third of modern executions (although only two death sentences were imposed in Texas during 2015, with the courts preferring to issue sentences of life without parole instead) and over four times as many as Oklahoma, the state with the second-highest number. California has the greatest number of prisoners on death row, has issued the highest number of death sentences but has held relatively few executions. In New Jersey and Illinois, all death row inmates had their sentences commuted to life in prison without parole when the death penalty repeal bills were signed into law. In Maryland, Governor Martin O'Malley commuted the state's four remaining death sentences to life in prison without parole in January 2015. While the bill repealing capital punishment in Connecticut was not retroactive, the Connecticut Supreme Court ruled in 2015 in State v. Santiago that the legislature's decision to prospectively abolish capital punishment rendered it an offense to ""evolving standards of decency,"" thus commuting the sentences of the 11 men remaining on death row to life in prison without parole. New Mexico may yet execute two condemned inmates sentenced prior to abolition, and Nebraska has ten death row inmates who may still be executed despite abolition."
"Charleston,_South_Carolina","The first settlers primarily came from England, its Caribbean colony of Barbados, and its Atlantic colony of Bermuda. Among these were free people of color, born in the West Indies of alliances and marriages between Africans and Englanders, when color lines were looser among the working class in the early colonial years, and some wealthy whites took black consorts or concubines. Charles Town attracted a mixture of ethnic and religious groups. French, Scottish, Irish, and Germans migrated to the developing seacoast town, representing numerous Protestant denominations. Because of the battles between English ""royalty"" and the Roman Catholic Church, practicing Catholics were not allowed to settle in South Carolina until after the American Revolution. Jews were allowed, and Sephardic Jews migrated to the city in such numbers that by the beginning of the 19th century, the city was home to the largest and wealthiest Jewish community in North America—a status it held until about 1830. By the mid-18th century, Charles Town had become a bustling trade center, the hub of the Atlantic trade for the southern colonies. Charles Towne was also the wealthiest and largest city south of Philadelphia, in part because of the lucrative slave trade. By 1770, it was the fourth-largest port in the colonies, after Boston, New York, and Philadelphia, with a population of 11,000—slightly more than half of them slaves. By 1708, the majority of the colony's population was slaves, and the future state would continue to be a majority of African descent until after the Great Migration of the early 20th century. Interstate 26 begins in downtown Charleston, with exits to the Septima Clark Expressway, the Arthur Ravenel, Jr. Bridge and Meeting Street. Heading northwest, it connects the city to North Charleston, the Charleston International Airport, Interstate 95, and Columbia. The Arthur Ravenel, Jr. Bridge and Septima Clark Expressway are part of U.S. Highway 17, which travels east-west through the cities of Charleston and Mount Pleasant. The Mark Clark Expressway, or Interstate 526, is the bypass around the city and begins and ends at U.S. Highway 17. U.S. Highway 52 is Meeting Street and its spur is East Bay Street, which becomes Morrison Drive after leaving the east side. This highway merges with King Street in the city's Neck area (industrial district). U.S. Highway 78 is King Street in the downtown area, eventually merging with Meeting Street. The early settlement was often subject to attack from sea and land, including periodic assaults from Spain and France (both of whom contested England's claims to the region), and pirates. These were combined with raids by Native Americans, who tried to protect themselves from so-called European ""settlers,"" who in turn wanted to expand the settlement. The heart of the city was fortified according to a 1704 plan by Governor Johnson. Except those fronting Cooper River, the walls were largely removed during the 1720s. The city also had a large class of free people of color. By 1860, 3,785 free people of color were in Charleston, nearly 18% of the city's black population, and 8% of the total population. Free people of color were far more likely to be of mixed racial background than slaves. Many were educated, practiced skilled crafts, and some even owned substantial property, including slaves. In 1790, they established the Brown Fellowship Society for mutual aid, initially as a burial society. It continued until 1945. The traditional Charleston accent has long been noted in the state and throughout the South. It is typically heard in wealthy white families who trace their families back generations in the city. It has ingliding or monophthongal long mid-vowels, raises ay and aw in certain environments, and is nonrhotic. Sylvester Primer of the College of Charleston wrote about aspects of the local dialect in his late 19th-century works: ""Charleston Provincialisms"" (1887)  and ""The Huguenot Element in Charleston's Provincialisms"", published in a German journal. He believed the accent was based on the English as it was spoken by the earliest settlers, therefore derived from Elizabethan England and preserved with modifications by Charleston speakers. The rapidly disappearing ""Charleston accent"" is still noted in the local pronunciation of the city's name. Some elderly (and usually upper-class) Charleston natives ignore the 'r' and elongate the first vowel, pronouncing the name as ""Chah-l-ston"". Some observers attribute these unique features of Charleston's speech to its early settlement by French Huguenots and Sephardic Jews (who were primarily English speakers from London), both of whom played influential roles in Charleston's early development and history.[citation needed] During this period, the Weapons Station was the Atlantic Fleet's loadout base for all nuclear ballistic missile submarines. Two SSBN ""Boomer"" squadrons and a submarine tender were homeported at the Weapons Station, while one SSN attack squadron, Submarine Squadron 4, and a submarine tender were homeported at the Naval Base. At the 1996 closure of the station's Polaris Missile Facility Atlantic (POMFLANT), over 2,500 nuclear warheads and their UGM-27 Polaris, UGM-73 Poseidon, and UGM-96 Trident I delivery missiles (SLBM) were stored and maintained, guarded by a U.S. Marine Corps security force company. In Charleston, the African American population increased as freedmen moved from rural areas to the major city: from 17,000 in 1860 to over 27,000 in 1880. Historian Eric Foner noted that blacks were glad to be relieved of the many regulations of slavery and to operate outside of white surveillance. Among other changes, most blacks quickly left the Southern Baptist Church, setting up their own black Baptist congregations or joining new African Methodist Episcopal Church and AME Zion churches, both independent black denominations first established in the North. Freedmen ""acquired dogs, guns, and liquor (all barred to them under slavery), and refused to yield the sidewalks to whites"". In 1832, South Carolina passed an ordinance of nullification, a procedure by which a state could, in effect, repeal a federal law; it was directed against the most recent tariff acts. Soon, federal soldiers were dispensed to Charleston's forts, and five United States Coast Guard cutters were detached to Charleston Harbor ""to take possession of any vessel arriving from a foreign port, and defend her against any attempt to dispossess the Customs Officers of her custody until all the requirements of law have been complied with."" This federal action became known as the Charleston incident. The state's politicians worked on a compromise law in Washington to gradually reduce the tariffs. Charleston has one official sister city, Spoleto, Umbria, Italy. The relationship between the two cities began when Pulitzer Prize-winning Italian composer Gian Carlo Menotti selected Charleston as the city to host the American version of Spoleto's annual Festival of Two Worlds. ""Looking for a city that would provide the charm of Spoleto, as well as its wealth of theaters, churches, and other performance spaces, they selected Charleston, South Carolina, as the ideal location. The historic city provided a perfect fit: intimate enough that the Festival would captivate the entire city, yet cosmopolitan enough to provide an enthusiastic audience and robust infrastructure."" The City of Charleston is served by the Charleston International Airport. It is located in the City of North Charleston and is about 12 miles (20 km) northwest of downtown Charleston. It is the busiest passenger airport in South Carolina (IATA: CHS, ICAO: KCHS). The airport shares runways with the adjacent Charleston Air Force Base. Charleston Executive Airport is a smaller airport located in the John's Island section of the city of Charleston and is used by noncommercial aircraft. Both airports are owned and operated by the Charleston County Aviation Authority. The community was established by several shiploads of settlers from Bermuda (which lies due east of South Carolina, although at 1,030 km or 640 mi, it is closest to Cape Hatteras, North Carolina), under the leadership of governor William Sayle, on the west bank of the Ashley River, a few miles northwest of the present-day city center. It was soon predicted by the Earl of Shaftesbury, one of the Lords Proprietors, to become a ""great port towne"", a destiny the city quickly fulfilled. In 1680, the settlement was moved east of the Ashley River to the peninsula between the Ashley and Cooper Rivers. Not only was this location more defensible, but it also offered access to a fine natural harbor. As many as five bands were on tour during the 1920s. The Jenkins Orphanage Band played in the inaugural parades of Presidents Theodore Roosevelt and William Taft and toured the USA and Europe. The band also played on Broadway for the play ""Porgy"" by DuBose and Dorothy Heyward, a stage version of their novel of the same title. The story was based in Charleston and featured the Gullah community. The Heywards insisted on hiring the real Jenkins Orphanage Band to portray themselves on stage. Only a few years later, DuBose Heyward collaborated with George and Ira Gershwin to turn his novel into the now famous opera, Porgy and Bess (so named so as to distinguish it from the play). George Gershwin and Heyward spent the summer of 1934 at Folly Beach outside of Charleston writing this ""folk opera"", as Gershwin called it. Porgy and Bess is considered the Great American Opera[citation needed] and is widely performed. The Jenkins Orphanage was established in 1891 by the Rev. Daniel J. Jenkins in Charleston. The orphanage accepted donations of musical instruments and Rev. Jenkins hired local Charleston musicians and Avery Institute Graduates to tutor the boys in music. As a result, Charleston musicians became proficient on a variety of instruments and were able to read music expertly. These traits set Jenkins musicians apart and helped land some of them positions in big bands with Duke Ellington and Count Basie. William ""Cat"" Anderson, Jabbo Smith, and Freddie Green are but a few of the alumni from the Jenkins Orphanage band who became professional musicians in some of the best bands of the day. Orphanages around the country began to develop brass bands in the wake of the Jenkins Orphanage Band's success. At the Colored Waif's Home Brass Band in New Orleans, for example, a young trumpeter named Louis Armstrong first began to draw attention. Charles Town was a hub of the deerskin trade, the basis of its early economy. Trade alliances with the Cherokee and Creek nations insured a steady supply of deer hides. Between 1699 and 1715, colonists exported an average of 54,000 deer skins annually to Europe through Charles Town. Between 1739 and 1761, the height of the deerskin trade era, an estimated 500,000 to 1,250,000 deer were slaughtered. During the same period, Charles Town records show an export of 5,239,350 pounds of deer skins. Deer skins were used in the production of men's fashionable and practical buckskin pantaloons, gloves, and book bindings. The City of Charleston Fire Department consists over 300 full-time firefighters. These firefighters operate out of 19 companies located throughout the city: 16 engine companies, two tower companies, and one ladder company. Training, Fire Marshall, Operations, and Administration are the divisions of the department. The department operates on a 24/48 schedule and had a Class 1 ISO rating until late 2008, when ISO officially lowered it to Class 3. Russell (Rusty) Thomas served as Fire Chief until June 2008, and was succeeded by Chief Thomas Carr in November 2008. On August 31, 1886, Charleston was nearly destroyed by an earthquake. The shock was estimated to have a moment magnitude of 7.0 and a maximum Mercalli intensity of X (Extreme). It was felt as far away as Boston to the north, Chicago and Milwaukee to the northwest, as far west as New Orleans, as far south as Cuba, and as far east as Bermuda. It damaged 2,000 buildings in Charleston and caused $6 million worth of damage ($133 million in 2006 dollars), at a time when all the city's buildings were valued around $24 million ($531 million in 2006 dollars). Charleston's oldest community theater group, the Footlight Players, has provided theatrical productions since 1931. A variety of performing arts venues includes the historic Dock Street Theatre. The annual Charleston Fashion Week held each spring in Marion Square brings in designers, journalists, and clients from across the nation. Charleston is known for its local seafood, which plays a key role in the city's renowned cuisine, comprising staple dishes such as gumbo, she-crab soup, fried oysters, Lowcountry boil, deviled crab cakes, red rice, and shrimp and grits. Rice is the staple in many dishes, reflecting the rice culture of the Low Country. The cuisine in Charleston is also strongly influenced by British and French elements. Public institutions of higher education in Charleston include the College of Charleston (the nation's 13th-oldest university), The Citadel, The Military College of South Carolina, and the Medical University of South Carolina. The city is also home to private universities, including the Charleston School of Law . Charleston is also home to the Roper Hospital School of Practical Nursing, and the city has a downtown satellite campus for the region's technical school, Trident Technical College. Charleston is also the location for the only college in the country that offers bachelor's degrees in the building arts, The American College of the Building Arts. The Art Institute of Charleston, located downtown on North Market Street, opened in 2007. The City of Charleston Police Department, with a total of 452 sworn officers, 137 civilians, and 27 reserve police officers, is South Carolina's largest police department. Their procedures on cracking down on drug use and gang violence in the city are used as models to other cities to do the same.[citation needed] According to the final 2005 FBI Crime Reports, Charleston crime level is worse than the national average in almost every major category. Greg Mullen, the former Deputy Chief of the Virginia Beach, Virginia Police Department, serves as the current Chief of the Charleston Police Department. The former Charleston police chief was Reuben Greenberg, who resigned August 12, 2005. Greenberg was credited with creating a polite police force that kept police brutality well in check, even as it developed a visible presence in community policing and a significant reduction in crime rates. On June 17, 2015, 21-year-old Dylann Roof entered the historic Emanuel African Methodist Episcopal Church during a Bible study and killed nine people. Senior pastor Clementa Pinckney, who also served as a state senator, was among those killed during the attack. The deceased also included congregation members Susie Jackson, 87; Rev. Daniel Simmons Sr., 74; Ethel Lance, 70; Myra Thompson, 59; Cynthia Hurd, 54; Rev. Depayne Middleton-Doctor, 49; Rev. Sharonda Coleman-Singleton, 45; and Tywanza Sanders, 26. The attack garnered national attention, and sparked a debate on historical racism, Confederate symbolism in Southern states, and gun violence. On July 10, 2015, the Confederate battle flag was removed from the South Carolina State House. A memorial service on the campus of the College of Charleston was attended by President Barack Obama, Michelle Obama, Vice President Joe Biden, Jill Biden, and Speaker of the House John Boehner. Charleston is the oldest and second-largest city in the U.S. state of South Carolina, the county seat of Charleston County, and the principal city in the Charleston–North Charleston–Summerville Metropolitan Statistical Area. The city lies just south of the geographical midpoint of South Carolina's coastline and is located on Charleston Harbor, an inlet of the Atlantic Ocean formed by the confluence of the Ashley and Cooper Rivers, or, as is locally expressed, ""where the Cooper and Ashley Rivers come together to form the Atlantic Ocean."" The traditional parish system persisted until the Reconstruction Era, when counties were imposed.[citation needed] Nevertheless, traditional parishes still exist in various capacities, mainly as public service districts. When the city of Charleston was formed, it was defined by the limits of the Parish of St. Philip and St. Michael, now also includes parts of St. James' Parish, St. George's Parish, St. Andrew's Parish, and St. John's Parish, although the last two are mostly still incorporated rural parishes. By 1820, Charleston's population had grown to 23,000, maintaining its black (and mostly slave) majority. When a massive slave revolt planned by Denmark Vesey, a free black, was revealed in May 1822, whites reacted with intense fear, as they were well aware of the violent retribution of slaves against whites during the Haitian Revolution. Soon after, Vesey was tried and executed, hanged in early July with five slaves. Another 28 slaves were later hanged. Later, the state legislature passed laws requiring individual legislative approval for manumission (the freeing of a slave) and regulating activities of free blacks and slaves. Founded in 1670 as Charles Town in honor of King Charles II of England, Charleston adopted its present name in 1783. It moved to its present location on Oyster Point in 1680 from a location on the west bank of the Ashley River known as Albemarle Point. By 1690, Charles Town was the fifth-largest city in North America, and it remained among the 10 largest cities in the United States through the 1840 census. With a 2010 census population of 120,083  (and a 2014 estimate of 130,113), current trends put Charleston as the fastest-growing municipality in South Carolina. The population of the Charleston metropolitan area, comprising Berkeley, Charleston, and Dorchester Counties, was counted by the 2014 estimate at 727,689 – the third-largest in the state – and the 78th-largest metropolitan statistical area in the United States. After Charles II of England (1630–1685) was restored to the English throne in 1660 following Oliver Cromwell's Protectorate, he granted the chartered Province of Carolina to eight of his loyal friends, known as the Lords Proprietors, on March 24, 1663. It took seven years before the group arranged for settlement expeditions. The first of these founded Charles Town, in 1670. Governance, settlement, and development were to follow a visionary plan known as the Grand Model prepared for the Lords Proprietors by John Locke. Colonial Lowcountry landowners experimented with cash crops ranging from tea to silkworms. African slaves brought knowledge of rice cultivation, which plantation owners cultivated and developed as a successful commodity crop by 1700. With the coerced help of African slaves from the Caribbean, Eliza Lucas, daughter of plantation owner George Lucas, learned how to raise and use indigo in the Lowcountry in 1747. Supported with subsidies from Britain, indigo was a leading export by 1750. Those and naval stores were exported in an extremely profitable shipping industry. Industries slowly brought the city and its inhabitants back to a renewed vitality and jobs attracted new residents. As the city's commerce improved, residents worked to restore or create community institutions. In 1865, the Avery Normal Institute was established by the American Missionary Association as the first free secondary school for Charleston's African American population. General William T. Sherman lent his support to the conversion of the United States Arsenal into the Porter Military Academy, an educational facility for former soldiers and boys left orphaned or destitute by the war. Porter Military Academy later joined with Gaud School and is now a university-preparatory school, Porter-Gaud School. Charleston annually hosts Spoleto Festival USA founded by Gian Carlo Menotti, a 17-day art festival featuring over 100 performances by individual artists in a variety of disciplines. The Spoleto Festival is internationally recognized as America's premier performing arts festival. The annual Piccolo Spoleto festival takes place at the same time and features local performers and artists, with hundreds of performances throughout the city. Other festivals and events include Historic Charleston Foundation's Festival of Houses and Gardens and Charleston Antiques Show, the Taste of Charleston, The Lowcountry Oyster Festival, the Cooper River Bridge Run, The Charleston Marathon, Southeastern Wildlife Exposition (SEWE), Charleston Food and Wine Festival, Charleston Fashion Week, the MOJA Arts Festival, and the Holiday Festival of Lights (at James Island County Park), and the Charleston International Film Festival. The Charleston-North Charleston-Summerville Metropolitan Statistical Area consists of three counties: Charleston, Berkeley, and Dorchester. As of the 2013 U.S. Census, the metropolitan statistical area had a total population of 712,239 people. North Charleston is the second-largest city in the Charleston-North Charleston-Summerville Metropolitan Statistical Area and ranks as the third-largest city in the state; Mount Pleasant and Summerville are the next-largest cities. These cities combined with other incorporated and unincorporated areas along with the city of Charleston form the Charleston-North Charleston Urban Area with a population of 548,404 as of 2010. The metropolitan statistical area also includes a separate and much smaller urban area within Berkeley County, Moncks Corner (with a 2000 population of 9,123). Africans were brought to Charles Town on the Middle Passage, first as ""servants"", then as slaves. Ethnic groups transported here included especially Wolof, Yoruba, Fulani, Igbo, Malinke, and other people of the Windward Coast. An estimated 40% of the total 400,000 Africans transported and sold as slaves into North America are estimated to have landed at Sullivan's Island, just off the port of Charles Town; it is described as a ""hellish Ellis Island of sorts .... Today nothing commemorates that ugly fact but a simple bench, established by the author Toni Morrison using private funds."" Charleston is the primary medical center for the eastern portion of the state. The city has several major hospitals located in the downtown area: Medical University of South Carolina Medical Center (MUSC), Ralph H. Johnson VA Medical Center, and Roper Hospital. MUSC is the state's first school of medicine, the largest medical university in the state, and the sixth-oldest continually operating school of medicine in the United States. The downtown medical district is experiencing rapid growth of biotechnology and medical research industries coupled with substantial expansions of all the major hospitals. Additionally, more expansions are planned or underway at another major hospital located in the West Ashley portion of the city: Bon Secours-St Francis Xavier Hospital. The Trident Regional Medical Center located in the City of North Charleston and East Cooper Regional Medical Center located in Mount Pleasant also serve the needs of residents of the city of Charleston. Charleston has a humid subtropical climate (Köppen climate classification Cfa), with mild winters, hot, humid summers, and significant rainfall all year long. Summer is the wettest season; almost half of the annual rainfall occurs from June to September in the form of thundershowers. Fall remains relatively warm through November. Winter is short and mild, and is characterized by occasional rain. Measurable snow (≥0.1 in or 0.25 cm) only occurs several times per decade at the most, with the last such event occurring December 26, 2010. However, 6.0 in (15 cm) fell at the airport on December 23, 1989, the largest single-day fall on record, contributing to a single-storm and seasonal record of 8.0 in (20 cm) snowfall. Violent incidents occurred throughout the Piedmont of the state as white insurgents struggled to maintain white supremacy in the face of social changes after the war and granting of citizenship to freedmen by federal constitutional amendments. After former Confederates were allowed to vote again, election campaigns from 1872 on were marked by violent intimidation of blacks and Republicans by white Democratic paramilitary groups, known as the Red Shirts. Violent incidents took place in Charleston on King Street in September 6 and in nearby Cainhoy on October 15, both in association with political meetings before the 1876 election. The Cainhoy incident was the only one statewide in which more whites were killed than blacks. The Red Shirts were instrumental in suppressing the black Republican vote in some areas in 1876 and narrowly electing Wade Hampton as governor, and taking back control of the state legislature. Another riot occurred in Charleston the day after the election, when a prominent Republican leader was mistakenly reported killed. Investment in the city continued. The William Enston Home, a planned community for the city's aged and infirm, was built in 1889. An elaborate public building, the United States Post Office and Courthouse, was completed by the federal government in 1896 in the heart of the city. The Democrat-dominated state legislature passed a new constitution in 1895 that disfranchised blacks, effectively excluding them entirely from the political process, a second-class status that was maintained for more than six decades in a state that was majority black until about 1930. The Roman Catholic Diocese of Charleston Office of Education also operates out of the city and oversees several K-8 parochial schools, such as Blessed Sacrament School, Christ Our King School, Charleston Catholic School, Nativity School, and Divine Redeemer School, all of which are ""feeder"" schools into Bishop England High School, a diocesan high school within the city. Bishop England, Porter-Gaud School, and Ashley Hall are the city's oldest and most prominent private schools, and are a significant part of Charleston history, dating back some 150 years. As Charles Town grew, so did the community's cultural and social opportunities, especially for the elite merchants and planters. The first theatre building in America was built in 1736 on the site of today's Dock Street Theatre. Benevolent societies were formed by different ethnic groups, from French Huguenots to free people of color to Germans to Jews. The Charles Towne Library Society was established in 1748 by well-born young men who wanted to share the financial cost to keep up with the scientific and philosophical issues of the day. This group also helped establish the College of Charles Towne in 1770, the oldest college in South Carolina. Until its transition to state ownership in 1970, this was the oldest municipally supported college in the United States. After the defeat of the Confederacy, federal forces remained in Charleston during the city's reconstruction. The war had shattered the prosperity of the antebellum city. Freed slaves were faced with poverty and discrimination, but a large community of free people of color had been well-established in the city before the war and became the leaders of the postwar Republican Party and its legislators. Men who had been free people of color before the war comprised 26% of those elected to state and federal office in South Carolina from 1868 to 1876. The highest temperature recorded within city limits was 104 °F (40 °C), on June 2, 1985, and June 24, 1944, and the lowest was 7 °F (−14 °C) on February 14, 1899, although at the airport, where official records are kept, the historical range is 105 °F (41 °C) on August 1, 1999 down to 6 °F (−14 °C) on January 21, 1985. Hurricanes are a major threat to the area during the summer and early fall, with several severe hurricanes hitting the area – most notably Hurricane Hugo on September 21, 1989 (a category 4 storm). Dewpoint in the summer ranges from 67.8 to 71.4 °F (20 to 22 °C). Clinton returned in 1780 with 14,000 soldiers. American General Benjamin Lincoln was trapped and surrendered his entire 5,400-man force after a long fight, and the Siege of Charles Towne was the greatest American defeat of the war. Several Americans who escaped the carnage joined other militias, including those of Francis Marion, the ""Swamp Fox""; and Andrew Pickens. The British retained control of the city until December 1782. After the British left, the city's name was officially changed to Charleston in 1783. Charleston is known for its unique culture, which blends traditional Southern U.S., English, French, and West African elements. The downtown peninsula has gained a reputation for its art, music, local cuisine, and fashion. Spoleto Festival USA, held annually in late spring, has become one of the world's major performing arts festivals. It was founded in 1977 by Pulitzer Prize-winning composer Gian Carlo Menotti, who sought to establish a counterpart to the Festival dei Due Mondi (the Festival of Two Worlds) in Spoleto, Italy. As it has on every aspect of Charleston culture, the Gullah community has had a tremendous influence on music in Charleston, especially when it comes to the early development of jazz music. In turn, the music of Charleston has had an influence on that of the rest of the country. The geechee dances that accompanied the music of the dock workers in Charleston followed a rhythm that inspired Eubie Blake's ""Charleston Rag"" and later James P. Johnson's ""The Charleston"", as well as the dance craze that defined a nation in the 1920s. ""Ballin' the Jack"", which was a popular dance in the years before ""The Charleston"", was written by native Charlestonian Chris Smith. On June 28, 1776, General Sir Henry Clinton along with 2,000 men and a naval squadron tried to seize Charles Towne, hoping for a simultaneous Loyalist uprising in South Carolina. When the fleet fired cannonballs, they failed to penetrate Fort Sullivan's unfinished, yet thick, palmetto-log walls. No local Loyalists attacked the town from the mainland side, as the British had hoped they would do. Col. Moultrie's men returned fire and inflicted heavy damage on several of the British ships. The British were forced to withdraw their forces, and the Americans renamed the defensive installation as Fort Moultrie in honor of its commander. Although the city lost the status of state capital to Columbia in 1786, Charleston became even more prosperous in the plantation-dominated economy of the post-Revolutionary years. The invention of the cotton gin in 1793 revolutionized the processing of this crop, making short-staple cotton profitable. It was more easily grown in the upland areas, and cotton quickly became South Carolina's major export commodity. The Piedmont region was developed into cotton plantations, to which the sea islands and Lowcountry were already devoted. Slaves were also the primary labor force within the city, working as domestics, artisans, market workers, and laborers. In 1875, blacks made up 57% of the city's population, and 73% of Charleston County. With leadership by members of the antebellum free black community, historian Melinda Meeks Hennessy described the community as ""unique"" in being able to defend themselves without provoking ""massive white retaliation"", as occurred in numerous other areas during Reconstruction. In the 1876 election cycle, two major riots between black Republicans and white Democrats occurred in the city, in September and the day after the election in November, as well as a violent incident in Cainhoy at an October joint discussion meeting. The Arthur Ravenel Jr. Bridge across the Cooper River opened on July 16, 2005, and was the second-longest cable-stayed bridge in the Americas at the time of its construction.[citation needed] The bridge links Mount Pleasant with downtown Charleston, and has eight lanes plus a 12-foot lane shared by pedestrians and bicycles. It replaced the Grace Memorial Bridge (built in 1929) and the Silas N. Pearman Bridge (built in 1966). They were considered two of the more dangerous bridges in America and were demolished after the Ravenel Bridge opened. According to the United States Census Bureau, the city has a total area of 127.5 square miles (330.2 km2), of which 109.0 square miles (282.2 km2) is land and 18.5 square miles (47.9 km2) is covered by water. The old city is located on a peninsula at the point where, as Charlestonians say, ""The Ashley and the Cooper Rivers come together to form the Atlantic Ocean."" The entire peninsula is very low, some is landfill material, and as such, frequently floods during heavy rains, storm surges, and unusually high tides. The city limits have expanded across the Ashley River from the peninsula, encompassing the majority of West Ashley as well as James Island and some of Johns Island. The city limits also have expanded across the Cooper River, encompassing Daniel Island and the Cainhoy area. North Charleston blocks any expansion up the peninsula, and Mount Pleasant occupies the land directly east of the Cooper River. By 1840, the Market Hall and Sheds, where fresh meat and produce were brought daily, became a hub of commercial activity. The slave trade also depended on the port of Charleston, where ships could be unloaded and the slaves bought and sold. The legal importation of African slaves had ended in 1808, although smuggling was significant. However, the domestic trade was booming. More than one million slaves were transported from the Upper South to the Deep South in the antebellum years, as cotton plantations were widely developed through what became known as the Black Belt. Many slaves were transported in the coastwise slave trade, with slave ships stopping at ports such as Charleston."
Department_store,"In Denmark you find three department store chains: Magasin (1868), Illum (1891), Salling (1906). Magasin is by far the largest with 6 stores all over the country, with the flagship store being Magasin du Nord on Kongens Nytorv in Copenhagen. Illums only store on Amagertorv in Copenhagen has the appearance of a department store with 20% run by Magasin, but has individual shop owners making it a shopping centre. But in people's mind it remains a department store. Salling has two stores in Jutland with one of these being the reason for the closure of a magasin store due to the competition. Since the opening policy in 1979, the Chinese department stores also develops swiftly along with the fast-growing economy. There are different department store groups dominating different regions. For example, INTIME department store has the biggest market presence in Zhejiang province, while Jinying department stores dominate Jiangsu Province. Besides, there are many other department store groups, such as Pacific, Parkson, Wangfujing，New World，etc., many of them are expanding quickly by listing in the financial market. All major cities have their distinctive local department stores, which anchored the downtown shopping district until the arrival of the malls in the 1960s. Washington, for example, after 1887 had Woodward & Lothrop and Garfinckel's starting in 1905. Garfield's went bankrupt in 1990, as did Woodward & Lothrop in 1994. Baltimore had four major department stores: Hutzler's was the prestige leader, followed by Hecht's, Hochschild's and Stewart's. They all operated branches in the suburbs, but all closed in the late twentieth century. By 2015, most locally owned department stores around the country had been consolidated into larger chains, or had closed down entirely. All the major British cities had flourishing department stores by the mid-or late nineteenth century. Increasingly, women became the major shoppers and middle-class households. Kendals (formerly Kendal Milne & Faulkner) in Manchester lays claim to being one of the first department stores and is still known to many of its customers as Kendal's, despite its 2005 name change to House of Fraser. The Manchester institution dates back to 1836 but had been trading as Watts Bazaar since 1796. At its zenith the store had buildings on both sides of Deansgate linked by a subterranean passage ""Kendals Arcade"" and an art nouveau tiled food hall. The store was especially known for its emphasis on quality and style over low prices giving it the nickname ""the Harrods of the North"", although this was due in part to Harrods acquiring the store in 1919. Other large Manchester stores included Paulden's (currently Debenhams) and Lewis's (now a Primark). A novelty shop called Au Bon Marché had been founded in Paris in 1838 to sell lace, ribbons, sheets, mattresses, buttons, umbrellas and other assorted goods. It originally had four departments, twelve employees, and a floor space of three hundred meters. The entrepreneur Aristide Boucicaut became a partner in 1852, and changed the marketing plan, instituting fixed prices and guarantees that allowed exchanges and refunds, advertising, and a much wider variety of merchandise. The annual income of the store increased from 500,000 francs in 1852 to five million in 1860. In 1869 he built much larger building at 24 rue de Sèvres on the Left Bank, and enlarged the store again in 1872, with help from the engineering firm of Gustave Eiffel, creator of the Eiffel Tower. The income rose from twenty million francs in 1870 to 72 million at the time of the Boucicaut's death in 1877. The floor space had increased from three hundred square meters in 1838 to fifty thousand, and the number of employees had increased from twelve in 1838 to 1788 in 1879. Boucicaut was famous for his marketing innovations; a reading room for husbands while their wives shopped; extensive newspaper advertising; entertainment for children; and six million catalogs sent out to customers. By 1880 half the employees were women; unmarried women employees lived in dormitories on the upper floors. John Lewis Newcastle (formerly Bainbridge) in Newcastle upon Tyne, is the world's oldest Department Store. It is still known to many of its customers as Bainbridge, despite the name change to 'John Lewis'. The Newcastle institution dates back to 1838 when Emerson Muschamp Bainbridge, aged 21, went into partnership with William Alder Dunn and opened a draper's and fashion in Market Street, Newcastle. In terms of retailing history, one of the most significant facts about the Newcastle Bainbridge shop, is that as early as 1849 weekly takings were recorded by department, making it the earliest of all department stores. This ledger survives and is kept in the John Lewis archives. John Lewis bought the Bainbridge store in 1952. The iconic department stores of New Zealand's three major centres are Smith & Caughey's (founded 1880), in New Zealand's most populous city, Auckland; Kirkcaldie & Stains (founded 1863) in the capital, Wellington; and Ballantynes (founded 1854) in New Zealand's second biggest city, Christchurch. These offer high-end and luxury items. Additionally, Arthur Barnett (1903) operates in Dunedin. H & J Smith is a small chain operating throughout Southland with a large flagship store in Invercargill. Farmers is a mid-range national chain of stores (originally a mail-order firm known as Laidlaw Leeds founded in 1909). Historical department stores include DIC. Discount chains include The Warehouse, Kmart Australia, and the now-defunct DEKA. Also, Kendals in Manchester can lay claim to being one of the oldest department stores in the UK. Beginning as a small shop owned by S. and J. Watts in 1796, its sold a variety of goods. Kendal Milne and Faulkner purchased the business in 1835. Expanding the space, rather than use it as a typical warehouse simply to showcase textiles, it became a vast bazaar. Serving Manchester's upmarket clientele for over 200 years, it was taken over by House of Fraser and recently rebranded as House of Fraser Manchester – although most Mancunians still refer to it as Kendals. The Kendal Milne signage still remains over the main entrance to the art deco building in the city's Deansgate. France's major upscale department stores are Galeries Lafayette and Le Printemps, which both have flagship stores on Boulevard Haussmann in Paris and branches around the country. The first department store in France, Le Bon Marché in Paris, was founded in 1852 and is now owned by the luxury goods conglomerate LVMH. La Samaritaine, another upscale department store also owned by LVMH, closed in 2005. Mid-range department stores chains also exist in France such as the BHV (Bazar de l'Hotel de Ville), part of the same group as Galeries Lafayette. Panama's first department stores such as Bazaar Francés, La Dalia and La Villa de Paris started as textile retailers at the turn of the nineteenth century. Later on in the twentieth century these eventually gave way to stores such as Felix B. Maduro, Sarah Panamá, Figali, Danté, Sears, Gran Morrison and smaller ones such as Bon Bini, Cocos, El Lider, Piccolo and Clubman among others. Of these only Felix B. Maduro (usually referred to as Felix by locals) and Danté remain strong. All the others have either folded or declined although Cocos has managed to secure a good position in the market. The design and function of department stores in Germany followed the lead of London, Paris and New York. Germany used to have a number of department stores; nowadays only a few of them remain. Next to some smaller, independent department stores these are Karstadt (in 2010 taken over by Nicolas Berggruen, also operating the KaDeWe in Berlin, the Alsterhaus in Hamburg and the Oberpollinger in Munich), GALERIA Kaufhof (part of the Metro AG). Others like Hertie, Wertheim and Horten AG were taken over by others and either fully integrated or later closed. From its origins in the fur trade, the Hudson's Bay Company is the oldest corporation in North America and was the largest department store operator in Canada until the mid-1980s, with locations across the country. It also previously owned Zellers, another major Canadian department store which ceased to exist in March 2013 after selling its lease holdings to Target Canada. Other department stores in Canada are: Canadian Tire, Sears Canada, Ogilvy, Les Ailes de la Mode, Giant Tiger, Co-op, Costco and Holt Renfrew. Grocery giant Superstores carry many non-grocery items akin to a department store. Woolco had 160 stores in Canada when operations ceased (Walmart bought out Woolco in 1994). Today low-price Walmart is by far the most dominant department store retailer in Canada with outlets throughout the country. Historically, department stores were a significant component in Canadian economic life, and chain stores such as Eaton's, Charles Ogilvy Limited, Freiman's, Spencer's, Simpsons, Morgan's, and Woodward's were staples in their respective communities. Department stores in Canada are similar in design and style to department stores in the United States. After World War II Hudson's realized that the limited parking space at its downtown skyscraper would increasingly be a problem for its customers. The solution in 1954 was to open the Northland Center in nearby Southfield, just beyond the city limits. It was the largest suburban shopping center in the world, and quickly became the main shopping destination for northern and western Detroit, and for much of the suburbs. By 1961 the downtown skyscraper accounted for only half of Hudson's sales; it closed in 1986. The Northland Center Hudson's, rebranded Macy's in 2006 following acquisition by Federated Department Stores, was closed along with the remaining stores in the center in March 2015 due to the mall's high storefront vacancy, decaying infrastructure, and financial mismanagement. The first department store in Spain was Almacenes el Siglo opened in October 1881 in Barcelona. Following the 2002 closure by the Australian group Partridges of their SEPU (Sociedad Española de Precios Unicos) department store chain, which was one of Spain's oldest, the market is now dominated by El Corte Inglés, founded in 1934 as a drapery store. El Corte Inglés stores tend to be vast buildings, selling a very broad range of products and the group also controls a number of other retail formats including supermarket chain 'Supercor' and hypermarket chain 'Hipercor'. Other competitors such as 'Simago' and 'Galerías Preciados' closed in the 1990s, however El Corte Inglés, faces major competition from French discount operators such as Carrefour and Auchan. The middle up segment is mainly occupied by Metro Department Store originated from Singapore and Sogo from Japan. 2007 saw the re-opening of Jakarta's Seibu, poised to be the largest and second most upscale department store in Indonesia after Harvey Nichols, which the latter closed in 2010 and yet plans to return. Other international department stores include Debenhams and Marks & Spencer. Galeries Lafayette also joins the Indonesian market in 2013 inside Pacific Place Mall. This department store is targeting middle up market with price range from affordable to luxury, poised to be the largest upscale department store. Galeries Lafayette, Debenhams, Harvey Nichols, Marks & Spencer, Seibu and Sogo are all operated by PT. Mitra Adiperkasa. Ireland developed a strong middle class, especially in the major cities, by the mid-nineteenth century. They were active patrons of department stores. Delany's New Mart was opened in 1853 in Dublin, Ireland. Unlike others, Delany's had not evolved gradually from a smaller shop on site. Thus it could claim to be the first purpose-built Department Store in the world. The word department store had not been invented at that time and thus it was called the ""Monster House"". The store was completely destroyed in the 1916 Easter Rising, but reopened in 1922. Socialism confronted consumerism in the chain State Department Stores (GUM), set up by Lenin in 1921 as a model retail enterprise. It operated stores throughout Russia and targeted consumers across class, gender, and ethnic lines. GUM was designed to advance the Bolsheviks' goals of eliminating private enterprise and rebuilding consumerism along socialist lines, as well as democratizing consumption for workers and peasants nationwide. GUM became a major propaganda purveyor, with advertising and promotional campaigns that taught Russians the goals of the regime and attempted to inculcate new attitudes and behavior. In trying to create a socialist consumer culture from scratch, GUM recast the functions and meanings of buying and selling, turning them into politically charged acts that could either contribute to or delay the march toward utopian communism. By the late 1920s, however, GUM's gandiose goals had proven unrealistic and largely alienated consumers, who instead learned a culture of complaint and entitlement. GUM's main function became one of distributing whatever the factories sent them, regardless of consumer demand or quality. In the 21st century the most famous department store in Russia is GUM in Moscow, followed by TsUM and the Petrovsky Passage. Other popular stores are Mega (shopping malls), Stockmann, and Marks & Spencer. Media Markt, M-video, Technosila, and White Wind (Beliy Veter) sell large number of electronic devices. In St. Petersburg The Passage has been popular since the 1840s. 1956 Soviet film Behind Store Window (За витриной универмага) on YouTube depicts operation of a Moscow department store in 1950's. In New York City in 1846, Alexander Turney Stewart established the ""Marble Palace"" on Broadway, between Chambers and Reade streets. He offered European retail merchandise at fixed prices on a variety of dry goods, and advertised a policy of providing ""free entrance"" to all potential customers. Though it was clad in white marble to look like a Renaissance palazzo, the building's cast iron construction permitted large plate glass windows that permitted major seasonal displays, especially in the Christmas shopping season. In 1862, Stewart built a new store on a full city block with eight floors and nineteen departments of dress goods and furnishing materials, carpets, glass and china, toys and sports equipment, ranged around a central glass-covered court. His innovations included buying from manufacturers for cash and in large quantities, keeping his markup small and prices low, truthful presentation of merchandise, the one-price policy (so there was no haggling), simple merchandise returns and cash refund policy, selling for cash and not credit, buyers who searched worldwide for quality merchandise, departmentalization, vertical and horizontal integration, volume sales, and free services for customers such as waiting rooms and free delivery of purchases. His innovations were quickly copied by other department stores. David Jones was started by David Jones, a Welsh merchant who met Hobart businessman Charles Appleton in London. Appleton established a store in Sydney in 1825 and Jones subsequently established a partnership with Appleton, moved to Australia in 1835, and the Sydney store became known as Appleton & Jones. When the partnership was dissolved in 1838, Jones moved his business to premises on the corner of George Street and Barrack Lane, Sydney. David Jones claims to be the oldest department store in the world still trading under its original name. Parkson enters by acquiring local brand Centro Department Store in 2011. Centro still operates for middle market while the 'Parkson' brand itself, positioned for middle-up segment, enters in 2014 by opening its first store in Medan, followed by its second store in Jakarta. Lotte, meanwhile, enters the market by inking partnership with Ciputra Group, creating what its called 'Lotte Shopping Avenue' inside the Ciputra World Jakarta complex, as well as acquiring Makro and rebranding it into Lotte Mart. Arnold, Constable was the first American department store. It was founded in 1825 by Aaron Arnold (1794?-1876), an emigrant from Great Britain, as a small dry goods store on Pine Street in New York City. In 1857 the store moved into a five-story white marble dry goods palace known as the Marble House. During the Civil War Arnold, Constable was one of the first stores to issue charge bills of credit to its customers each month instead of on a bi-annual basis. Recognized as an emporium for high-quality fashions, the store soon outgrew the Marble House and erected a cast-iron building on Broadway and Nineteenth Street in 1869; this “Palace of Trade” expanded over the years until it was necessary to move into a larger space in 1914. In 1925, Arnold, Constable merged with Stewart & Company and expanded into the suburbs, first with a 1937 store in New Rochelle, New York and later in Hempstead and Manhasset on Long Island, and in New Jersey. Financial problems led to bankruptcy in 1975. The first department stores Lane Crawford was opened in 1850 by Scots Thomas Ash Lane and Ninian Crawford on Des Voeux Road, Hong Kong Island. At the beginning, the store mainly catered visiting ships' crews as well as British Navy staff and their families. In 1900, the first ethnic-Chinese owned Sincere Department Store was opened by Ma Ying Piu, who returned from Australia and inspired by David Jones. In 1907, another former Hong Kong expatriate in Australia, the Kwok's family, returned to Hong Kong and founded Wing On. The Paris department store had its roots in the magasin de nouveautés, or novelty store; the first, the Tapis Rouge, was created in 1784. They flourished in the early 19th century, with La Belle Jardiniere (1824), Aux Trois Quartiers (1829), and Le Petit Saint Thomas (1830). Balzac described their functioning in his novel César Birotteau. In the 1840s, with the arrival of the railroads in Paris and the increased number of shoppers they brought, they grew in size, and began to have large plate glass display windows, fixed prices and price tags, and advertising in newspapers. Although there were a number of department stores in Australia for much of the 20th Century, including chains such as Grace Bros. and Waltons, many disappeared during the 1980s and 1990s. Today Myer and David Jones, located nationally, are practically the national department stores duopoly in Australia. When Russian-born migrant, Sidney Myer, came to Australia in 1899 he formed the Myer retail group with his brother, Elcon Myer. In 1900, they opened the first Myer department store, in Bendigo, Victoria. Since then, the Myer retail group has grown to be Australia's largest retailer. Both, Myer and David Jones, are up-market chains, offering a wide variety of products from mid-range names to luxury brands. Other retail chain stores such as Target (unrelated to the American chain of the same name), Venture (now defunct), Kmart and Big W, also located nationally, are considered to be Australia's discount department stores. Harris Scarfe, though only operating in four states and one territory, is a department store using both the large full-line and small discount department store formats. Most department stores in Australia have their own credit card companies, each having their own benefits while the discount department stores do not have their own credit card rights. Chain department stores grew rapidly after 1920, and provided competition for the downtown upscale department stores, as well as local department stores in small cities. J. C. Penney had four stores in 1908, 312 in 1920, and 1452 in 1930. Sears, Roebuck & Company, a giant mail-order house, opened its first eight retail stores in 1925, and operated 338 by 1930, and 595 by 1940. The chains reached a middle-class audience, that was more interested in value than in upscale fashions. Sears was a pioneer in creating department stores that catered to men as well as women, especially with lines of hardware and building materials. It deemphasized the latest fashions in favor of practicality and durability, and allowed customers to select goods without the aid of a clerk. Its stores were oriented to motorists – set apart from existing business districts amid residential areas occupied by their target audience; had ample, free, off-street parking; and communicated a clear corporate identity. In the 1930s, the company designed fully air-conditioned, ""windowless"" stores whose layout was driven wholly by merchandising concerns. George Dayton had founded his Dayton's Dry Goods store in Minneapolis in 1902 and the AMC cooperative in 1912. His descendants built Southdale Center in 1956, opened the Target discount store chain in 1962 and the B. Dalton Bookseller chain in 1966. Dayton's grew to 19 stores under the Dayton's name plus five other regional names acquired by Dayton-Hudson. The Dayton-Hudson Corporation closed the flagship J. L. Hudson Department Store in downtown Detroit in 1983, but expanded its other retail operations. It acquired Mervyn's in 1978, Marshall Field's in 1990, and renamed itself the Target Corporation in 2000. In 2002, Dayton's and Hudson's were consolidated into the Marshall Field's name. In 2005, May Department Stores acquired all of the Marshall Field's stores and shortly thereafter, Macy's acquired May. Mexico has a large number of department stores based in Mexico, of which the most traditional are El Palacio de Hierro (High end and luxury goods) and Liverpool (Upper-middle income), with its middle income sister store Fabricas de Francia. Sanborns owns over 100 middle income level stores throughout the country. Grupo Carso operates Sears Mexico and two high-end Saks 5th Avenue stores. Other large chains are Coppel and Elektra, which offer items for the bargain price seeker. Wal-Mart operates Suburbia for lower income shoppers, along with stores under the brand names of Wal-Mart, Bodega Aurrera, and Superama. Selfridges was established in 1909 by American-born Harry Gordon Selfridge on Oxford Street. The company's innovative marketing promoted the radical notion of shopping for pleasure rather than necessity and its techniques were adopted by modern department stores the world over. The store was extensively promoted through paid advertising. The shop floors were structured so that goods could be made more accessible to customers. There were elegant restaurants with modest prices, a library, reading and writing rooms, special reception rooms for French, German, American and ""Colonial"" customers, a First Aid Room, and a Silence Room, with soft lights, deep chairs, and double-glazing, all intended to keep customers in the store as long as possible. Staff members were taught to be on hand to assist customers, but not too aggressively, and to sell the merchandise. Selfridge attracted shoppers with educational and scientific exhibits; – in 1909, Louis Blériot's monoplane was exhibited at Selfridges (Blériot was the first to fly over the English Channel), and the first public demonstration of television by John Logie Baird took place in the department store in 1925. The Grands Magasins Dufayel was a huge department store with inexpensive prices built in 1890 in the northern part of Paris, where it reached a very large new customer base in the working class. In a neighborhood with few public spaces, it provided a consumer version of the public square. It educated workers to approach shopping as an exciting social activity not just a routine exercise in obtaining necessities, just as the bourgeoisie did at the famous department stores in the central city. Like the bourgeois stores, it helped transform consumption from a business transaction into a direct relationship between consumer and sought-after goods. Its advertisements promised the opportunity to participate in the newest, most fashionable consumerism at reasonable cost. The latest technology was featured, such as cinemas and exhibits of inventions like X-ray machines (that could be used to fit shoes) and the gramophone. Marshall Field & Company originated in 1852. It was the premier department store on the main shopping street in the Midwest, State Street in Chicago. Upscale shoppers came by train from throughout the region, patronizing nearby hotels. It grew to become a major chain before converting to the Macy's nameplate on 9 September 2006. Marshall Field's Served as a model for other departments stores in that it had exceptional customer service. Field's also brought with it the now famous Frango mints brand that became so closely identified with Marshall Field's and Chicago from the now defunct Frederick & Nelson Department store. Marshall Field's also had the firsts, among many innovations by Marshall Field's. Field's had the first European buying office, which was located in Manchester, England, and the first bridal registry. The company was the first to introduce the concept of the personal shopper, and that service was provided without charge in every Field's store, until the chain's last days under the Marshall Field's name. It was the first store to offer revolving credit and the first department store to use escalators. Marshall Field's book department in the State Street store was legendary; it pioneered the concept of the ""book signing."" Moreover, every year at Christmas, Marshall Field's downtown store windows were filled with animated displays as part of the downtown shopping district display; the ""theme"" window displays became famous for their ingenuity and beauty, and visiting the Marshall Field's windows at Christmas became a tradition for Chicagoans and visitors alike, as popular a local practice as visiting the Walnut Room with its equally famous Christmas tree or meeting ""under the clock"" on State Street. The first department store in the Philippines is the Hoskyn's Department Store of Hoskyn & Co. established in 1877 in Iloilo by the Englishman Henry Hoskyn, nephew of Nicholas Loney, the first British vice-consul in Iloilo. Some of the earliest department stores in the Philippines were located in Manila as early as 1898 with the opening of the American Bazaar, which was later named Beck's. During the course of the American occupation of the Philippines, many department stores were built throughout the city, many of which were located in Escolta. Heacock's, a luxury department store, was considered as the best department store in the Orient. Other department stores included Aguinaldo's, La Puerta del Sol, Estrella del Norte, and the Crystal Arcade, all of which were destroyed during the Battle of Manila in 1945. After the war, department stores were once again alive with the establishment of Shoemart (now SM), and Rustan's. Since the foundation of these companies in the 1950s, there are now more than one hundred department stores to date. At present, due to the huge success of shopping malls, department stores in the Philippines usually are anchor tenants within malls. SM Supermalls and Robinsons Malls are two of the country's most prominent mall chains, all of which has Department Store sections. The five most prevalent chains are Lotte, Hyundai, Shinsegae, Galleria, AK plaza. Lotte Department Store is the largest, operating more than 40 stores (include outlet, young plaza, foreign branches). Hyundai Department Store has about 14 stores (13dept, 1outlet), and there are 10 stores in Shinsegae. Shinsegae has 3 outlet store with Simon. Galleria has 5, AK has 5 stores. Galleriaeast and west is well known by luxury goods. These five department stores are known to people as representative corporations in the field of distirution in South Korea. From fashion items to electric appliances, people can buy various kinds of products. Every weekend, people are fond of going around these department stores, because their location is usually easy to visit. As of 2010 the Shinsegae department store in Centum City, Busan, is the largest department store in the world. In 1877, John Wanamaker opened the United State's first modern department store in a former Pennsylvania Railroad freight terminal in Philadelphia. Wanamakers was the first department store to offer fixed prices marked on every article and also introduced electrical illumination (1878), the telephone (1879), and the use of pneumatic tubes to transport cash and documents (1880) to the department store business. Subsequent department stores founded in Philadelphia included Strawbridge and Clothier, Gimbels, Lit Brothers, and Snellenbergs. The origins of the department store lay in the growth of the conspicuous consumer society at the turn of the 19th century. As the Industrial Revolution accelerated economy expansion, the affluent middle-class grew in size and wealth. This urbanized social group, sharing a culture of consumption and changing fashion, was the catalyst for the retail revolution. As rising prosperity and social mobility increased the number of people, especially women (who found they could shop unaccompanied at department stores without damaging their reputation), with disposable income in the late Georgian period, window shopping was transformed into a leisure activity and entrepreneurs, like the potter Josiah Wedgwood, pioneered the use of marketing techniques to influence the prevailing tastes and preferences of society. In 1849, Horne's began operations and soon became a leading Pittsburgh department store. In 1879, it opened a seven-story landmark which was the first department store in the city's downtown. In 1972, Associated Dry Goods acquired Horne's, and ADG expanded operations of Horne's to several stores in suburban malls throughout the Pittsburgh region as well as in Erie, Pennsylvania and Northeast Ohio. In December 1986, Horne's was acquired by a local investor group following ADG's acquisition by May Department Stores. By 1994, Federated Department Stores acquired the remaining ten Horne's stores and merged them with its Lazarus division, completely ceasing all operations of any store under the Horne's name. The site where the Saint Petersburg Passage sprawls had been devoted to trade since the city's foundation in the early 18th century. It had been occupied by various shops and warehouses (Maly Gostiny Dvor, Schukin Dvor, Apraksin Dvor) until 1846, when Count Essen-Stenbock-Fermor acquired the grounds to build an elite shopping mall for the Russian nobility and wealthy bourgeoisie. Stenbock-Fermor conceived of the Passage as more than a mere shopping mall, but also as a cultural and social centre for the people of St Petersburg. The edifice contained coffee-houses, confectioneries, panorama installations, an anatomical museum, a wax museum, and even a small zoo, described by Dostoyevsky in his extravaganza ""Crocodile, or Passage through the Passage"". The concert hall became renowned as a setting for literary readings attended by the likes of Dostoevsky and Turgenev. Parenthetically, the Passage premises have long been associated with the entertainment industry and still remains home to the Komissarzhevskaya Theatre. Before the 1950s, the department store held an eminent place in both Canada and Australia, during both the Great Depression and World War II. Since then, they have suffered from strong competition from specialist stores. Most recently the competition has intensified with the advent of larger-scale superstores (Jones et al. 1994; Merrilees and Miller 1997). Competition was not the only reason for the department stores' weakening strength; the changing structure of cities also affected them. The compact and centralized 19th century city with its mass transit lines converging on the downtown was a perfect environment for department store growth. But as residents moved out of the downtown areas to the suburbs, the large, downtown department stores became inconvenient and lost business to the newer suburban shopping malls. In 2003, U.S. department store sales were surpassed by big-box store sales for the first time (though some stores may be classified as ""big box"" by physical layout and ""department store"" by merchandise). Department stores today have sections that sell the following: clothing, furniture, home appliances, toys, cosmetics, gardening, toiletries, sporting goods, do it yourself, paint, and hardware and additionally select other lines of products such as food, books, jewelry, electronics, stationery, photographic equipment, baby products, and products for pets. Customers check out near the front of the store or, alternatively, at sales counters within each department. Some are part of a retail chain of many stores, while others may be independent retailers. In the 1970s, they came under heavy pressure from discounters. Since 2010, they have come under even heavier pressure from online stores such as Amazon. In Puerto Rico, various department stores have operated, such as Sears, JC Penney, Macy's, Kmart, Wal-Mart, Marshalls, Burlington Coat Factory, T.J. Maxx, Costco, Sam's Club and others. La New York was a Puerto Rican department store. Topeka, Capri and Pitusa are competitors on the Puerto Rican market that also have hypermarkets operating under their names. Retailers Nordstrom and Saks Fifth Avenue also have plans to come to the Mall of San Juan, a new high-end retail project with over 100 tenants. The mall is set to open in March 2015."
Melbourne,"A long list of AM and FM radio stations broadcast to greater Melbourne. These include ""public"" (i.e., state-owned ABC and SBS) and community stations. Many commercial stations are networked-owned: DMG has Nova 100 and Smooth; ARN controls Gold 104.3 and KIIS 101.1; and Southern Cross Austereo runs both Fox and Triple M. Stations from towns in regional Victoria may also be heard (e.g. 93.9 Bay FM, Geelong). Youth alternatives include ABC Triple J and youth run SYN. Triple J, and similarly PBS and Triple R, strive to play under represented music. JOY 94.9 caters for gay, lesbian, bisexual and transgender audiences. For fans of classical music there are 3MBS and ABC Classic FM. Light FM is a contemporary Christian station. AM stations include ABC: 774, Radio National, and News Radio; also Fairfax affiliates 3AW (talk) and Magic (easy listening). For sport fans and enthusiasts there is SEN 1116. Melbourne has many community run stations that serve alternative interests, such as 3CR and 3KND (Indigenous). Many suburbs have low powered community run stations serving local audiences. Melbourne (/ˈmɛlbərn/, AU i/ˈmɛlbən/) is the capital and most populous city in the Australian state of Victoria, and the second most populous city in Australia and Oceania. The name ""Melbourne"" refers to the area of urban agglomeration (as well as a census statistical division) spanning 9,900 km2 (3,800 sq mi) which comprises the broader metropolitan area, as well as being the common name for its city centre. The metropolis is located on the large natural bay of Port Phillip and expands into the hinterlands towards the Dandenong and Macedon mountain ranges, Mornington Peninsula and Yarra Valley. Melbourne consists of 31 municipalities. It has a population of 4,347,955 as of 2013, and its inhabitants are called Melburnians. Height limits in the Melbourne CBD were lifted in 1958, after the construction of ICI House, transforming the city's skyline with the introduction of skyscrapers. Suburban expansion then intensified, serviced by new indoor malls beginning with Chadstone Shopping Centre. The post-war period also saw a major renewal of the CBD and St Kilda Road which significantly modernised the city. New fire regulations and redevelopment saw most of the taller pre-war CBD buildings either demolished or partially retained through a policy of facadism. Many of the larger suburban mansions from the boom era were also either demolished or subdivided. Another recent environmental issue in Melbourne was the Victorian government project of channel deepening Melbourne Ports by dredging Port Phillip Bay—the Port Phillip Channel Deepening Project. It was subject to controversy and strict regulations among fears that beaches and marine wildlife could be affected by the disturbance of heavy metals and other industrial sediments. Other major pollution problems in Melbourne include levels of bacteria including E. coli in the Yarra River and its tributaries caused by septic systems, as well as litter. Up to 350,000 cigarette butts enter the storm water runoff every day. Several programs are being implemented to minimise beach and river pollution. In February 2010, The Transition Decade, an initiative to transition human society, economics and environment towards sustainability, was launched in Melbourne. Melbourne is experiencing high population growth, generating high demand for housing. This housing boom has increased house prices and rents, as well as the availability of all types of housing. Subdivision regularly occurs in the outer areas of Melbourne, with numerous developers offering house and land packages. However, after 10 years[when?] of planning policies to encourage medium-density and high-density development in existing areas with greater access to public transport and other services, Melbourne's middle and outer-ring suburbs have seen significant brownfields redevelopment. Ship transport is an important component of Melbourne's transport system. The Port of Melbourne is Australia's largest container and general cargo port and also its busiest. The port handled two million shipping containers in a 12-month period during 2007, making it one of the top five ports in the Southern Hemisphere. Station Pier on Port Phillip Bay is the main passenger ship terminal with cruise ships and the Spirit of Tasmania ferries which cross Bass Strait to Tasmania docking there. Ferries and water taxis run from berths along the Yarra River as far upstream as South Yarra and across Port Phillip Bay. An influx of interstate and overseas migrants, particularly Irish, German and Chinese, saw the development of slums including a temporary ""tent city"" established on the southern banks of the Yarra. Chinese migrants founded the Melbourne Chinatown in 1851, which remains the longest continuous Chinese settlement in the Western World. In the aftermath of the Eureka Stockade, mass public support for the plight of the miners resulted in major political changes to the colony, including changes to working conditions across local industries including mining, agriculture and manufacturing. The nationalities involved in the Eureka revolt and Burke and Wills expedition gave an indication of immigration flows in the second half of the nineteenth century. Melbourne has a highly diversified economy with particular strengths in finance, manufacturing, research, IT, education, logistics, transportation and tourism. Melbourne houses the headquarters for many of Australia's largest corporations, including five of the ten largest in the country (based on revenue), and four of the largest six in the country (based on market capitalisation) (ANZ, BHP Billiton (the world's largest mining company), the National Australia Bank and Telstra), as well as such representative bodies and think tanks as the Business Council of Australia and the Australian Council of Trade Unions. Melbourne's suburbs also have the Head Offices of Wesfarmers companies Coles (including Liquorland), Bunnings, Target, K-Mart & Officeworks. The city is home to Australia's largest and busiest seaport which handles more than $75 billion in trade every year and 39% of the nation's container trade. Melbourne Airport provides an entry point for national and international visitors, and is Australia's second busiest airport.[citation needed] The Melbourne rail network has its origins in privately built lines from the 1850s gold rush era, and today the suburban network consists of 209 suburban stations on 16 lines which radiate from the City Loop, a partially underground metro section of the network beneath the Central Business District (Hoddle Grid). Flinders Street Station is Melbourne's busiest railway station, and was the world's busiest passenger station in 1926. It remains a prominent Melbourne landmark and meeting place. The city has rail connections with regional Victorian cities, as well as direct interstate rail services to Sydney and Adelaide and beyond which depart from Melbourne's other major rail terminus, Southern Cross Station in Spencer Street. In the 2013–2014 financial year, the Melbourne rail network recorded 232.0 million passenger trips, the highest in its history. Many rail lines, along with dedicated lines and rail yards are also used for freight. The Overland to Adelaide departs Southern Cross twice a week, while the XPT to Sydney departs twice a day. Three daily newspapers serve Melbourne: the Herald Sun (tabloid), The Age (formerly broadsheet, now compact) and The Australian (national broadsheet). Six free-to-air television stations service Greater Melbourne and Geelong: ABC Victoria, (ABV), SBS Victoria (SBS), Seven Melbourne (HSV), Nine Melbourne (GTV), Ten Melbourne (ATV), C31 Melbourne (MGV) – community television. Each station (excluding C31) broadcasts a primary channel and several multichannels. C31 is only broadcast from the transmitters at Mount Dandenong and South Yarra. Hybrid digital/print media companies such as Broadsheet and ThreeThousand are based in and primarily serve Melbourne. Melbourne is an international cultural centre, with cultural endeavours spanning major events and festivals, drama, musicals, comedy, music, art, architecture, literature, film and television. The climate, waterfront location and nightlife make it one of the most vibrant destinations in Australia. For five years in a row (as of 2015) it has held the top position in a survey by The Economist Intelligence Unit of the world's most liveable cities on the basis of a number of attributes which include its broad cultural offerings. The city celebrates a wide variety of annual cultural events and festivals of all types, including Australia's largest free community festival—Moomba, the Melbourne International Arts Festival, Melbourne International Film Festival, Melbourne International Comedy Festival and the Melbourne Fringe Festival. The culture of the city is an important drawcard for tourists, of which just under two million international overnight visitors and 57.7 million domestic overnight visited during the year ending March 2014. Melbourne is typical of Australian capital cities in that after the turn of the 20th century, it expanded with the underlying notion of a 'quarter acre home and garden' for every family, often referred to locally as the Australian Dream. This, coupled with the popularity of the private automobile after 1945, led to the auto-centric urban structure now present today in the middle and outer suburbs. Much of metropolitan Melbourne is accordingly characterised by low density sprawl, whilst its inner city areas feature predominantly medium-density, transit-oriented urban forms. The city centre, Docklands, St. Kilda Road and Southbank areas feature high-density forms. From 2006, the growth of the city extended into ""green wedges"" and beyond the city's urban growth boundary. Predictions of the city's population reaching 5 million people pushed the state government to review the growth boundary in 2008 as part of its Melbourne @ Five Million strategy. In 2009, Melbourne was less affected by the Late-2000s financial crisis in comparison to other Australian cities. At this time, more new jobs were created in Melbourne than any other Australian city—almost as many as the next two fastest growing cities, Brisbane and Perth, combined, and Melbourne's property market remained strong, resulting in historically high property prices and widespread rent increases. In 2012, the city contained a total of 594 high-rise buildings, with 8 under construction, 71 planned and 39 at proposal stage making the city's skyline the second largest in Australia. The CBD is dominated by modern office buildings including the Rialto Towers (1986), built on the site of several grand classical Victorian buildings, two of which — the Rialto Building (1889) designed by William Pitt and the Winfield Building (1890) designed by Charles D'Ebro and Richard Speight — still remain today and more recently hi-rise apartment buildings including Eureka Tower (2006), which is listed as the 13th tallest residential building in the world in January 2014. The main passenger airport serving the metropolis and the state is Melbourne Airport (also called Tullamarine Airport), which is the second busiest in Australia, and the Port of Melbourne is Australia's busiest seaport for containerised and general cargo. Melbourne has an extensive transport network. The main metropolitan train terminus is Flinders Street Station, and the main regional train and coach terminus is Southern Cross Station. Melbourne is also home to Australia's most extensive freeway network and has the world's largest urban tram network. A brash boosterism that had typified Melbourne during this time ended in the early 1890s with a severe depression of the city's economy, sending the local finance and property industries into a period of chaos during which 16 small ""land banks"" and building societies collapsed, and 133 limited companies went into liquidation. The Melbourne financial crisis was a contributing factor in the Australian economic depression of the 1890s and the Australian banking crisis of 1893. The effects of the depression on the city were profound, with virtually no new construction until the late 1890s. The city is recognised for its mix of modern architecture which intersects with an extensive range of nineteenth and early twentieth century buildings. Some of the most architecturally noteworthy historic buildings include the World Heritage Site-listed Royal Exhibition Building, constructed over a two-year period for the Melbourne International Exhibition in 1880, A.C. Goode House, a Neo Gothic building located on Collins Street designed by Wright, Reed & Beaver (1891), William Pitt's Venetian Gothic style Old Stock Exchange (1888), William Wardell's Gothic Bank (1883) which features some of Melbourne's finest interiors, the incomplete Parliament House, St Paul's Cathedral (1891) and Flinders Street Station (1909), which was the busiest commuter railway station in the world in the mid-1920s. Since the mid-1990s, Melbourne has maintained significant population and employment growth. There has been substantial international investment in the city's industries and property market. Major inner-city urban renewal has occurred in areas such as Southbank, Port Melbourne, Melbourne Docklands and more recently, South Wharf. According to the Australian Bureau of Statistics, Melbourne sustained the highest population increase and economic growth rate of any Australian capital city in the three years ended June 2004. These factors have led to population growth and further suburban expansion through the 2000s. The city is home to many professional franchises/teams in national competitions including: cricket clubs Melbourne Stars, Melbourne Renegades and Victorian Bushrangers, which play in the Big Bash League and other domestic cricket competitions; soccer clubs Melbourne Victory and Melbourne City FC (known until June 2014 as Melbourne Heart), which play in the A-League competition, both teams play their home games at AAMI Park, with the Victory also playing home games at Etihad Stadium. Rugby league club Melbourne Storm which plays in the NRL competition; rugby union clubs Melbourne Rebels and Melbourne Rising, which play in the Super Rugby and National Rugby Championship competitions respectively; netball club Melbourne Vixens, which plays in the trans-Tasman trophy ANZ Championship; basketball club Melbourne United, which plays in the NBL competition; Bulleen Boomers and Dandenong Rangers, which play in the WNBL; ice hockey teams Melbourne Ice and Melbourne Mustangs, who play in the Australian Ice Hockey League; and baseball club Melbourne Aces, which plays in the Australian Baseball League. Rowing is also a large part of Melbourne's sporting identity, with a number of clubs located on the Yarra River, out of which many Australian Olympians trained. The city previously held the nation's premier long distance swimming event the annual Race to Prince's Bridge, in the Yarra River. Melbourne has an integrated public transport system based around extensive train, tram, bus and taxi systems. Flinders Street Station was the world's busiest passenger station in 1927 and Melbourne's tram network overtook Sydney's to become the world's largest in the 1940s, at which time 25% of travellers used public transport but by 2003 it had declined to just 7.6%. The public transport system was privatised in 1999, symbolising the peak of the decline. Despite privatisation and successive governments persisting with auto-centric urban development into the 21st century, there have since been large increases in public transport patronage, with the mode share for commuters increasing to 14.8% and 8.4% of all trips. A target of 20% public transport mode share for Melbourne by 2020 was set by the state government in 2006. Since 2006 public transport patronage has grown by over 20%. In recent years, Melton, Wyndham and Casey, part of the Melbourne statistical division, have recorded the highest growth rate of all local government areas in Australia. Melbourne could overtake Sydney in population by 2028, The ABS has projected in two scenarios that Sydney will remain larger than Melbourne beyond 2056, albeit by a margin of less than 3% compared to a margin of 12% today. Melbourne's population could overtake that of Sydney by 2037 or 2039, according to the first scenario projected by the ABS; primarily due to larger levels of internal migration losses assumed for Sydney. Another study claims that Melbourne will surpass Sydney in population by 2040. Melbourne's CBD, compared with other Australian cities, has comparatively unrestricted height limits and as a result of waves of post-war development contains five of the six tallest buildings in Australia, the tallest of which is the Eureka Tower, situated in Southbank. It has an observation deck near the top from where you can see above all of Melbourne's structures. The Rialto tower, the city's second tallest, remains the tallest building in the old CBD; its observation deck for visitors has recently closed. Melbourne is also an important financial centre. Two of the big four banks, NAB and ANZ, are headquartered in Melbourne. The city has carved out a niche as Australia's leading centre for superannuation (pension) funds, with 40% of the total, and 65% of industry super-funds including the $109 billion-dollar Federal Government Future Fund. The city was rated 41st within the top 50 financial cities as surveyed by the MasterCard Worldwide Centers of Commerce Index (2008), second only to Sydney (12th) in Australia. Melbourne is Australia's second-largest industrial centre. It is the Australian base for a number of significant manufacturers including Boeing, truck-makers Kenworth and Iveco, Cadbury as well as Bombardier Transportation and Jayco, among many others. It is also home to a wide variety of other manufacturers, ranging from petrochemicals and pharmaceuticals to fashion garments, paper manufacturing and food processing. The south-eastern suburb of Scoresby is home to Nintendo's Australian headquarters. The city also boasts a research and development hub for Ford Australia, as well as a global design studio and technical centre for General Motors and Toyota respectively. Australian rules football and cricket are the most popular sports in Melbourne. It is considered the spiritual home of the two sports in Australia. The first official Test cricket match was played at the Melbourne Cricket Ground in March 1877. The origins of Australian rules football can be traced to matches played next to the MCG in 1858. The Australian Football League is headquartered at Docklands Stadium. Nine of the League's teams are based in the Melbourne metropolitan area: Carlton, Collingwood, Essendon, Hawthorn, Melbourne, North Melbourne, Richmond, St Kilda, and Western Bulldogs. Up to five AFL matches are played each week in Melbourne, attracting an average 40,000 people per game. Additionally, the city annually hosts the AFL Grand Final. As the centre of Australia's ""rust belt"", Melbourne experienced an economic downturn between 1989 to 1992, following the collapse of several local financial institutions. In 1992 the newly elected Kennett government began a campaign to revive the economy with an aggressive development campaign of public works coupled with the promotion of the city as a tourist destination with a focus on major events and sports tourism. During this period the Australian Grand Prix moved to Melbourne from Adelaide. Major projects included the construction of a new facility for the Melbourne Museum, Federation Square, the Melbourne Exhibition and Convention Centre, Crown Casino and the CityLink tollway. Other strategies included the privatisation of some of Melbourne's services, including power and public transport, and a reduction in funding to public services such as health, education and public transport infrastructure. During a visit in 1885 English journalist George Augustus Henry Sala coined the phrase ""Marvellous Melbourne"", which stuck long into the twentieth century and is still used today by Melburnians. Growing building activity culminated in a ""land boom"" which, in 1888, reached a peak of speculative development fuelled by consumer confidence and escalating land value. As a result of the boom, large commercial buildings, coffee palaces, terrace housing and palatial mansions proliferated in the city. The establishment of a hydraulic facility in 1887 allowed for the local manufacture of elevators, resulting in the first construction of high-rise buildings; most notably the APA Building, amongst the world's tallest commercial buildings upon completion in 1889. This period also saw the expansion of a major radial rail-based transport network. With the gold rush largely over by 1860, Melbourne continued to grow on the back of continuing gold mining, as the major port for exporting the agricultural products of Victoria, especially wool, and a developing manufacturing sector protected by high tariffs. An extensive radial railway network centred on Melbourne and spreading out across the suburbs and into the countryside was established from the late 1850s. Further major public buildings were begun in the 1860s and 1870s such as the Supreme Court, Government House, and the Queen Victoria Market. The central city filled up with shops and offices, workshops, and warehouses. Large banks and hotels faced the main streets, with fine townhouses in the east end of Collins Street, contrasting with tiny cottages down laneways within the blocks. The Aboriginal population continued to decline with an estimated 80% total decrease by 1863, due primarily to introduced diseases, particularly smallpox, frontier violence and dispossession from their lands. RMIT University was also ranked among the top 51–100 universities in the world in the subjects of: accounting, Business and Management, communication and media studies, computer science and information systems. The Swinburne University of Technology, based in the inner city Melbourne suburb of Hawthorn is ranked 76–100 in the world for Physics by the Academic Ranking of World Universities making Swinburne the only Australian university outside the Group of Eight to achieve a top 100 rating in a science discipline. Deakin University maintains two major campuses in Melbourne and Geelong, and is the third largest university in Victoria. In recent years, the number of international students at Melbourne's universities has risen rapidly, a result of an increasing number of places being made available to full fee paying students. Education in Melbourne is overseen by the Victorian Department of Education and Early Childhood Development (DEECD), whose role is to 'provide policy and planning advice for the delivery of education'. The Hoddle Grid (dimensions of 1 by 1⁄2 mile (1.61 by 0.80 km)) forms the centre of Melbourne's central business district. The grid's southern edge fronts onto the Yarra River. Office, commercial and public developments in the adjoining districts of Southbank and Docklands have made these redeveloped areas into extensions of the CBD in all but name. The city centre has a reputation for its historic and prominent lanes and arcades (most notably Block Place and Royal Arcade) which contain a variety of shops and cafés and are a byproduct of the city's layout. The decade began with the Melbourne International Exhibition in 1880, held in the large purpose-built Exhibition Building. In 1880 a telephone exchange was established and in the same year the foundations of St Paul's, were laid; in 1881 electric light was installed in the Eastern Market, and in the following year a generating station capable of supplying 2,000 incandescent lamps was in operation. In 1885 the first line of the Melbourne cable tramway system was built, becoming one of the worlds most extensive systems by 1890. Melbourne has a temperate oceanic climate (Köppen climate classification Cfb) and is well known for its changeable weather conditions. This is mainly due to Melbourne's location situated on the boundary of the very hot inland areas and the cool southern ocean. This temperature differential is most pronounced in the spring and summer months and can cause very strong cold fronts to form. These cold fronts can be responsible for all sorts of severe weather from gales to severe thunderstorms and hail, large temperature drops, and heavy rain. The local councils are responsible for providing the functions set out in the Local Government Act 1989 such as urban planning and waste management. Most other government services are provided or regulated by the Victorian state government, which governs from Parliament House in Spring Street. These include services which are associated with local government in other countries and include public transport, main roads, traffic control, policing, education above preschool level, health and planning of major infrastructure projects. The state government retains the right to override certain local government decisions, including urban planning, and Melburnian issues often feature prominently in state election. With the wealth brought on by the gold rush following closely on the heels of the establishment of Victoria as a separate colony and the subsequent need for public buildings, a program of grand civic construction soon began. The 1850s and 1860s saw the commencement of Parliament House, the Treasury Building, the Old Melbourne Gaol, Victoria Barracks, the State Library, University, General Post Office, Customs House, the Melbourne Town Hall, St Patrick's cathedral, though many remained uncompleted for decades, with some still not finished. Batman's Treaty with the Aborigines was annulled by the New South Wales governor (who at the time governed all of eastern mainland Australia), with compensation paid to members of the association. In 1836, Governor Bourke declared the city the administrative capital of the Port Phillip District of New South Wales, and commissioned the first plan for the city, the Hoddle Grid, in 1837. The settlement was named Batmania after Batman. However, later that year the settlement was named ""Melbourne"" after the British Prime Minister, William Lamb, 2nd Viscount Melbourne, whose seat was Melbourne Hall in the market town of Melbourne, Derbyshire. On 13 April 1837 the settlement's general post office officially opened with that name. The layout of the inner suburbs on a largely one-mile grid pattern, cut through by wide radial boulevards, and string of gardens surrounding the central city was largely established in the 1850s and 1860s. These areas were rapidly filled from the mid 1850s by the ubiquitous terrace house, as well as detached houses and some grand mansions in large grounds, while some of the major roads developed as shopping streets. Melbourne quickly became a major finance centre, home to several banks, the Royal Mint, and Australia's first stock exchange in 1861. In 1855 the Melbourne Cricket Club secured possession of its now famous ground, the MCG. Members of the Melbourne Football Club codified Australian football in 1859, and Yarra rowing clubs and ""regattas"" became popular about the same time. In 1861 the Melbourne Cup was first run. In 1864 Melbourne acquired its first public monument—the Burke and Wills statue. Melbourne's air quality is generally good and has improved significantly since the 1980s. Like many urban environments, the city faces significant environmental issues, many of them relating to the city's large urban footprint and urban sprawl and the demand for infrastructure and services. One such issue is water usage, drought and low rainfall. Drought in Victoria, low rainfalls and high temperatures deplete Melbourne water supplies and climate change may have a long-term impact on the water supplies of Melbourne. In response to low water supplies and low rainfall due to drought, the government implemented water restrictions and a range of other options including: water recycling schemes for the city, incentives for household water tanks, greywater systems, water consumption awareness initiatives, and other water saving and reuse initiatives; also, in June 2007, the Bracks Government announced that a $3.1 billion Wonthaggi desalination plant would be built on Victoria's south-east coast, capable of treating 150 billion litres of water per year, as well as a 70 km (43 mi) pipeline from the Goulburn area in Victoria's north to Melbourne and a new water pipeline linking Melbourne and Geelong. Both projects are being conducted under controversial Public-Private Partnerships and a multitude of independent reports have found that neither project is required to supply water to the city and that Sustainable Water Management is the best solution. In the meantime, the drought must be weathered. The city reaches south-east through Dandenong to the growth corridor of Pakenham towards West Gippsland, and southward through the Dandenong Creek valley, the Mornington Peninsula and the city of Frankston taking in the peaks of Olivers Hill, Mount Martha and Arthurs Seat, extending along the shores of Port Phillip as a single conurbation to reach the exclusive suburb of Portsea and Point Nepean. In the west, it extends along the Maribyrnong River and its tributaries north towards Sunbury and the foothills of the Macedon Ranges, and along the flat volcanic plain country towards Melton in the west, Werribee at the foothills of the You Yangs granite ridge south west of the CBD. The Little River, and the township of the same name, marks the border between Melbourne and neighbouring Geelong city. Water storage and supply for Melbourne is managed by Melbourne Water, which is owned by the Victorian Government. The organisation is also responsible for management of sewerage and the major water catchments in the region as well as the Wonthaggi desalination plant and North–South Pipeline. Water is stored in a series of reservoirs located within and outside the Greater Melbourne area. The largest dam, the Thomson River Dam, located in the Victorian Alps, is capable of holding around 60% of Melbourne's water capacity, while smaller dams such as the Upper Yarra Dam, Yan Yean Reservoir, and the Cardinia Reservoir carry secondary supplies. In response to attribution of recent climate change, the City of Melbourne, in 2002, set a target to reduce carbon emissions to net zero by 2020 and Moreland City Council established the Zero Moreland program, however not all metropolitan municipalities have followed, with the City of Glen Eira notably deciding in 2009 not to become carbon neutral. Melbourne has one of the largest urban footprints in the world due to its low density housing, resulting in a vast suburban sprawl, with a high level of car dependence and minimal public transport outside of inner areas. Much of the vegetation within the city are non-native species, most of European origin, and in many cases plays host to invasive species and noxious weeds. Significant introduced urban pests include the common myna, feral pigeon, brown rat, European wasp, common starling and red fox. Many outlying suburbs, particularly towards the Yarra Valley and the hills to the north-east and east, have gone for extended periods without regenerative fires leading to a lack of saplings and undergrowth in urbanised native bushland. The Department of Sustainability and Environment partially addresses this problem by regularly burning off. Several national parks have been designated around the urban area of Melbourne, including the Mornington Peninsula National Park, Port Phillip Heads Marine National Park and Point Nepean National Park in the south east, Organ Pipes National Park to the north and Dandenong Ranges National Park to the east. There are also a number of significant state parks just outside Melbourne. Responsibility for regulating pollution falls under the jurisdiction of the EPA Victoria and several local councils. Air pollution, by world standards, is classified as being good. Summer and autumn are the worst times of year for atmospheric haze in the urban area. Melbourne has four airports. Melbourne Airport, at Tullamarine, is the city's main international and domestic gateway and second busiest in Australia. The airport is home base for passenger airlines Jetstar Airways and Tiger Airways Australia and cargo airlines Australian air Express and Toll Priority; and is a major hub for Qantas and Virgin Australia. Avalon Airport, located between Melbourne and Geelong, is a secondary hub of Jetstar. It is also used as a freight and maintenance facility. Buses and taxis are the only forms of public transport to and from the city's main airports. Air Ambulance facilities are available for domestic and international transportation of patients. Melbourne also has a significant general aviation airport, Moorabbin Airport in the city's south east that also handles a small number of passenger flights. Essendon Airport, which was once the city's main airport also handles passenger flights, general aviation and some cargo flights. Melbourne rates highly in education, entertainment, health care, research and development, tourism and sport, making it the world's most liveable city—for the fifth year in a row in 2015, according to the Economist Intelligence Unit. It is a leading financial centre in the Asia-Pacific region, and ranks among the top 30 cities in the world in the Global Financial Centres Index. Referred to as Australia's ""cultural capital"", it is the birthplace of Australian impressionism, Australian rules football, the Australian film and television industries, and Australian contemporary dance such as the Melbourne Shuffle. It is recognised as a UNESCO City of Literature and a major centre for street art, music and theatre. It is home to many of Australia's largest and oldest cultural institutions such as the Melbourne Cricket Ground, the National Gallery of Victoria, the State Library of Victoria and the UNESCO World Heritage-listed Royal Exhibition Building. Melbourne has the largest tram network in the world which had its origins in the city's 1880s land boom. In 2013–2014, 176.9 million passenger trips were made by tram. Melbourne's is Australia's only tram network to comprise more than a single line and consists of 250 km (155.3 mi) of track, 487 trams, 25 routes, and 1,763 tram stops. Around 80 per cent of Melbourne's tram network shares road space with other vehicles, while the rest of the network is separated or are light rail routes. Melbourne's trams are recognised as iconic cultural assets and a tourist attraction. Heritage trams operate on the free City Circle route, intended for visitors to Melbourne, and heritage restaurant trams travel through the city and surrounding areas during the evening. Melbourne is currently building 50 new E Class trams with some already in service in 2014. The E Class trams are about 30 metres long and are superior to the C2 class tram of similar length. Melbourne's bus network consists of almost 300 routes which mainly service the outer suburbs and fill the gaps in the network between rail and tram services. 127.6 million passenger trips were recorded on Melbourne's buses in 2013–2014, an increase of 10.2 percent on the previous year. In May and June 1835, the area which is now central and northern Melbourne was explored by John Batman, a leading member of the Port Phillip Association in Van Diemen's Land (now known as Tasmania), who claimed to have negotiated a purchase of 600,000 acres (2,400 km2) with eight Wurundjeri elders. Batman selected a site on the northern bank of the Yarra River, declaring that ""this will be the place for a village"". Batman then returned to Launceston in Tasmania. In early August 1835 a different group of settlers, including John Pascoe Fawkner, left Launceston on the ship Enterprize. Fawkner was forced to disembark at Georgetown, Tasmania, because of outstanding debts. The remainder of the party continued and arrived at the mouth of the Yarra River on 15 August 1835. On 30 August 1835 the party disembarked and established a settlement at the site of the current Melbourne Immigration Museum. Batman and his group arrived on 2 September 1835 and the two groups ultimately agreed to share the settlement. Melbourne is often referred to as Australia's garden city, and the state of Victoria was once known as the garden state. There is an abundance of parks and gardens in Melbourne, many close to the CBD with a variety of common and rare plant species amid landscaped vistas, pedestrian pathways and tree-lined avenues. Melbourne's parks are often considered the best public parks in all of Australia's major cities. There are also many parks in the surrounding suburbs of Melbourne, such as in the municipalities of Stonnington, Boroondara and Port Phillip, south east of the central business district. The extensive area covered by urban Melbourne is formally divided into hundreds of suburbs (for addressing and postal purposes), and administered as local government areas 31 of which are located within the metropolitan area. Some of Australia's most prominent and well known schools are based in Melbourne. Of the top twenty high schools in Australia according to the Better Education ranking, six are located in Melbourne. There has also been a rapid increase in the number of International students studying in the city. Furthermore, Melbourne was ranked the world's fourth top university city in 2008 after London, Boston and Tokyo in a poll commissioned by the Royal Melbourne Institute of Technology. Melbourne is the home of seven public universities: the University of Melbourne, Monash University, Royal Melbourne Institute of Technology (RMIT University), Deakin University, La Trobe University, Swinburne University of Technology and Victoria University. To counter the trend towards low-density suburban residential growth, the government began a series of controversial public housing projects in the inner city by the Housing Commission of Victoria, which resulted in demolition of many neighbourhoods and a proliferation of high-rise towers. In later years, with the rapid rise of motor vehicle ownership, the investment in freeway and highway developments greatly accelerated the outward suburban sprawl and declining inner city population. The Bolte government sought to rapidly accelerate the modernisation of Melbourne. Major road projects including the remodelling of St Kilda Junction, the widening of Hoddle Street and then the extensive 1969 Melbourne Transportation Plan changed the face of the city into a car-dominated environment. Melbourne is notable as the host city for the 1956 Summer Olympic Games (the first Olympic Games held in the southern hemisphere and Oceania, with all previous games held in Europe and the United States), along with the 2006 Commonwealth Games. Melbourne is so far the southernmost city to host the games. The city is home to three major annual international sporting events: the Australian Open (one of the four Grand Slam tennis tournaments); the Melbourne Cup (horse racing); and the Australian Grand Prix (Formula One). Also, the Australian Masters golf tournament is held at Melbourne since 1979, having been co-sanctioned by the European Tour from 2006 to 2009. Melbourne was proclaimed the ""World's Ultimate Sports City"", in 2006, 2008 and 2010. The city is home to the National Sports Museum, which until 2003 was located outside the members pavilion at the Melbourne Cricket Ground. It reopened in 2008 in the Olympic Stand. Port Phillip is often warmer than the surrounding oceans and/or the land mass, particularly in spring and autumn; this can set up a ""bay effect"" similar to the ""lake effect"" seen in colder climates where showers are intensified leeward of the bay. Relatively narrow streams of heavy showers can often affect the same places (usually the eastern suburbs) for an extended period, while the rest of Melbourne and surrounds stays dry. Overall, Melbourne is, owing to the rain shadow of the Otway Ranges, nonetheless drier than average for southern Victoria. Within the city and surrounds, however, rainfall varies widely, from around 425 millimetres (17 in) at Little River to 1,250 millimetres (49 in) on the eastern fringe at Gembrook. Melbourne receives 48.6 clear days annually. Dewpoint temperatures in the summer range from 9.5 °C (49.1 °F) to 11.7 °C (53.1 °F). After a trend of declining population density since World War II, the city has seen increased density in the inner and western suburbs, aided in part by Victorian Government planning, such as Postcode 3000 and Melbourne 2030 which have aimed to curtail urban sprawl. According to the Australian Bureau of Statistics as of June 2013, inner city Melbourne had the highest population density with 12,400 people per km2. Surrounding inner city suburbs experienced an increase in population density between 2012 and 2013; Carlton (9,000 people per km2) and Fitzroy (7,900). At the time of Australia's federation on 1 January 1901, Melbourne became the seat of government of the federation. The first federal parliament was convened on 9 May 1901 in the Royal Exhibition Building, subsequently moving to the Victorian Parliament House where it was located until 1927, when it was moved to Canberra. The Governor-General of Australia resided at Government House in Melbourne until 1930 and many major national institutions remained in Melbourne well into the twentieth century. Melbourne universities have campuses all over Australia and some internationally. Swinburne University has campuses in Malaysia, while Monash has a research centre based in Prato, Italy. The University of Melbourne, the second oldest university in Australia, was ranked first among Australian universities in the 2010 THES international rankings. The 2012–2013 Times Higher Education Supplement ranked the University of Melbourne as the 28th (30th by QS ranking) best university in the world. Monash University was ranked as the 99th (60th by QS ranking) best university in the world. Both universities are members of the Group of Eight, a coalition of leading Australian tertiary institutions offering comprehensive and leading education. The Story of the Kelly Gang, the world's first feature film, was shot in Melbourne in 1906. Melbourne filmmakers continued to produce bushranger films until they were banned by Victorian politicians in 1912 for the perceived promotion of crime, thus contributing to the decline of one of the silent film era's most productive industries. A notable film shot and set in Melbourne during Australia's cinematic lull is On the Beach (1959). The 1970s saw the rise of the Australian New Wave and its Ozploitation offshoot, instigated by Melbourne-based productions Stork and Alvin Purple. Picnic at Hanging Rock and Mad Max, both shot in and around Melbourne, achieved worldwide acclaim. 2004 saw the construction of Melbourne's largest film and television studio complex, Docklands Studios Melbourne, which has hosted many domestic productions, as well as international features. Melbourne is also home to the headquarters of Village Roadshow Pictures, Australia's largest film production company. Famous modern day actors from Melbourne include Cate Blanchett, Rachel Griffiths, Guy Pearce, Geoffrey Rush and Eric Bana. Between 1836 and 1842 Victorian Aboriginal groups were largely dispossessed[by whom?] of their land. By January 1844, there were said to be 675 Aborigines resident in squalid camps in Melbourne. The British Colonial Office appointed five Aboriginal Protectors for the Aborigines of Victoria, in 1839, however their work was nullified by a land policy that favoured squatters to take possession of Aboriginal lands. By 1845, fewer than 240 wealthy Europeans held all the pastoral licences then issued in Victoria and became a powerful political and economic force in Victoria for generations to come. Melbourne has the largest Greek-speaking population outside of Europe, a population comparable to some larger Greek cities like Larissa and Volos. Thessaloniki is Melbourne's Greek sister city. The Vietnamese surname Nguyen is the second most common in Melbourne's phone book after Smith. The city also features substantial Indian, Sri Lankan, and Malaysian-born communities, in addition to recent South African and Sudanese influxes. The cultural diversity is reflected in the city's restaurants that serve international cuisines. Melbourne's rich and diverse literary history was recognised in 2008 when it became the second UNESCO City of Literature. The State Library of Victoria is one of Australia's oldest cultural institutions and one of many public and university libraries across the city. Melbourne also has Australia's widest range of bookstores, as well the nation's largest publishing sector. The city is home to significant writers' festivals, most notably the Melbourne Writers Festival. Several major literary prizes are open to local writers including the Melbourne Prize for Literature and the Victorian Premier's Literary Awards. Significant novels set in Melbourne include Fergus Hume's The Mystery of a Hansom Cab, Helen Garner's Monkey Grip and Christos Tsiolkas' The Slap. Notable writers and poets from Melbourne include Thomas Browne, C. J. Dennis, Germaine Greer and Peter Carey. Television shows are produced in Melbourne, most notably Neighbours, Kath & Kim, Winners and Losers, Offspring, Underbelly , House Husbands, Wentworth and Miss Fisher's Murder Mysteries, along with national news-based programs such as The Project, Insiders and ABC News Breakfast. Melbourne is also known as the game show capital of Australia; productions such as Million Dollar Minute, Millionaire Hot Seat and Family Feud are all based in Melbourne. Reality television productions such as Dancing with the Stars, MasterChef, The Block and The Real Housewives of Melbourne are all filmed in and around Melbourne. Founded by free settlers from the British Crown colony of Van Diemen's Land on 30 August 1835, in what was then the colony of New South Wales, it was incorporated as a Crown settlement in 1837. It was named ""Melbourne"" by the Governor of New South Wales, Sir Richard Bourke, in honour of the British Prime Minister of the day, William Lamb, 2nd Viscount Melbourne. It was officially declared a city by Queen Victoria in 1847, after which it became the capital of the newly founded colony of Victoria in 1851. During the Victorian gold rush of the 1850s, it was transformed into one of the world's largest and wealthiest cities. After the federation of Australia in 1901, it served as the nation's interim seat of government until 1927. Like many Australian cities, Melbourne has a high dependency on the automobile for transport, particularly in the outer suburban areas where the largest number of cars are bought, with a total of 3.6 million private vehicles using 22,320 km (13,870 mi) of road, and one of the highest lengths of road per capita in the world. The early 20th century saw an increase in popularity of automobiles, resulting in large-scale suburban expansion. By the mid 1950s there was just under 200 passenger vehicles per 1000 people by 2013 there was 600 passenger vehicles per 1000 people. Today it has an extensive network of freeways and arterial roadways used by private vehicles including freight as well as public transport systems including bus and taxis. Major highways feeding into the city include the Eastern Freeway, Monash Freeway and West Gate Freeway (which spans the large West Gate Bridge), whilst other freeways circumnavigate the city or lead to other major cities, including CityLink (which spans the large Bolte Bridge), Eastlink, the Western Ring Road, Calder Freeway, Tullamarine Freeway (main airport link) and the Hume Freeway which links Melbourne and Sydney. Melbourne's live performance institutions date from the foundation of the city, with the first theatre, the Pavilion, opening in 1841. The city's East End Theatre District includes theatres that similarly date from 1850s to the 1920s, including the Princess Theatre, Regent Theatre, Her Majesty's Theatre, Forum Theatre, Comedy Theatre, and the Athenaeum Theatre. The Melbourne Arts Precinct in Southbank is home to Arts Centre Melbourne, which includes the State Theatre, Hamer Hall, the Playhouse and the Fairfax Studio. The Melbourne Recital Centre and Southbank Theatre (principal home of the MTC, which includes the Sumner and Lawler performance spaces) are also located in Southbank. The Sidney Myer Music Bowl, which dates from 1955, is located in the gardens of Kings Domain; and the Palais Theatre is a feature of the St Kilda Beach foreshore. Residential architecture is not defined by a single architectural style, but rather an eclectic mix of houses, townhouses, condominiums, and apartment buildings in the metropolitan area (particularly in areas of urban sprawl). Free standing dwellings with relatively large gardens are perhaps the most common type of housing outside inner city Melbourne. Victorian terrace housing, townhouses and historic Italianate, Tudor revival and Neo-Georgian mansions are all common in neighbourhoods such as Toorak. The discovery of gold in Victoria in mid 1851 led to the Victorian gold rush, and Melbourne, which served as the major port and provided most services for the region, experienced rapid growth. Within months, the city's population had increased by nearly three-quarters, from 25,000 to 40,000 inhabitants. Thereafter, growth was exponential and by 1865, Melbourne had overtaken Sydney as Australia's most populous city. Additionally, Melbourne along with the Victorian regional cities of Ballarat and Geelong became the wealthiest cities in the world during the Gold Rush era. Melbourne is also prone to isolated convective showers forming when a cold pool crosses the state, especially if there is considerable daytime heating. These showers are often heavy and can contain hail and squalls and significant drops in temperature, but they pass through very quickly at times with a rapid clearing trend to sunny and relatively calm weather and the temperature rising back to what it was before the shower. This often occurs in the space of minutes and can be repeated many times in a day, giving Melbourne a reputation for having ""four seasons in one day"", a phrase that is part of local popular culture and familiar to many visitors to the city. The lowest temperature on record is −2.8 °C (27.0 °F), on 21 July 1869. The highest temperature recorded in Melbourne city was 46.4 °C (115.5 °F), on 7 February 2009. While snow is occasionally seen at higher elevations in the outskirts of the city, it has not been recorded in the Central Business District since 1986. Over two-thirds of Melburnians speak only English at home (68.1%). Chinese (mainly Cantonese and Mandarin) is the second-most-common language spoken at home (3.6%), with Greek third, Italian fourth and Vietnamese fifth, each with more than 100,000 speakers. Although Victoria's net interstate migration has fluctuated, the population of the Melbourne statistical division has grown by about 70,000 people a year since 2005. Melbourne has now attracted the largest proportion of international overseas immigrants (48,000) finding it outpacing Sydney's international migrant intake on percentage, along with having strong interstate migration from Sydney and other capitals due to more affordable housing and cost of living. CSL, one of the world's top five biotech companies, and Sigma Pharmaceuticals have their headquarters in Melbourne. The two are the largest listed Australian pharmaceutical companies. Melbourne has an important ICT industry that employs over 60,000 people (one third of Australia's ICT workforce), with a turnover of $19.8 billion and export revenues of $615 million. In addition, tourism also plays an important role in Melbourne's economy, with about 7.6 million domestic visitors and 1.88 million international visitors in 2004. In 2008, Melbourne overtook Sydney with the amount of money that domestic tourists spent in the city, accounting for around $15.8 billion annually. Melbourne has been attracting an increasing share of domestic and international conference markets. Construction began in February 2006 of a $1 billion 5000-seat international convention centre, Hilton Hotel and commercial precinct adjacent to the Melbourne Exhibition and Convention Centre to link development along the Yarra River with the Southbank precinct and multibillion-dollar Docklands redevelopment."
Greeks,"The Greek shipping tradition recovered during Ottoman rule when a substantial merchant middle class developed, which played an important part in the Greek War of Independence. Today, Greek shipping continues to prosper to the extent that Greece has the largest merchant fleet in the world, while many more ships under Greek ownership fly flags of convenience. The most notable shipping magnate of the 20th century was Aristotle Onassis, others being Yiannis Latsis, George Livanos, and Stavros Niarchos. The evolution of Proto-Greek should be considered within the context of an early Paleo-Balkan sprachbund that makes it difficult to delineate exact boundaries between individual languages. The characteristically Greek representation of word-initial laryngeals by prothetic vowels is shared, for one, by the Armenian language, which also seems to share some other phonological and morphological peculiarities of Greek; this has led some linguists to propose a hypothetical closer relationship between Greek and Armenian, although evidence remains scant. The most obvious link between modern and ancient Greeks is their language, which has a documented tradition from at least the 14th century BC to the present day, albeit with a break during the Greek Dark Ages (lasting from the 11th to the 8th century BC). Scholars compare its continuity of tradition to Chinese alone. Since its inception, Hellenism was primarily a matter of common culture and the national continuity of the Greek world is a lot more certain than its demographic. Yet, Hellenism also embodied an ancestral dimension through aspects of Athenian literature that developed and influenced ideas of descent based on autochthony. During the later years of the Eastern Roman Empire, areas such as Ionia and Constantinople experienced a Hellenic revival in language, philosophy, and literature and on classical models of thought and scholarship. This revival provided a powerful impetus to the sense of cultural affinity with ancient Greece and its classical heritage. The cultural changes undergone by the Greeks are, despite a surviving common sense of ethnicity, undeniable. At the same time, the Greeks have retained their language and alphabet, certain values and cultural traditions, customs, a sense of religious and cultural difference and exclusion, (the word barbarian was used by 12th-century historian Anna Komnene to describe non-Greek speakers), a sense of Greek identity and common sense of ethnicity despite the global political and social changes of the past two millennia. There is a sizeable Greek minority of about 105,000 (disputed, sources claim higher) people, in Albania. The Greek minority of Turkey, which numbered upwards of 200,000 people after the 1923 exchange, has now dwindled to a few thousand, after the 1955 Constantinople Pogrom and other state sponsored violence and discrimination. This effectively ended, though not entirely, the three-thousand-year-old presence of Hellenism in Asia Minor. There are smaller Greek minorities in the rest of the Balkan countries, the Levant and the Black Sea states, remnants of the Old Greek Diaspora (pre-19th century). Following the Fall of Constantinople on 29 May 1453, many Greeks sought better employment and education opportunities by leaving for the West, particularly Italy, Central Europe, Germany and Russia. Greeks are greatly credited for the European cultural revolution, later called, the Renaissance. In Greek-inhabited territory itself, Greeks came to play a leading role in the Ottoman Empire, due in part to the fact that the central hub of the empire, politically, culturally, and socially, was based on Western Thrace and Greek Macedonia, both in Northern Greece, and of course was centred on the mainly Greek-populated, former Byzantine capital, Constantinople. As a direct consequence of this situation, Greek-speakers came to play a hugely important role in the Ottoman trading and diplomatic establishment, as well as in the church. Added to this, in the first half of the Ottoman period men of Greek origin made up a significant proportion of the Ottoman army, navy, and state bureaucracy, having been levied as adolescents (along with especially Albanians and Serbs) into Ottoman service through the devshirme. Many Ottomans of Greek (or Albanian or Serb) origin were therefore to be found within the Ottoman forces which governed the provinces, from Ottoman Egypt, to Ottomans occupied Yemen and Algeria, frequently as provincial governors. Notable modern Greek artists include Renaissance painter Dominikos Theotokopoulos (El Greco), Panagiotis Doxaras, Nikolaos Gyzis, Nikiphoros Lytras, Yannis Tsarouchis, Nikos Engonopoulos, Constantine Andreou, Jannis Kounellis, sculptors such as Leonidas Drosis, Georgios Bonanos, Yannoulis Chalepas and Joannis Avramidis, conductor Dimitri Mitropoulos, soprano Maria Callas, composers such as Mikis Theodorakis, Nikos Skalkottas, Iannis Xenakis, Manos Hatzidakis, Eleni Karaindrou, Yanni and Vangelis, one of the best-selling singers worldwide Nana Mouskouri and poets such as Kostis Palamas, Dionysios Solomos, Angelos Sikelianos and Yannis Ritsos. Alexandrian Constantine P. Cavafy and Nobel laureates Giorgos Seferis and Odysseas Elytis are among the most important poets of the 20th century. Novel is also represented by Alexandros Papadiamantis and Nikos Kazantzakis. Greek colonies and communities have been historically established on the shores of the Mediterranean Sea and Black Sea, but the Greek people have always been centered around the Aegean and Ionian seas, where the Greek language has been spoken since the Bronze Age. Until the early 20th century, Greeks were distributed between the Greek peninsula, the western coast of Asia Minor, the Black Sea coast, Cappadocia in central Anatolia, Egypt, the Balkans, Cyprus, and Constantinople. Many of these regions coincided to a large extent with the borders of the Byzantine Empire of the late 11th century and the Eastern Mediterranean areas of ancient Greek colonization. The cultural centers of the Greeks have included Athens, Thessalonica, Alexandria, Smyrna, and Constantinople at various periods. As of 2007, Greece had the eighth highest percentage of tertiary enrollment in the world (with the percentages for female students being higher than for male) while Greeks of the Diaspora are equally active in the field of education. Hundreds of thousands of Greek students attend western universities every year while the faculty lists of leading Western universities contain a striking number of Greek names. Notable modern Greek scientists of modern times include Dimitrios Galanos, Georgios Papanikolaou (inventor of the Pap test), Nicholas Negroponte, Constantin Carathéodory, Manolis Andronikos, Michael Dertouzos, John Argyris, Panagiotis Kondylis, John Iliopoulos (2007 Dirac Prize for his contributions on the physics of the charm quark, a major contribution to the birth of the Standard Model, the modern theory of Elementary Particles), Joseph Sifakis (2007 Turing Award, the ""Nobel Prize"" of Computer Science), Christos Papadimitriou (2002 Knuth Prize, 2012 Gödel Prize), Mihalis Yannakakis (2005 Knuth Prize) and Dimitri Nanopoulos. The roots of Greek success in the Ottoman Empire can be traced to the Greek tradition of education and commerce. It was the wealth of the extensive merchant class that provided the material basis for the intellectual revival that was the prominent feature of Greek life in the half century and more leading to the outbreak of the Greek War of Independence in 1821. Not coincidentally, on the eve of 1821, the three most important centres of Greek learning were situated in Chios, Smyrna and Aivali, all three major centres of Greek commerce. Greek success was also favoured by Greek domination of the Christian Orthodox church. The history of the Greek people is closely associated with the history of Greece, Cyprus, Constantinople, Asia Minor and the Black Sea. During the Ottoman rule of Greece, a number of Greek enclaves around the Mediterranean were cut off from the core, notably in Southern Italy, the Caucasus, Syria and Egypt. By the early 20th century, over half of the overall Greek-speaking population was settled in Asia Minor (now Turkey), while later that century a huge wave of migration to the United States, Australia, Canada and elsewhere created the modern Greek diaspora. Today, Greeks are the majority ethnic group in the Hellenic Republic, where they constitute 93% of the country's population, and the Republic of Cyprus where they make up 78% of the island's population (excluding Turkish settlers in the occupied part of the country). Greek populations have not traditionally exhibited high rates of growth; nonetheless, the population of Greece has shown regular increase since the country's first census in 1828. A large percentage of the population growth since the state's foundation has resulted from annexation of new territories and the influx of 1.5 million Greek refugees after the 1923 population exchange between Greece and Turkey. About 80% of the population of Greece is urban, with 28% concentrated in the city of Athens The Eastern Roman Empire – today conventionally named the Byzantine Empire, a name not in use during its own time – became increasingly influenced by Greek culture after the 7th century, when Emperor Heraclius (AD 575 - 641) decided to make Greek the empire's official language. Certainly from then on, but likely earlier, the Roman and Greek cultures were virtually fused into a single Greco-Roman world. Although the Latin West recognized the Eastern Empire's claim to the Roman legacy for several centuries, after Pope Leo III crowned Charlemagne, king of the Franks, as the ""Roman Emperor"" on 25 December 800, an act which eventually led to the formation of the Holy Roman Empire, the Latin West started to favour the Franks and began to refer to the Eastern Roman Empire largely as the Empire of the Greeks (Imperium Graecorum). For those that remained under the Ottoman Empire's millet system, religion was the defining characteristic of national groups (milletler), so the exonym ""Greeks"" (Rumlar from the name Rhomaioi) was applied by the Ottomans to all members of the Orthodox Church, regardless of their language or ethnic origin. The Greek speakers were the only ethnic group to actually call themselves Romioi, (as opposed to being so named by others) and, at least those educated, considered their ethnicity (genos) to be Hellenic. There were, however, many Greeks who escaped the second-class status of Christians inherent in the Ottoman millet system, according to which Muslims were explicitly awarded senior status and preferential treatment. These Greeks either emigrated, particularly to their fellow Greek Orthodox protector, the Russian Empire, or simply converted to Islam, often only very superficially and whilst remaining crypto-Christian. The most notable examples of large-scale conversion to Turkish Islam among those today defined as Greek Muslims - excluding those who had to convert as a matter of course on being recruited through the devshirme - were to be found in Crete (Cretan Turks), Greek Macedonia (for example among the Vallahades of western Macedonia), and among Pontic Greeks in the Pontic Alps and Armenian Highlands. Several Ottoman sultans and princes were also of part Greek origin, with mothers who were either Greek concubines or princesses from Byzantine noble families, one famous example being sultan Selim the Grim, whose mother Gülbahar Hatun was a Pontic Greek. The terms used to define Greekness have varied throughout history but were never limited or completely identified with membership to a Greek state. By Western standards, the term Greeks has traditionally referred to any native speakers of the Greek language, whether Mycenaean, Byzantine or modern Greek. Byzantine Greeks called themselves Romioi and considered themselves the political heirs of Rome, but at least by the 12th century a growing number of those educated, deemed themselves the heirs of ancient Greece as well, although for most of the Greek speakers, ""Hellene"" still meant pagan. On the eve of the Fall of Constantinople the Last Emperor urged his soldiers to remember that they were the descendants of Greeks and Romans. Notable Greek seafarers include people such as Pytheas of Marseilles, Scylax of Caryanda who sailed to Iberia and beyond, Nearchus, the 6th century merchant and later monk Cosmas Indicopleustes (Cosmas who sailed to India) and the explorer of the Northwestern passage Juan de Fuca. In later times, the Romioi plied the sea-lanes of the Mediterranean and controlled trade until an embargo imposed by the Roman Emperor on trade with the Caliphate opened the door for the later Italian pre-eminence in trade. During and after the Greek War of Independence, Greeks of the diaspora were important in establishing the fledgling state, raising funds and awareness abroad. Greek merchant families already had contacts in other countries and during the disturbances many set up home around the Mediterranean (notably Marseilles in France, Livorno in Italy, Alexandria in Egypt), Russia (Odessa and Saint Petersburg), and Britain (London and Liverpool) from where they traded, typically in textiles and grain. Businesses frequently comprised the extended family, and with them they brought schools teaching Greek and the Greek Orthodox Church. Modern Greek has, in addition to Standard Modern Greek or Dimotiki, a wide variety of dialects of varying levels of mutual intelligibility, including Cypriot, Pontic, Cappadocian, Griko and Tsakonian (the only surviving representative of ancient Doric Greek). Yevanic is the language of the Romaniotes, and survives in small communities in Greece, New York and Israel. In addition to Greek, many Greeks in Greece and the Diaspora are bilingual in other languages or dialects such as English, Arvanitika/Albanian, Aromanian, Macedonian Slavic, Russian and Turkish. Greek culture has evolved over thousands of years, with its beginning in the Mycenaean civilization, continuing through the Classical period, the Roman and Eastern Roman periods and was profoundly affected by Christianity, which it in turn influenced and shaped. Ottoman Greeks had to endure through several centuries of adversity that culminated in genocide in the 20th century but nevertheless included cultural exchanges and enriched both cultures. The Diafotismos is credited with revitalizing Greek culture and giving birth to the synthesis of ancient and medieval elements that characterize it today. The traditional Greek homelands have been the Greek peninsula and the Aegean Sea, the Southern Italy (Magna Graecia), the Black Sea, the Ionian coasts of Asia Minor and the islands of Cyprus and Sicily. In Plato's Phaidon, Socrates remarks, ""we (Greeks) live around a sea like frogs around a pond"" when describing to his friends the Greek cities of the Aegean. This image is attested by the map of the Old Greek Diaspora, which corresponded to the Greek world until the creation of the Greek state in 1832. The sea and trade were natural outlets for Greeks since the Greek peninsula is rocky and does not offer good prospects for agriculture. Greek art has a long and varied history. Greeks have contributed to the visual, literary and performing arts. In the West, ancient Greek art was influential in shaping the Roman and later the modern western artistic heritage. Following the Renaissance in Europe, the humanist aesthetic and the high technical standards of Greek art inspired generations of European artists. Well into the 19th century, the classical tradition derived from Greece played an important role in the art of the western world. In the East, Alexander the Great's conquests initiated several centuries of exchange between Greek, Central Asian and Indian cultures, resulting in Greco-Buddhist art, whose influence reached as far as Japan. In Homer's Iliad, the names Danaans (or Danaoi: Δαναοί) and Argives (Argives: Αργείοι) are used to designate the Greek forces opposed to the Trojans. The myth of Danaus, whose origin is Egypt, is a foundation legend of Argos. His daughters Danaides, were forced in Tartarus to carry a jug to fill a bathtub without a bottom. This myth is connected with a task that can never be fulfilled (Sisyphos) and the name can be derived from the PIE root *danu: ""river"". There is not any satisfactory theory on their origin. Some scholars connect Danaans with the Denyen, one of the groups of the sea peoples who attacked Egypt during the reign of Ramesses III (1187-1156 BCE). The same inscription mentions the Weshesh who might have been the Achaeans. The Denyen seem to have been inhabitants of the city Adana in Cilicia. Pottery similar to that of Mycenae itself has been found in Tarsus of Cilicia and it seems that some refugees from the Aegean went there after the collapse of the Mycenean civilization. These Cilicians seem to have been called Dananiyim, the same word as Danaoi who attacked Egypt in 1191 BC along with the Quaouash (or Weshesh) who may be Achaeans. They were also called Danuna according to a Hittite inscription and the same name is mentioned in the Amarna letters. Julius Pokorny reconstructs the name from the PIE root da:-: ""flow, river"", da:-nu: ""any moving liquid, drops"", da: navo ""people living by the river, Skyth. nomadic people (in Rigveda water-demons, fem.Da:nu primordial goddess), in Greek Danaoi, Egypt. Danuna"". It is also possible that the name Danaans is pre-Greek. A country Danaja with a city Mukana (propaply: Mycenea) is mentioned in inscriptions from Egypt from Amenophis III (1390-1352 BC), Thutmosis III (1437 BC). The total number of Greeks living outside Greece and Cyprus today is a contentious issue. Where Census figures are available, they show around 3 million Greeks outside Greece and Cyprus. Estimates provided by the SAE - World Council of Hellenes Abroad put the figure at around 7 million worldwide. According to George Prevelakis of Sorbonne University, the number is closer to just below 5 million. Integration, intermarriage, and loss of the Greek language influence the self-identification of the Omogeneia. Important centres of the New Greek Diaspora today are London, New York, Melbourne and Toronto. In 2010, the Hellenic Parliament introduced a law that enables Diaspora Greeks in Greece to vote in the elections of the Greek state. This law was later repealed in early 2014. Around 1200 BC, the Dorians, another Greek-speaking people, followed from Epirus. Traditionally, historians have believed that the Dorian invasion caused the collapse of the Mycenaean civilization, but it is likely the main attack was made by seafaring raiders (sea peoples) who sailed into the eastern Mediterranean around 1180 BC. The Dorian invasion was followed by a poorly attested period of migrations, appropriately called the Greek Dark Ages, but by 800 BC the landscape of Archaic and Classical Greece was discernible. In the Hesiodic Catalogue of Women, Graecus is presented as the son of Zeus and Pandora II, sister of Hellen the patriarch of Hellenes. Hellen was the son of Deucalion who ruled around Phthia in central Greece. The Parian Chronicle mentions that when Deucalion became king of Phthia, the previously called Graikoi were named Hellenes. Aristotle notes that the Hellenes were related with Grai/Greeks (Meteorologica I.xiv) a native name of a Dorian tribe in Epirus which was used by the Illyrians. He also claims that the great deluge must have occurred in the region around Dodona, where the Selloi dwelt. However, according to the Greek tradition it is more possible that the homeland of the Greeks was originally in central Greece. A modern theory derives the name Greek (Latin Graeci) from Graikos, ""inhabitant of Graia/Graea,"" a town on the coast of Boeotia. Greek colonists from Graia helped to found Cumae (900 BC) in Italy, where they were called Graeces. When the Romans encountered them they used this name for the colonists and then for all Greeks (Graeci.) The word γραῖα graia ""old woman"" comes from the PIE root *ǵerh2-/*ǵreh2-, ""to grow old"" via Proto-Greek *gera-/grau-iu; the same root later gave γέρας geras (/keras/), ""gift of honour"" in Mycenean Greek. The Germanic languages borrowed the word Greeks with an initial ""k"" sound which probably was their initial sound closest to the Latin ""g"" at the time (Goth. Kreks). The area out of ancient Attica including Boeotia was called Graïke and is connected with the older deluge of Ogyges, the mythological ruler of Boeotia. The region was originally occupied by the Minyans who were autochthonous or Proto-Greek speaking people. In ancient Greek the name Ogygios came to mean ""from earliest days"". The classical period of Greek civilization covers a time spanning from the early 5th century BC to the death of Alexander the Great, in 323 BC (some authors prefer to split this period into 'Classical', from the end of the Persian wars to the end of the Peloponnesian War, and 'Fourth Century', up to the death of Alexander). It is so named because it set the standards by which Greek civilization would be judged in later eras. The Classical period is also described as the ""Golden Age"" of Greek civilization, and its art, philosophy, architecture and literature would be instrumental in the formation and development of Western culture. Greek demonstrates several linguistic features that are shared with other Balkan languages, such as Albanian, Bulgarian and Eastern Romance languages (see Balkan sprachbund), and has absorbed many foreign words, primarily of Western European and Turkish origin. Because of the movements of Philhellenism and the Diafotismos in the 19th century, which emphasized the modern Greeks' ancient heritage, these foreign influences were excluded from official use via the creation of Katharevousa, a somewhat artificial form of Greek purged of all foreign influence and words, as the official language of the Greek state. In 1976, however, the Hellenic Parliament voted to make the spoken Dimotiki the official language, making Katharevousa obsolete. Greek surnames were widely in use by the 9th century supplanting the ancient tradition of using the father’s name, however Greek surnames are most commonly patronymics. Commonly, Greek male surnames end in -s, which is the common ending for Greek masculine proper nouns in the nominative case. Exceptionally, some end in -ou, indicating the genitive case of this proper noun for patronymic reasons. Although surnames in mainland Greece are static today, dynamic and changing patronymic usage survives in middle names where the genitive of father's first name is commonly the middle name (this usage having been passed on to the Russians). In Cyprus, by contrast, surnames follow the ancient tradition of being given according to the father’s name. Finally, in addition to Greek-derived surnames many have Latin, Turkish and Italian origin. In ancient times, the trading and colonizing activities of the Greek tribes and city states spread the Greek culture, religion and language around the Mediterranean and Black Sea basins, especially in Sicily and southern Italy (also known as Magna Grecia), Spain, the south of France and the Black sea coasts. Under Alexander the Great's empire and successor states, Greek and Hellenizing ruling classes were established in the Middle East, India and in Egypt. The Hellenistic period is characterized by a new wave of Greek colonization that established Greek cities and kingdoms in Asia and Africa. Under the Roman Empire, easier movement of people spread Greeks across the Empire and in the eastern territories, Greek became the lingua franca rather than Latin. The modern-day Griko community of southern Italy, numbering about 60,000, may represent a living remnant of the ancient Greek populations of Italy. Most Greeks are Christians, belonging to the Greek Orthodox Church. During the first centuries after Jesus Christ, the New Testament was originally written in Koine Greek, which remains the liturgical language of the Greek Orthodox Church, and most of the early Christians and Church Fathers were Greek-speaking. There are small groups of ethnic Greeks adhering to other Christian denominations like Greek Catholics, Greek Evangelicals, Pentecostals, and groups adhering to other religions including Romaniot and Sephardic Jews and Greek Muslims. About 2,000 Greeks are members of Hellenic Polytheistic Reconstructionism congregations. Another study from 2012 included 150 dental school students from University of Athens, the result showed that light hair colour (blonde/light ash brown) was predominant in 10.7% of the students. 36% had medium hair colour (Light brown/Medium darkest brown). 32% had darkest brown and 21% black (15.3 off black, 6% midnight black). In conclusion the hair colour of young Greeks are mostly brown, ranging from light to dark brown. with significant minorities having black and blonde hair. The same study also showed that the eye colour of the students was 14.6% blue/green, 28% medium (light brown) and 57.4% dark brown. The relationship between ethnic Greek identity and Greek Orthodox religion continued after the creation of the Modern Greek state in 1830. According to the second article of the first Greek constitution of 1822, a Greek was defined as any Christian resident of the Kingdom of Greece, a clause removed by 1840. A century later, when the Treaty of Lausanne was signed between Greece and Turkey in 1923, the two countries agreed to use religion as the determinant for ethnic identity for the purposes of population exchange, although most of the Greeks displaced (over a million of the total 1.5 million) had already been driven out by the time the agreement was signed.[note 1] The Greek genocide, in particular the harsh removal of Pontian Greeks from the southern shore area of the Black Sea, contemporaneous with and following the failed Greek Asia Minor Campaign, was part of this process of Turkification of the Ottoman Empire and the placement of its economy and trade, then largely in Greek hands under ethnic Turkish control. The ethnogenesis of the Greek nation is linked to the development of Pan-Hellenism in the 8th century BC. According to some scholars, the foundational event was the Olympic Games in 776 BC, when the idea of a common Hellenism among the Greek tribes was first translated into a shared cultural experience and Hellenism was primarily a matter of common culture. The works of Homer (i.e. Iliad and Odyssey) and Hesiod (i.e. Theogony) were written in the 8th century BC, becoming the basis of the national religion, ethos, history and mythology. The Oracle of Apollo at Delphi was established in this period. The Greeks of the Classical era made several notable contributions to science and helped lay the foundations of several western scientific traditions, like philosophy, historiography and mathematics. The scholarly tradition of the Greek academies was maintained during Roman times with several academic institutions in Constantinople, Antioch, Alexandria and other centres of Greek learning while Eastern Roman science was essentially a continuation of classical science. Greeks have a long tradition of valuing and investing in paideia (education). Paideia was one of the highest societal values in the Greek and Hellenistic world while the first European institution described as a university was founded in 5th century Constantinople and operated in various incarnations until the city's fall to the Ottomans in 1453. The University of Constantinople was Christian Europe's first secular institution of higher learning since no theological subjects were taught, and considering the original meaning of the world university as a corporation of students, the world’s first university as well. In the religious sphere, this was a period of profound change. The spiritual revolution that took place, saw a waning of the old Greek religion, whose decline beginning in the 3rd century BC continued with the introduction of new religious movements from the East. The cults of deities like Isis and Mithra were introduced into the Greek world. Greek-speaking communities of the Hellenized East were instrumental in the spread of early Christianity in the 2nd and 3rd centuries, and Christianity's early leaders and writers (notably St Paul) were generally Greek-speaking, though none were from Greece. However, Greece itself had a tendency to cling to paganism and was not one of the influential centers of early Christianity: in fact, some ancient Greek religious practices remained in vogue until the end of the 4th century, with some areas such as the southeastern Peloponnese remaining pagan until well into the 10th century AD. Of the new eastern religions introduced into the Greek world, the most successful was Christianity. From the early centuries of the Common Era, the Greeks identified as Romaioi (""Romans""), by that time the name ‘Hellenes’ denoted pagans. While ethnic distinctions still existed in the Roman Empire, they became secondary to religious considerations and the renewed empire used Christianity as a tool to support its cohesion and promoted a robust Roman national identity. Concurrently the secular, urban civilization of late antiquity survived in the Eastern Mediterranean along with Greco-Roman educational system, although it was from Christianity that the culture's essential values were drawn. This age saw the Greeks move towards larger cities and a reduction in the importance of the city-state. These larger cities were parts of the still larger Kingdoms of the Diadochi. Greeks, however, remained aware of their past, chiefly through the study of the works of Homer and the classical authors. An important factor in maintaining Greek identity was contact with barbarian (non-Greek) peoples, which was deepened in the new cosmopolitan environment of the multi-ethnic Hellenistic kingdoms. This led to a strong desire among Greeks to organize the transmission of the Hellenic paideia to the next generation. Greek science, technology and mathematics are generally considered to have reached their peak during the Hellenistic period. Before the establishment of the Modern Greek state, the link between ancient and modern Greeks was emphasized by the scholars of Greek Enlightenment especially by Rigas Feraios. In his ""Political Constitution"", he addresses to the nation as ""the people descendant of the Greeks"". The modern Greek state was created in 1829, when the Greeks liberated a part of their historic homelands, Peloponnese, from the Ottoman Empire. The large Greek diaspora and merchant class were instrumental in transmitting the ideas of western romantic nationalism and philhellenism, which together with the conception of Hellenism, formulated during the last centuries of the Byzantine Empire, formed the basis of the Diafotismos and the current conception of Hellenism. Homer uses the terms Achaeans and Danaans (Δαναοί) as a generic term for Greeks in Iliad, and they were probably a part of the Mycenean civilization. The names Achaioi and Danaoi seem to be pre-Dorian belonging to the people who were overthrown. They were forced to the region that later bore the name Achaea after the Dorian invasion. In the 5th century BC, they were redefined as contemporary speakers of Aeolic Greek which was spoken mainly in Thessaly, Boeotia and Lesbos. There are many controversial theories on the origin of the Achaeans. According to one view, the Achaeans were one of the fair-headed tribes of upper Europe, who pressed down over the Alps during the early Iron age (1300 BC) to southern Europe. Another theory suggests that the Peloponnesian Dorians were the Achaeans. These theories are rejected by other scholars who, based on linguistic criteria, suggest that the Achaeans were mainland pre-Dorian Greeks. There is also the theory that there was an Achaean ethnos that migrated from Asia minor to lower Thessaly prior to 2000 BC. Some Hittite texts mention a nation lying to the west called Ahhiyava or Ahhiya. Egyptian documents refer to Ekwesh, one of the groups of sea peoples who attached Egypt during the reign of Merneptah (1213-1203 BCE), who may have been Achaeans. The most widely used symbol is the flag of Greece, which features nine equal horizontal stripes of blue alternating with white representing the nine syllables of the Greek national motto Eleftheria i thanatos (freedom or death), which was the motto of the Greek War of Independence. The blue square in the upper hoist-side corner bears a white cross, which represents Greek Orthodoxy. The Greek flag is widely used by the Greek Cypriots, although Cyprus has officially adopted a neutral flag to ease ethnic tensions with the Turkish Cypriot minority – see flag of Cyprus). Greeks from Cyprus have a similar history of emigration, usually to the English-speaking world because of the island's colonization by the British Empire. Waves of emigration followed the Turkish invasion of Cyprus in 1974, while the population decreased between mid-1974 and 1977 as a result of emigration, war losses, and a temporary decline in fertility. After the ethnic cleansing of a third of the Greek population of the island in 1974, there was also an increase in the number of Greek Cypriots leaving, especially for the Middle East, which contributed to a decrease in population that tapered off in the 1990s. Today more than two-thirds of the Greek population in Cyprus is urban. Homer refers to the ""Hellenes"" (/ˈhɛliːnz/) as a relatively small tribe settled in Thessalic Phthia, with its warriors under the command of Achilleus. The Parian Chronicle says that Phthia was the homeland of the Hellenes and that this name was given to those previously called Greeks (Γραικοί). In Greek mythology, Hellen, the patriarch of Hellenes, was son of Pyrrha and Deucalion, who ruled around Phthia, the only survivors after the great deluge. It seems that the myth was invented when the Greek tribes started to separate from each other in certain areas of Greece and it indicates their common origin. Aristotle names ancient Hellas as an area in Epirus between Dodona and the Achelous river, the location of the great deluge of Deucalion, a land occupied by the Selloi and the ""Greeks"" who later came to be known as ""Hellenes"". Selloi were the priests of Dodonian Zeus and the word probably means ""sacrificers"" (compare Gothic saljan, ""present, sacrifice""). There is currently no satisfactory etymology of the name Hellenes. Some scholars assert that the name Selloi changed to Sellanes and then to Hellanes-Hellenes. However this etymology connects the name Hellenes with the Dorians who occupied Epirus and the relation with the name Greeks given by the Romans becomes uncertain. The name Hellenes seems to be older and it was probably used by the Greeks with the establishment of the Great Amphictyonic League. This was an ancient association of Greek tribes with twelve founders which was organized to protect the great temples of Apollo in Delphi (Phocis) and of Demeter near Thermopylae (Locris). According to the legend it was founded after the Trojan War by the eponymous Amphictyon, brother of Hellen. The Greeks of classical antiquity idealized their Mycenaean ancestors and the Mycenaean period as a glorious era of heroes, closeness of the gods and material wealth. The Homeric Epics (i.e. Iliad and Odyssey) were especially and generally accepted as part of the Greek past and it was not until the 19th century that scholars began to question Homer's historicity. As part of the Mycenaean heritage that survived, the names of the gods and goddesses of Mycenaean Greece (e.g. Zeus, Poseidon and Hades) became major figures of the Olympian Pantheon of later antiquity. In any case, Alexander's toppling of the Achaemenid Empire, after his victories at the battles of the Granicus, Issus and Gaugamela, and his advance as far as modern-day Pakistan and Tajikistan, provided an important outlet for Greek culture, via the creation of colonies and trade routes along the way. While the Alexandrian empire did not survive its creator's death intact, the cultural implications of the spread of Hellenism across much of the Middle East and Asia were to prove long lived as Greek became the lingua franca, a position it retained even in Roman times. Many Greeks settled in Hellenistic cities like Alexandria, Antioch and Seleucia. Two thousand years later, there are still communities in Pakistan and Afghanistan, like the Kalash, who claim to be descended from Greek settlers. A distinct Greek political identity re-emerged in the 11th century in educated circles and became more forceful after the fall of Constantinople to the Crusaders of the Fourth Crusade in 1204, so that when the empire was revived in 1261, it became in many ways a Greek national state. That new notion of nationhood engendered a deep interest in the classical past culminating in the ideas of the Neoplatonist philosopher Gemistus Pletho, who abandoned Christianity. However, it was the combination of Orthodox Christianity with a specifically Greek identity that shaped the Greeks' notion of themselves in the empire's twilight years. The interest in the Classical Greek heritage was complemented by a renewed emphasis on Greek Orthodox identity, which was reinforced in the late Medieval and Ottoman Greeks' links with their fellow Orthodox Christians in the Russian Empire. These were further strengthened following the fall of the Empire of Trebizond in 1461, after which and until the second Russo-Turkish War of 1828-29 hundreds of thousands of Pontic Greeks fled or migrated from the Pontic Alps and Armenian Highlands to southern Russia and the Russian South Caucasus (see also Greeks in Russia, Greeks in Armenia, Greeks in Georgia, and Caucasian Greeks)."
London,"Outward urban expansion is now prevented by the Metropolitan Green Belt, although the built-up area extends beyond the boundary in places, resulting in a separately defined Greater London Urban Area. Beyond this is the vast London commuter belt. Greater London is split for some purposes into Inner London and Outer London. The city is split by the River Thames into North and South, with an informal central London area in its interior. The coordinates of the nominal centre of London, traditionally considered to be the original Eleanor Cross at Charing Cross near the junction of Trafalgar Square and Whitehall, are approximately 51°30′26″N 00°07′39″W﻿ / ﻿51.50722°N 0.12750°W﻿ / 51.50722; -0.12750. The 2011 census showed that 36.7 per cent of Greater London's population were born outside the UK. The table to the right shows the 30 most common countries of birth of London residents in 2011, the date of the last published UK Census. A portion of the German-born population are likely to be British nationals born to parents serving in the British Armed Forces in Germany. Estimates produced by the Office for National Statistics indicate that the five largest foreign-born groups living in London in the period July 2009 to June 2010 were those born in India, Poland, the Republic of Ireland, Bangladesh and Nigeria. The London Natural History Society suggest that London is ""one of the World's Greenest Cities"" with more than 40 percent green space or open water. They indicate that 2000 species of flowering plant have been found growing there and that the tidal Thames supports 120 species of fish. They also state that over 60 species of bird nest in central London and that their members have recorded 47 species of butterfly, 1173 moths and more than 270 kinds of spider around London. London's wetland areas support nationally important populations of many water birds. London has 38 Sites of Special Scientific Interest (SSSIs), two National Nature Reserves and 76 Local Nature Reserves. Some international railway services to Continental Europe were operated during the 20th century as boat trains, such as the Admiraal de Ruijter to Amsterdam and the Night Ferry to Paris and Brussels. The opening of the Channel Tunnel in 1994 connected London directly to the continental rail network, allowing Eurostar services to begin. Since 2007, high-speed trains link St. Pancras International with Lille, Paris, Brussels and European tourist destinations via the High Speed 1 rail link and the Channel Tunnel. The first high-speed domestic trains started in June 2009 linking Kent to London. There are plans for a second high speed line linking London to the Midlands, North West England, and Yorkshire. In 2003, a congestion charge was introduced to reduce traffic volumes in the city centre. With a few exceptions, motorists are required to pay £10 per day to drive within a defined zone encompassing much of central London. Motorists who are residents of the defined zone can buy a greatly reduced season pass. London government initially expected the Congestion Charge Zone to increase daily peak period Underground and bus users by 20,000 people, reduce road traffic by 10 to 15 per cent, increase traffic speeds by 10 to 15 per cent, and reduce queues by 20 to 30 per cent. Over the course of several years, the average number of cars entering the centre of London on a weekday was reduced from 195,000 to 125,000 cars – a 35-per-cent reduction of vehicles driven per day. London has a diverse range of peoples and cultures, and more than 300 languages are spoken within Greater London. The Office for National Statistics estimated its mid-2014 population to be 8,538,689, the largest of any municipality in the European Union, and accounting for 12.5 percent of the UK population. London's urban area is the second most populous in the EU, after Paris, with 9,787,426 inhabitants according to the 2011 census. The city's metropolitan area is one of the most populous in Europe with 13,879,757 inhabitants,[note 4] while the Greater London Authority states the population of the city-region (covering a large part of the south east) as 22.7 million. London was the world's most populous city from around 1831 to 1925. London i/ˈlʌndən/ is the capital and most populous city of England and the United Kingdom. Standing on the River Thames in the south eastern part of the island of Great Britain, London has been a major settlement for two millennia. It was founded by the Romans, who named it Londinium. London's ancient core, the City of London, largely retains its 1.12-square-mile (2.9 km2) medieval boundaries and in 2011 had a resident population of 7,375, making it the smallest city in England. Since at least the 19th century, the term London has also referred to the metropolis developed around this core. The bulk of this conurbation forms Greater London,[note 1] a region of England governed by the Mayor of London and the London Assembly.[note 2] The conurbation also covers two English counties: the small district of the City of London and the county of Greater London. The latter constitutes the vast majority of London, though historically it was split between Middlesex (a now abolished county), Essex, Surrey, Kent and Hertfordshire. The largest parks in the central area of London are three of the eight Royal Parks, namely Hyde Park and its neighbour Kensington Gardens in the west, and Regent's Park to the north. Hyde Park in particular is popular for sports and sometimes hosts open-air concerts. Regent's Park contains London Zoo, the world's oldest scientific zoo, and is near the tourist attraction of Madame Tussauds Wax Museum. Primrose Hill in the northern part of Regent's Park at 256 feet (78 m) is a popular spot to view the city skyline. Primarily starting in the mid-1960s, London became a centre for the worldwide youth culture, exemplified by the Swinging London subculture associated with the King's Road, Chelsea and Carnaby Street. The role of trendsetter was revived during the punk era. In 1965 London's political boundaries were expanded to take into account the growth of the urban area and a new Greater London Council was created. During The Troubles in Northern Ireland, London was subjected to bombing attacks by the Provisional IRA. Racial inequality was highlighted by the 1981 Brixton riot. London was instrumental in the development of punk music, with figures such as the Sex Pistols, The Clash, and Vivienne Westwood all based in the city. More recent artists to emerge from the London music scene include George Michael, Kate Bush, Seal, Siouxsie and the Banshees, Bush, the Spice Girls, Jamiroquai, Blur, The Prodigy, Gorillaz, Mumford & Sons, Coldplay, Amy Winehouse, Adele, Ed Sheeran and One Direction. London is also a centre for urban music. In particular the genres UK garage, drum and bass, dubstep and grime evolved in the city from the foreign genres of hip hop and reggae, alongside local drum and bass. Black music station BBC Radio 1Xtra was set up to support the rise of home-grown urban music both in London and in the rest of the UK. Within London, both the City of London and the City of Westminster have city status and both the City of London and the remainder of Greater London are counties for the purposes of lieutenancies. The area of Greater London has incorporated areas that were once part of the historic counties of Middlesex, Kent, Surrey, Essex and Hertfordshire. London's status as the capital of England, and later the United Kingdom, has never been granted or confirmed officially—by statute or in written form.[note 6] The administration of London is formed of two tiers—a city-wide, strategic tier and a local tier. City-wide administration is coordinated by the Greater London Authority (GLA), while local administration is carried out by 33 smaller authorities. The GLA consists of two elected components; the Mayor of London, who has executive powers, and the London Assembly, which scrutinises the mayor's decisions and can accept or reject the mayor's budget proposals each year. The headquarters of the GLA is City Hall, Southwark; the mayor is Boris Johnson. The mayor's statutory planning strategy is published as the London Plan, which was most recently revised in 2011. The local authorities are the councils of the 32 London boroughs and the City of London Corporation. They are responsible for most local services, such as local planning, schools, social services, local roads and refuse collection. Certain functions, such as waste management, are provided through joint arrangements. In 2009–2010 the combined revenue expenditure by London councils and the GLA amounted to just over £22 billion (£14.7 billion for the boroughs and £7.4 billion for the GLA). London is a leading global city, with strengths in the arts, commerce, education, entertainment, fashion, finance, healthcare, media, professional services, research and development, tourism, and transport all contributing to its prominence. It is one of the world's leading financial centres and has the fifth-or sixth-largest metropolitan area GDP in the world depending on measurement.[note 3] London is a world cultural capital. It is the world's most-visited city as measured by international arrivals and has the world's largest city airport system measured by passenger traffic. London is one of the world's leading investment destinations, hosting more international retailers and ultra high-net-worth individuals than any other city. London's 43 universities form the largest concentration of higher education institutes in Europe, and a 2014 report placed it first in the world university rankings. According to the report London also ranks first in the world in software, multimedia development and design, and shares first position in technology readiness. In 2012, London became the first city to host the modern Summer Olympic Games three times. The Monument in the City of London provides views of the surrounding area while commemorating the Great Fire of London, which originated nearby. Marble Arch and Wellington Arch, at the north and south ends of Park Lane respectively, have royal connections, as do the Albert Memorial and Royal Albert Hall in Kensington. Nelson's Column is a nationally recognised monument in Trafalgar Square, one of the focal points of central London. Older buildings are mainly brick built, most commonly the yellow London stock brick or a warm orange-red variety, often decorated with carvings and white plaster mouldings. Following his victory in the Battle of Hastings, William, Duke of Normandy, was crowned King of England in the newly finished Westminster Abbey on Christmas Day 1066. William constructed the Tower of London, the first of the many Norman castles in England to be rebuilt in stone, in the southeastern corner of the city, to intimidate the native inhabitants. In 1097, William II began the building of Westminster Hall, close by the abbey of the same name. The hall became the basis of a new Palace of Westminster. The majority of British Jews live in London, with significant Jewish communities in Stamford Hill, Stanmore, Golders Green, Finchley, Hampstead, Hendon and Edgware in North London. Bevis Marks Synagogue in the City of London is affiliated to London's historic Sephardic Jewish community. It is the only synagogue in Europe which has held regular services continuously for over 300 years. Stanmore and Canons Park Synagogue has the largest membership of any single Orthodox synagogue in the whole of Europe, overtaking Ilford synagogue (also in London) in 1998. The community set up the London Jewish Forum in 2006 in response to the growing significance of devolved London Government. Transport is one of the four main areas of policy administered by the Mayor of London, however the mayor's financial control does not extend to the longer distance rail network that enters London. In 2007 he assumed responsibility for some local lines, which now form the London Overground network, adding to the existing responsibility for the London Underground, trams and buses. The public transport network is administered by Transport for London (TfL) and is one of the most extensive in the world. There is a variety of annual events, beginning with the relatively new New Year's Day Parade, fireworks display at the London Eye, the world's second largest street party, the Notting Hill Carnival is held during the late August Bank Holiday each year. Traditional parades include November's Lord Mayor's Show, a centuries-old event celebrating the annual appointment of a new Lord Mayor of the City of London with a procession along the streets of the City, and June's Trooping the Colour, a formal military pageant performed by regiments of the Commonwealth and British armies to celebrate the Queen's Official Birthday. Islington's 1 mile (1.6 km) long Upper Street, extending northwards from Angel, has more bars and restaurants than any other street in the United Kingdom. Europe's busiest shopping area is Oxford Street, a shopping street nearly 1 mile (1.6 km) long, making it the longest shopping street in the United Kingdom. Oxford Street is home to vast numbers of retailers and department stores, including the world-famous Selfridges flagship store. Knightsbridge, home to the equally renowned Harrods department store, lies to the south-west. London is a major international air transport hub with the busiest city airspace in the world. Eight airports use the word London in their name, but most traffic passes through six of these. London Heathrow Airport, in Hillingdon, West London, is the busiest airport in the world for international traffic, and is the major hub of the nation's flag carrier, British Airways. In March 2008 its fifth terminal was opened. There were plans for a third runway and a sixth terminal; however, these were cancelled by the Coalition Government on 12 May 2010. The pilgrims in Geoffrey Chaucer's late 14th-century Canterbury Tales set out for Canterbury from London – specifically, from the Tabard inn, Southwark. William Shakespeare spent a large part of his life living and working in London; his contemporary Ben Jonson was also based there, and some of his work—most notably his play The Alchemist—was set in the city. A Journal of the Plague Year (1722) by Daniel Defoe is a fictionalisation of the events of the 1665 Great Plague. Later important depictions of London from the 19th and early 20th centuries are Dickens' novels, and Arthur Conan Doyle's Sherlock Holmes stories. Modern writers pervasively influenced by the city include Peter Ackroyd, author of a ""biography"" of London, and Iain Sinclair, who writes in the genre of psychogeography. Close to Richmond Park is Kew Gardens which has the world's largest collection of living plants. In 2003, the gardens were put on the UNESCO list of World Heritage Sites. There are also numerous parks administered by London's borough Councils, including Victoria Park in the East End and Battersea Park in the centre. Some more informal, semi-natural open spaces also exist, including the 320-hectare (790-acre) Hampstead Heath of North London, and Epping Forest, which covers 2,476 hectares (6,118.32 acres) in the east. Both are controlled by the City of London Corporation. Hampstead Heath incorporates Kenwood House, the former stately home and a popular location in the summer months where classical musical concerts are held by the lake, attracting thousands of people every weekend to enjoy the music, scenery and fireworks. Epping Forest is a popular venue for various outdoor activities, including mountain biking, walking, horse riding, golf, angling, and orienteering. Among other inhabitants of London are 10,000 foxes, so that there are now 16 foxes for every square mile (2.6 square kilometres) of London. These urban foxes are noticeably bolder than their country cousins, sharing the pavement with pedestrians and raising cubs in people's backyards. Foxes have even sneaked into the Houses of Parliament, where one was found asleep on a filing cabinet. Another broke into the grounds of Buckingham Palace, reportedly killing some of Queen Elizabeth II's prized pink flamingos. Generally, however, foxes and city folk appear to get along. A survey in 2001 by the London-based Mammal Society found that 80 percent of 3,779 respondents who volunteered to keep a diary of garden mammal visits liked having them around. This sample cannot be taken to represent Londoners as a whole. London is home to five major medical schools – Barts and The London School of Medicine and Dentistry (part of Queen Mary), King's College London School of Medicine (the largest medical school in Europe), Imperial College School of Medicine, UCL Medical School and St George's, University of London – and has a large number of affiliated teaching hospitals. It is also a major centre for biomedical research, and three of the UK's five academic health science centres are based in the city – Imperial College Healthcare, King's Health Partners and UCL Partners (the largest such centre in Europe). The Vikings established Danelaw over much of the eastern and northern part of England with its boundary roughly stretching from London to Chester. It was an area of political and geographical control imposed by the Viking incursions which was formally agreed to by the Danish warlord, Guthrum and west-Saxon king, Alfred the Great in 886 AD. The Anglo-Saxon Chronicle recorded that London was ""refounded"" by Alfred the Great in 886. Archaeological research shows that this involved abandonment of Lundenwic and a revival of life and trade within the old Roman walls. London then grew slowly until about 950, after which activity increased dramatically. The 2011 census recorded that 2,998,264 people or 36.7% of London's population are foreign-born making London the city with the second largest immigrant population, behind New York City, in terms of absolute numbers. The table to the right shows the most common countries of birth of London residents. Note that some of the German-born population, in 18th position, are British citizens from birth born to parents serving in the British Armed Forces in Germany. With increasing industrialisation, London's population grew rapidly throughout the 19th and early 20th centuries, and it was for some time in the late 19th and early 20th centuries the most populous city in the world. Its population peaked at 8,615,245 in 1939 immediately before the outbreak of the Second World War, but had declined to 7,192,091 at the 2001 Census. However, the population then grew by just over a million between the 2001 and 2011 Censuses, to reach 8,173,941 in the latter enumeration. In the latter half of the 19th century the locale of South Kensington was developed as ""Albertopolis"", a cultural and scientific quarter. Three major national museums are there: the Victoria and Albert Museum (for the applied arts), the Natural History Museum and the Science Museum. The National Portrait Gallery was founded in 1856 to house depictions of figures from British history; its holdings now comprise the world's most extensive collection of portraits. The national gallery of British art is at Tate Britain, originally established as an annexe of the National Gallery in 1897. The Tate Gallery, as it was formerly known, also became a major centre for modern art; in 2000 this collection moved to Tate Modern, a new gallery housed in the former Bankside Power Station. There are a number of business schools in London, including the London School of Business and Finance, Cass Business School (part of City University London), Hult International Business School, ESCP Europe, European Business School London, Imperial College Business School and the London Business School. London is also home to many specialist arts education institutions, including the Academy of Live and Recorded Arts, Central School of Ballet, LAMDA, London College of Contemporary Arts (LCCA), London Contemporary Dance School, National Centre for Circus Arts, RADA, Rambert School of Ballet and Contemporary Dance, the Royal College of Art, the Royal College of Music and Trinity Laban. London's bus network is one of the largest in the world, running 24 hours a day, with about 8,500 buses, more than 700 bus routes and around 19,500 bus stops. In 2013, the network had more than 2 billion commuter trips per annum, more than the Underground. Around £850 million is taken in revenue each year. London has the largest wheelchair accessible network in the world and, from the 3rd quarter of 2007, became more accessible to hearing and visually impaired passengers as audio-visual announcements were introduced. The distinctive red double-decker buses are an internationally recognised trademark of London transport along with black cabs and the Tube. During the English Civil War the majority of Londoners supported the Parliamentary cause. After an initial advance by the Royalists in 1642 culminating in the battles of Brentford and Turnham Green, London was surrounded by defensive perimeter wall known as the Lines of Communication. The lines were built by an up to 20,000 people, and were completed in under two months. The fortifications failed their only test when the New Model Army entered London in 1647, and they were levelled by Parliament the same year. Summers are generally warm and sometimes hot. London's average July high is 24 °C (75.2 °F). On average London will see 31 days above 25 °C (77.0 °F) each year, and 4.2 days above 30.0 °C (86.0 °F) every year. During the 2003 European heat wave there were 14 consecutive days above 30 °C (86.0 °F) and 2 consecutive days where temperatures reached 38 °C (100.4 °F), leading to hundreds of heat related deaths. Winters are generally cool and damp with little temperature variation. Snowfall does occur from time to time, and can cause travel disruption when this happens. Spring and autumn are mixed seasons and can be pleasant. As a large city, London has a considerable urban heat island effect, making the centre of London at times 5 °C (9 °F) warmer than the suburbs and outskirts. The effect of this can be seen below when comparing London Heathrow, 15 miles west of London, with the London Weather Centre, in the city centre. London is also home to sizeable Muslim, Hindu, Sikh, and Jewish communities. Notable mosques include the East London Mosque in Tower Hamlets, London Central Mosque on the edge of Regent's Park and the Baitul Futuh Mosque of the Ahmadiyya Muslim Community. Following the oil boom, increasing numbers of wealthy Hindus and Middle-Eastern Muslims have based themselves around Mayfair and Knightsbridge in West London. There are large Muslim communities in the eastern boroughs of Tower Hamlets and Newham. Large Hindu communities are in the north-western boroughs of Harrow and Brent, the latter of which is home to Europe's largest Hindu temple, Neasden Temple. London is also home to 42 Hindu temples. There are Sikh communities in East and West London, particularly in Southall, home to one of the largest Sikh populations and the largest Sikh temple outside India. Three Aviva Premiership rugby union teams are based in London, (London Irish, Saracens, and Harlequins), although currently only Harlequins and Saracens play their home games within Greater London. London Scottish and London Welsh play in the RFU Championship club and other rugby union clubs in the city include Richmond F.C., Rosslyn Park F.C., Westcombe Park R.F.C. and Blackheath F.C.. Twickenham Stadium in south-west London is the national rugby union stadium, and has a capacity of 82,000 now that the new south stand has been completed. Other mammals found in Greater London are hedgehogs, rats, mice, rabbit, shrew, vole, and squirrels, In wilder areas of Outer London, such as Epping Forest, a wide variety of mammals are found including hare, badger, field, bank and water vole, wood mouse, yellow-necked mouse, mole, shrew, and weasel, in addition to fox, squirrel and hedgehog. A dead otter was found at The Highway, in Wapping, about a mile from the Tower Bridge, which would suggest that they have begun to move back after being absent a hundred years from the city. Ten of England's eighteen species of bats have been recorded in Epping Forest: soprano, nathusius and common pipistrelles, noctule, serotine, barbastelle, daubenton's, brown Long-eared, natterer's and leisler's. London has been the setting for many works of literature. The literary centres of London have traditionally been hilly Hampstead and (since the early 20th century) Bloomsbury. Writers closely associated with the city are the diarist Samuel Pepys, noted for his eyewitness account of the Great Fire, Charles Dickens, whose representation of a foggy, snowy, grimy London of street sweepers and pickpockets has been a major influence on people's vision of early Victorian London, and Virginia Woolf, regarded as one of the foremost modernist literary figures of the 20th century. London is the world's most expensive office market for the last three years according to world property journal (2015) report. As of 2015[update] the residential property in London is worth $2.2 trillion - same value as that of Brazil annual GDP. The city has the highest property prices of any European city according to the Office for National Statistics and the European Office of Statistics. On average the price per square metre in central London is €24,252 (April 2014). This is higher than the property prices in other G8 European capital cities; Berlin €3,306, Rome €6,188 and Paris €11,229. London contains four World Heritage Sites: the Tower of London; Kew Gardens; the site comprising the Palace of Westminster, Westminster Abbey, and St Margaret's Church; and the historic settlement of Greenwich (in which the Royal Observatory, Greenwich marks the Prime Meridian, 0° longitude, and GMT). Other famous landmarks include Buckingham Palace, the London Eye, Piccadilly Circus, St Paul's Cathedral, Tower Bridge, Trafalgar Square, and The Shard. London is home to numerous museums, galleries, libraries, sporting events and other cultural institutions, including the British Museum, National Gallery, Tate Modern, British Library and 40 West End theatres. The London Underground is the oldest underground railway network in the world. Along with professional services, media companies are concentrated in London and the media distribution industry is London's second most competitive sector. The BBC is a significant employer, while other broadcasters also have headquarters around the City. Many national newspapers are edited in London. London is a major retail centre and in 2010 had the highest non-food retail sales of any city in the world, with a total spend of around £64.2 billion. The Port of London is the second-largest in the United Kingdom, handling 45 million tonnes of cargo each year. London is home to many museums, galleries, and other institutions, many of which are free of admission charges and are major tourist attractions as well as playing a research role. The first of these to be established was the British Museum in Bloomsbury, in 1753. Originally containing antiquities, natural history specimens and the national library, the museum now has 7 million artefacts from around the globe. In 1824 the National Gallery was founded to house the British national collection of Western paintings; this now occupies a prominent position in Trafalgar Square. In 1762, George III acquired Buckingham House and it was enlarged over the next 75 years. During the 18th century, London was dogged by crime, and the Bow Street Runners were established in 1750 as a professional police force. In total, more than 200 offences were punishable by death, including petty theft. Most children born in the city died before reaching their third birthday. The coffeehouse became a popular place to debate ideas, with growing literacy and the development of the printing press making news widely available; and Fleet Street became the centre of the British press. London is one of the major classical and popular music capitals of the world and is home to major music corporations, such as EMI and Warner Music Group as well as countless bands, musicians and industry professionals. The city is also home to many orchestras and concert halls, such as the Barbican Arts Centre (principal base of the London Symphony Orchestra and the London Symphony Chorus), Cadogan Hall (Royal Philharmonic Orchestra) and the Royal Albert Hall (The Proms). London's two main opera houses are the Royal Opera House and the London Coliseum. The UK's largest pipe organ is at the Royal Albert Hall. Other significant instruments are at the cathedrals and major churches. Several conservatoires are within the city: Royal Academy of Music, Royal College of Music, Guildhall School of Music and Drama and Trinity Laban. London was the world's largest city from about 1831 to 1925. London's overcrowded conditions led to cholera epidemics, claiming 14,000 lives in 1848, and 6,000 in 1866. Rising traffic congestion led to the creation of the world's first local urban rail network. The Metropolitan Board of Works oversaw infrastructure expansion in the capital and some of the surrounding counties; it was abolished in 1889 when the London County Council was created out of those areas of the counties surrounding the capital. London was bombed by the Germans during the First World War while during the Second World War, the Blitz and other bombings by the German Luftwaffe, killed over 30,000 Londoners and destroyed large tracts of housing and other buildings across the city. Immediately after the war, the 1948 Summer Olympics were held at the original Wembley Stadium, at a time when London had barely recovered from the war. London has numerous venues for rock and pop concerts, including the world's busiest arena the o2 arena and other large arenas such as Earls Court, Wembley Arena, as well as many mid-sized venues, such as Brixton Academy, the Hammersmith Apollo and the Shepherd's Bush Empire. Several music festivals, including the Wireless Festival, South West Four, Lovebox, and Hyde Park's British Summer Time are all held in London. The city is home to the first and original Hard Rock Cafe and the Abbey Road Studios where The Beatles recorded many of their hits. In the 1960s, 1970s and 1980s, musicians and groups like Elton John, Pink Floyd, David Bowie, Queen, The Kinks, The Rolling Stones, The Who, Eric Clapton, Led Zeppelin, The Small Faces, Iron Maiden, Fleetwood Mac, Elvis Costello, Cat Stevens, The Police, The Cure, Madness, The Jam, Dusty Springfield, Phil Collins, Rod Stewart and Sade, derived their sound from the streets and rhythms vibrating through London. A number of world-leading education institutions are based in London. In the 2014/15 QS World University Rankings, Imperial College London is ranked joint 2nd in the world (alongside The University of Cambridge), University College London (UCL) is ranked 5th, and King's College London (KCL) is ranked 16th. The London School of Economics has been described as the world's leading social science institution for both teaching and research. The London Business School is considered one of the world's leading business schools and in 2015 its MBA programme was ranked second best in the world by the Financial Times. With the collapse of Roman rule in the early 5th century, London ceased to be a capital and the walled city of Londinium was effectively abandoned, although Roman civilisation continued in the St Martin-in-the-Fields area until around 450. From around 500, an Anglo-Saxon settlement known as Lundenwic developed in the same area, slightly to the west of the old Roman city. By about 680, it had revived sufficiently to become a major port, although there is little evidence of large-scale production of goods. From the 820s the town declined because of repeated Viking invasions. There are three recorded Viking assaults on London; two of which were successful in 851 and 886 AD, although they were defeated during the attack of 994 AD. The majority of primary and secondary schools and further-education colleges in London are controlled by the London boroughs or otherwise state-funded; leading examples include City and Islington College, Ealing, Hammersmith and West London College, Leyton Sixth Form College, Tower Hamlets College and Bethnal Green Academy. There are also a number of private schools and colleges in London, some old and famous, such as City of London School, Harrow, St Paul's School, Haberdashers' Aske's Boys' School, University College School, The John Lyon School, Highgate School and Westminster School. Within the City of Westminster in London the entertainment district of the West End has its focus around Leicester Square, where London and world film premieres are held, and Piccadilly Circus, with its giant electronic advertisements. London's theatre district is here, as are many cinemas, bars, clubs and restaurants, including the city's Chinatown district (in Soho), and just to the east is Covent Garden, an area housing speciality shops. The city is the home of Andrew Lloyd Webber, whose musicals have dominated the West End theatre since the late 20th century. The United Kingdom's Royal Ballet, English National Ballet, Royal Opera and English National Opera are based in London and perform at the Royal Opera House, the London Coliseum, Sadler's Wells Theatre and the Royal Albert Hall as well as touring the country. There are 366 railway stations in the London Travelcard Zones on an extensive above-ground suburban railway network. South London, particularly, has a high concentration of railways as it has fewer Underground lines. Most rail lines terminate around the centre of London, running into eighteen terminal stations, with the exception of the Thameslink trains connecting Bedford in the north and Brighton in the south via Luton and Gatwick airports. London has Britain's busiest station by number of passengers – Waterloo, with over 184 million people using the interchange station complex (which includes Waterloo East station) each year. Clapham Junction is the busiest station in Europe by the number of trains passing. A number of universities in London are outside the University of London system, including Brunel University, City University London, Imperial College London, Kingston University, London Metropolitan University, Middlesex University, University of East London, University of West London and University of Westminster, (with over 34,000 students, the largest unitary university in London), London South Bank University, Middlesex University, University of the Arts London (the largest university of art, design, fashion, communication and the performing arts in Europe), University of East London, the University of West London and the University of Westminster. In addition there are three international universities in London – Regent's University London, Richmond, The American International University in London and Schiller International University. Stansted Airport, north east of London in Essex, is a local UK hub and Luton Airport to the north of London in Bedfordshire, caters mostly for cheap short-haul flights. London City Airport, the smallest and most central airport, in Newham, East London, is focused on business travellers, with a mixture of full service short-haul scheduled flights and considerable business jet traffic. London Southend Airport, east of London in Essex, is a smaller, regional airport that mainly caters for cheap short-haul flights. During the 12th century, the institutions of central government, which had hitherto accompanied the royal English court as it moved around the country, grew in size and sophistication and became increasingly fixed in one place. In most cases this was Westminster, although the royal treasury, having been moved from Winchester, came to rest in the Tower. While the City of Westminster developed into a true capital in governmental terms, its distinct neighbour, the City of London, remained England's largest city and principal commercial centre, and it flourished under its own unique administration, the Corporation of London. In 1100, its population was around 18,000; by 1300 it had grown to nearly 100,000. Although there is evidence of scattered Brythonic settlements in the area, the first major settlement was founded by the Romans after the invasion of 43 AD. This lasted only until around 61, when the Iceni tribe led by Queen Boudica stormed it, burning it to the ground. The next, heavily planned, incarnation of Londinium prospered, and it superseded Colchester as the capital of the Roman province of Britannia in 100. At its height in the 2nd century, Roman London had a population of around 60,000. Greater London encompasses a total area of 1,583 square kilometres (611 sq mi), an area which had a population of 7,172,036 in 2001 and a population density of 4,542 inhabitants per square kilometre (11,760/sq mi). The extended area known as the London Metropolitan Region or the London Metropolitan Agglomeration, comprises a total area of 8,382 square kilometres (3,236 sq mi) has a population of 13,709,000 and a population density of 1,510 inhabitants per square kilometre (3,900/sq mi). Modern London stands on the Thames, its primary geographical feature, a navigable river which crosses the city from the south-west to the east. The Thames Valley is a floodplain surrounded by gently rolling hills including Parliament Hill, Addington Hills, and Primrose Hill. The Thames was once a much broader, shallower river with extensive marshlands; at high tide, its shores reached five times their present width. With 120,000 students in London, the federal University of London is the largest contact teaching university in the UK. It includes four large multi-faculty universities – King's College London, Queen Mary, Royal Holloway and UCL – and a number of smaller and more specialised institutions including Birkbeck, the Courtauld Institute of Art, Goldsmiths, Guildhall School of Music and Drama, the Institute of Education, the London Business School, the London School of Economics, the London School of Hygiene & Tropical Medicine, the Royal Academy of Music, the Central School of Speech and Drama, the Royal Veterinary College and the School of Oriental and African Studies. Members of the University of London have their own admissions procedures, and some award their own degrees. London is one of the leading tourist destinations in the world and in 2015 was ranked as the most visited city in the world with over 65 million visits. It is also the top city in the world by visitor cross-border spending, estimated at US$20.23 billion in 2015 Tourism is one of London's prime industries, employing the equivalent of 350,000 full-time workers in 2003, and the city accounts for 54% of all inbound visitor spend in UK. As of 2016 London is rated as the world top ranked city destination by TripAdvisor users. From 1898, it was commonly accepted that the name was of Celtic origin and meant place belonging to a man called *Londinos; this explanation has since been rejected. Richard Coates put forward an explanation in 1998 that it is derived from the pre-Celtic Old European *(p)lowonida, meaning 'river too wide to ford', and suggested that this was a name given to the part of the River Thames which flows through London; from this, the settlement gained the Celtic form of its name, *Lowonidonjon; this requires quite a serious amendment however. The ultimate difficulty lies in reconciling the Latin form Londinium with the modern Welsh Llundain, which should demand a form *(h)lōndinion (as opposed to *londīnion), from earlier *loundiniom. The possibility cannot be ruled out that the Welsh name was borrowed back in from English at a later date, and thus cannot be used as a basis from which to reconstruct the original name. Although the majority of journeys involving central London are made by public transport, car travel is common in the suburbs. The inner ring road (around the city centre), the North and South Circular roads (in the suburbs), and the outer orbital motorway (the M25, outside the built-up area) encircle the city and are intersected by a number of busy radial routes—but very few motorways penetrate into inner London. A plan for a comprehensive network of motorways throughout the city (the Ringways Plan) was prepared in the 1960s but was mostly cancelled in the early 1970s. The M25 is the longest ring-road motorway in the world at 121.5 mi (195.5 km) long. The A1 and M1 connect London to Leeds, and Newcastle and Edinburgh. London is the seat of the Government of the United Kingdom. Many government departments are based close to the Palace of Westminster, particularly along Whitehall, including the Prime Minister's residence at 10 Downing Street. The British Parliament is often referred to as the ""Mother of Parliaments"" (although this sobriquet was first applied to England itself by John Bright) because it has been the model for most other parliamentary systems. There are 73 Members of Parliament (MPs) from London, who correspond to local parliamentary constituencies in the national Parliament. As of May 2015, 45 are from the Labour Party, 27 are Conservatives, and one is a Liberal Democrat. Policing in Greater London, with the exception of the City of London, is provided by the Metropolitan Police Service, overseen by the Mayor through the Mayor's Office for Policing and Crime (MOPAC). The City of London has its own police force – the City of London Police. The British Transport Police are responsible for police services on National Rail, London Underground, Docklands Light Railway and Tramlink services. A fourth police force in London, the Ministry of Defence Police, do not generally become involved with policing the general public. There are many accents that are traditionally thought of as London accents. The most well known of the London accents long ago acquired the Cockney label, which is heard both in London itself, and across the wider South East England region more generally. The accent of a 21st-century 'Londoner' varies widely; what is becoming more and more common amongst the under-30s however is some fusion of Cockney with a whole array of 'ethnic' accents, in particular Caribbean, which form an accent labelled Multicultural London English (MLE). The other widely heard and spoken accent is RP (Received Pronunciation) in various forms, which can often be heard in the media and many of other traditional professions and beyond, although this accent is not limited to London and South East England, and can also be heard selectively throughout the whole UK amongst certain social groupings. The region covers an area of 1,579 square kilometres (610 sq mi). The population density is 5,177 inhabitants per square kilometre (13,410/sq mi), more than ten times that of any other British region. In terms of population, London is the 19th largest city and the 18th largest metropolitan region in the world. As of 2014[update], London has the largest number of billionaires (British Pound Sterling) in the world, with 72 residing in the city. London ranks as one of the most expensive cities in the world, alongside Tokyo and Moscow. In the dense areas, most of the concentration is via medium- and high-rise buildings. London's skyscrapers such as 30 St Mary Axe, Tower 42, the Broadgate Tower and One Canada Square are mostly in the two financial districts, the City of London and Canary Wharf. High-rise development is restricted at certain sites if it would obstruct protected views of St Paul's Cathedral and other historic buildings. Nevertheless, there are a number of very tall skyscrapers in central London (see Tall buildings in London), including the 95-storey Shard London Bridge, the tallest building in the European Union. Walking is a popular recreational activity in London. Areas that provide for walks include Wimbledon Common, Epping Forest, Hampton Court Park, Hampstead Heath, the eight Royal Parks, canals and disused railway tracks. Access to canals and rivers has improved recently, including the creation of the Thames Path, some 28 miles (45 km) of which is within Greater London, and The Wandle Trail; this runs 12 miles (19 km) through South London along the River Wandle, a tributary of the River Thames. Other long distance paths, linking green spaces, have also been created, including the Capital Ring, the Green Chain Walk, London Outer Orbital Path (""Loop""), Jubilee Walkway, Lea Valley Walk, and the Diana, Princess of Wales Memorial Walk. London's first and only cable car, known as the Emirates Air Line, opened in June 2012. Crossing the River Thames, linking Greenwich Peninsula and the Royal Docks in the east of the city, the cable car is integrated with London's Oyster Card ticketing system, although special fares are charged. Costing £60 million to build, it carries over 3,500 passengers every day, although this is very much lower than its capacity. Similar to the Santander Cycles bike hire scheme, the cable car is sponsored in a 10-year deal by the airline Emirates. London is a major global centre of higher education teaching and research and its 43 universities form the largest concentration of higher education institutes in Europe. According to the QS World University Rankings 2015/16, London has the greatest concentration of top class universities in the world and the international student population around 110,000 which is also more than any other city in the world. A 2014 PricewaterhouseCoopers report termed London as the global capital of higher education London's most popular sport is football and it has fourteen League football clubs, including five in the Premier League: Arsenal, Chelsea, Crystal Palace, Tottenham Hotspur, and West Ham United. Among other professional teams based in London include Fulham, Queens Park Rangers, Millwall and Charlton Athletic. In May 2012, Chelsea became the first London club to win the UEFA Champions League. Aside from Arsenal, Chelsea and Tottenham, none of the other London clubs have ever won the national league title. During the Tudor period the Reformation produced a gradual shift to Protestantism, much of London passing from church to private ownership. The traffic in woollen cloths shipped undyed and undressed from London to the nearby shores of the Low Countries, where it was considered indispensable. But the tentacles of English maritime enterprise hardly extended beyond the seas of north-west Europe. The commercial route to Italy and the Mediterranean Sea normally lay through Antwerp and over the Alps; any ships passing through the Strait of Gibraltar to or from England were likely to be Italian or Ragusan. Upon the re-opening of the Netherlands to English shipping in January 1565, there ensued a strong outburst of commercial activity. The Royal Exchange was founded. Mercantilism grew, and monopoly trading companies such as the East India Company were established, with trade expanding to the New World. London became the principal North Sea port, with migrants arriving from England and abroad. The population rose from an estimated 50,000 in 1530 to about 225,000 in 1605. Two recent discoveries indicate probable very early settlements near the Thames in the London area. In 1999, the remains of a Bronze Age bridge were found on the foreshore north of Vauxhall Bridge. This bridge either crossed the Thames, or went to a now lost island in the river. Dendrology dated the timbers to 1500 BC. In 2010 the foundations of a large timber structure, dated to 4500 BC, were found on the Thames foreshore, south of Vauxhall Bridge. The function of the mesolithic structure is not known. Both structures are on South Bank, at a natural crossing point where the River Effra flows into the River Thames. London's buildings are too diverse to be characterised by any particular architectural style, partly because of their varying ages. Many grand houses and public buildings, such as the National Gallery, are constructed from Portland stone. Some areas of the city, particularly those just west of the centre, are characterised by white stucco or whitewashed buildings. Few structures in central London pre-date the Great Fire of 1666, these being a few trace Roman remains, the Tower of London and a few scattered Tudor survivors in the City. Further out is, for example, the Tudor period Hampton Court Palace, England's oldest surviving Tudor palace, built by Cardinal Thomas Wolsey c.1515. Across London, Black and Asian children outnumber White British children by about six to four in state schools. Altogether at the 2011 census, of London's 1,624,768 population aged 0 to 15, 46.4 per cent were White, 19.8 per cent were Asian, 19 per cent were Black, 10.8 per cent were Mixed and 4 per cent represented another ethnic group. In January 2005, a survey of London's ethnic and religious diversity claimed that there were more than 300 languages spoken in London and more than 50 non-indigenous communities with a population of more than 10,000. Figures from the Office for National Statistics show that, in 2010[update], London's foreign-born population was 2,650,000 (33 per cent), up from 1,630,000 in 1997. Greater London's population declined steadily in the decades after the Second World War, from an estimated peak of 8.6 million in 1939 to around 6.8 million in the 1980s. The principal ports for London moved downstream to Felixstowe and Tilbury, with the London Docklands area becoming a focus for regeneration, including the Canary Wharf development. This was borne out of London's ever-increasing role as a major international financial centre during the 1980s. The Thames Barrier was completed in the 1980s to protect London against tidal surges from the North Sea. The Greater London Council was abolished in 1986, which left London as the only large metropolis in the world without a central administration. In 2000, London-wide government was restored, with the creation of the Greater London Authority. To celebrate the start of the 21st century, the Millennium Dome, London Eye and Millennium Bridge were constructed. On 6 July 2005 London was awarded the 2012 Summer Olympics, making London the first city to stage the Olympic Games three times. In January 2015, Greater London's population was estimated to be 8.63 million, the highest level since 1939. London's largest industry is finance, and its financial exports make it a large contributor to the UK's balance of payments. Around 325,000 people were employed in financial services in London until mid-2007. London has over 480 overseas banks, more than any other city in the world. Over 85 percent (3.2 million) of the employed population of greater London works in the services industries. Because of its prominent global role, London's economy had been affected by the Late-2000s financial crisis. However, by 2010 the City has recovered; put in place new regulatory powers, proceeded to regain lost ground and re-established London's economic dominance. The City of London is home to the Bank of England, London Stock Exchange, and Lloyd's of London insurance market. Herds of red and fallow deer also roam freely within much of Richmond and Bushy Park. A cull takes place each November and February to ensure numbers can be sustained. Epping Forest is also known for its fallow deer, which can frequently be seen in herds to the north of the Forest. A rare population of melanistic, black fallow deer is also maintained at the Deer Sanctuary near Theydon Bois. Muntjac deer, which escaped from deer parks at the turn of the twentieth century, are also found in the forest. While Londoners are accustomed to wildlife such as birds and foxes sharing the city, more recently urban deer have started becoming a regular feature, and whole herds of fallow and white-tailed deer come into residential areas at night to take advantage of the London's green spaces. The London Fire Brigade is the statutory fire and rescue service for Greater London. It is run by the London Fire and Emergency Planning Authority and is the third largest fire service in the world. National Health Service ambulance services are provided by the London Ambulance Service (LAS) NHS Trust, the largest free-at-the-point-of-use emergency ambulance service in the world. The London Air Ambulance charity operates in conjunction with the LAS where required. Her Majesty's Coastguard and the Royal National Lifeboat Institution operate on the River Thames, which is under the jurisdiction of the Port of London Authority from Teddington Lock to the sea. London has played a significant role in the film industry, and has major studios at Ealing and a special effects and post-production community centred in Soho. Working Title Films has its headquarters in London. London has been the setting for films including Oliver Twist (1948), Scrooge (1951), Peter Pan (1953), The 101 Dalmatians (1961), My Fair Lady (1964), Mary Poppins (1964), Blowup (1966), The Long Good Friday (1980), Notting Hill (1999), Love Actually (2003), V For Vendetta (2005), Sweeney Todd: The Demon Barber Of Fleet Street (2008) and The King's Speech (2010). Notable actors and filmmakers from London include; Charlie Chaplin, Alfred Hitchcock, Michael Caine, Helen Mirren, Gary Oldman, Christopher Nolan, Jude Law, Tom Hardy, Keira Knightley and Daniel Day-Lewis. As of 2008[update], the British Academy Film Awards have taken place at the Royal Opera House. London is a major centre for television production, with studios including BBC Television Centre, The Fountain Studios and The London Studios. Many television programmes have been set in London, including the popular television soap opera EastEnders, broadcast by the BBC since 1985. By the 11th century, London was beyond all comparison the largest town in England. Westminster Abbey, rebuilt in the Romanesque style by King Edward the Confessor, was one of the grandest churches in Europe. Winchester had previously been the capital of Anglo-Saxon England, but from this time on, London became the main forum for foreign traders and the base for defence in time of war. In the view of Frank Stenton: ""It had the resources, and it was rapidly developing the dignity and the political self-consciousness appropriate to a national capital."""
Age_of_Enlightenment,"Coffeehouses represent a turning point in history during which people discovered that they could have enjoyable social lives within their communities. Coffeeshops became homes away from home for many who sought, for the first time, to engage in discourse with their neighbors and discuss intriguing and thought-provoking matters, especially those regarding philosophy to politics. Coffeehouses were essential to the Enlightenment, for they were centers of free-thinking and self-discovery. Although many coffeehouse patrons were scholars, a great deal were not. Coffeehouses attracted a diverse set of people, including not only the educated wealthy but also members of the bourgeoisie and the lower class. While it may seem positive that patrons, being doctors, lawyers, merchants, etc. represented almost all classes, the coffeeshop environment sparked fear in those who sought to preserve class distinction. One of the most popular critiques of the coffeehouse claimed that it ""allowed promiscuous association among people from different rungs of the social ladder, from the artisan to the aristocrat"" and was therefore compared to Noah's Ark, receiving all types of animals, clean or unclean. This unique culture served as a catalyst for journalism when Joseph Addison and Richard Steele recognized its potential as an audience. Together, Steele and Addison published The Spectator (1711), a daily publication which aimed, through fictional narrator Mr. Spectator, both to entertain and to provoke discussion regarding serious philosophical matters. The predominant educational psychology from the 1750s onward, especially in northern European countries was associationism, the notion that the mind associates or dissociates ideas through repeated routines. In addition to being conducive to Enlightenment ideologies of liberty, self-determination and personal responsibility, it offered a practical theory of the mind that allowed teachers to transform longstanding forms of print and manuscript culture into effective graphic tools of learning for the lower and middle orders of society. Children were taught to memorize facts through oral and graphic methods that originated during the Renaissance. The term ""Enlightenment"" emerged in English in the later part of the 19th century, with particular reference to French philosophy, as the equivalent of the French term 'Lumières' (used first by Dubos in 1733 and already well established by 1751). From Immanuel Kant's 1784 essay ""Beantwortung der Frage: Was ist Aufklärung?"" (""Answering the Question: What is Enlightenment?"") the German term became 'Aufklärung' (aufklären = to illuminate; sich aufklären = to clear up). However, scholars have never agreed on a definition of the Enlightenment, or on its chronological or geographical extent. Terms like ""les Lumières"" (French), ""illuminismo"" (Italian), ""ilustración"" (Spanish) and ""Aufklärung"" (German) referred to partly overlapping movements. Not until the late nineteenth century did English scholars agree they were talking about ""the Enlightenment."" However, the prime example of reference works that systematized scientific knowledge in the age of Enlightenment were universal encyclopedias rather than technical dictionaries. It was the goal of universal encyclopedias to record all human knowledge in a comprehensive reference work. The most well-known of these works is Denis Diderot and Jean le Rond d'Alembert's Encyclopédie, ou dictionnaire raisonné des sciences, des arts et des métiers. The work, which began publication in 1751, was composed of thirty-five volumes and over 71 000 separate entries. A great number of the entries were dedicated to describing the sciences and crafts in detail, and provided intellectuals across Europe with a high-quality survey of human knowledge. In d'Alembert's Preliminary Discourse to the Encyclopedia of Diderot, the work's goal to record the extent of human knowledge in the arts and sciences is outlined: The increased consumption of reading materials of all sorts was one of the key features of the ""social"" Enlightenment. Developments in the Industrial Revolution allowed consumer goods to be produced in greater quantities at lower prices, encouraging the spread of books, pamphlets, newspapers and journals – ""media of the transmission of ideas and attitudes"". Commercial development likewise increased the demand for information, along with rising populations and increased urbanisation. However, demand for reading material extended outside of the realm of the commercial, and outside the realm of the upper and middle classes, as evidenced by the Bibliothèque Bleue. Literacy rates are difficult to gauge, but in France at least, the rates doubled over the course of the 18th century. Reflecting the decreasing influence of religion, the number of books about science and art published in Paris doubled from 1720 to 1780, while the number of books about religion dropped to just one-tenth of the total. As musicians depended more and more on public support, public concerts became increasingly popular and helped supplement performers' and composers' incomes. The concerts also helped them to reach a wider audience. Handel, for example, epitomized this with his highly public musical activities in London. He gained considerable fame there with performances of his operas and oratorios. The music of Haydn and Mozart, with their Viennese Classical styles, are usually regarded as being the most in line with the Enlightenment ideals. There is little consensus on the precise beginning of the Age of Enlightenment; the beginning of the 18th century (1701) or the middle of the 17th century (1650) are often used as epochs. French historians usually place the period, called the Siècle des Lumières (Century of Enlightenments), between 1715 and 1789, from the beginning of the reign of Louis XV until the French Revolution. If taken back to the mid-17th century, the Enlightenment would trace its origins to Descartes' Discourse on Method, published in 1637. In France, many cited the publication of Isaac Newton's Principia Mathematica in 1687. It is argued by several historians and philosophers that the beginning of the Enlightenment is when Descartes shifted the epistemological basis from external authority to internal certainty by his cogito ergo sum published in 1637. As to its end, most scholars use the last years of the century, often choosing the French Revolution of 1789 or the beginning of the Napoleonic Wars (1804–15) as a convenient point in time with which to date the end of the Enlightenment. Several Americans, especially Benjamin Franklin and Thomas Jefferson, played a major role in bringing Enlightenment ideas to the New World and in influencing British and French thinkers. Franklin was influential for his political activism and for his advances in physics. The cultural exchange during the Age of Enlightenment ran in both directions across the Atlantic. Thinkers such as Paine, Locke, and Rousseau all take Native American cultural practices as examples of natural freedom. The Americans closely followed English and Scottish political ideas, as well as some French thinkers such as Montesquieu. As deists, they were influenced by ideas of John Toland (1670–1722) and Matthew Tindal (1656–1733). During the Enlightenment there was a great emphasis upon liberty, democracy, republicanism and religious tolerance. Attempts to reconcile science and religion resulted in a widespread rejection of prophecy, miracle and revealed religion in preference for Deism – especially by Thomas Paine in The Age of Reason and by Thomas Jefferson in his short Jefferson Bible – from which all supernatural aspects were removed. Hume and other Scottish Enlightenment thinkers developed a 'science of man', which was expressed historically in works by authors including James Burnett, Adam Ferguson, John Millar, and William Robertson, all of whom merged a scientific study of how humans behaved in ancient and primitive cultures with a strong awareness of the determining forces of modernity. Modern sociology largely originated from this movement, and Hume's philosophical concepts that directly influenced James Madison (and thus the U.S. Constitution) and as popularised by Dugald Stewart, would be the basis of classical liberalism. The Enlightenment – known in French as the Siècle des Lumières, the Century of Enlightenment, and in German as the Aufklärung – was a philosophical movement which dominated the world of ideas in Europe in the 18th century. The Enlightenment included a range of ideas centered on reason as the primary source of authority and legitimacy, and came to advance ideals such as liberty, progress, tolerance, fraternity, constitutional government and ending the abuses of the church and state. In France, the central doctrines of the Lumières were individual liberty and religious tolerance, in opposition to the principle of absolute monarchy and the fixed dogmas of the Roman Catholic Church. The Enlightenment was marked by increasing empiricism, scientific rigor, and reductionism, along with increased questioning of religious orthodoxy. Historians have long debated the extent to which the secret network of Freemasonry was a main factor in the Enlightenment. The leaders of the Enlightenment included Freemasons such as Diderot, Montesquieu, Voltaire, Pope, Horace Walpole, Sir Robert Walpole, Mozart, Goethe, Frederick the Great, Benjamin Franklin, and George Washington. Norman Davies said that Freemasonry was a powerful force on behalf of Liberalism in Europe, from about 1700 to the twentieth century. It expanded rapidly during the Age of Enlightenment, reaching practically every country in Europe. It was especially attractive to powerful aristocrats and politicians as well as intellectuals, artists and political activists. Alexis de Tocqueville described the French Revolution as the inevitable result of the radical opposition created in the 18th century between the monarchy and the men of letters of the Enlightenment. These men of letters constituted a sort of ""substitute aristocracy that was both all-powerful and without real power"". This illusory power came from the rise of ""public opinion"", born when absolutist centralization removed the nobility and the bourgeoisie from the political sphere. The ""literary politics"" that resulted promoted a discourse of equality and was hence in fundamental opposition to the monarchical regime. De Tocqueville ""clearly designates ... the cultural effects of transformation in the forms of the exercise of power"". Nevertheless, it took another century before cultural approach became central to the historiography, as typified by Robert Darnton, The Business of Enlightenment: A Publishing History of the Encyclopédie, 1775–1800 (1979). Jonathan Israel called the journals the most influential cultural innovation of European intellectual culture. They shifted the attention of the ""cultivated public"" away from established authorities to novelty and innovation, and promoted the ""enlightened"" ideals of toleration and intellectual objectivity. Being a source of knowledge derived from science and reason, they were an implicit critique of existing notions of universal truth monopolized by monarchies, parliaments, and religious authorities. They also advanced Christian enlightenment that upheld ""the legitimacy of God-ordained authority""—the Bible—in which there had to be agreement between the biblical and natural theories. The Age of Enlightenment was preceded by and closely associated with the scientific revolution. Earlier philosophers whose work influenced the Enlightenment included Francis Bacon, Descartes, Locke, and Spinoza. The major figures of the Enlightenment included Cesare Beccaria, Voltaire, Denis Diderot, Jean-Jacques Rousseau, David Hume, Adam Smith, and Immanuel Kant. Some European rulers, including Catherine II of Russia, Joseph II of Austria and Frederick I of Prussia, tried to apply Enlightenment thought on religious and political tolerance, which became known as enlightened absolutism. The Americans Benjamin Franklin and Thomas Jefferson came to Europe during the period and contributed actively to the scientific and political debate, and the ideals of the Enlightenment were incorporated into the United States Declaration of Independence and the Constitution of the United States. Locke is known for his statement that individuals have a right to ""Life, Liberty and Property"", and his belief that the natural right to property is derived from labor. Tutored by Locke, Anthony Ashley-Cooper, 3rd Earl of Shaftesbury wrote in 1706: ""There is a mighty Light which spreads its self over the world especially in those two free Nations of England and Holland; on whom the Affairs of Europe now turn"". Locke's theory of natural rights has influenced many political documents, including the United States Declaration of Independence and the French National Constituent Assembly's Declaration of the Rights of Man and of the Citizen. A number of novel ideas about religion developed with the Enlightenment, including Deism and talk of atheism. Deism, according to Thomas Paine, is the simple belief in God the Creator, with no reference to the Bible or any other miraculous source. Instead, the Deist relies solely on personal reason to guide his creed, which was eminently agreeable to many thinkers of the time. Atheism was much discussed, but there were few proponents. Wilson and Reill note that, ""In fact, very few enlightened intellectuals, even when they were vocal critics of Christianity, were true atheists. Rather, they were critics of orthodox belief, wedded rather to skepticism, deism, vitalism, or perhaps pantheism."" Some followed Pierre Bayle and argued that atheists could indeed be moral men. Many others like Voltaire held that without belief in a God who punishes evil, the moral order of society was undermined. That is, since atheists gave themselves to no Supreme Authority and no law, and had no fear of eternal consequences, they were far more likely to disrupt society. Bayle (1647–1706) observed that in his day, ""prudent persons will always maintain an appearance of [religion]."". He believed that even atheists could hold concepts of honor and go beyond their own self-interest to create and interact in society. Locke said that if there were no God and no divine law, the result would be moral anarchy: every individual ""could have no law but his own will, no end but himself. He would be a god to himself, and the satisfaction of his own will the sole measure and end of all his actions"". The ""Radical Enlightenment"" promoted the concept of separating church and state, an idea that often credited to English philosopher John Locke (1632–1704). According to his principle of the social contract, Locke said that the government lacked authority in the realm of individual conscience, as this was something rational people could not cede to the government for it or others to control. For Locke, this created a natural right in the liberty of conscience, which he said must therefore remain protected from any government authority. There were two distinct lines of Enlightenment thought: the radical enlightenment, inspired by the philosophy of Spinoza, advocating democracy, individual liberty, freedom of expression, and eradication of religious authority; and a second, more moderate variety, supported by René Descartes, John Locke, Christian Wolff, Isaac Newton and others, which sought accommodation between reform and the traditional systems of power and faith. Both lines of thought were opposed by the conservative Counter-Enlightenment. The first significant work that expressed scientific theory and knowledge expressly for the laity, in the vernacular, and with the entertainment of readers in mind, was Bernard de Fontenelle's Conversations on the Plurality of Worlds (1686). The book was produced specifically for women with an interest in scientific writing and inspired a variety of similar works. These popular works were written in a discursive style, which was laid out much more clearly for the reader than the complicated articles, treatises, and books published by the academies and scientists. Charles Leadbetter's Astronomy (1727) was advertised as ""a Work entirely New"" that would include ""short and easie  [sic] Rules and Astronomical Tables."" The first French introduction to Newtonianism and the Principia was Eléments de la philosophie de Newton, published by Voltaire in 1738. Émilie du Châtelet's translation of the Principia, published after her death in 1756, also helped to spread Newton's theories beyond scientific academies and the university. Francesco Algarotti, writing for a growing female audience, published Il Newtonianism per le dame, which was a tremendously popular work and was translated from Italian into English by Elizabeth Carter. A similar introduction to Newtonianism for women was produced by Henry Pembarton. His A View of Sir Isaac Newton's Philosophy was published by subscription. Extant records of subscribers show that women from a wide range of social standings purchased the book, indicating the growing number of scientifically inclined female readers among the middling class. During the Enlightenment, women also began producing popular scientific works themselves. Sarah Trimmer wrote a successful natural history textbook for children titled The Easy Introduction to the Knowledge of Nature (1782), which was published for many years after in eleven editions. Many of the leading universities associated with Enlightenment progressive principles were located in northern Europe, with the most renowned being the universities of Leiden, Göttingen, Halle, Montpellier, Uppsala and Edinburgh. These universities, especially Edinburgh, produced professors whose ideas had a significant impact on Britain's North American colonies and, later, the American Republic. Within the natural sciences, Edinburgh's medical also led the way in chemistry, anatomy and pharmacology. In other parts of Europe, the universities and schools of France and most of Europe were bastions of traditionalism and were not hospitable to the Enlightenment. In France, the major exception was the medical university at Montpellier. The desire to explore, record and systematize knowledge had a meaningful impact on music publications. Jean-Jacques Rousseau's Dictionnaire de musique (published 1767 in Geneva and 1768 in Paris) was a leading text in the late 18th century. This widely available dictionary gave short definitions of words like genius and taste, and was clearly influenced by the Enlightenment movement. Another text influenced by Enlightenment values was Charles Burney's A General History of Music: From the Earliest Ages to the Present Period (1776), which was a historical survey and an attempt to rationalize elements in music systematically over time. Recently, musicologists have shown renewed interest in the ideas and consequences of the Enlightenment. For example, Rose Rosengard Subotnik's Deconstructive Variations (subtitled Music and Reason in Western Society) compares Mozart's Die Zauberflöte (1791) using the Enlightenment and Romantic perspectives, and concludes that the work is ""an ideal musical representation of the Enlightenment"". French historians traditionally place the Enlightenment between 1715, the year that Louis XIV died, and 1789, the beginning of the French Revolution. Some recent historians begin the period in the 1620s, with the start of the scientific revolution. The Philosophes, the French term for the philosophers of the period, widely circulated their ideas through meetings at scientific academies, Masonic lodges, literary salons and coffee houses, and through printed books and pamphlets. The ideas of the Enlightenment undermined the authority of the monarchy and the church, and paved the way for the revolutions of the 18th and 19th centuries. A variety of 19th-century movements, including liberalism and neo-classicism, trace their intellectual heritage back to the Enlightenment. In Russia, the government began to actively encourage the proliferation of arts and sciences in the mid-18th century. This era produced the first Russian university, library, theatre, public museum, and independent press. Like other enlightened despots, Catherine the Great played a key role in fostering the arts, sciences, and education. She used her own interpretation of Enlightenment ideals, assisted by notable international experts such as Voltaire (by correspondence) and, in residence, world class scientists such as Leonhard Euler and Peter Simon Pallas. The national Enlightenment differed from its Western European counterpart in that it promoted further modernization of all aspects of Russian life and was concerned with attacking the institution of serfdom in Russia. The Russian enlightenment centered on the individual instead of societal enlightenment and encouraged the living of an enlightened life. Jonathan Israel rejects the attempts of postmodern and Marxian historians to understand the revolutionary ideas of the period purely as by-products of social and economic transformations. He instead focuses on the history of ideas in the period from 1650 to the end of the 18th century, and claims that it was the ideas themselves that caused the change that eventually led to the revolutions of the latter half of the 18th century and the early 19th century. Israel argues that until the 1650s Western civilization ""was based on a largely shared core of faith, tradition and authority"". The creation of the public sphere has been associated with two long-term historical trends: the rise of the modern nation state and the rise of capitalism. The modern nation state, in its consolidation of public power, created by counterpoint a private realm of society independent of the state, which allowed for the public sphere. Capitalism also increased society's autonomy and self-awareness, and an increasing need for the exchange of information. As the nascent public sphere expanded, it embraced a large variety of institutions; the most commonly cited were coffee houses and cafés, salons and the literary public sphere, figuratively localized in the Republic of Letters. In France, the creation of the public sphere was helped by the aristocracy's move from the King's palace at Versailles to Paris in about 1720, since their rich spending stimulated the trade in luxuries and artistic creations, especially fine paintings. One of the most important developments that the Enlightenment era brought to the discipline of science was its popularization. An increasingly literate population seeking knowledge and education in both the arts and the sciences drove the expansion of print culture and the dissemination of scientific learning. The new literate population was due to a high rise in the availability of food. This enabled many people to rise out of poverty, and instead of paying more for food, they had money for education. Popularization was generally part of an overarching Enlightenment ideal that endeavoured ""to make information available to the greatest number of people."" As public interest in natural philosophy grew during the 18th century, public lecture courses and the publication of popular texts opened up new roads to money and fame for amateurs and scientists who remained on the periphery of universities and academies. More formal works included explanations of scientific theories for individuals lacking the educational background to comprehend the original scientific text. Sir Isaac Newton's celebrated Philosophiae Naturalis Principia Mathematica was published in Latin and remained inaccessible to readers without education in the classics until Enlightenment writers began to translate and analyze the text in the vernacular. The massive work was arranged according to a ""tree of knowledge."" The tree reflected the marked division between the arts and sciences, which was largely a result of the rise of empiricism. Both areas of knowledge were united by philosophy, or the trunk of the tree of knowledge. The Enlightenment's desacrilization of religion was pronounced in the tree's design, particularly where theology accounted for a peripheral branch, with black magic as a close neighbour. As the Encyclopédie gained popularity, it was published in quarto and octavo editions after 1777. The quarto and octavo editions were much less expensive than previous editions, making the Encyclopédie more accessible to the non-elite. Robert Darnton estimates that there were approximately 25 000 copies of the Encyclopédie in circulation throughout France and Europe before the French Revolution. The extensive, yet affordable encyclopedia came to represent the transmission of Enlightenment and scientific education to an expanding audience. The strongest contribution of the French Academies to the public sphere comes from the concours académiques (roughly translated as 'academic contests') they sponsored throughout France. These academic contests were perhaps the most public of any institution during the Enlightenment. The practice of contests dated back to the Middle Ages, and was revived in the mid-17th century. The subject matter had previously been generally religious and/or monarchical, featuring essays, poetry, and painting. By roughly 1725, however, this subject matter had radically expanded and diversified, including ""royal propaganda, philosophical battles, and critical ruminations on the social and political institutions of the Old Regime."" Topics of public controversy were also discussed such as the theories of Newton and Descartes, the slave trade, women's education, and justice in France. In France, the established men of letters (gens de lettres) had fused with the elites (les grands) of French society by the mid-18th century. This led to the creation of an oppositional literary sphere, Grub Street, the domain of a ""multitude of versifiers and would-be authors"". These men came to London to become authors, only to discover that the literary market simply could not support large numbers of writers, who, in any case, were very poorly remunerated by the publishing-bookselling guilds. Frederick the Great, the king of Prussia from 1740 to 1786, saw himself as a leader of the Enlightenment and patronized philosophers and scientists at his court in Berlin. Voltaire, who had been imprisoned and maltreated by the French government, was eager to accept Frederick's invitation to live at his palace. Frederick explained, ""My principal occupation is to combat ignorance and prejudice ... to enlighten minds, cultivate morality, and to make people as happy as it suits human nature, and as the means at my disposal permit."" As the economy and the middle class expanded, there was an increasing number of amateur musicians. One manifestation of this involved women, who became more involved with music on a social level. Women were already engaged in professional roles as singers, and increased their presence in the amateur performers' scene, especially with keyboard music. Music publishers begin to print music that amateurs could understand and play. The majority of the works that were published were for keyboard, voice and keyboard, and chamber ensemble. After these initial genres were popularized, from the mid-century on, amateur groups sang choral music, which then became a new trend for publishers to capitalize on. The increasing study of the fine arts, as well as access to amateur-friendly published works, led to more people becoming interested in reading and discussing music. Music magazines, reviews, and critical works which suited amateurs as well as connoisseurs began to surface. Though much of Enlightenment political thought was dominated by social contract theorists, both David Hume and Adam Ferguson criticized this camp. Hume's essay Of the Original Contract argues that governments derived from consent are rarely seen, and civil government is grounded in a ruler's habitual authority and force. It is precisely because of the ruler's authority over-and-against the subject, that the subject tacitly consents; Hume says that the subjects would ""never imagine that their consent made him sovereign"", rather the authority did so. Similarly, Ferguson did not believe citizens built the state, rather polities grew out of social development. In his 1767 An Essay on the History of Civil Society, Ferguson uses the four stages of progress, a theory that was very popular in Scotland at the time, to explain how humans advance from a hunting and gathering society to a commercial and civil society without ""signing"" a social contract. The first technical dictionary was drafted by John Harris and entitled Lexicon Technicum: Or, An Universal English Dictionary of Arts and Sciences. Harris' book avoided theological and biographical entries; instead it concentrated on science and technology. Published in 1704, the Lexicon technicum was the first book to be written in English that took a methodical approach to describing mathematics and commercial arithmetic along with the physical sciences and navigation. Other technical dictionaries followed Harris' model, including Ephraim Chambers' Cyclopaedia (1728), which included five editions, and was a substantially larger work than Harris'. The folio edition of the work even included foldout engravings. The Cyclopaedia emphasized Newtonian theories, Lockean philosophy, and contained thorough examinations of technologies, such as engraving, brewing, and dyeing. Bertrand Russell saw the Enlightenment as a phase in a progressive development, which began in antiquity, and that reason and challenges to the established order were constant ideals throughout that time. Russell said that the Enlightenment was ultimately born out of the Protestant reaction against the Catholic counter-reformation, and that philosophical views such as affinity for democracy against monarchy originated among 16th-century Protestants to justify their desire to break away from the Catholic Church. Though many of these philosophical ideals were picked up by Catholics, Russell argues, by the 18th century the Enlightenment was the principal manifestation of the schism that began with Martin Luther. In the mid-18th century, Paris became the center of an explosion of philosophic and scientific activity challenging traditional doctrines and dogmas. The philosophic movement was led by Voltaire and Jean-Jacques Rousseau, who argued for a society based upon reason rather than faith and Catholic doctrine, for a new civil order based on natural law, and for science based on experiments and observation. The political philosopher Montesquieu introduced the idea of a separation of powers in a government, a concept which was enthusiastically adopted by the authors of the United States Constitution. While the Philosophes of the French Enlightenment were not revolutionaries, and many were members of the nobility, their ideas played an important part in undermining the legitimacy of the Old Regime and shaping the French Revolution. Enlightenment scholars sought to curtail the political power of organized religion and thereby prevent another age of intolerant religious war. Spinoza determined to remove politics from contemporary and historical theology (e.g. disregarding Judaic law). Moses Mendelssohn advised affording no political weight to any organized religion, but instead recommended that each person follow what they found most convincing. A good religion based in instinctive morals and a belief in God should not theoretically need force to maintain order in its believers, and both Mendelssohn and Spinoza judged religion on its moral fruits, not the logic of its theology. The first English coffeehouse opened in Oxford in 1650. Brian Cowan said that Oxford coffeehouses developed into ""penny universities"", offering a locus of learning that was less formal than structured institutions. These penny universities occupied a significant position in Oxford academic life, as they were frequented by those consequently referred to as the ""virtuosi"", who conducted their research on some of the resulting premises. According to Cowan, ""the coffeehouse was a place for like-minded scholars to congregate, to read, as well as learn from and to debate with each other, but was emphatically not a university institution, and the discourse there was of a far different order than any university tutorial."" Along with secular matters, readers also favoured an alphabetical ordering scheme over cumbersome works arranged along thematic lines. The historian Charles Porset, commenting on alphabetization, has said that ""as the zero degree of taxonomy, alphabetical order authorizes all reading strategies; in this respect it could be considered an emblem of the Enlightenment."" For Porset, the avoidance of thematic and hierarchical systems thus allows free interpretation of the works and becomes an example of egalitarianism. Encyclopedias and dictionaries also became more popular during the Age of Reason as the number of educated consumers who could afford such texts began to multiply. In the later half of the 18th century, the number of dictionaries and encyclopedias published by decade increased from 63 between 1760 and 1769 to approximately 148 in the decade proceeding the French Revolution (1780–1789). Along with growth in numbers, dictionaries and encyclopedias also grew in length, often having multiple print runs that sometimes included in supplemented editions. Intellectuals such as Robert Darnton and Jürgen Habermas have focused on the social conditions of the Enlightenment. Habermas described the creation of the ""bourgeois public sphere"" in 18th-century Europe, containing the new venues and modes of communication allowing for rational exchange. Habermas said that the public sphere was bourgeois, egalitarian, rational, and independent from the state, making it the ideal venue for intellectuals to critically examine contemporary politics and society, away from the interference of established authority. While the public sphere is generally an integral component of the social study of the Enlightenment, other historians have questioned whether the public sphere had these characteristics. Broadly speaking, Enlightenment science greatly valued empiricism and rational thought, and was embedded with the Enlightenment ideal of advancement and progress. The study of science, under the heading of natural philosophy, was divided into physics and a conglomerate grouping of chemistry and natural history, which included anatomy, biology, geology, mineralogy, and zoology. As with most Enlightenment views, the benefits of science were not seen universally; Rousseau criticized the sciences for distancing man from nature and not operating to make people happier. Science during the Enlightenment was dominated by scientific societies and academies, which had largely replaced universities as centres of scientific research and development. Societies and academies were also the backbone of the maturation of the scientific profession. Another important development was the popularization of science among an increasingly literate population. Philosophes introduced the public to many scientific theories, most notably through the Encyclopédie and the popularization of Newtonianism by Voltaire and Émilie du Châtelet. Some historians have marked the 18th century as a drab period in the history of science; however, the century saw significant advancements in the practice of medicine, mathematics, and physics; the development of biological taxonomy; a new understanding of magnetism and electricity; and the maturation of chemistry as a discipline, which established the foundations of modern chemistry. John Locke, one of the most influential Enlightenment thinkers, based his governance philosophy in social contract theory, a subject that permeated Enlightenment political thought. The English philosopher Thomas Hobbes ushered in this new debate with his work Leviathan in 1651. Hobbes also developed some of the fundamentals of European liberal thought: the right of the individual; the natural equality of all men; the artificial character of the political order (which led to the later distinction between civil society and the state); the view that all legitimate political power must be ""representative"" and based on the consent of the people; and a liberal interpretation of law which leaves people free to do whatever the law does not explicitly forbid. German historian Reinhart Koselleck claimed that ""On the Continent there were two social structures that left a decisive imprint on the Age of Enlightenment: the Republic of Letters and the Masonic lodges."" Scottish professor Thomas Munck argues that ""although the Masons did promote international and cross-social contacts which were essentially non-religious and broadly in agreement with enlightened values, they can hardly be described as a major radical or reformist network in their own right."" Many of the Masons values seemed to greatly appeal to Enlightenment values and thinkers. Diderot discusses the link between Freemason ideals and the enlightenment in D'Alembert's Dream, exploring masonry as a way of spreading enlightenment beliefs. Historian Margaret Jacob stresses the importance of the Masons in indirectly inspiring enlightened political thought. On the negative side, Daniel Roche contests claims that Masonry promoted egalitarianism. He argues that the lodges only attracted men of similar social backgrounds. The presence of noble women in the French ""lodges of adoption"" that formed in the 1780s was largely due to the close ties shared between these lodges and aristocratic society. A healthy, and legal, publishing industry existed throughout Europe, although established publishers and book sellers occasionally ran afoul of the law. The Encyclopédie, for example, condemned not only by the King but also by Clement XII, nevertheless found its way into print with the help of the aforementioned Malesherbes and creative use of French censorship law. But many works were sold without running into any legal trouble at all. Borrowing records from libraries in England, Germany and North America indicate that more than 70 percent of books borrowed were novels. Less than 1 percent of the books were of a religious nature, indicating the general trend of declining religiosity. The vast majority of the reading public could not afford to own a private library, and while most of the state-run ""universal libraries"" set up in the 17th and 18th centuries were open to the public, they were not the only sources of reading material. On one end of the spectrum was the Bibliothèque Bleue, a collection of cheaply produced books published in Troyes, France. Intended for a largely rural and semi-literate audience these books included almanacs, retellings of medieval romances and condensed versions of popular novels, among other things. While some historians have argued against the Enlightenment's penetration into the lower classes, the Bibliothèque Bleue represents at least a desire to participate in Enlightenment sociability. Moving up the classes, a variety of institutions offered readers access to material without needing to buy anything. Libraries that lent out their material for a small price started to appear, and occasionally bookstores would offer a small lending library to their patrons. Coffee houses commonly offered books, journals and sometimes even popular novels to their customers. The Tatler and The Spectator, two influential periodicals sold from 1709 to 1714, were closely associated with coffee house culture in London, being both read and produced in various establishments in the city. This is an example of the triple or even quadruple function of the coffee house: reading material was often obtained, read, discussed and even produced on the premises. Both Locke and Rousseau developed social contract theories in Two Treatises of Government and Discourse on Inequality, respectively. While quite different works, Locke, Hobbes, and Rousseau agreed that a social contract, in which the government's authority lies in the consent of the governed, is necessary for man to live in civil society. Locke defines the state of nature as a condition in which humans are rational and follow natural law; in which all men are born equal and with the right to life, liberty and property. However, when one citizen breaks the Law of Nature, both the transgressor and the victim enter into a state of war, from which it is virtually impossible to break free. Therefore, Locke said that individuals enter into civil society to protect their natural rights via an ""unbiased judge"" or common authority, such as courts, to appeal to. Contrastingly, Rousseau's conception relies on the supposition that ""civil man"" is corrupted, while ""natural man"" has no want he cannot fulfill himself. Natural man is only taken out of the state of nature when the inequality associated with private property is established. Rousseau said that people join into civil society via the social contract to achieve unity while preserving individual freedom. This is embodied in the sovereignty of the general will, the moral and collective legislative body constituted by citizens. The writers of Grub Street, the Grub Street Hacks, were left feeling bitter about the relative success of the men of letters, and found an outlet for their literature which was typified by the libelle. Written mostly in the form of pamphlets, the libelles ""slandered the court, the Church, the aristocracy, the academies, the salons, everything elevated and respectable, including the monarchy itself"". Le Gazetier cuirassé by Charles Théveneau de Morande was a prototype of the genre. It was Grub Street literature that was most read by the public during the Enlightenment. More importantly, according to Darnton, the Grub Street hacks inherited the ""revolutionary spirit"" once displayed by the philosophes, and paved the way for the French Revolution by desacralizing figures of political, moral and religious authority in France. Cesare Beccaria, a jurist and one of the great Enlightenment writers, became famous for his masterpiece Of Crimes and Punishments (1764), which was later translated into 22 languages. Another prominent intellectual was Francesco Mario Pagano, who wrote important studies such as Saggi Politici (Political Essays, 1783), one of the major works of the Enlightenment in Naples, and Considerazioni sul processo criminale (Considerations on the criminal trial, 1787), which established him as an international authority on criminal law. In several nations, rulers welcomed leaders of the Enlightenment at court and asked them to help design laws and programs to reform the system, typically to build stronger national states. These rulers are called ""enlightened despots"" by historians. They included Frederick the Great of Prussia, Catherine the Great of Russia, Leopold II of Tuscany, and Joseph II of Austria. Joseph was over-enthusiastic, announcing so many reforms that had so little support that revolts broke out and his regime became a comedy of errors and nearly all his programs were reversed. Senior ministers Pombal in Portugal and Struensee in Denmark also governed according to Enlightenment ideals. In Poland, the model constitution of 1791 expressed Enlightenment ideals, but was in effect for only one year as the nation was partitioned among its neighbors. More enduring were the cultural achievements, which created a nationalist spirit in Poland. The Enlightenment has been frequently linked to the French Revolution of 1789. One view of the political changes that occurred during the Enlightenment is that the ""consent of the governed"" philosophy as delineated by Locke in Two Treatises of Government (1689) represented a paradigm shift from the old governance paradigm under feudalism known as the ""divine right of kings"". In this view, the revolutions of the late 1700s and early 1800s were caused by the fact that this governance paradigm shift often could not be resolved peacefully, and therefore violent revolution was the result. Clearly a governance philosophy where the king was never wrong was in direct conflict with one whereby citizens by natural law had to consent to the acts and rulings of their government. The major opponent of Freemasonry was the Roman Catholic Church, so that in countries with a large Catholic element, such as France, Italy, Spain, and Mexico, much of the ferocity of the political battles involve the confrontation between what Davies calls the reactionary Church and enlightened Freemasonry. Even in France, Masons did not act as a group. American historians, while noting that Benjamin Franklin and George Washington were indeed active Masons, have downplayed the importance of Freemasonry in causing the American Revolution because the Masonic order was non-political and included both Patriots and their enemy the Loyalists. The word ""public"" implies the highest level of inclusivity – the public sphere by definition should be open to all. However, this sphere was only public to relative degrees. Enlightenment thinkers frequently contrasted their conception of the ""public"" with that of the people: Condorcet contrasted ""opinion"" with populace, Marmontel ""the opinion of men of letters"" with ""the opinion of the multitude,"" and d'Alembert the ""truly enlightened public"" with ""the blind and noisy multitude"". Additionally, most institutions of the public sphere excluded both women and the lower classes. Cross-class influences occurred through noble and lower class participation in areas such as the coffeehouses and the Masonic lodges. The target audience of natural history was French polite society, evidenced more by the specific discourse of the genre than by the generally high prices of its works. Naturalists catered to polite society's desire for erudition – many texts had an explicit instructive purpose. However, natural history was often a political affair. As E. C. Spary writes, the classifications used by naturalists ""slipped between the natural world and the social ... to establish not only the expertise of the naturalists over the natural, but also the dominance of the natural over the social"". The idea of taste (le goût) was a social indicator: to truly be able to categorize nature, one had to have the proper taste, an ability of discretion shared by all members of polite society. In this way natural history spread many of the scientific developments of the time, but also provided a new source of legitimacy for the dominant class. From this basis, naturalists could then develop their own social ideals based on their scientific works. Enlightenment era religious commentary was a response to the preceding century of religious conflict in Europe, especially the Thirty Years' War. Theologians of the Enlightenment wanted to reform their faith to its generally non-confrontational roots and to limit the capacity for religious controversy to spill over into politics and warfare while still maintaining a true faith in God. For moderate Christians, this meant a return to simple Scripture. John Locke abandoned the corpus of theological commentary in favor of an ""unprejudiced examination"" of the Word of God alone. He determined the essence of Christianity to be a belief in Christ the redeemer and recommended avoiding more detailed debate. Thomas Jefferson in the Jefferson Bible went further; he dropped any passages dealing with miracles, visitations of angels, and the resurrection of Jesus after his death. He tried to extract the practical Christian moral code of the New Testament. A genre that greatly rose in importance was that of scientific literature. Natural history in particular became increasingly popular among the upper classes. Works of natural history include René-Antoine Ferchault de Réaumur's Histoire naturelle des insectes and Jacques Gautier d'Agoty's La Myologie complète, ou description de tous les muscles du corps humain (1746). Outside ancien régime France, natural history was an important part of medicine and industry, encompassing the fields of botany, zoology, meteorology, hydrology and mineralogy. Students in Enlightenment universities and academies were taught these subjects to prepare them for careers as diverse as medicine and theology. As shown by M D Eddy, natural history in this context was a very middle class pursuit and operated as a fertile trading zone for the interdisciplinary exchange of diverse scientific ideas. Enlightenment historiography began in the period itself, from what Enlightenment figures said about their work. A dominant element was the intellectual angle they took. D'Alembert's Preliminary Discourse of l'Encyclopédie provides a history of the Enlightenment which comprises a chronological list of developments in the realm of knowledge – of which the Encyclopédie forms the pinnacle. In 1783, Jewish philosopher Moses Mendelssohn referred to Enlightenment as a process by which man was educated in the use of reason. Immanuel Kant called Enlightenment ""man's release from his self-incurred tutelage"", tutelage being ""man's inability to make use of his understanding without direction from another"". ""For Kant, Enlightenment was mankind's final coming of age, the emancipation of the human consciousness from an immature state of ignorance."" The German scholar Ernst Cassirer called the Enlightenment ""a part and a special phase of that whole intellectual development through which modern philosophic thought gained its characteristic self-confidence and self-consciousness"". According to historian Roy Porter, the liberation of the human mind from a dogmatic state of ignorance is the epitome of what the Age of Enlightenment was trying to capture. Many women played an essential part in the French Enlightenment, due to the role they played as salonnières in Parisian salons, as the contrast to the male philosophes. The salon was the principal social institution of the republic, and ""became the civil working spaces of the project of Enlightenment."" Women, as salonnières, were ""the legitimate governors of [the] potentially unruly discourse"" that took place within. While women were marginalized in the public culture of the Ancien Régime, the French Revolution destroyed the old cultural and economic restraints of patronage and corporatism (guilds), opening French society to female participation, particularly in the literary sphere. The influence of science also began appearing more commonly in poetry and literature during the Enlightenment. Some poetry became infused with scientific metaphor and imagery, while other poems were written directly about scientific topics. Sir Richard Blackmore committed the Newtonian system to verse in Creation, a Philosophical Poem in Seven Books (1712). After Newton's death in 1727, poems were composed in his honour for decades. James Thomson (1700–1748) penned his ""Poem to the Memory of Newton,"" which mourned the loss of Newton, but also praised his science and legacy. During the Age of Enlightenment, Freemasons comprised an international network of like-minded men, often meeting in secret in ritualistic programs at their lodges. they promoted the ideals of the Enlightenment, and helped diffuse these values across Britain and France and other places. Freemasonry as a systematic creed with its own myths, values and set of rituals originated in Scotland around 1600 and spread first to England and then across the Continent in the eighteenth century. They fostered new codes of conduct – including a communal understanding of liberty and equality inherited from guild sociability – ""liberty, fraternity, and equality"" Scottish soldiers and Jacobite Scots brought to the Continent ideals of fraternity which reflected not the local system of Scottish customs but the institutions and ideals originating in the English Revolution against royal absolutism. Freemasonry was particularly prevalent in France – by 1789, there were perhaps as many as 100,000 French Masons, making Freemasonry the most popular of all Enlightenment associations. The Freemasons displayed a passion for secrecy and created new degrees and ceremonies. Similar societies, partially imitating Freemasonry, emerged in France, Germany, Sweden and Russia. One example was the ""Illuminati"" founded in Bavaria in 1776, which was copied after the Freemasons but was never part of the movement. The Illuminati was an overtly political group, which most Masonic lodges decidedly were not. The most influential publication of the Enlightenment was the Encyclopédie, compiled by Denis Diderot and (until 1759) by Jean le Rond d'Alembert and a team of 150 scientists and philosophers. It was published between 1751 and 1772 in thirty-five volumes, and spread the ideas of the Enlightenment across Europe and beyond. Other landmark publications were the Dictionnaire philosophique (Philosophical Dictionary, 1764) and Letters on the English (1733) written by Voltaire; Rousseau's Discourse on Inequality (1754) and The Social Contract (1762); and Montesquieu's Spirit of the Laws (1748). The ideas of the Enlightenment played a major role in inspiring the French Revolution, which began in 1789. After the Revolution, the Enlightenment was followed by an opposing intellectual movement known as Romanticism. In England, the Royal Society of London also played a significant role in the public sphere and the spread of Enlightenment ideas. It was founded by a group of independent scientists and given a royal charter in 1662. The Society played a large role in spreading Robert Boyle's experimental philosophy around Europe, and acted as a clearinghouse for intellectual correspondence and exchange. Boyle was ""a founder of the experimental world in which scientists now live and operate,"" and his method based knowledge on experimentation, which had to be witnessed to provide proper empirical legitimacy. This is where the Royal Society came into play: witnessing had to be a ""collective act"", and the Royal Society's assembly rooms were ideal locations for relatively public demonstrations. However, not just any witness was considered to be credible; ""Oxford professors were accounted more reliable witnesses than Oxfordshire peasants."" Two factors were taken into account: a witness's knowledge in the area; and a witness's ""moral constitution"". In other words, only civil society were considered for Boyle's public. Immanuel Kant (1724–1804) tried to reconcile rationalism and religious belief, individual freedom and political authority, as well as map out a view of the public sphere through private and public reason. Kant's work continued to shape German thought, and indeed all of European philosophy, well into the 20th century. Mary Wollstonecraft was one of England's earliest feminist philosophers. She argued for a society based on reason, and that women, as well as men, should be treated as rational beings. She is best known for her work A Vindication of the Rights of Woman (1791). Although the existence of dictionaries and encyclopedias spanned into ancient times, the texts changed from simply defining words in a long running list to far more detailed discussions of those words in 18th-century encyclopedic dictionaries. The works were part of an Enlightenment movement to systematize knowledge and provide education to a wider audience than the elite. As the 18th century progressed, the content of encyclopedias also changed according to readers' tastes. Volumes tended to focus more strongly on secular affairs, particularly science and technology, rather than matters of theology. The Enlightenment took hold in most European countries, often with a specific local emphasis. For example, in France it became associated with anti-government and anti-Church radicalism while in Germany it reached deep into the middle classes and where it expressed a spiritualistic and nationalistic tone without threatening governments or established churches. Government responses varied widely. In France, the government was hostile, and the philosophes fought against its censorship, sometimes being imprisoned or hounded into exile. The British government for the most part ignored the Enlightenment's leaders in England and Scotland, although it did give Isaac Newton a knighthood and a very lucrative government office. These views on religious tolerance and the importance of individual conscience, along with the social contract, became particularly influential in the American colonies and the drafting of the United States Constitution. Thomas Jefferson called for a ""wall of separation between church and state"" at the federal level. He previously had supported successful efforts to disestablish the Church of England in Virginia, and authored the Virginia Statute for Religious Freedom. Jefferson's political ideals were greatly influenced by the writings of John Locke, Francis Bacon, and Isaac Newton whom he considered the three greatest men that ever lived. Both Rousseau and Locke's social contract theories rest on the presupposition of natural rights, which are not a result of law or custom, but are things that all men have in pre-political societies, and are therefore universal and inalienable. The most famous natural right formulation comes from John Locke in his Second Treatise, when he introduces the state of nature. For Locke the law of nature is grounded on mutual security, or the idea that one cannot infringe on another's natural rights, as every man is equal and has the same inalienable rights. These natural rights include perfect equality and freedom, and the right to preserve life and property. Locke also argued against slavery on the basis that enslaving yourself goes against the law of nature; you cannot surrender your own rights, your freedom is absolute and no one can take it from you. Additionally, Locke argues that one person cannot enslave another because it is morally reprehensible, although he introduces a caveat by saying that enslavement of a lawful captive in time of war would not go against one's natural rights. The context for the rise of the public sphere was the economic and social change commonly associated with the Industrial Revolution: ""economic expansion, increasing urbanization, rising population and improving communications in comparison to the stagnation of the previous century""."" Rising efficiency in production techniques and communication lowered the prices of consumer goods and increased the amount and variety of goods available to consumers (including the literature essential to the public sphere). Meanwhile, the colonial experience (most European states had colonial empires in the 18th century) began to expose European society to extremely heterogeneous cultures, leading to the breaking down of ""barriers between cultural systems, religious divides, gender differences and geographical areas"". Masonic lodges created a private model for public affairs. They ""reconstituted the polity and established a constitutional form of self-government, complete with constitutions and laws, elections and representatives."" In other words, the micro-society set up within the lodges constituted a normative model for society as a whole. This was especially true on the Continent: when the first lodges began to appear in the 1730s, their embodiment of British values was often seen as threatening by state authorities. For example, the Parisian lodge that met in the mid 1720s was composed of English Jacobite exiles. Furthermore, freemasons all across Europe explicitly linked themselves to the Enlightenment as a whole. In French lodges, for example, the line ""As the means to be enlightened I search for the enlightened"" was a part of their initiation rites. British lodges assigned themselves the duty to ""initiate the unenlightened"". This did not necessarily link lodges to the irreligious, but neither did this exclude them from the occasional heresy. In fact, many lodges praised the Grand Architect, the masonic terminology for the deistic divine being who created a scientifically ordered universe. The Enlightenment has always been contested territory. Its supporters ""hail it as the source of everything that is progressive about the modern world. For them, it stands for freedom of thought, rational inquiry, critical thinking, religious tolerance, political liberty, scientific achievement, the pursuit of happiness, and hope for the future."" However, its detractors accuse it of 'shallow' rationalism, naïve optimism, unrealistic universalism, and moral darkness. From the start there was a Counter-Enlightenment in which conservative and clerical defenders of traditional religion attacked materialism and skepticism as evil forces that encouraged immorality. By 1794, they pointed to the Terror during the French Revolution as confirmation of their predictions. As the Enlightenment was ending, Romantic philosophers argued that excessive dependence on reason was a mistake perpetuated by the Enlightenment, because it disregarded the bonds of history, myth, faith and tradition that were necessary to hold society together. In Germany, practical reference works intended for the uneducated majority became popular in the 18th century. The Marperger Curieuses Natur-, Kunst-, Berg-, Gewerkund Handlungs-Lexicon (1712) explained terms that usefully described the trades and scientific and commercial education. Jablonksi Allgemeines Lexicon (1721) was better known than the Handlungs-Lexicon, and underscored technical subjects rather than scientific theory. For example, over five columns of text were dedicated to wine, while geometry and logic were allocated only twenty-two and seventeen lines, respectively. The first edition of the Encyclopædia Britannica (1771) was modelled along the same lines as the German lexicons. One of the primary elements of the culture of the Enlightenment was the rise of the public sphere, a ""realm of communication marked by new arenas of debate, more open and accessible forms of urban public space and sociability, and an explosion of print culture,"" in the late 17th century and 18th century. Elements of the public sphere included: it was egalitarian, it discussed the domain of ""common concern,"" and argument was founded on reason. Habermas uses the term ""common concern"" to describe those areas of political/social knowledge and discussion that were previously the exclusive territory of the state and religious authorities, now open to critical examination by the public sphere. The values of this bourgeois public sphere included holding reason to be supreme, considering everything to be open to criticism (the public sphere is critical), and the opposition of secrecy of all sorts. More importantly, the contests were open to all, and the enforced anonymity of each submission guaranteed that neither gender nor social rank would determine the judging. Indeed, although the ""vast majority"" of participants belonged to the wealthier strata of society (""the liberal arts, the clergy, the judiciary, and the medical profession""), there were some cases of the popular classes submitting essays, and even winning. Similarly, a significant number of women participated – and won – the competitions. Of a total of 2300 prize competitions offered in France, women won 49 – perhaps a small number by modern standards, but very significant in an age in which most women did not have any academic training. Indeed, the majority of the winning entries were for poetry competitions, a genre commonly stressed in women's education. The debating societies discussed an extremely wide range of topics. Before the Enlightenment, most intellectual debates revolved around ""confessional"" – that is, Catholic, Lutheran, Reformed (Calvinist), or Anglican issues, and the main aim of these debates was to establish which bloc of faith ought to have the ""monopoly of truth and a God-given title to authority"". After this date everything thus previously rooted in tradition was questioned and often replaced by new concepts in the light of philosophical reason. After the second half of the 17th century and during the 18th century, a ""general process of rationalization and secularization set in,"" and confessional disputes were reduced to a secondary status in favor of the ""escalating contest between faith and incredulity"". The history of Academies in France during the Enlightenment begins with the Academy of Science, founded in 1635 in Paris. It was closely tied to the French state, acting as an extension of a government seriously lacking in scientists. It helped promote and organize new disciplines, and it trained new scientists. It also contributed to the enhancement of scientists' social status, considering them to be the ""most useful of all citizens"". Academies demonstrate the rising interest in science along with its increasing secularization, as evidenced by the small number of clerics who were members (13 percent). The presence of the French academies in the public sphere cannot be attributed to their membership; although the majority of their members were bourgeois, the exclusive institution was only open to elite Parisian scholars. They perceived themselves as ""interpreters of the sciences for the people"". For example, it was with this in mind that academicians took it upon themselves to disprove the popular pseudo-science of mesmerism. In addition to debates on religion, societies discussed issues such as politics and the role of women. It is important to note, however, that the critical subject matter of these debates did not necessarily translate into opposition to the government. In other words, the results of the debate quite frequently upheld the status quo. From a historical standpoint, one of the most important features of the debating society was their openness to the public; women attended and even participated in almost every debating society, which were likewise open to all classes providing they could pay the entrance fee. Once inside, spectators were able to participate in a largely egalitarian form of sociability that helped spread Enlightenment ideas. Francis Hutcheson, a moral philosopher, described the utilitarian and consequentialist principle that virtue is that which provides, in his words, ""the greatest happiness for the greatest numbers"". Much of what is incorporated in the scientific method (the nature of knowledge, evidence, experience, and causation) and some modern attitudes towards the relationship between science and religion were developed by his protégés David Hume and Adam Smith. Hume became a major figure in the skeptical philosophical and empiricist traditions of philosophy. Science came to play a leading role in Enlightenment discourse and thought. Many Enlightenment writers and thinkers had backgrounds in the sciences and associated scientific advancement with the overthrow of religion and traditional authority in favour of the development of free speech and thought. Scientific progress during the Enlightenment included the discovery of carbon dioxide (fixed air) by the chemist Joseph Black, the argument for deep time by the geologist James Hutton, and the invention of the steam engine by James Watt. The experiments of Lavoisier were used to create the first modern chemical plants in Paris, and the experiments of the Montgolfier Brothers enabled them to launch the first manned flight in a hot-air balloon on 21 November 1783, from the Château de la Muette, near the Bois de Boulogne. Across continental Europe, but in France especially, booksellers and publishers had to negotiate censorship laws of varying strictness. The Encyclopédie, for example, narrowly escaped seizure and had to be saved by Malesherbes, the man in charge of the French censure. Indeed, many publishing companies were conveniently located outside France so as to avoid overzealous French censors. They would smuggle their merchandise across the border, where it would then be transported to clandestine booksellers or small-time peddlers. The records of clandestine booksellers may give a better representation of what literate Frenchmen might have truly read, since their clandestine nature provided a less restrictive product choice. In one case, political books were the most popular category, primarily libels and pamphlets. Readers were more interested in sensationalist stories about criminals and political corruption than they were in political theory itself. The second most popular category, ""general works"" (those books ""that did not have a dominant motif and that contained something to offend almost everyone in authority"") demonstrated a high demand for generally low-brow subversive literature. However, these works never became part of literary canon, and are largely forgotten today as a result. The Café Procope was established in Paris in 1686; by the 1720s there were around 400 cafés in the city. The Café Procope in particular became a center of Enlightenment, welcoming such celebrities as Voltaire and Rousseau. The Café Procope was where Diderot and D'Alembert decided to create the Encyclopédie. The cafés were one of the various ""nerve centers"" for bruits publics, public noise or rumour. These bruits were allegedly a much better source of information than were the actual newspapers available at the time. In the Scottish Enlightenment, Scotland's major cities created an intellectual infrastructure of mutually supporting institutions such as universities, reading societies, libraries, periodicals, museums and masonic lodges. The Scottish network was ""predominantly liberal Calvinist, Newtonian, and 'design' oriented in character which played a major role in the further development of the transatlantic Enlightenment"". In France, Voltaire said ""we look to Scotland for all our ideas of civilization."" The focus of the Scottish Enlightenment ranged from intellectual and economic matters to the specifically scientific as in the work of William Cullen, physician and chemist; James Anderson, an agronomist; Joseph Black, physicist and chemist; and James Hutton, the first modern geologist. Scientific academies and societies grew out of the Scientific Revolution as the creators of scientific knowledge in contrast to the scholasticism of the university. During the Enlightenment, some societies created or retained links to universities. However, contemporary sources distinguished universities from scientific societies by claiming that the university's utility was in the transmission of knowledge, while societies functioned to create knowledge. As the role of universities in institutionalized science began to diminish, learned societies became the cornerstone of organized science. Official scientific societies were chartered by the state in order to provide technical expertise. Most societies were granted permission to oversee their own publications, control the election of new members, and the administration of the society. After 1700, a tremendous number of official academies and societies were founded in Europe, and by 1789 there were over seventy official scientific societies. In reference to this growth, Bernard de Fontenelle coined the term ""the Age of Academies"" to describe the 18th century. The first scientific and literary journals were established during the Enlightenment. The first journal, the Parisian Journal des Sçavans, appeared in 1665. However, it was not until 1682 that periodicals began to be more widely produced. French and Latin were the dominant languages of publication, but there was also a steady demand for material in German and Dutch. There was generally low demand for English publications on the Continent, which was echoed by England's similar lack of desire for French works. Languages commanding less of an international market – such as Danish, Spanish and Portuguese – found journal success more difficult, and more often than not, a more international language was used instead. French slowly took over Latin's status as the lingua franca of learned circles. This in turn gave precedence to the publishing industry in Holland, where the vast majority of these French language periodicals were produced. Coffeehouses were especially important to the spread of knowledge during the Enlightenment because they created a unique environment in which people from many different walks of life gathered and shared ideas. They were frequently criticized by nobles who feared the possibility of an environment in which class and its accompanying titles and privileges were disregarded. Such an environment was especially intimidating to monarchs who derived much of their power from the disparity between classes of people. If classes were to join together under the influence of Enlightenment thinking, they might recognize the all-encompassing oppression and abuses of their monarchs and, because of their size, might be able to carry out successful revolts. Monarchs also resented the idea of their subjects convening as one to discuss political matters, especially those concerning foreign affairs - rulers thought political affairs to be their business only, a result of their supposed divine right to rule.  The Republic of Letters was the sum of a number of Enlightenment ideals: an egalitarian realm governed by knowledge that could act across political boundaries and rival state power. It was a forum that supported ""free public examination of questions regarding religion or legislation"". Immanuel Kant considered written communication essential to his conception of the public sphere; once everyone was a part of the ""reading public"", then society could be said to be enlightened. The people who participated in the Republic of Letters, such as Diderot and Voltaire, are frequently known today as important Enlightenment figures. Indeed, the men who wrote Diderot's Encyclopédie arguably formed a microcosm of the larger ""republic""."
Pain,"A much smaller number of people are insensitive to pain due to an inborn abnormality of the nervous system, known as ""congenital insensitivity to pain"". Children with this condition incur carelessly-repeated damage to their tongues, eyes, joints, skin, and muscles. Some die before adulthood, and others have a reduced life expectancy.[citation needed] Most people with congenital insensitivity to pain have one of five hereditary sensory and autonomic neuropathies (which includes familial dysautonomia and congenital insensitivity to pain with anhidrosis). These conditions feature decreased sensitivity to pain together with other neurological abnormalities, particularly of the autonomic nervous system. A very rare syndrome with isolated congenital insensitivity to pain has been linked with mutations in the SCN9A gene, which codes for a sodium channel (Nav1.7) necessary in conducting pain nerve stimuli. Spinal cord fibers dedicated to carrying A-delta fiber pain signals, and others that carry both A-delta and C fiber pain signals up the spinal cord to the thalamus in the brain have been identified. Other spinal cord fibers, known as wide dynamic range neurons, respond to A-delta and C fibers, but also to the large A-beta fibers that carry touch, pressure and vibration signals. Pain-related activity in the thalamus spreads to the insular cortex (thought to embody, among other things, the feeling that distinguishes pain from other homeostatic emotions such as itch and nausea) and anterior cingulate cortex (thought to embody, among other things, the motivational element of pain); and pain that is distinctly located also activates the primary and secondary somatosensory cortices. Melzack and Casey's 1968 picture of the dimensions of pain is as influential today as ever, firmly framing theory and guiding research in the functional neuroanatomy and psychology of pain. Paraplegia, the loss of sensation and voluntary motor control after serious spinal cord damage, may be accompanied by girdle pain at the level of the spinal cord damage, visceral pain evoked by a filling bladder or bowel, or, in five to ten per cent of paraplegics, phantom body pain in areas of complete sensory loss. This phantom body pain is initially described as burning or tingling but may evolve into severe crushing or pinching pain, or the sensation of fire running down the legs or of a knife twisting in the flesh. Onset may be immediate or may not occur until years after the disabling injury. Surgical treatment rarely provides lasting relief. The pain signal travels from the periphery to the spinal cord along an A-delta or C fiber. Because the A-delta fiber is thicker than the C fiber, and is thinly sheathed in an electrically insulating material (myelin), it carries its signal faster (5–30 m/s) than the unmyelinated C fiber (0.5–2 m/s). Pain evoked by the (faster) A-delta fibers is described as sharp and is felt first. This is followed by a duller pain, often described as burning, carried by the C fibers. These first order neurons enter the spinal cord via Lissauer's tract. Cultural barriers can also keep a person from telling someone they are in pain. Religious beliefs may prevent the individual from seeking help. They may feel certain pain treatment is against their religion. They may not report pain because they feel it is a sign that death is near. Many people fear the stigma of addiction and avoid pain treatment so as not to be prescribed potentially addicting drugs. Many Asians do not want to lose respect in society by admitting they are in pain and need help, believing the pain should be borne in silence, while other cultures feel they should report pain right away and get immediate relief. Gender can also be a factor in reporting pain. Sexual differences can be the result of social and cultural expectations, with women expected to be emotional and show pain and men stoic, keeping pain to themselves. Pain is usually transitory, lasting only until the noxious stimulus is removed or the underlying damage or pathology has healed, but some painful conditions, such as rheumatoid arthritis, peripheral neuropathy, cancer and idiopathic pain, may persist for years. Pain that lasts a long time is called chronic or persistent, and pain that resolves quickly is called acute. Traditionally, the distinction between acute and chronic pain has relied upon an arbitrary interval of time from onset; the two most commonly used markers being 3 months and 6 months since the onset of pain, though some theorists and researchers have placed the transition from acute to chronic pain at 12 months.:93 Others apply acute to pain that lasts less than 30 days, chronic to pain of more than six months' duration, and subacute to pain that lasts from one to six months. A popular alternative definition of chronic pain, involving no arbitrarily fixed durations, is ""pain that extends beyond the expected period of healing"". Chronic pain may be classified as cancer pain or else as benign. The Multidimensional Pain Inventory (MPI) is a questionnaire designed to assess the psychosocial state of a person with chronic pain. Analysis of MPI results by Turk and Rudy (1988) found three classes of chronic pain patient: ""(a) dysfunctional, people who perceived the severity of their pain to be high, reported that pain interfered with much of their lives, reported a higher degree of psychological distress caused by pain, and reported low levels of activity; (b) interpersonally distressed, people with a common perception that significant others were not very supportive of their pain problems; and (c) adaptive copers, patients who reported high levels of social support, relatively low levels of pain and perceived interference, and relatively high levels of activity."" Combining the MPI characterization of the person with their IASP five-category pain profile is recommended for deriving the most useful case description. Pain is the most common reason for physician consultation in most developed countries. It is a major symptom in many medical conditions, and can interfere with a person's quality of life and general functioning. Psychological factors such as social support, hypnotic suggestion, excitement, or distraction can significantly affect pain's intensity or unpleasantness. In some arguments put forth in physician-assisted suicide or euthanasia debates, pain has been used as an argument to permit terminally ill patients to end their lives.  In 1955, DC Sinclair and G Weddell developed peripheral pattern theory, based on a 1934 suggestion by John Paul Nafe. They proposed that all skin fiber endings (with the exception of those innervating hair cells) are identical, and that pain is produced by intense stimulation of these fibers. Another 20th-century theory was gate control theory, introduced by Ronald Melzack and Patrick Wall in the 1965 Science article ""Pain Mechanisms: A New Theory"". The authors proposed that both thin (pain) and large diameter (touch, pressure, vibration) nerve fibers carry information from the site of injury to two destinations in the dorsal horn of the spinal cord, and that the more large fiber activity relative to thin fiber activity at the inhibitory cell, the less pain is felt. Both peripheral pattern theory and gate control theory have been superseded by more modern theories of pain[citation needed]. A person's self-report is the most reliable measure of pain, with health care professionals tending to underestimate severity. A definition of pain widely employed in nursing, emphasizing its subjective nature and the importance of believing patient reports, was introduced by Margo McCaffery in 1968: ""Pain is whatever the experiencing person says it is, existing whenever he says it does"". To assess intensity, the patient may be asked to locate their pain on a scale of 0 to 10, with 0 being no pain at all, and 10 the worst pain they have ever felt. Quality can be established by having the patient complete the McGill Pain Questionnaire indicating which words best describe their pain. Local anesthetic injections into the nerves or sensitive areas of the stump may relieve pain for days, weeks, or sometimes permanently, despite the drug wearing off in a matter of hours; and small injections of hypertonic saline into the soft tissue between vertebrae produces local pain that radiates into the phantom limb for ten minutes or so and may be followed by hours, weeks or even longer of partial or total relief from phantom pain. Vigorous vibration or electrical stimulation of the stump, or current from electrodes surgically implanted onto the spinal cord, all produce relief in some patients. Differences in pain perception and tolerance thresholds are associated with, among other factors, ethnicity, genetics, and sex. People of Mediterranean origin report as painful some radiant heat intensities that northern Europeans describe as nonpainful. And Italian women tolerate less intense electric shock than Jewish or Native American women. Some individuals in all cultures have significantly higher than normal pain perception and tolerance thresholds. For instance, patients who experience painless heart attacks have higher pain thresholds for electric shock, muscle cramp and heat. The International Association for the Study of Pain advocates that the relief of pain should be recognized as a human right, that chronic pain should be considered a disease in its own right, and that pain medicine should have the full status of a specialty. It is a specialty only in China and Australia at this time. Elsewhere, pain medicine is a subspecialty under disciplines such as anesthesiology, physiatry, neurology, palliative medicine and psychiatry. In 2011, Human Rights Watch alerted that tens of millions of people worldwide are still denied access to inexpensive medications for severe pain. When a person is non-verbal and cannot self-report pain, observation becomes critical, and specific behaviors can be monitored as pain indicators. Behaviors such as facial grimacing and guarding indicate pain, as well as an increase or decrease in vocalizations, changes in routine behavior patterns and mental status changes. Patients experiencing pain may exhibit withdrawn social behavior and possibly experience a decreased appetite and decreased nutritional intake. A change in condition that deviates from baseline such as moaning with movement or when manipulating a body part, and limited range of motion are also potential pain indicators. In patients who possess language but are incapable of expressing themselves effectively, such as those with dementia, an increase in confusion or display of aggressive behaviors or agitation may signal that discomfort exists, and further assessment is necessary. The prevalence of phantom pain in upper limb amputees is nearly 82%, and in lower limb amputees is 54%. One study found that eight days after amputation, 72 percent of patients had phantom limb pain, and six months later, 65 percent reported it. Some amputees experience continuous pain that varies in intensity or quality; others experience several bouts a day, or it may occur only once every week or two. It is often described as shooting, crushing, burning or cramping. If the pain is continuous for a long period, parts of the intact body may become sensitized, so that touching them evokes pain in the phantom limb, or phantom limb pain may accompany urination or defecation. A number of meta-analyses have found clinical hypnosis to be effective in controlling pain associated with diagnostic and surgical procedures in both adults and children, as well as pain associated with cancer and childbirth. A 2007 review of 13 studies found evidence for the efficacy of hypnosis in the reduction of chronic pain in some conditions, though the number of patients enrolled in the studies was low, bringing up issues of power to detect group differences, and most lacked credible controls for placebo and/or expectation. The authors concluded that ""although the findings provide support for the general applicability of hypnosis in the treatment of chronic pain, considerably more research will be needed to fully determine the effects of hypnosis for different chronic-pain conditions."" The most reliable method for assessing pain in most humans is by asking a question: a person may report pain that cannot be detected by any known physiological measure. However, like infants (Latin infans meaning ""unable to speak""), animals cannot answer questions about whether they feel pain; thus the defining criterion for pain in humans cannot be applied to them. Philosophers and scientists have responded to this difficulty in a variety of ways. René Descartes for example argued that animals lack consciousness and therefore do not experience pain and suffering in the way that humans do. Bernard Rollin of Colorado State University, the principal author of two U.S. federal laws regulating pain relief for animals, writes that researchers remained unsure into the 1980s as to whether animals experience pain, and that veterinarians trained in the U.S. before 1989 were simply taught to ignore animal pain. In his interactions with scientists and other veterinarians, he was regularly asked to ""prove"" that animals are conscious, and to provide ""scientifically acceptable"" grounds for claiming that they feel pain. Carbone writes that the view that animals feel pain differently is now a minority view. Academic reviews of the topic are more equivocal, noting that although the argument that animals have at least simple conscious thoughts and feelings has strong support, some critics continue to question how reliably animal mental states can be determined. The ability of invertebrate species of animals, such as insects, to feel pain and suffering is also unclear. Pain is a distressing feeling often caused by intense or damaging stimuli, such as stubbing a toe, burning a finger, putting alcohol on a cut, and bumping the ""funny bone"". Because it is a complex, subjective phenomenon, defining pain has been a challenge. The International Association for the Study of Pain's widely used definition states: ""Pain is an unpleasant sensory and emotional experience associated with actual or potential tissue damage, or described in terms of such damage."" In medical diagnosis, pain is a symptom. In his book, The Greatest Show on Earth: The Evidence for Evolution, biologist Richard Dawkins grapples with the question of why pain has to be so very painful. He describes the alternative as a simple, mental raising of a ""red flag"". To argue why that red flag might be insufficient, Dawkins explains that drives must compete with each other within living beings. The most fit creature would be the one whose pains are well balanced. Those pains which mean certain death when ignored will become the most powerfully felt. The relative intensities of pain, then, may resemble the relative importance of that risk to our ancestors (lack of food, too much cold, or serious injuries are felt as agony, whereas minor damage is felt as mere discomfort). This resemblance will not be perfect, however, because natural selection can be a poor designer. The result is often glitches in animals, including supernormal stimuli. Such glitches help explain pains which are not, or at least no longer directly adaptive (e.g. perhaps some forms of toothache, or injury to fingernails). In 1994, responding to the need for a more useful system for describing chronic pain, the International Association for the Study of Pain (IASP) classified pain according to specific characteristics: (1) region of the body involved (e.g. abdomen, lower limbs), (2) system whose dysfunction may be causing the pain (e.g., nervous, gastrointestinal), (3) duration and pattern of occurrence, (4) intensity and time since onset, and (5) etiology. However, this system has been criticized by Clifford J. Woolf and others as inadequate for guiding research and treatment. Woolf suggests three classes of pain : (1) nociceptive pain, (2) inflammatory pain which is associated with tissue damage and the infiltration of immune cells, and (3) pathological pain which is a disease state caused by damage to the nervous system or by its abnormal function (e.g. fibromyalgia, irritable bowel syndrome, tension type headache, etc.). Breakthrough pain is transitory acute pain that comes on suddenly and is not alleviated by the patient's normal pain management. It is common in cancer patients who often have background pain that is generally well-controlled by medications, but who also sometimes experience bouts of severe pain that from time to time ""breaks through"" the medication. The characteristics of breakthrough cancer pain vary from person to person and according to the cause. Management of breakthrough pain can entail intensive use of opioids, including fentanyl. Although unpleasantness is an essential part of the IASP definition of pain, it is possible to induce a state described as intense pain devoid of unpleasantness in some patients, with morphine injection or psychosurgery. Such patients report that they have pain but are not bothered by it; they recognize the sensation of pain but suffer little, or not at all. Indifference to pain can also rarely be present from birth; these people have normal nerves on medical investigations, and find pain unpleasant, but do not avoid repetition of the pain stimulus. Nociceptive pain may also be divided into ""visceral"", ""deep somatic"" and ""superficial somatic"" pain. Visceral structures are highly sensitive to stretch, ischemia and inflammation, but relatively insensitive to other stimuli that normally evoke pain in other structures, such as burning and cutting. Visceral pain is diffuse, difficult to locate and often referred to a distant, usually superficial, structure. It may be accompanied by nausea and vomiting and may be described as sickening, deep, squeezing, and dull. Deep somatic pain is initiated by stimulation of nociceptors in ligaments, tendons, bones, blood vessels, fasciae and muscles, and is dull, aching, poorly-localized pain. Examples include sprains and broken bones. Superficial pain is initiated by activation of nociceptors in the skin or other superficial tissue, and is sharp, well-defined and clearly located. Examples of injuries that produce superficial somatic pain include minor wounds and minor (first degree) burns.  Sugar taken orally reduces the total crying time but not the duration of the first cry in newborns undergoing a painful procedure (a single lancing of the heel). It does not moderate the effect of pain on heart rate and a recent single study found that sugar did not significantly affect pain-related electrical activity in the brains of newborns one second after the heel lance procedure. Sweet oral liquid moderately reduces the incidence and duration of crying caused by immunization injection in children between one and twelve months of age. In 1968 Ronald Melzack and Kenneth Casey described pain in terms of its three dimensions: ""sensory-discriminative"" (sense of the intensity, location, quality and duration of the pain), ""affective-motivational"" (unpleasantness and urge to escape the unpleasantness), and ""cognitive-evaluative"" (cognitions such as appraisal, cultural values, distraction and hypnotic suggestion). They theorized that pain intensity (the sensory discriminative dimension) and unpleasantness (the affective-motivational dimension) are not simply determined by the magnitude of the painful stimulus, but ""higher"" cognitive activities can influence perceived intensity and unpleasantness. Cognitive activities ""may affect both sensory and affective experience or they may modify primarily the affective-motivational dimension. Thus, excitement in games or war appears to block both dimensions of pain, while suggestion and placebos may modulate the affective-motivational dimension and leave the sensory-discriminative dimension relatively undisturbed."" (p. 432) The paper ends with a call to action: ""Pain can be treated not only by trying to cut down the sensory input by anesthetic block, surgical intervention and the like, but also by influencing the motivational-affective and cognitive factors as well."" (p. 435) People with long-term pain frequently display psychological disturbance, with elevated scores on the Minnesota Multiphasic Personality Inventory scales of hysteria, depression and hypochondriasis (the ""neurotic triad""). Some investigators have argued that it is this neuroticism that causes acute pain to turn chronic, but clinical evidence points the other way, to chronic pain causing neuroticism. When long-term pain is relieved by therapeutic intervention, scores on the neurotic triad and anxiety fall, often to normal levels. Self-esteem, often low in chronic pain patients, also shows improvement once pain has resolved. Physical pain is an important political topic in relation to various issues, including pain management policy, drug control, animal rights or animal welfare, torture, and pain compliance. In various contexts, the deliberate infliction of pain in the form of corporal punishment is used as retribution for an offence, or for the purpose of disciplining or reforming a wrongdoer, or to deter attitudes or behaviour deemed unacceptable. In some cultures, extreme practices such as mortification of the flesh or painful rites of passage are highly regarded. Wilhelm Erb's (1874) ""intensive"" theory, that a pain signal can be generated by intense enough stimulation of any sensory receptor, has been soundly disproved. Some sensory fibers do not differentiate between noxious and non-noxious stimuli, while others, nociceptors, respond only to noxious, high intensity stimuli. At the peripheral end of the nociceptor, noxious stimuli generate currents that, above a given threshold, begin to send signals along the nerve fiber to the spinal cord. The ""specificity"" (whether it responds to thermal, chemical or mechanical features of its environment) of a nociceptor is determined by which ion channels it expresses at its peripheral end. Dozens of different types of nociceptor ion channels have so far been identified, and their exact functions are still being determined. In 1644, René Descartes theorized that pain was a disturbance that passed down along nerve fibers until the disturbance reached the brain, a development that transformed the perception of pain from a spiritual, mystical experience to a physical, mechanical sensation[citation needed]. Descartes's work, along with Avicenna's, prefigured the 19th-century development of specificity theory. Specificity theory saw pain as ""a specific sensation, with its own sensory apparatus independent of touch and other senses"". Another theory that came to prominence in the 18th and 19th centuries was intensive theory, which conceived of pain not as a unique sensory modality, but an emotional state produced by stronger than normal stimuli such as intense light, pressure or temperature. By the mid-1890s, specificity was backed mostly by physiologists and physicians, and the intensive theory was mostly backed by psychologists. However, after a series of clinical observations by Henry Head and experiments by Max von Frey, the psychologists migrated to specificity almost en masse, and by century's end, most textbooks on physiology and psychology were presenting pain specificity as fact. The experience of pain has many cultural dimensions. For instance, the way in which one experiences and responds to pain is related to sociocultural characteristics, such as gender, ethnicity, and age. An aging adult may not respond to pain in the way that a younger person would. Their ability to recognize pain may be blunted by illness or the use of multiple prescription drugs. Depression may also keep the older adult from reporting they are in pain. The older adult may also quit doing activities they love because it hurts too much. Decline in self-care activities (dressing, grooming, walking, etc.) may also be indicators that the older adult is experiencing pain. The older adult may refrain from reporting pain because they are afraid they will have to have surgery or will be put on a drug they might become addicted to. They may not want others to see them as weak, or may feel there is something impolite or shameful in complaining about pain, or they may feel the pain is deserved punishment for past transgressions. The presence of pain in an animal cannot be known for certain, but it can be inferred through physical and behavioral reactions. Specialists currently believe that all vertebrates can feel pain, and that certain invertebrates, like the octopus, might too. As for other animals, plants, or other entities, their ability to feel physical pain is at present a question beyond scientific reach, since no mechanism is known by which they could have such a feeling. In particular, there are no known nociceptors in groups such as plants, fungi, and most insects, except for instance in fruit flies. Pain is the most common reason for people to use complementary and alternative medicine. An analysis of the 13 highest quality studies of pain treatment with acupuncture, published in January 2009, concluded there is little difference in the effect of real, sham and no acupuncture. However other reviews have found benefit. Additionally, there is tentative evidence for a few herbal medicine. There is interest in the relationship between vitamin D and pain, but the evidence so far from controlled trials for such a relationship, other than in osteomalacia, is unconvincing. Nociceptive pain is caused by stimulation of peripheral nerve fibers that respond to stimuli approaching or exceeding harmful intensity (nociceptors), and may be classified according to the mode of noxious stimulation. The most common categories are ""thermal"" (e.g. heat or cold), ""mechanical"" (e.g. crushing, tearing, shearing, etc.) and ""chemical"" (e.g. iodine in a cut or chemicals released during inflammation). Some nociceptors respond to more than one of these modalities and are consequently designated polymodal."
Oxygen,"Reactive oxygen species, such as superoxide ion (O−
2) and hydrogen peroxide (H
2O
2), are dangerous by-products of oxygen use in organisms. Parts of the immune system of higher organisms create peroxide, superoxide, and singlet oxygen to destroy invading microbes. Reactive oxygen species also play an important role in the hypersensitive response of plants against pathogen attack. Oxygen is toxic to obligately anaerobic organisms, which were the dominant form of early life on Earth until O
2 began to accumulate in the atmosphere about 2.5 billion years ago during the Great Oxygenation Event, about a billion years after the first appearance of these organisms. Oxygen toxicity to the lungs and central nervous system can also occur in deep scuba diving and surface supplied diving. Prolonged breathing of an air mixture with an O
2 partial pressure more than 60 kPa can eventually lead to permanent pulmonary fibrosis. Exposure to a O
2 partial pressures greater than 160 kPa (about 1.6 atm) may lead to convulsions (normally fatal for divers). Acute oxygen toxicity (causing seizures, its most feared effect for divers) can occur by breathing an air mixture with 21% O
2 at 66 m or more of depth; the same thing can occur by breathing 100% O
2 at only 6 m. Many major classes of organic molecules in living organisms, such as proteins, nucleic acids, carbohydrates, and fats, contain oxygen, as do the major inorganic compounds that are constituents of animal shells, teeth, and bone. Most of the mass of living organisms is oxygen as it is a part of water, the major constituent of lifeforms. Oxygen is used in cellular respiration and released by photosynthesis, which uses the energy of sunlight to produce oxygen from water. It is too chemically reactive to remain a free element in air without being continuously replenished by the photosynthetic action of living organisms. Another form (allotrope) of oxygen, ozone (O
3), strongly absorbs UVB radiation and consequently the high-altitude ozone layer helps protect the biosphere from ultraviolet radiation, but is a pollutant near the surface where it is a by-product of smog. At even higher low earth orbit altitudes, sufficient atomic oxygen is present to cause erosion for spacecraft. Oxygen, as a supposed mild euphoric, has a history of recreational use in oxygen bars and in sports. Oxygen bars are establishments, found in Japan, California, and Las Vegas, Nevada since the late 1990s that offer higher than normal O
2 exposure for a fee. Professional athletes, especially in American football, also sometimes go off field between plays to wear oxygen masks in order to get a ""boost"" in performance. The pharmacological effect is doubtful; a placebo effect is a more likely explanation. Available studies support a performance boost from enriched O
2 mixtures only if they are breathed during aerobic exercise. Among the most important classes of organic compounds that contain oxygen are (where ""R"" is an organic group): alcohols (R-OH); ethers (R-O-R); ketones (R-CO-R); aldehydes (R-CO-H); carboxylic acids (R-COOH); esters (R-COO-R); acid anhydrides (R-CO-O-CO-R); and amides (R-C(O)-NR
2). There are many important organic solvents that contain oxygen, including: acetone, methanol, ethanol, isopropanol, furan, THF, diethyl ether, dioxane, ethyl acetate, DMF, DMSO, acetic acid, and formic acid. Acetone ((CH
3)
2CO) and phenol (C
6H
5OH) are used as feeder materials in the synthesis of many different substances. Other important organic compounds that contain oxygen are: glycerol, formaldehyde, glutaraldehyde, citric acid, acetic anhydride, and acetamide. Epoxides are ethers in which the oxygen atom is part of a ring of three atoms. In this dioxygen, the two oxygen atoms are chemically bonded to each other. The bond can be variously described based on level of theory, but is reasonably and simply described as a covalent double bond that results from the filling of molecular orbitals formed from the atomic orbitals of the individual oxygen atoms, the filling of which results in a bond order of two. More specifically, the double bond is the result of sequential, low-to-high energy, or Aufbau, filling of orbitals, and the resulting cancellation of contributions from the 2s electrons, after sequential filling of the low σ and σ* orbitals; σ overlap of the two atomic 2p orbitals that lie along the O-O molecular axis and π overlap of two pairs of atomic 2p orbitals perpendicular to the O-O molecular axis, and then cancellation of contributions from the remaining two of the six 2p electrons after their partial filling of the lowest π and π* orbitals. The element is found in almost all biomolecules that are important to (or generated by) life. Only a few common complex biomolecules, such as squalene and the carotenes, contain no oxygen. Of the organic compounds with biological relevance, carbohydrates contain the largest proportion by mass of oxygen. All fats, fatty acids, amino acids, and proteins contain oxygen (due to the presence of carbonyl groups in these acids and their ester residues). Oxygen also occurs in phosphate (PO3−
4) groups in the biologically important energy-carrying molecules ATP and ADP, in the backbone and the purines (except adenine) and pyrimidines of RNA and DNA, and in bones as calcium phosphate and hydroxylapatite. Hyperbaric (high-pressure) medicine uses special oxygen chambers to increase the partial pressure of O
2 around the patient and, when needed, the medical staff. Carbon monoxide poisoning, gas gangrene, and decompression sickness (the 'bends') are sometimes treated using these devices. Increased O
2 concentration in the lungs helps to displace carbon monoxide from the heme group of hemoglobin. Oxygen gas is poisonous to the anaerobic bacteria that cause gas gangrene, so increasing its partial pressure helps kill them. Decompression sickness occurs in divers who decompress too quickly after a dive, resulting in bubbles of inert gas, mostly nitrogen and helium, forming in their blood. Increasing the pressure of O
2 as soon as possible is part of the treatment. Free oxygen gas was almost nonexistent in Earth's atmosphere before photosynthetic archaea and bacteria evolved, probably about 3.5 billion years ago. Free oxygen first appeared in significant quantities during the Paleoproterozoic eon (between 3.0 and 2.3 billion years ago). For the first billion years, any free oxygen produced by these organisms combined with dissolved iron in the oceans to form banded iron formations. When such oxygen sinks became saturated, free oxygen began to outgas from the oceans 3–2.7 billion years ago, reaching 10% of its present level around 1.7 billion years ago. John Dalton's original atomic hypothesis assumed that all elements were monatomic and that the atoms in compounds would normally have the simplest atomic ratios with respect to one another. For example, Dalton assumed that water's formula was HO, giving the atomic mass of oxygen as 8 times that of hydrogen, instead of the modern value of about 16. In 1805, Joseph Louis Gay-Lussac and Alexander von Humboldt showed that water is formed of two volumes of hydrogen and one volume of oxygen; and by 1811 Amedeo Avogadro had arrived at the correct interpretation of water's composition, based on what is now called Avogadro's law and the assumption of diatomic elemental molecules.[a] Oxygen gas (O
2) can be toxic at elevated partial pressures, leading to convulsions and other health problems.[j] Oxygen toxicity usually begins to occur at partial pressures more than 50 kilopascals (kPa), equal to about 50% oxygen composition at standard pressure or 2.5 times the normal sea-level O
2 partial pressure of about 21 kPa. This is not a problem except for patients on mechanical ventilators, since gas supplied through oxygen masks in medical applications is typically composed of only 30%–50% O
2 by volume (about 30 kPa at standard pressure). (although this figure also is subject to wide variation, depending on type of mask). Oxygen is a chemical element with symbol O and atomic number 8. It is a member of the chalcogen group on the periodic table and is a highly reactive nonmetal and oxidizing agent that readily forms compounds (notably oxides) with most elements. By mass, oxygen is the third-most abundant element in the universe, after hydrogen and helium. At standard temperature and pressure, two atoms of the element bind to form dioxygen, a colorless and odorless diatomic gas with the formula O
2. Diatomic oxygen gas constitutes 20.8% of the Earth's atmosphere. However, monitoring of atmospheric oxygen levels show a global downward trend, because of fossil-fuel burning. Oxygen is the most abundant element by mass in the Earth's crust as part of oxide compounds such as silicon dioxide, making up almost half of the crust's mass. Oxygen is present in the atmosphere in trace quantities in the form of carbon dioxide (CO
2). The Earth's crustal rock is composed in large part of oxides of silicon (silica SiO
2, as found in granite and quartz), aluminium (aluminium oxide Al
2O
3, in bauxite and corundum), iron (iron(III) oxide Fe
2O
3, in hematite and rust), and calcium carbonate (in limestone). The rest of the Earth's crust is also made of oxygen compounds, in particular various complex silicates (in silicate minerals). The Earth's mantle, of much larger mass than the crust, is largely composed of silicates of magnesium and iron. The other major method of producing O
2 gas involves passing a stream of clean, dry air through one bed of a pair of identical zeolite molecular sieves, which absorbs the nitrogen and delivers a gas stream that is 90% to 93% O
2. Simultaneously, nitrogen gas is released from the other nitrogen-saturated zeolite bed, by reducing the chamber operating pressure and diverting part of the oxygen gas from the producer bed through it, in the reverse direction of flow. After a set cycle time the operation of the two beds is interchanged, thereby allowing for a continuous supply of gaseous oxygen to be pumped through a pipeline. This is known as pressure swing adsorption. Oxygen gas is increasingly obtained by these non-cryogenic technologies (see also the related vacuum swing adsorption). In 1891 Scottish chemist James Dewar was able to produce enough liquid oxygen to study. The first commercially viable process for producing liquid oxygen was independently developed in 1895 by German engineer Carl von Linde and British engineer William Hampson. Both men lowered the temperature of air until it liquefied and then distilled the component gases by boiling them off one at a time and capturing them. Later, in 1901, oxyacetylene welding was demonstrated for the first time by burning a mixture of acetylene and compressed O
2. This method of welding and cutting metal later became common. Singlet oxygen is a name given to several higher-energy species of molecular O
2 in which all the electron spins are paired. It is much more reactive towards common organic molecules than is molecular oxygen per se. In nature, singlet oxygen is commonly formed from water during photosynthesis, using the energy of sunlight. It is also produced in the troposphere by the photolysis of ozone by light of short wavelength, and by the immune system as a source of active oxygen. Carotenoids in photosynthetic organisms (and possibly also in animals) play a major role in absorbing energy from singlet oxygen and converting it to the unexcited ground state before it can cause harm to tissues. The unusually high concentration of oxygen gas on Earth is the result of the oxygen cycle. This biogeochemical cycle describes the movement of oxygen within and between its three main reservoirs on Earth: the atmosphere, the biosphere, and the lithosphere. The main driving factor of the oxygen cycle is photosynthesis, which is responsible for modern Earth's atmosphere. Photosynthesis releases oxygen into the atmosphere, while respiration and decay remove it from the atmosphere. In the present equilibrium, production and consumption occur at the same rate of roughly 1/2000th of the entire atmospheric oxygen per year. In the late 17th century, Robert Boyle proved that air is necessary for combustion. English chemist John Mayow (1641–1679) refined this work by showing that fire requires only a part of air that he called spiritus nitroaereus or just nitroaereus. In one experiment he found that placing either a mouse or a lit candle in a closed container over water caused the water to rise and replace one-fourteenth of the air's volume before extinguishing the subjects. From this he surmised that nitroaereus is consumed in both respiration and combustion. People who climb mountains or fly in non-pressurized fixed-wing aircraft sometimes have supplemental O
2 supplies.[h] Passengers traveling in (pressurized) commercial airplanes have an emergency supply of O
2 automatically supplied to them in case of cabin depressurization. Sudden cabin pressure loss activates chemical oxygen generators above each seat, causing oxygen masks to drop. Pulling on the masks ""to start the flow of oxygen"" as cabin safety instructions dictate, forces iron filings into the sodium chlorate inside the canister. A steady stream of oxygen gas is then produced by the exothermic reaction. Free oxygen also occurs in solution in the world's water bodies. The increased solubility of O
2 at lower temperatures (see Physical properties) has important implications for ocean life, as polar oceans support a much higher density of life due to their higher oxygen content. Water polluted with plant nutrients such as nitrates or phosphates may stimulate growth of algae by a process called eutrophication and the decay of these organisms and other biomaterials may reduce amounts of O
2 in eutrophic water bodies. Scientists assess this aspect of water quality by measuring the water's biochemical oxygen demand, or the amount of O
2 needed to restore it to a normal concentration. This combination of cancellations and σ and π overlaps results in dioxygen's double bond character and reactivity, and a triplet electronic ground state. An electron configuration with two unpaired electrons as found in dioxygen (see the filled π* orbitals in the diagram), orbitals that are of equal energy—i.e., degenerate—is a configuration termed a spin triplet state. Hence, the ground state of the O
2 molecule is referred to as triplet oxygen.[b] The highest energy, partially filled orbitals are antibonding, and so their filling weakens the bond order from three to two. Because of its unpaired electrons, triplet oxygen reacts only slowly with most organic molecules, which have paired electron spins; this prevents spontaneous combustion. In the triplet form, O
2 molecules are paramagnetic. That is, they impart magnetic character to oxygen when it is in the presence of a magnetic field, because of the spin magnetic moments of the unpaired electrons in the molecule, and the negative exchange energy between neighboring O
2 molecules. Liquid oxygen is attracted to a magnet to a sufficient extent that, in laboratory demonstrations, a bridge of liquid oxygen may be supported against its own weight between the poles of a powerful magnet.[c] Uptake of O
2 from the air is the essential purpose of respiration, so oxygen supplementation is used in medicine. Treatment not only increases oxygen levels in the patient's blood, but has the secondary effect of decreasing resistance to blood flow in many types of diseased lungs, easing work load on the heart. Oxygen therapy is used to treat emphysema, pneumonia, some heart disorders (congestive heart failure), some disorders that cause increased pulmonary artery pressure, and any disease that impairs the body's ability to take up and use gaseous oxygen. Oxygen storage methods include high pressure oxygen tanks, cryogenics and chemical compounds. For reasons of economy, oxygen is often transported in bulk as a liquid in specially insulated tankers, since one liter of liquefied oxygen is equivalent to 840 liters of gaseous oxygen at atmospheric pressure and 20 °C (68 °F). Such tankers are used to refill bulk liquid oxygen storage containers, which stand outside hospitals and other institutions with a need for large volumes of pure oxygen gas. Liquid oxygen is passed through heat exchangers, which convert the cryogenic liquid into gas before it enters the building. Oxygen is also stored and shipped in smaller cylinders containing the compressed gas; a form that is useful in certain portable medical applications and oxy-fuel welding and cutting. Oxygen was discovered independently by Carl Wilhelm Scheele, in Uppsala, in 1773 or earlier, and Joseph Priestley in Wiltshire, in 1774, but Priestley is often given priority because his work was published first. The name oxygen was coined in 1777 by Antoine Lavoisier, whose experiments with oxygen helped to discredit the then-popular phlogiston theory of combustion and corrosion. Its name derives from the Greek roots ὀξύς oxys, ""acid"", literally ""sharp"", referring to the sour taste of acids and -γενής -genes, ""producer"", literally ""begetter"", because at the time of naming, it was mistakenly thought that all acids required oxygen in their composition. Common uses of oxygen includes the production cycle of steel, plastics and textiles, brazing, welding and cutting of steels and other metals, rocket propellant, in oxygen therapy and life support systems in aircraft, submarines, spaceflight and diving. In the meantime, on August 1, 1774, an experiment conducted by the British clergyman Joseph Priestley focused sunlight on mercuric oxide (HgO) inside a glass tube, which liberated a gas he named ""dephlogisticated air"". He noted that candles burned brighter in the gas and that a mouse was more active and lived longer while breathing it. After breathing the gas himself, he wrote: ""The feeling of it to my lungs was not sensibly different from that of common air, but I fancied that my breast felt peculiarly light and easy for some time afterwards."" Priestley published his findings in 1775 in a paper titled ""An Account of Further Discoveries in Air"" which was included in the second volume of his book titled Experiments and Observations on Different Kinds of Air. Because he published his findings first, Priestley is usually given priority in the discovery. Highly combustible materials that leave little residue, such as wood or coal, were thought to be made mostly of phlogiston; whereas non-combustible substances that corrode, such as iron, contained very little. Air did not play a role in phlogiston theory, nor were any initial quantitative experiments conducted to test the idea; instead, it was based on observations of what happens when something burns, that most common objects appear to become lighter and seem to lose something in the process. The fact that a substance like wood gains overall weight in burning was hidden by the buoyancy of the gaseous combustion products. Indeed, one of the first clues that the phlogiston theory was incorrect was that metals, too, gain weight in rusting (when they were supposedly losing phlogiston). Trioxygen (O
3) is usually known as ozone and is a very reactive allotrope of oxygen that is damaging to lung tissue. Ozone is produced in the upper atmosphere when O
2 combines with atomic oxygen made by the splitting of O
2 by ultraviolet (UV) radiation. Since ozone absorbs strongly in the UV region of the spectrum, the ozone layer of the upper atmosphere functions as a protective radiation shield for the planet. Near the Earth's surface, it is a pollutant formed as a by-product of automobile exhaust. The metastable molecule tetraoxygen (O
4) was discovered in 2001, and was assumed to exist in one of the six phases of solid oxygen. It was proven in 2006 that this phase, created by pressurizing O
2 to 20 GPa, is in fact a rhombohedral O
8 cluster. This cluster has the potential to be a much more powerful oxidizer than either O
2 or O
3 and may therefore be used in rocket fuel. A metallic phase was discovered in 1990 when solid oxygen is subjected to a pressure of above 96 GPa and it was shown in 1998 that at very low temperatures, this phase becomes superconducting. Oxygen is the most abundant chemical element by mass in the Earth's biosphere, air, sea and land. Oxygen is the third most abundant chemical element in the universe, after hydrogen and helium. About 0.9% of the Sun's mass is oxygen. Oxygen constitutes 49.2% of the Earth's crust by mass and is the major component of the world's oceans (88.8% by mass). Oxygen gas is the second most common component of the Earth's atmosphere, taking up 20.8% of its volume and 23.1% of its mass (some 1015 tonnes).[d] Earth is unusual among the planets of the Solar System in having such a high concentration of oxygen gas in its atmosphere: Mars (with 0.1% O
2 by volume) and Venus have far lower concentrations. The O
2 surrounding these other planets is produced solely by ultraviolet radiation impacting oxygen-containing molecules such as carbon dioxide. Paleoclimatologists measure the ratio of oxygen-18 and oxygen-16 in the shells and skeletons of marine organisms to determine what the climate was like millions of years ago (see oxygen isotope ratio cycle). Seawater molecules that contain the lighter isotope, oxygen-16, evaporate at a slightly faster rate than water molecules containing the 12% heavier oxygen-18; this disparity increases at lower temperatures. During periods of lower global temperatures, snow and rain from that evaporated water tends to be higher in oxygen-16, and the seawater left behind tends to be higher in oxygen-18. Marine organisms then incorporate more oxygen-18 into their skeletons and shells than they would in a warmer climate. Paleoclimatologists also directly measure this ratio in the water molecules of ice core samples that are up to several hundreds of thousands of years old. Oxygen presents two spectrophotometric absorption bands peaking at the wavelengths 687 and 760 nm. Some remote sensing scientists have proposed using the measurement of the radiance coming from vegetation canopies in those bands to characterize plant health status from a satellite platform. This approach exploits the fact that in those bands it is possible to discriminate the vegetation's reflectance from its fluorescence, which is much weaker. The measurement is technically difficult owing to the low signal-to-noise ratio and the physical structure of vegetation; but it has been proposed as a possible method of monitoring the carbon cycle from satellites on a global scale. Breathing pure O
2 in space applications, such as in some modern space suits, or in early spacecraft such as Apollo, causes no damage due to the low total pressures used. In the case of spacesuits, the O
2 partial pressure in the breathing gas is, in general, about 30 kPa (1.4 times normal), and the resulting O
2 partial pressure in the astronaut's arterial blood is only marginally more than normal sea-level O
2 partial pressure (for more information on this, see space suit and arterial blood gas). The common allotrope of elemental oxygen on Earth is called dioxygen, O
2. It is the form that is a major part of the Earth's atmosphere (see Occurrence). O2 has a bond length of 121 pm and a bond energy of 498 kJ·mol−1, which is smaller than the energy of other double bonds or pairs of single bonds in the biosphere and responsible for the exothermic reaction of O2 with any organic molecule. Due to its energy content, O2 is used by complex forms of life, such as animals, in cellular respiration (see Biological role). Other aspects of O
2 are covered in the remainder of this article. Concentrated O
2 will allow combustion to proceed rapidly and energetically. Steel pipes and storage vessels used to store and transmit both gaseous and liquid oxygen will act as a fuel; and therefore the design and manufacture of O
2 systems requires special training to ensure that ignition sources are minimized. The fire that killed the Apollo 1 crew in a launch pad test spread so rapidly because the capsule was pressurized with pure O
2 but at slightly more than atmospheric pressure, instead of the 1⁄3 normal pressure that would be used in a mission.[k] Due to its electronegativity, oxygen forms chemical bonds with almost all other elements to give corresponding oxides. The surface of most metals, such as aluminium and titanium, are oxidized in the presence of air and become coated with a thin film of oxide that passivates the metal and slows further corrosion. Many oxides of the transition metals are non-stoichiometric compounds, with slightly less metal than the chemical formula would show. For example, the mineral FeO (wüstite) is written as Fe
1 − xO, where x is usually around 0.05. Oxygen is more soluble in water than nitrogen is. Water in equilibrium with air contains approximately 1 molecule of dissolved O
2 for every 2 molecules of N
2, compared to an atmospheric ratio of approximately 1:4. The solubility of oxygen in water is temperature-dependent, and about twice as much (14.6 mg·L−1) dissolves at 0 °C than at 20 °C (7.6 mg·L−1). At 25 °C and 1 standard atmosphere (101.3 kPa) of air, freshwater contains about 6.04 milliliters (mL) of oxygen per liter, whereas seawater contains about 4.95 mL per liter. At 5 °C the solubility increases to 9.0 mL (50% more than at 25 °C) per liter for water and 7.2 mL (45% more) per liter for sea water. Oxygen gas can also be produced through electrolysis of water into molecular oxygen and hydrogen. DC electricity must be used: if AC is used, the gases in each limb consist of hydrogen and oxygen in the explosive ratio 2:1. Contrary to popular belief, the 2:1 ratio observed in the DC electrolysis of acidified water does not prove that the empirical formula of water is H2O unless certain assumptions are made about the molecular formulae of hydrogen and oxygen themselves. A similar method is the electrocatalytic O
2 evolution from oxides and oxoacids. Chemical catalysts can be used as well, such as in chemical oxygen generators or oxygen candles that are used as part of the life-support equipment on submarines, and are still part of standard equipment on commercial airliners in case of depressurization emergencies. Another air separation technology involves forcing air to dissolve through ceramic membranes based on zirconium dioxide by either high pressure or an electric current, to produce nearly pure O
2 gas. One of the first known experiments on the relationship between combustion and air was conducted by the 2nd century BCE Greek writer on mechanics, Philo of Byzantium. In his work Pneumatica, Philo observed that inverting a vessel over a burning candle and surrounding the vessel's neck with water resulted in some water rising into the neck. Philo incorrectly surmised that parts of the air in the vessel were converted into the classical element fire and thus were able to escape through pores in the glass. Many centuries later Leonardo da Vinci built on Philo's work by observing that a portion of air is consumed during combustion and respiration. In one experiment, Lavoisier observed that there was no overall increase in weight when tin and air were heated in a closed container. He noted that air rushed in when he opened the container, which indicated that part of the trapped air had been consumed. He also noted that the tin had increased in weight and that increase was the same as the weight of the air that rushed back in. This and other experiments on combustion were documented in his book Sur la combustion en général, which was published in 1777. In that work, he proved that air is a mixture of two gases; 'vital air', which is essential to combustion and respiration, and azote (Gk. ἄζωτον ""lifeless""), which did not support either. Azote later became nitrogen in English, although it has kept the name in French and several other European languages. Planetary geologists have measured different abundances of oxygen isotopes in samples from the Earth, the Moon, Mars, and meteorites, but were long unable to obtain reference values for the isotope ratios in the Sun, believed to be the same as those of the primordial solar nebula. Analysis of a silicon wafer exposed to the solar wind in space and returned by the crashed Genesis spacecraft has shown that the Sun has a higher proportion of oxygen-16 than does the Earth. The measurement implies that an unknown process depleted oxygen-16 from the Sun's disk of protoplanetary material prior to the coalescence of dust grains that formed the Earth. By the late 19th century scientists realized that air could be liquefied, and its components isolated, by compressing and cooling it. Using a cascade method, Swiss chemist and physicist Raoul Pierre Pictet evaporated liquid sulfur dioxide in order to liquefy carbon dioxide, which in turn was evaporated to cool oxygen gas enough to liquefy it. He sent a telegram on December 22, 1877 to the French Academy of Sciences in Paris announcing his discovery of liquid oxygen. Just two days later, French physicist Louis Paul Cailletet announced his own method of liquefying molecular oxygen. Only a few drops of the liquid were produced in either case so no meaningful analysis could be conducted. Oxygen was liquified in stable state for the first time on March 29, 1883 by Polish scientists from Jagiellonian University, Zygmunt Wróblewski and Karol Olszewski. Highly concentrated sources of oxygen promote rapid combustion. Fire and explosion hazards exist when concentrated oxidants and fuels are brought into close proximity; an ignition event, such as heat or a spark, is needed to trigger combustion. Oxygen is the oxidant, not the fuel, but nevertheless the source of most of the chemical energy released in combustion. Combustion hazards also apply to compounds of oxygen with a high oxidative potential, such as peroxides, chlorates, nitrates, perchlorates, and dichromates because they can donate oxygen to a fire. Oxygen condenses at 90.20 K (−182.95 °C, −297.31 °F), and freezes at 54.36 K (−218.79 °C, −361.82 °F). Both liquid and solid O
2 are clear substances with a light sky-blue color caused by absorption in the red (in contrast with the blue color of the sky, which is due to Rayleigh scattering of blue light). High-purity liquid O
2 is usually obtained by the fractional distillation of liquefied air. Liquid oxygen may also be produced by condensation out of air, using liquid nitrogen as a coolant. It is a highly reactive substance and must be segregated from combustible materials."
Ministry_of_Defence_(United_Kingdom),"Additionally, there are a number of Assistant Chiefs of Defence Staff, including the Assistant Chief of the Defence Staff (Reserves and Cadets) and the Defence Services Secretary in the Royal Household of the Sovereign of the United Kingdom, who is also the Assistant Chief of Defence Staff (Personnel). The Ministry of Defence (MoD) is the British government department responsible for implementing the defence policy set by Her Majesty's Government, and is the headquarters of the British Armed Forces. The current Chief of the Defence Staff, the professional head of the British Armed Forces, is General Sir Nicholas Houghton, late Green Howards. He is supported by the Vice Chief of the Defence Staff, by the professional heads of the three services of HM Armed Forces and by the Commander of Joint Forces Command. There are also three Deputy Chiefs of the Defence Staff with particular remits, Deputy Chief of the Defence Staff (Capability), Deputy CDS (Personnel and Training) and Deputy CDS (Operations). The Surgeon General, represents the Defence Medical Services on the Defence Staff, and is the clinical head of that service. The MoD states that its principal objectives are to defend the United Kingdom of Great Britain and Northern Ireland and its interests and to strengthen international peace and stability. With the collapse of the Soviet Union and the end of the Cold War, the MoD does not foresee any short-term conventional military threat; rather, it has identified weapons of mass destruction, international terrorism, and failed and failing states as the overriding threats to Britain's interests. The MoD also manages day-to-day running of the armed forces, contingency planning and defence procurement. The Ministry of Defence is one of the United Kingdom's largest landowners, owning 227,300 hectares of land and foreshore (either freehold or leasehold) at April 2014, which was valued at ""about £20 billion"". The MoD also has ""rights of access"" to a further 222,000 hectares. In total, this is about 1.8% of the UK land mass. The total annual cost to support the defence estate is ""in excess of £3.3 billion"". In 2013 it was found that the Ministry of Defence had overspent on its equipment budget by £6.5bn on orders that could take up to 39 years to fulfil. The Ministry of Defence has been criticised in the past for poor management and financial control, investing in projects that have taken up to 10 and even as much as 15 years to be delivered. The Ministers and Chiefs of the Defence Staff are supported by a number of civilian, scientific and professional military advisors. The Permanent Under-Secretary of State for Defence (generally known as the Permanent Secretary) is the senior civil servant at the MoD. His or her role is to ensure the MoD operates effectively as a department of the government. The most notable fraud conviction was that of Gordon Foxley, head of defence procurement at the Ministry of Defence from 1981 to 1984. Police claimed he received at least £3.5m in total in corrupt payments, such as substantial bribes from overseas arms contractors aiming to influence the allocation of contracts. From 1946 to 1964 five Departments of State did the work of the modern Ministry of Defence: the Admiralty, the War Office, the Air Ministry, the Ministry of Aviation, and an earlier form of the Ministry of Defence. These departments merged in 1964; the defence functions of the Ministry of Aviation Supply merged into the Ministry of Defence in 1971. In April 2008, a £90m contract was signed with Boeing for a ""quick fix"" solution, so they can fly by 2010: QinetiQ will downgrade the Chinooks—stripping out some of their more advanced equipment. During the 1920s and 1930s, British civil servants and politicians, looking back at the performance of the state during World War I, concluded that there was a need for greater co-ordination between the three Services that made up the armed forces of the United Kingdom—the British Army, the Royal Navy, and the Royal Air Force. The formation of a united ministry of defence was rejected by David Lloyd George's coalition government in 1921; but the Chiefs of Staff Committee was formed in 1923, for the purposes of inter-Service co-ordination. As rearmament became a concern during the 1930s, Stanley Baldwin created the position of Minister for Coordination of Defence. Lord Chatfield held the post until the fall of Neville Chamberlain's government in 1940; his success was limited by his lack of control over the existing Service departments and his limited political influence. The Strategic Defence and Security Review 2015 included £178 billion investment in new equipment and capabilities. The review set a defence policy with four primary missions for the Armed Forces: Following the end of the Cold War, the threat of direct conventional military confrontation with other states has been replaced by terrorism. Sir Richard Dannatt predicted British forces to be involved in combating ""predatory non-state actors"" for the foreseeable future, in what he called an ""era of persistent conflict"". He told the Chatham House think tank that the fight against al-Qaeda and other militant Islamist groups was ""probably the fight of our generation"". Henry VIII's wine cellar at the Palace of Whitehall, built in 1514–1516 for Cardinal Wolsey, is in the basement of Main Building, and is used for entertainment. The entire vaulted brick structure of the cellar was encased in steel and concrete and relocated nine feet to the west and nearly 19 feet (5.8 m) deeper in 1949, when construction was resumed at the site after World War II. This was carried out without any significant damage to the structure. The defence estate is divided as training areas & ranges (84.0%), research & development (5.4%), airfields (3.4%), barracks & camps (2.5%), storage & supply depots (1.6%), and other (3.0%). These are largely managed by the Defence Infrastructure Organisation. Dannatt criticised a remnant ""Cold War mentality"", with military expenditures based on retaining a capability against a direct conventional strategic threat; He said currently only 10% of the MoD's equipment programme budget between 2003 and 2018 was to be invested in the ""land environment""—at a time when Britain was engaged in land-based wars in Afghanistan and Iraq. The MoD has been criticised for an ongoing fiasco, having spent £240m on eight Chinook HC3 helicopters which only started to enter service in 2010, years after they were ordered in 1995 and delivered in 2001. A National Audit Office report reveals that the helicopters have been stored in air conditioned hangars in Britain since their 2001[why?] delivery, while troops in Afghanistan have been forced to rely on helicopters which are flying with safety faults. By the time the Chinooks are airworthy, the total cost of the project could be as much as £500m. The headquarters of the MoD are in Whitehall and are now known as Main Building. This structure is neoclassical in style and was originally built between 1938 and 1959 to designs by Vincent Harris to house the Air Ministry and the Board of Trade. The northern entrance in Horse Guards Avenue is flanked by two monumental statues, Earth and Water, by Charles Wheeler. Opposite stands the Gurkha Monument, sculpted by Philip Jackson and unveiled in 1997 by Queen Elizabeth II. Within it is the Victoria Cross and George Cross Memorial, and nearby are memorials to the Fleet Air Arm and RAF (to its east, facing the riverside). A major refurbishment of the building was completed under a PFI contract by Skanska in 2004. A government report covered by the Guardian in 2002 indicates that between 1940 and 1979, the Ministry of Defence ""turned large parts of the country into a giant laboratory to conduct a series of secret germ warfare tests on the public"" and many of these tests ""involved releasing potentially dangerous chemicals and micro-organisms over vast swaths of the population without the public being told."" The Ministry of Defence claims that these trials were to simulate germ warfare and that the tests were harmless. Still, families who have been in the area of many of the tests are experiencing children with birth defects and physical and mental handicaps and many are asking for a public inquiry. According to the report these tests affected estimated millions of people including one period between 1961 and 1968 where ""more than a million people along the south coast of England, from Torquay to the New Forest, were exposed to bacteria including e.coli and bacillus globigii, which mimics anthrax."" Two scientists commissioned by the Ministry of Defence stated that these trials posed no risk to the public. This was confirmed by Sue Ellison, a representative of Porton Down who said that the results from these trials ""will save lives, should the country or our forces face an attack by chemical and biological weapons."" Asked whether such tests are still being carried out, she said: ""It is not our policy to discuss ongoing research."" It is unknown whether or not the harmlessness of the trials was known at the time of their occurrence. In October 2009, the MoD was heavily criticized for withdrawing the bi-annual non-operational training £20m budget for the volunteer Territorial Army (TA), ending all non-operational training for 6 months until April 2010. The government eventually backed down and restored the funding. The TA provides a small percentage of the UK's operational troops. Its members train on weekly evenings and monthly weekends, as well as two-week exercises generally annually and occasionally bi-annually for troops doing other courses. The cuts would have meant a significant loss of personnel and would have had adverse effects on recruitment. The 1998 Strategic Defence Review and the 2003 Delivering Security in a Changing World White Paper outlined the following posture for the British Armed Forces: The Defence Committee—Third Report ""Defence Equipment 2009"" cites an article from the Financial Times website stating that the Chief of Defence Materiel, General Sir Kevin O’Donoghue, had instructed staff within Defence Equipment and Support (DE&S) through an internal memorandum to reprioritize the approvals process to focus on supporting current operations over the next three years; deterrence related programmes; those that reflect defence obligations both contractual or international; and those where production contracts are already signed. The report also cites concerns over potential cuts in the defence science and technology research budget; implications of inappropriate estimation of Defence Inflation within budgetary processes; underfunding in the Equipment Programme; and a general concern over striking the appropriate balance over a short-term focus (Current Operations) and long-term consequences of failure to invest in the delivery of future UK defence capabilities on future combatants and campaigns. The then Secretary of State for Defence, Bob Ainsworth MP, reinforced this reprioritisation of focus on current operations and had not ruled out ""major shifts"" in defence spending. In the same article the First Sea Lord and Chief of the Naval Staff, Admiral Sir Mark Stanhope, Royal Navy, acknowledged that there was not enough money within the defence budget and it is preparing itself for tough decisions and the potential for cutbacks. According to figures published by the London Evening Standard the defence budget for 2009 is ""more than 10% overspent"" (figures cannot be verified) and the paper states that this had caused Gordon Brown to say that the defence spending must be cut. The MoD has been investing in IT to cut costs and improve services for its personnel. The MoD has since been regarded as a leader in elaborating the post-Cold War organising concept of ""defence diplomacy"". As a result of the Strategic Defence and Security Review 2010, Prime Minister David Cameron signed a 50-year treaty with French President Nicolas Sarkozy that would have the two countries co-operate intensively in military matters. The UK is establishing air and naval bases in the Persian Gulf, located in the UAE and Bahrain. A presence in Oman is also being considered. Winston Churchill, on forming his government in 1940, created the office of Minister of Defence to exercise ministerial control over the Chiefs of Staff Committee and to co-ordinate defence matters. The post was held by the Prime Minister of the day until Clement Attlee's government introduced the Ministry of Defence Act of 1946. The new ministry was headed by a Minister of Defence who possessed a seat in the Cabinet. The three existing service Ministers—the Secretary of State for War, the First Lord of the Admiralty, and the Secretary of State for Air—remained in direct operational control of their respective services, but ceased to attend Cabinet."
War_on_Terror,"In 2005, the UN Security Council adopted Resolution 1624 concerning incitement to commit acts of terrorism and the obligations of countries to comply with international human rights laws. Although both resolutions require mandatory annual reports on counter-terrorism activities by adopting nations, the United States and Israel have both declined to submit reports. In the same year, the United States Department of Defense and the Chairman of the Joint Chiefs of Staff issued a planning document, by the name ""National Military Strategic Plan for the War on Terrorism"", which stated that it constituted the ""comprehensive military plan to prosecute the Global War on Terror for the Armed Forces of the United States...including the findings and recommendations of the 9/11 Commission and a rigorous examination with the Department of Defense"". Because the actions involved in the ""war on terrorism"" are diffuse, and the criteria for inclusion are unclear, political theorist Richard Jackson has argued that ""the 'war on terrorism' therefore, is simultaneously a set of actual practices—wars, covert operations, agencies, and institutions—and an accompanying series of assumptions, beliefs, justifications, and narratives—it is an entire language or discourse."" Jackson cites among many examples a statement by John Ashcroft that ""the attacks of September 11 drew a bright line of demarcation between the civil and the savage"". Administration officials also described ""terrorists"" as hateful, treacherous, barbarous, mad, twisted, perverted, without faith, parasitical, inhuman, and, most commonly, evil. Americans, in contrast, were described as brave, loving, generous, strong, resourceful, heroic, and respectful of human rights. The USA PATRIOT Act of October 2001 dramatically reduces restrictions on law enforcement agencies' ability to search telephone, e-mail communications, medical, financial, and other records; eases restrictions on foreign intelligence gathering within the United States; expands the Secretary of the Treasury's authority to regulate financial transactions, particularly those involving foreign individuals and entities; and broadens the discretion of law enforcement and immigration authorities in detaining and deporting immigrants suspected of terrorism-related acts. The act also expanded the definition of terrorism to include domestic terrorism, thus enlarging the number of activities to which the USA PATRIOT Act's expanded law enforcement powers could be applied. A new Terrorist Finance Tracking Program monitored the movements of terrorists' financial resources (discontinued after being revealed by The New York Times). Global telecommunication usage, including those with no links to terrorism, is being collected and monitored through the NSA electronic surveillance program. The Patriot Act is still in effect. In a major split in the ranks of Al Qaeda's organization, the Iraqi franchise, known as Al Qaeda in Iraq covertly invaded Syria and the Levant and began participating in the ongoing Syrian Civil War, gaining enough support and strength to re-invade Iraq's western provinces under the name of the Islamic State of Iraq and the Levant (ISIS/ISIL), taking over much of the country in a blitzkrieg-like action and combining the Iraq insurgency and Syrian Civil War into a single conflict. Due to their extreme brutality and a complete change in their overall ideology, Al Qaeda's core organization in Central Asia eventually denounced ISIS and directed their affiliates to cut off all ties with this organization. Many analysts[who?] believe that because of this schism, Al Qaeda and ISIL are now in a competition to retain the title of the world's most powerful terrorist organization. On 14 September 2009, U.S. Special Forces killed two men and wounded and captured two others near the Somali village of Baarawe. Witnesses claim that helicopters used for the operation launched from French-flagged warships, but that could not be confirmed. A Somali-based al-Qaida affiliated group, the Al-Shabaab, has confirmed the death of ""sheik commander"" Saleh Ali Saleh Nabhan along with an unspecified number of militants. Nabhan, a Kenyan, was wanted in connection with the 2002 Mombasa attacks. U.S. President Barack Obama has rarely used the term, but in his inaugural address on 20 January 2009, he stated ""Our nation is at war, against a far-reaching network of violence and hatred."" In March 2009 the Defense Department officially changed the name of operations from ""Global War on Terror"" to ""Overseas Contingency Operation"" (OCO). In March 2009, the Obama administration requested that Pentagon staff members avoid use of the term, instead using ""Overseas Contingency Operation"". Basic objectives of the Bush administration ""war on terror"", such as targeting al Qaeda and building international counterterrorism alliances, remain in place. In December 2012, Jeh Johnson, the General Counsel of the Department of Defense, stated that the military fight will be replaced by a law enforcement operation when speaking at Oxford University, predicting that al Qaeda will be so weakened to be ineffective, and has been ""effectively destroyed"", and thus the conflict will not be an armed conflict under international law. In May 2013, Obama stated that the goal is ""to dismantle specific networks of violent extremists that threaten America""; which coincided with the U.S. Office of Management and Budget having changed the wording from ""Overseas Contingency Operations"" to ""Countering Violent Extremism"" in 2010. In September 2009, a U.S. Drone strike reportedly killed Ilyas Kashmiri, who was the chief of Harkat-ul-Jihad al-Islami, a Kashmiri militant group associated with Al-Qaeda. Kashmiri was described by Bruce Riedel as a 'prominent' Al-Qaeda member, while others described him as the head of military operations for Al-Qaeda. Waziristan had now become the new battlefield for Kashmiri militants, who were now fighting NATO in support of Al-Qaeda. On 8 July 2012, Al-Badar Mujahideen, a breakaway faction of Kashmir centric terror group Hizbul Mujahideen, on conclusion of their two-day Shuhada Conference called for mobilisation of resources for continuation of jihad in Kashmir. The first ground attack came at the Battle of Umm Qasr on 21 March 2003 when a combined force of British, American and Polish forces seized control of the port city of Umm Qasr. Baghdad, Iraq's capital city, fell to American forces in April 2003 and Saddam Hussein's government quickly dissolved. On 1 May 2003, Bush announced that major combat operations in Iraq had ended. However, an insurgency arose against the U.S.-led coalition and the newly developing Iraqi military and post-Saddam government. The insurgency, which included al-Qaeda affiliated groups, led to far more coalition casualties than the invasion. Other elements of the insurgency were led by fugitive members of President Hussein's Ba'ath regime, which included Iraqi nationalists and pan-Arabists. Many insurgency leaders are Islamists and claim to be fighting a religious war to reestablish the Islamic Caliphate of centuries past. Iraq's former president, Saddam Hussein was captured by U.S. forces in December 2003. He was executed in 2006. The British 16th Air Assault Brigade (later reinforced by Royal Marines) formed the core of the force in southern Afghanistan, along with troops and helicopters from Australia, Canada and the Netherlands. The initial force consisted of roughly 3,300 British, 2,000 Canadian, 1,400 from the Netherlands and 240 from Australia, along with special forces from Denmark and Estonia and small contingents from other nations. The monthly supply of cargo containers through Pakistani route to ISAF in Afghanistan is over 4,000 costing around 12 billion in Pakistani Rupees. The Obama administration began to reengage in Iraq with a series of airstrikes aimed at ISIS beginning on 10 August 2014. On 9 September 2014 President Obama said that he had the authority he needed to take action to destroy the militant group known as the Islamic State of Iraq and the Levant, citing the 2001 Authorization for Use of Military Force Against Terrorists, and thus did not require additional approval from Congress. The following day on 10 September 2014 President Barack Obama made a televised speech about ISIL, which he stated ""Our objective is clear: We will degrade, and ultimately destroy, ISIL through a comprehensive and sustained counter-terrorism strategy"". Obama has authorized the deployment of additional U.S. Forces into Iraq, as well as authorizing direct military operations against ISIL within Syria. On the night of 21/22 September the United States, Saudi Arabia, Bahrain, the UAE, Jordan and Qatar started air attacks against ISIS in Syria.[citation needed] The origins of al-Qaeda can be traced to the Soviet war in Afghanistan (December 1979 – February 1989). The United States, United Kingdom, Saudi Arabia, Pakistan, and the People's Republic of China supported the Islamist Afghan mujahadeen guerillas against the military forces of the Soviet Union and the Democratic Republic of Afghanistan. A small number of ""Afghan Arab"" volunteers joined the fight against the Soviets, including Osama bin Laden, but there is no evidence they received any external assistance. In May 1996 the group World Islamic Front for Jihad Against Jews and Crusaders (WIFJAJC), sponsored by bin Laden (and later re-formed as al-Qaeda), started forming a large base of operations in Afghanistan, where the Islamist extremist regime of the Taliban had seized power earlier in the year. In February 1998, Osama bin Laden signed a fatwā, as head of al-Qaeda, declaring war on the West and Israel, later in May of that same year al-Qaeda released a video declaring war on the U.S. and the West. Political interest groups have stated that these laws remove important restrictions on governmental authority, and are a dangerous encroachment on civil liberties, possible unconstitutional violations of the Fourth Amendment. On 30 July 2003, the American Civil Liberties Union (ACLU) filed the first legal challenge against Section 215 of the Patriot Act, claiming that it allows the FBI to violate a citizen's First Amendment rights, Fourth Amendment rights, and right to due process, by granting the government the right to search a person's business, bookstore, and library records in a terrorist investigation, without disclosing to the individual that records were being searched. Also, governing bodies in a number of communities have passed symbolic resolutions against the act. Following the ceasefire agreement that suspended hostilities (but not officially ended) in the 1991 Gulf War, the United States and its allies instituted and began patrolling Iraqi no-fly zones, to protect Iraq's Kurdish and Shi'a Arab population—both of which suffered attacks from the Hussein regime before and after the Gulf War—in Iraq's northern and southern regions, respectively. U.S. forces continued in combat zone deployments through November 1995 and launched Operation Desert Fox against Iraq in 1998 after it failed to meet U.S. demands of ""unconditional cooperation"" in weapons inspections. On the morning of 11 September 2001, 19 men affiliated with al-Qaeda hijacked four airliners all bound for California. Once the hijackers assumed control of the airliners, they told the passengers that they had the bomb on board and would spare the lives of passengers and crew once their demands were met – no passenger and crew actually suspected that they would use the airliners as suicide weapons since it had never happened before in history. The hijackers – members of al-Qaeda's Hamburg cell – intentionally crashed two airliners into the Twin Towers of the World Trade Center in New York City. Both buildings collapsed within two hours from fire damage related to the crashes, destroying nearby buildings and damaging others. The hijackers crashed a third airliner into the Pentagon in Arlington County, Virginia, just outside Washington D.C. The fourth plane crashed into a field near Shanksville, Pennsylvania, after some of its passengers and flight crew attempted to retake control of the plane, which the hijackers had redirected toward Washington D.C., to target the White House, or the U.S. Capitol. No flights had survivors. A total of 2,977 victims and the 19 hijackers perished in the attacks. On 12 January 2002, Musharraf gave a speech against Islamic extremism. He unequivocally condemned all acts of terrorism and pledged to combat Islamic extremism and lawlessness within Pakistan itself. He stated that his government was committed to rooting out extremism and made it clear that the banned militant organizations would not be allowed to resurface under any new name. He said, ""the recent decision to ban extremist groups promoting militancy was taken in the national interest after thorough consultations. It was not taken under any foreign influence"". Criticism of the War on Terror addresses the issues, morality, efficiency, economics, and other questions surrounding the War on Terror and made against the phrase itself, calling it a misnomer. The notion of a ""war"" against ""terrorism"" has proven highly contentious, with critics charging that it has been exploited by participating governments to pursue long-standing policy/military objectives, reduce civil liberties, and infringe upon human rights. It is argued that the term war is not appropriate in this context (as in War on Drugs), since there is no identifiable enemy, and that it is unlikely international terrorism can be brought to an end by military means. In the following months, NATO took a wide range of measures to respond to the threat of terrorism. On 22 November 2002, the member states of the Euro-Atlantic Partnership Council (EAPC) decided on a Partnership Action Plan against Terrorism, which explicitly states, ""EAPC States are committed to the protection and promotion of fundamental freedoms and human rights, as well as the rule of law, in combating terrorism."" NATO started naval operations in the Mediterranean Sea designed to prevent the movement of terrorists or weapons of mass destruction as well as to enhance the security of shipping in general called Operation Active Endeavour. Following the 11 September 2001 attacks, former President of Pakistan Pervez Musharraf sided with the U.S. against the Taliban government in Afghanistan after an ultimatum by then U.S. President George W. Bush. Musharraf agreed to give the U.S. the use of three airbases for Operation Enduring Freedom. United States Secretary of State Colin Powell and other U.S. administration officials met with Musharraf. On 19 September 2001, Musharraf addressed the people of Pakistan and stated that, while he opposed military tactics against the Taliban, Pakistan risked being endangered by an alliance of India and the U.S. if it did not cooperate. In 2006, Musharraf testified that this stance was pressured by threats from the U.S., and revealed in his memoirs that he had ""war-gamed"" the United States as an adversary and decided that it would end in a loss for Pakistan. Support for the U.S. cooled when America made clear its determination to invade Iraq in late 2002. Even so, many of the ""coalition of the willing"" countries that unconditionally supported the U.S.-led military action have sent troops to Afghanistan, particular neighboring Pakistan, which has disowned its earlier support for the Taliban and contributed tens of thousands of soldiers to the conflict. Pakistan was also engaged in the War in North-West Pakistan (Waziristan War). Supported by U.S. intelligence, Pakistan was attempting to remove the Taliban insurgency and al-Qaeda element from the northern tribal areas. The Taliban regrouped in western Pakistan and began to unleash an insurgent-style offensive against Coalition forces in late 2002. Throughout southern and eastern Afghanistan, firefights broke out between the surging Taliban and Coalition forces. Coalition forces responded with a series of military offensives and an increase in the amount of troops in Afghanistan. In February 2010, Coalition forces launched Operation Moshtarak in southern Afghanistan along with other military offensives in the hopes that they would destroy the Taliban insurgency once and for all. Peace talks are also underway between Taliban affiliated fighters and Coalition forces. In September 2014, Afghanistan and the United States signed a security agreement, which permits United States and NATO forces to remain in Afghanistan until at least 2024. The United States and other NATO and non-NATO forces are planning to withdraw; with the Taliban claiming it has defeated the United States and NATO, and the Obama Administration viewing it as a victory. In December 2014, ISAF encasing its colors, and Resolute Support began as the NATO operation in Afghanistan. Continued United States operations within Afghanistan will continue under the name ""Operation Freedom's Sentinel"". Subsequently, in October 2001, U.S. forces (with UK and coalition allies) invaded Afghanistan to oust the Taliban regime. On 7 October 2001, the official invasion began with British and U.S. forces conducting airstrike campaigns over enemy targets. Kabul, the capital city of Afghanistan, fell by mid-November. The remaining al-Qaeda and Taliban remnants fell back to the rugged mountains of eastern Afghanistan, mainly Tora Bora. In December, Coalition forces (the U.S. and its allies) fought within that region. It is believed that Osama bin Laden escaped into Pakistan during the battle. On 7 August 1998, al-Qaeda struck the U.S. embassies in Kenya and Tanzania, killing 224 people, including 12 Americans. In retaliation, U.S. President Bill Clinton launched Operation Infinite Reach, a bombing campaign in Sudan and Afghanistan against targets the U.S. asserted were associated with WIFJAJC, although others have questioned whether a pharmaceutical plant in Sudan was used as a chemical warfare plant. The plant produced much of the region's antimalarial drugs and around 50% of Sudan's pharmaceutical needs. The strikes failed to kill any leaders of WIFJAJC or the Taliban. Other critics, such as Francis Fukuyama, note that ""terrorism"" is not an enemy, but a tactic; calling it a ""war on terror"", obscures differences between conflicts such as anti-occupation insurgents and international mujahideen. With a military presence in Iraq and Afghanistan and its associated collateral damage Shirley Williams maintains this increases resentment and terrorist threats against the West. There is also perceived U.S. hypocrisy, media-induced hysteria, and that differences in foreign and security policy have damaged America's image in most of the world. On 16 September 2001, at Camp David, President George W. Bush used the phrase war on terrorism in an unscripted and controversial comment when he said, ""This crusade – this war on terrorism – is going to take a while, ... "" Bush later apologized for this remark due to the negative connotations the term crusade has to people, e.g. of Muslim faith. The word crusade was not used again. On 20 September 2001, during a televised address to a joint session of congress, Bush stated that, ""(o)ur 'war on terror' begins with al-Qaeda, but it does not end there. It will not end until every terrorist group of global reach has been found, stopped, and defeated."" In a 'Letter to American People' written by Osama bin Laden in 2002, he stated that one of the reasons he was fighting America is because of its support of India on the Kashmir issue. While on a trip to Delhi in 2002, U.S. Secretary of Defense Donald Rumsfeld suggested that Al-Qaeda was active in Kashmir, though he did not have any hard evidence. An investigation in 2002 unearthed evidence that Al-Qaeda and its affiliates were prospering in Pakistan-administered Kashmir with tacit approval of Pakistan's National Intelligence agency Inter-Services Intelligence. A team of Special Air Service and Delta Force was sent into Indian-administered Kashmir in 2002 to hunt for Osama bin Laden after reports that he was being sheltered by the Kashmiri militant group Harkat-ul-Mujahideen. U.S. officials believed that Al-Qaeda was helping organize a campaign of terror in Kashmir in order to provoke conflict between India and Pakistan. Fazlur Rehman Khalil, the leader of the Harkat-ul-Mujahideen, signed al-Qaeda's 1998 declaration of holy war, which called on Muslims to attack all Americans and their allies. Indian sources claimed that In 2006, Al-Qaeda claimed they had established a wing in Kashmir; this worried the Indian government. India also claimed that Al-Qaeda has strong ties with the Kashmir militant groups Lashkar-e-Taiba and Jaish-e-Mohammed in Pakistan. While on a visit to Pakistan in January 2010, U.S. Defense secretary Robert Gates stated that Al-Qaeda was seeking to destabilize the region and planning to provoke a nuclear war between India and Pakistan. The Authorization for Use of Military Force Against Terrorists or ""AUMF"" was made law on 14 September 2001, to authorize the use of United States Armed Forces against those responsible for the attacks on 11 September 2001. It authorized the President to use all necessary and appropriate force against those nations, organizations, or persons he determines planned, authorized, committed, or aided the terrorist attacks that occurred on 11 September 2001, or harbored such organizations or persons, in order to prevent any future acts of international terrorism against the United States by such nations, organizations or persons. Congress declares this is intended to constitute specific statutory authorization within the meaning of section 5(b) of the War Powers Resolution of 1973. In addition to military efforts abroad, in the aftermath of 9/11 the Bush Administration increased domestic efforts to prevent future attacks. Various government bureaucracies that handled security and military functions were reorganized. A new cabinet-level agency called the United States Department of Homeland Security was created in November 2002 to lead and coordinate the largest reorganization of the U.S. federal government since the consolidation of the armed forces into the Department of Defense.[citation needed] In January 2002, the United States Special Operations Command, Pacific deployed to the Philippines to advise and assist the Armed Forces of the Philippines in combating Filipino Islamist groups. The operations were mainly focused on removing the Abu Sayyaf group and Jemaah Islamiyah (JI) from their stronghold on the island of Basilan. The second portion of the operation was conducted as a humanitarian program called ""Operation Smiles"". The goal of the program was to provide medical care and services to the region of Basilan as part of a ""Hearts and Minds"" program. Joint Special Operations Task Force – Philippines disbanded in June 2014, ending a 14-year mission. After JSOTF-P disbanded, as late as November 2014, American forces continued to operate in the Philippines under the name ""PACOM Augmentation Team"". The use of drones by the Central Intelligence Agency in Pakistan to carry out operations associated with the Global War on Terror sparks debate over sovereignty and the laws of war. The U.S. Government uses the CIA rather than the U.S. Air Force for strikes in Pakistan in order to avoid breaching sovereignty through military invasion. The United States was criticized by[according to whom?] a report on drone warfare and aerial sovereignty for abusing the term 'Global War on Terror' to carry out military operations through government agencies without formally declaring war. The conflict in northern Mali began in January 2012 with radical Islamists (affiliated to al-Qaeda) advancing into northern Mali. The Malian government had a hard time maintaining full control over their country. The fledgling government requested support from the international community on combating the Islamic militants. In January 2013, France intervened on behalf of the Malian government's request and deployed troops into the region. They launched Operation Serval on 11 January 2013, with the hopes of dislodging the al-Qaeda affiliated groups from northern Mali. In 2002, the Musharraf-led government took a firm stand against the jihadi organizations and groups promoting extremism, and arrested Maulana Masood Azhar, head of the Jaish-e-Mohammed, and Hafiz Muhammad Saeed, chief of the Lashkar-e-Taiba, and took dozens of activists into custody. An official ban was imposed on the groups on 12 January. Later that year, the Saudi born Zayn al-Abidn Muhammed Hasayn Abu Zubaydah was arrested by Pakistani officials during a series of joint U.S.-Pakistan raids. Zubaydah is said to have been a high-ranking al-Qaeda official with the title of operations chief and in charge of running al-Qaeda training camps. Other prominent al-Qaeda members were arrested in the following two years, namely Ramzi bin al-Shibh, who is known to have been a financial backer of al-Qaeda operations, and Khalid Sheikh Mohammed, who at the time of his capture was the third highest-ranking official in al-Qaeda and had been directly in charge of the planning for the 11 September attacks."
Chinese_characters,"One of the most complex characters found in modern Chinese dictionaries[g] is 齉 (U+9F49) (nàng,  listen (help·info), pictured below, middle image), meaning ""snuffle"" (that is, a pronunciation marred by a blocked nose), with ""just"" thirty-six strokes. However, this is not in common use. The most complex character that can be input using the Microsoft New Phonetic IME 2002a for traditional Chinese is 龘 (dá, ""the appearance of a dragon flying""). It is composed of the dragon radical represented three times, for a total of 16 × 3 = 48 strokes. Among the most complex characters in modern dictionaries and also in frequent modern use are 籲 (yù, ""to implore""), with 32 strokes; 鬱 (yù, ""luxuriant, lush; gloomy""), with 29 strokes, as in 憂鬱 (yōuyù, ""depressed""); 豔 (yàn, ""colorful""), with 28 strokes; and 釁 (xìn, ""quarrel""), with 25 strokes, as in 挑釁 (tiǎoxìn, ""to pick a fight""). Also in occasional modern use is 鱻 (xiān ""fresh""; variant of 鮮 xiān) with 33 strokes. After Kim Jong Il, the second ruler of North Korea, died in December 2011, Kim Jong Un stepped up and began mandating the use of Hanja as a source of definition for the Korean language. Currently, it is said that North Korea teaches around 3,000 Hanja characters to North Korean students, and in some cases, the characters appear within advertisements and newspapers. However, it is also said that the authorities implore students not to use the characters in public. Due to North Korea's strict isolationism, accurate reports about hanja use in North Korea are hard to obtain. In the Republic of China (Taiwan), which uses traditional Chinese characters, the Ministry of Education's Chángyòng Guózì Biāozhǔn Zìtǐ Biǎo (常用國字標準字體表, Chart of Standard Forms of Common National Characters) lists 4,808 characters; the Cì Chángyòng Guózì Biāozhǔn Zìtǐ Biǎo (次常用國字標準字體表, Chart of Standard Forms of Less-Than-Common National Characters) lists another 6,341 characters. The Chinese Standard Interchange Code (CNS11643)—the official national encoding standard—supports 48,027 characters, while the most widely used encoding scheme, BIG-5, supports only 13,053. In addition, there are a number of dialect characters (方言字) that are not used in formal written Chinese but represent colloquial terms in non-Mandarin varieties of Chinese. One such variety is Written Cantonese, in widespread use in Hong Kong even for certain formal documents, due to the former British colonial administration's recognition of Cantonese for use for official purposes. In Taiwan, there is also an informal body of characters used to represent Hokkien Chinese. Many varieties have specific characters for words exclusive to them. For example, the vernacular character 㓾, pronounced cii11 in Hakka, means ""to kill"". Furthermore, Shanghainese and Sichuanese also have their own series of written text, but these are not widely used in actual texts, Mandarin being the preference for all mainland regions. Modern examples particularly include Chinese characters for SI units. In Chinese these units are disyllabic and standardly written with two characters, as 厘米 límǐ ""centimeter"" (厘 centi-, 米 meter) or 千瓦 qiānwǎ ""kilowatt"". However, in the 19th century these were often written via compound characters, pronounced disyllabically, such as 瓩 for 千瓦 or 糎 for 厘米 – some of these characters were also used in Japan, where they were pronounced with borrowed European readings instead. These have now fallen out of general use, but are occasionally seen. Less systematic examples include 圕 túshūguǎn ""library"", a contraction of 圖書館, A four-morpheme word, 社会主义 shèhuì zhǔyì ""socialism"", is commonly written with a single character formed by combining the last character, 义, with the radical of the first, 社, yielding roughly 礻义. Examples are 河 hé ""river"", 湖 hú ""lake"", 流 liú ""stream"", 沖 chōng ""riptide"" (or ""flush""), 滑 huá ""slippery"". All these characters have on the left a radical of three short strokes (氵), which is a reduced form of the character 水 shuǐ meaning ""water"", indicating that the character has a semantic connection with water. The right-hand side in each case is a phonetic indicator. For example, in the case of 沖 chōng (Old Chinese *ɡ-ljuŋ), the phonetic indicator is 中 zhōng (Old Chinese *k-ljuŋ), which by itself means ""middle"". In this case it can be seen that the pronunciation of the character is slightly different from that of its phonetic indicator; the process of historical phonetic change means that the composition of such characters can sometimes seem arbitrary today. For instance, to look up the character where the sound is not known, e.g., 松 (pine tree), the user first determines which part of the character is the radical (here 木), then counts the number of strokes in the radical (four), and turns to the radical index (usually located on the inside front or back cover of the dictionary). Under the number ""4"" for radical stroke count, the user locates 木, then turns to the page number listed, which is the start of the listing of all the characters containing this radical. This page will have a sub-index giving remainder stroke numbers (for the non-radical portions of characters) and page numbers. The right half of the character also contains four strokes, so the user locates the number 4, and turns to the page number given. From there, the user must scan the entries to locate the character he or she is seeking. Some dictionaries have a sub-index which lists every character containing each radical, and if the user knows the number of strokes in the non-radical portion of the character, he or she can locate the correct page directly. Most modern Chinese dictionaries and Chinese dictionaries sold to English speakers use the traditional radical-based character index in a section at the front, while the main body of the dictionary arranges the main character entries alphabetically according to their pinyin spelling. To find a character with unknown sound using one of these dictionaries, the reader finds the radical and stroke number of the character, as before, and locates the character in the radical index. The character's entry will have the character's pronunciation in pinyin written down; the reader then turns to the main dictionary section and looks up the pinyin spelling alphabetically. In addition to strictness in character size and shape, Chinese characters are written with very precise rules. The most important rules regard the strokes employed, stroke placement, and stroke order. Just as each region that uses Chinese characters has standardized character forms, each also has standardized stroke orders, with each standard being different. Most characters can be written with just one correct stroke order, though some words also have many valid stroke orders, which may occasionally result in different stroke counts. Some characters are also written with different stroke orders due to character simplification. According to the Rev. John Gulick: ""The inhabitants of other Asiatic nations, who have had occasion to represent the words of their several languages by Chinese characters, have as a rule used unaspirated characters for the sounds, g, d, b. The Muslims from Arabia and Persia have followed this method … The Mongols, Manchu, and Japanese also constantly select unaspirated characters to represent the sounds g, d, b, and j of their languages. These surrounding Asiatic nations, in writing Chinese words in their own alphabets, have uniformly used g, d, b, & c., to represent the unaspirated sounds."" Written Japanese also includes a pair of syllabaries known as kana, derived by simplifying Chinese characters selected to represent syllables of Japanese. The syllabaries differ because they sometimes selected different characters for a syllable, and because they used different strategies to reduce these characters for easy writing: the angular katakana were obtained by selecting a part of each character, while hiragana were derived from the cursive forms of whole characters. Modern Japanese writing uses a composite system, using kanji for word stems, hiragana for inflexional endings and grammatical words, and katakana to transcribe non-Chinese loanwords as well as serve as a method to emphasize native words (similar to how italics are used in Romance languages). In the years after World War II, the Japanese government also instituted a series of orthographic reforms. Some characters were given simplified forms called shinjitai 新字体 (lit. ""new character forms"", the older forms were then labelled the kyūjitai 旧字体, lit. ""old character forms""). The number of characters in common use was restricted, and formal lists of characters to be learned during each grade of school were established, first the 1850-character tōyō kanji 当用漢字 list in 1945, the 1945-character jōyō kanji 常用漢字 list in 1981, and a 2136-character reformed version of the jōyō kanji in 2010. Many variant forms of characters and obscure alternatives for common characters were officially discouraged. This was done with the goal of facilitating learning for children and simplifying kanji use in literature and periodicals. These are simply guidelines, hence many characters outside these standards are still widely known and commonly used, especially those used for personal and place names (for the latter, see jinmeiyō kanji),[citation needed] as well as for some common words such as ""dragon"" (Japanese kana: たつ, Rōmaji: tatsu) in which both the shinjitai 竜 and the kyūjitai 龍 forms of the kanji are both acceptable and widely known amongst native Japanese speakers. The majority of simplified characters are drawn from conventional abbreviated forms, or ancient standard forms. For example, the orthodox character 來 lái (""come"") was written with the structure 来 in the clerical script (隶书 / 隸書, lìshū) of the Han dynasty. This clerical form uses one fewer stroke, and was thus adopted as a simplified form. The character 雲 yún (""cloud"") was written with the structure 云 in the oracle bone script of the Shang dynasty, and had remained in use later as a phonetic loan in the meaning of ""to say"" while the 雨 radical was added to differentiate meanings. The simplified form adopts the original structure. Chinese character dictionaries often allow users to locate entries in several ways. Many Chinese, Japanese, and Korean dictionaries of Chinese characters list characters in radical order: characters are grouped together by radical, and radicals containing fewer strokes come before radicals containing more strokes (radical-and-stroke sorting). Under each radical, characters are listed by their total number of strokes. It is often also possible to search for characters by sound, using pinyin (in Chinese dictionaries), zhuyin (in Taiwanese dictionaries), kana (in Japanese dictionaries) or hangul (in Korean dictionaries). Most dictionaries also allow searches by total number of strokes, and individual dictionaries often allow other search methods as well. While new characters can be easily coined by writing on paper, they are difficult to represent on a computer – they must generally be represented as a picture, rather than as text – which presents a significant barrier to their use or widespread adoption. Compare this with the use of symbols as names in 20th century musical albums such as Led Zeppelin IV (1971) and Love Symbol Album (1993); an album cover may potentially contain any graphics, but in writing and other computation these symbols are difficult to use. Semantic-phonetic compounds or pictophonetic compounds are by far the most numerous characters. These characters are composed of two parts: one of a limited set of characters (the semantic indicator, often graphically simplified) which suggests the general meaning of the compound character, and another character (the phonetic indicator) whose pronunciation suggests the pronunciation of the compound character. In most cases the semantic indicator is also the radical under which the character is listed in dictionaries. Chinese characters represent words of the language using several strategies. A few characters, including some of the most commonly used, were originally pictograms, which depicted the objects denoted, or simple ideograms, in which meaning was expressed iconically. Some other words were expressed by compound ideograms, but the vast majority were written using the rebus principle, in which a character for a similarly sounding word was either simply borrowed or (more commonly) extended with a disambiguating semantic marker to form a phono-semantic compound character. Regular script has been attributed to Zhong Yao, of the Eastern Han to Cao Wei period (c. 151–230 AD), who has been called the ""father of regular script"". However, some scholars postulate that one person alone could not have developed a new script which was universally adopted, but could only have been a contributor to its gradual formation. The earliest surviving pieces written in regular script are copies of Yao's works, including at least one copied by Wang Xizhi. This new script, which is the dominant modern Chinese script, developed out of a neatly written form of early semi-cursive, with addition of the pause (頓/顿 dùn) technique to end horizontal strokes, plus heavy tails on strokes which are written to the downward-right diagonal. Thus, early regular script emerged from a neat, formal form of semi-cursive, which had itself emerged from neo-clerical (a simplified, convenient form of clerical script). It then matured further in the Eastern Jin dynasty in the hands of the ""Sage of Calligraphy"", Wang Xizhi, and his son Wang Xianzhi. It was not, however, in widespread use at that time, and most writers continued using neo-clerical, or a somewhat semi-cursive form of it, for daily writing, while the conservative bafen clerical script remained in use on some stelae, alongside some semi-cursive, but primarily neo-clerical. New characters can in principle be coined at any time, just as new words can be, but they may not be adopted. Significant historically recent coinages date to scientific terms of the 19th century. Specifically, Chinese coined new characters for chemical elements – see chemical elements in East Asian languages – which continue to be used and taught in schools in China and Taiwan. In Japan, in the Meiji era (specifically, late 19th century), new characters were coined for some (but not all) SI units, such as 粁 (米 ""meter"" + 千 ""thousand, kilo-"") for kilometer. These kokuji (Japanese-coinages) have found use in China as well – see Chinese characters for SI units for details. The Shang dynasty oracle bone script and the Zhou dynasty scripts found on Chinese bronze inscriptions are no longer used; the oldest script that is still in use today is the Seal Script (篆書(书), zhuànshū). It evolved organically out of the Spring and Autumn period Zhou script, and was adopted in a standardized form under the first Emperor of China, Qin Shi Huang. The seal script, as the name suggests, is now used only in artistic seals. Few people are still able to read it effortlessly today, although the art of carving a traditional seal in the script remains alive; some calligraphers also work in this style. There is a clear trend toward the exclusive use of hangul in day-to-day South Korean society. Hanja are still used to some extent, particularly in newspapers, weddings, place names and calligraphy (although it is nowhere near the extent of kanji use in day-to-day Japanese society). Hanja is also extensively used in situations where ambiguity must be avoided,[citation needed] such as academic papers, high-level corporate reports, government documents, and newspapers; this is due to the large number of homonyms that have resulted from extensive borrowing of Chinese words. It was not until the Northern and Southern dynasties that regular script rose to dominant status. During that period, regular script continued evolving stylistically, reaching full maturity in the early Tang dynasty. Some call the writing of the early Tang calligrapher Ouyang Xun (557–641) the first mature regular script. After this point, although developments in the art of calligraphy and in character simplification still lay ahead, there were no more major stages of evolution for the mainstream script. The art of writing Chinese characters is called Chinese calligraphy. It is usually done with ink brushes. In ancient China, Chinese calligraphy is one of the Four Arts of the Chinese Scholars. There is a minimalist set of rules of Chinese calligraphy. Every character from the Chinese scripts is built into a uniform shape by means of assigning it a geometric area in which the character must occur. Each character has a set number of brushstrokes; none must be added or taken away from the character to enhance it visually, lest the meaning be lost. Finally, strict regularity is not required, meaning the strokes may be accentuated for dramatic effect of individual style. Calligraphy was the means by which scholars could mark their thoughts and teachings for immortality, and as such, represent some of the more precious treasures that can be found from ancient China. Contrary to the popular belief of there being only one script per period, there were in fact multiple scripts in use during the Han period. Although mature clerical script, also called 八分 (bāfēn) script, was dominant at that time, an early type of cursive script was also in use by the Han by at least as early as 24 BC (during the very late Western Han period),[b] incorporating cursive forms popular at the time, well as many elements from the vulgar writing of the Warring State of Qin. By around the time of the Eastern Jin dynasty, this Han cursive became known as 章草 zhāngcǎo (also known as 隶草 / 隸草 lìcǎo today), or in English sometimes clerical cursive, ancient cursive, or draft cursive. Some believe that the name, based on 章 zhāng meaning ""orderly"", arose because the script was a more orderly form of cursive than the modern form, which emerged during the Eastern Jin dynasty and is still in use today, called 今草 jīncǎo or ""modern cursive"". Although most of the simplified Chinese characters in use today are the result of the works moderated by the government of the People's Republic of China in the 1950s and 60s, character simplification predates the republic's formation in 1949. One of the earliest proponents of character simplification was Lufei Kui, who proposed in 1909 that simplified characters should be used in education. In the years following the May Fourth Movement in 1919, many anti-imperialist Chinese intellectuals sought ways to modernise China. In the 1930s and 1940s, discussions on character simplification took place within the Kuomintang government, and many Chinese intellectuals and writers have long maintained that character simplification would help boost literacy in China. In many world languages, literacy has been promoted as a justification for spelling reforms. The People's Republic of China issued its first round of official character simplifications in two documents, the first in 1956 and the second in 1964. In the 1950s and 1960s, while confusion about simplified characters was still rampant, transitional characters that mixed simplified parts with yet-to-be simplified parts of characters together appeared briefly, then disappeared. One man who has encountered this problem is Taiwanese politician Yu Shyi-kun, due to the rarity of the last character in his name. Newspapers have dealt with this problem in varying ways, including using software to combine two existing, similar characters, including a picture of the personality, or, especially as is the case with Yu Shyi-kun, simply substituting a homophone for the rare character in the hope that the reader would be able to make the correct inference. Taiwanese political posters, movie posters etc. will often add the bopomofo phonetic symbols next to such a character. Japanese newspapers may render such names and words in katakana instead of kanji, and it is accepted practice for people to write names for which they are unsure of the correct kanji in katakana instead. When learning how to write hanja, students are taught to memorize the native Korean pronunciation for the hanja's meaning and the Sino-Korean pronunciations (the pronunciation based on the Chinese pronunciation of the characters) for each hanja respectively so that students know what the syllable and meaning is for a particular hanja. For example, the name for the hanja 水 is 물 수 (mul-su) in which 물 (mul) is the native Korean pronunciation for ""water"", while 수 (su) is the Sino-Korean pronunciation of the character. The naming of hanja is similar to if ""water"" were named ""water-aqua"", ""horse-equus"", or ""gold-aurum"" based on a hybridization of both the English and the Latin names. Other examples include 사람 인 (saram-in) for 人 ""person/people"", 큰 대 (keun-dae) for 大 ""big/large//great"", 작을 소 (jakeul-so) for 小 ""small/little"", 아래 하 (arae-ha) for 下 ""underneath/below/low"", 아비 부 (abi-bu) for 父 ""father"", and 나라이름 한 (naraimreum-han) for 韓 ""Han/Korea"". The use of such contractions is as old as Chinese characters themselves, and they have frequently been found in religious or ritual use. In the Oracle Bone script, personal names, ritual items, and even phrases such as 受又(祐) shòu yòu ""receive blessings"" are commonly contracted into single characters. A dramatic example is that in medieval manuscripts 菩薩 púsà ""bodhisattva"" (simplified: 菩萨) is sometimes written with a single character formed of a 2×2 grid of four 十 (derived from the grass radical over two 十). However, for the sake of consistency and standardization, the CPC seeks to limit the use of such polysyllabic characters in public writing to ensure that every character only has one syllable. Chinese characters are primarily morphosyllabic, meaning that most Chinese morphemes are monosyllabic and are written with a single character, though in modern Chinese most words are disyllabic and dimorphemic, consisting of two syllables, each of which is a morpheme. In modern Chinese 10% of morphemes only occur as part of a given compound. However, a few morphemes are disyllabic, some of them dating back to Classical Chinese. Excluding foreign loan words, these are typically words for plants and small animals. They are usually written with a pair of phono-semantic compound characters sharing a common radical. Examples are 蝴蝶 húdié ""butterfly"" and 珊瑚 shānhú ""coral"". Note that the 蝴 hú of húdié and the 瑚 hú of shānhú have the same phonetic, 胡, but different radicals (""insect"" and ""jade"", respectively). Neither exists as an independent morpheme except as a poetic abbreviation of the disyllabic word. A commonly seen example is the double happiness symbol 囍, formed as a ligature of 喜喜 and referred to by its disyllabic name (simplified Chinese: 双喜; traditional Chinese: 雙喜; pinyin: shuāngxǐ). In handwriting, numbers are very frequently squeezed into one space or combined – common ligatures include 廿 niàn, ""twenty"", normally read as 二十 èrshí, 卅 sà, ""thirty"", normally read as 三十 sānshí, and 卌 xì ""forty"", normally read as 四十 ""sìshí"". In some cases counters are also merged into one character, such as 七十人 qīshí rén ""seventy people"". Another common abbreviation is 门 with a ""T"" written inside it, for 問題, 问题, wèntí (""question; problem""), where the ""T"" is from pinyin for the second syllable tí 题. Since polysyllabic characters are often non-standard, they are often excluded incharcter dictionaries. Chinese characters are logograms used in the writing of Chinese and some other Asian languages. In Standard Chinese they are called Hanzi (simplified Chinese: 汉字; traditional Chinese: 漢字). They have been adapted to write a number of other languages including: Japanese, where they are known as kanji, Korean, where they are known as hanja, and Vietnamese in a system known as chữ Nôm. Collectively, they are known as CJKV characters. In English, they are sometimes called Han characters. Chinese characters constitute the oldest continuously used system of writing in the world. By virtue of their widespread current use in East Asia, and historic use throughout the Sinosphere, Chinese characters are among the most widely adopted writing systems in the world. In Old Chinese, (e.g. Classical Chinese) most words were monosyllabic and there was a close correspondence between characters and words. In modern Chinese (esp. Mandarin Chinese), characters do not necessarily correspond to words; indeed the majority of Chinese words today consist of two or more characters due to the merging and loss of sounds in the Chinese language over time. Rather, a character almost always corresponds to a single syllable that is also a morpheme. However, there are a few exceptions to this general correspondence, including bisyllabic morphemes (written with two characters), bimorphemic syllables (written with two characters) and cases where a single character represents a polysyllabic word or phrase. The following is a comparison of Chinese characters in the Standard Form of National Characters, a common traditional Chinese standard used in Taiwan, the Table of General Standard Chinese Characters, the standard for Mainland Chinese simplified Chinese characters, and the jōyō kanji, the standard for Japanese kanji. Generally, the jōyō kanji are more similar to traditional Chinese characters than simplified Chinese characters are to traditional Chinese characters. ""Simplified"" refers to having significant differences from the Taiwan standard, not necessarily being a newly created character or a newly performed substitution. The characters in the Hong Kong standard and the Kangxi Dictionary are also known as ""Traditional,"" but are not shown. The earliest confirmed evidence of the Chinese script yet discovered is the body of inscriptions on oracle bones from the late Shang dynasty (c. 1200–1050 BC). These symbols, carved on pieces of bone and turtle shell being sold as ""dragon bones"" for medicinal purposes, were identified as Chinese writing by scholars in 1899. By 1928, the source of the oracle bones had been traced to a village near Anyang in Henan Province, which was excavated by the Academia Sinica between 1928 and 1937. Over 150,000 fragments have been found. Although Chinese characters in Vietnam are now limited to ceremonial uses, they were once in widespread use. Until the early 20th century, Literary Chinese was used in Vietnam for all official and scholarly writing. Around the 13th century the Nôm script was developed to record folk literature in the Vietnamese language. The script used Chinese characters to represent both borrowed Sino-Vietnamese vocabulary and native words with similar pronunciation or meaning. In addition thousands of new compound characters were created to write Vietnamese words. This process resulted in a highly complex system that was never mastered by more than 5% of the population. Both Literary Chinese and Nôm were replaced in the early 20th century by Vietnamese written with the Latin-based Vietnamese alphabet. Seal script, which had evolved slowly in the state of Qin during the Eastern Zhou dynasty, became standardized and adopted as the formal script for all of China in the Qin dynasty (leading to a popular misconception that it was invented at that time), and was still widely used for decorative engraving and seals (name chops, or signets) in the Han dynasty period. However, despite the Qin script standardization, more than one script remained in use at the time. For example, a little-known, rectilinear and roughly executed kind of common (vulgar) writing had for centuries coexisted with the more formal seal script in the Qin state, and the popularity of this vulgar writing grew as the use of writing itself became more widespread. By the Warring States period, an immature form of clerical script called ""early clerical"" or ""proto-clerical"" had already developed in the state of Qin based upon this vulgar writing, and with influence from seal script as well. The coexistence of the three scripts – small seal, vulgar and proto-clerical, with the latter evolving gradually in the Qin to early Han dynasties into clerical script – runs counter to the traditional belief that the Qin dynasty had one script only, and that clerical script was suddenly invented in the early Han dynasty from the small seal script. The traditional picture of an orderly series of scripts, each one invented suddenly and then completely displacing the previous one, has been conclusively demonstrated to be fiction by the archaeological finds and scholarly research of the later 20th and early 21st centuries. Gradual evolution and the coexistence of two or more scripts was more often the case. As early as the Shang dynasty, oracle-bone script coexisted as a simplified form alongside the normal script of bamboo books (preserved in typical bronze inscriptions), as well as the extra-elaborate pictorial forms (often clan emblems) found on many bronzes. By the late Eastern Han period, an early form of semi-cursive script appeared, developing out of a cursively written form of neo-clerical script[c] and simple cursive. This semi-cursive script was traditionally attributed to Liu Desheng c. 147–188 AD,[d] although such attributions refer to early masters of a script rather than to their actual inventors, since the scripts generally evolved into being over time. Qiu gives examples of early semi-cursive script, showing that it had popular origins rather than being purely Liu’s invention. The cursive script (草書(书), cǎoshū, literally ""grass script"") is used informally. The basic character shapes are suggested, rather than explicitly realized, and the abbreviations are sometimes extreme. Despite being cursive to the point where individual strokes are no longer differentiable and the characters often illegible to the untrained eye, this script (also known as draft) is highly revered for the beauty and freedom that it embodies. Some of the simplified Chinese characters adopted by the People's Republic of China, and some simplified characters used in Japan, are derived from the cursive script. The Japanese hiragana script is also derived from this script. In times past, until the 15th century, in Korea, Literary Chinese was the dominant form of written communication, prior to the creation of hangul, the Korean alphabet. Much of the vocabulary, especially in the realms of science and sociology, comes directly from Chinese, comparable to Latin or Greek root words in European languages. However, due to the lack of tones in Korean,[citation needed] as the words were imported from Chinese, many dissimilar characters took on identical sounds, and subsequently identical spelling in hangul.[citation needed] Chinese characters are sometimes used to this day for either clarification in a practical manner, or to give a distinguished appearance, as knowledge of Chinese characters is considered a high class attribute and an indispensable part of a classical education.[citation needed] It is also observed that the preference for Chinese characters is treated as being conservative and Confucian. In recent decades, a series of inscribed graphs and pictures have been found at Neolithic sites in China, including Jiahu (c. 6500 BC), Dadiwan and Damaidi from the 6th millennium BC, and Banpo (5th millennium BC). Often these finds are accompanied by media reports that push back the purported beginnings of Chinese writing by thousands of years. However, because these marks occur singly, without any implied context, and are made crudely and simply, Qiu Xigui concluded that ""we do not have any basis for stating that these constituted writing nor is there reason to conclude that they were ancestral to Shang dynasty Chinese characters."" They do however demonstrate a history of sign use in the Yellow River valley during the Neolithic through to the Shang period. Just as Roman letters have a characteristic shape (lower-case letters mostly occupying the x-height, with ascenders or descenders on some letters), Chinese characters occupy a more or less square area in which the components of every character are written to fit in order to maintain a uniform size and shape, especially with small printed characters in Ming and sans-serif styles. Because of this, beginners often practise writing on squared graph paper, and the Chinese sometimes use the term ""Square-Block Characters"" (方块字 / 方塊字, fāngkuàizì), sometimes translated as tetragraph, in reference to Chinese characters. The use of traditional Chinese characters versus simplified Chinese characters varies greatly, and can depend on both the local customs and the medium. Before the official reform, character simplifications were not officially sanctioned and generally adopted vulgar variants and idiosyncratic substitutions. Orthodox variants were mandatory in printed works, while the (unofficial) simplified characters would be used in everyday writing or quick notes. Since the 1950s, and especially with the publication of the 1964 list, the People's Republic of China has officially adopted simplified Chinese characters for use in mainland China, while Hong Kong, Macau, and the Republic of China (Taiwan) were not affected by the reform. There is no absolute rule for using either system, and often it is determined by what the target audience understands, as well as the upbringing of the writer. Based on studies of these bronze inscriptions, it is clear that, from the Shang dynasty writing to that of the Western Zhou and early Eastern Zhou, the mainstream script evolved in a slow, unbroken fashion, until assuming the form that is now known as seal script in the late Eastern Zhou in the state of Qin, without any clear line of division. Meanwhile, other scripts had evolved, especially in the eastern and southern areas during the late Zhou dynasty, including regional forms, such as the gǔwén (""ancient forms"") of the eastern Warring States preserved as variant forms in the Han dynasty character dictionary Shuowen Jiezi, as well as decorative forms such as bird and insect scripts. Occasionally a bisyllabic word is written with two characters that contain the same radical, as in 蝴蝶 húdié ""butterfly"", where both characters have the insect radical 虫. A notable example is pipa (a Chinese lute, also a fruit, the loquat, of similar shape) – originally written as 批把 with the hand radical, referring to the down and up strokes when playing this instrument, which was then changed to 枇杷 (tree radical), which is still used for the fruit, while the character was changed to 琵琶 when referring to the instrument. In other cases a compound word may coincidentally share a radical without this being meaningful. Even the Zhonghua Zihai does not include characters in the Chinese family of scripts created to represent non-Chinese languages. Characters formed by Chinese principles in other languages include the roughly 1,500 Japanese-made kokuji given in the Kokuji no Jiten, the Korean-made gukja, the over 10,000 Sawndip characters still in use in Guangxi, and the almost 20,000 Nôm characters formerly used in Vietnam.[citation needed] More divergent descendents of Chinese script include Tangut script, which created over 5,000 characters with similar strokes but different formation principles to Chinese characters. Modern Chinese has many homophones; thus the same spoken syllable may be represented by many characters, depending on meaning. A single character may also have a range of meanings, or sometimes quite distinct meanings; occasionally these correspond to different pronunciations. Cognates in the several varieties of Chinese are generally written with the same character. They typically have similar meanings, but often quite different pronunciations. In other languages, most significantly today in Japanese and sometimes in Korean, characters are used to represent Chinese loanwords, to represent native words independent of the Chinese pronunciation, and as purely phonetic elements based on their pronunciation in the historical variety of Chinese from which they were acquired. These foreign adaptations of Chinese pronunciation are known as Sino-Xenic pronunciations, and have been useful in the reconstruction of Middle Chinese. In certain cases compound words and set phrases may be contracted into single characters. Some of these can be considered logograms, where characters represent whole words rather than syllable-morphemes, though these are generally instead considered ligatures or abbreviations (similar to scribal abbreviations, such as & for ""et""), and as non-standard. These do see use, particularly in handwriting or decoration, but also in some cases in print. In Chinese, these ligatures are called héwén (合文), héshū (合書) or hétǐzì (合体字), and in the special case of combining two characters, these are known as ""two-syllable Chinese characters"" (双音节汉字, 雙音節漢字). Chinese characters number in the tens of thousands, though most of them are minor graphic variants encountered only in historical texts. Studies in China have shown that functional literacy in written Chinese requires a knowledge of between three and four thousand characters. In Japan, 2,136 are taught through secondary school (the Jōyō kanji); hundreds more are in everyday use. There are various national standard lists of characters, forms, and pronunciations. Simplified forms of certain characters are used in China, Singapore, and Malaysia; the corresponding traditional characters are used in Taiwan, Hong Kong, Macau, and to a limited extent in South Korea. In Japan, common characters are written in post-WWII Japan-specific simplified forms (shinjitai), which are closer to traditional forms than Chinese simplifications, while uncommon characters are written in Japanese traditional forms (kyūjitai), which are virtually identical to Chinese traditional forms. In South Korea, when Chinese characters are used they are of the traditional variant and are almost identical to those used in places like Taiwan and Hong Kong. Teaching of Chinese characters in South Korea starts in the 7th grade and continues until the 12th grade where 1,800 total characters are taught albeit these characters are only used in certain cases (on signs, academic papers, historical writings, etc.) and are slowly declining in use. The total number of Chinese characters from past to present remains unknowable because new ones are developed all the time – for instance, brands may create new characters when none of the existing ones allow for the intended meaning. Chinese characters are theoretically an open set and anyone can create new characters, though such inventions are rarely included in official character sets. The number of entries in major Chinese dictionaries is the best means of estimating the historical growth of character inventory. Modified radicals and new variants are two common reasons for the ever-increasing number of characters. There are about 300 radicals and 100 are in common use. Creating a new character by modifying the radical is an easy way to disambiguate homographs among xíngshēngzì pictophonetic compounds. This practice began long before the standardization of Chinese script by Qin Shi Huang and continues to the present day. The traditional 3rd-person pronoun tā (他 ""he, she, it""), which is written with the ""person radical"", illustrates modifying significs to form new characters. In modern usage, there is a graphic distinction between tā (她 ""she"") with the ""woman radical"", tā (牠 ""it"") with the ""animal radical"", tā (它 ""it"") with the ""roof radical"", and tā (祂 ""He"") with the ""deity radical"", One consequence of modifying radicals is the fossilization of rare and obscure variant logographs, some of which are not even used in Classical Chinese. For instance, he 和 ""harmony, peace"", which combines the ""grain radical"" with the ""mouth radical"", has infrequent variants 咊 with the radicals reversed and 龢 with the ""flute radical"". The People's Republic of China issued its first round of official character simplifications in two documents, the first in 1956 and the second in 1964. A second round of character simplifications (known as erjian, or ""second round simplified characters"") was promulgated in 1977. It was poorly received, and in 1986 the authorities rescinded the second round completely, while making six revisions to the 1964 list, including the restoration of three traditional characters that had been simplified: 叠 dié, 覆 fù, 像 xiàng. In China, which uses simplified Chinese characters, the Xiàndài Hànyǔ Chángyòng Zìbiǎo (现代汉语常用字表, Chart of Common Characters of Modern Chinese) lists 2,500 common characters and 1,000 less-than-common characters, while the Xiàndài Hànyǔ Tōngyòng Zìbiǎo (现代汉语通用字表, Chart of Generally Utilized Characters of Modern Chinese) lists 7,000 characters, including the 3,500 characters already listed above. GB2312, an early version of the national encoding standard used in the People's Republic of China, has 6,763 code points. GB18030, the modern, mandatory standard, has a much higher number. The New Hànyǔ Shuǐpíng Kǎoshì (汉语水平考试, Chinese Proficiency Test) covers approximately 2,600 characters at its highest level (level six). Regular script typefaces are also commonly used, but not as common as Ming or sans-serif typefaces for body text. Regular script typefaces are often used to teach students Chinese characters, and often aim to match the standard forms of the region where they are meant to be used. Most typefaces in the Song dynasty were regular script typefaces which resembled a particular person's handwriting (e.g. the handwriting of Ouyang Xun, Yan Zhenqing, or Liu Gongquan), while most modern regular script typefaces tend toward anonymity and regularity. Although most often associated with the People's Republic of China, character simplification predates the 1949 communist victory. Caoshu, cursive written text, almost always includes character simplification, and simplified forms have always existed in print, albeit not for the most formal works. In the 1930s and 1940s, discussions on character simplification took place within the Kuomintang government, and a large number of Chinese intellectuals and writers have long maintained that character simplification would help boost literacy in China. Indeed, this desire by the Kuomintang to simplify the Chinese writing system (inherited and implemented by the Communist Party of China) also nursed aspirations of some for the adoption of a phonetic script based on the Latin script, and spawned such inventions as the Gwoyeu Romatzyh. There are also some extremely complex characters which have understandably become rather rare. According to Joël Bellassen (1989), the most complex Chinese character is /𪚥 (U+2A6A5) zhé  listen (help·info), meaning ""verbose"" and containing sixty-four strokes; this character fell from use around the 5th century. It might be argued, however, that while containing the most strokes, it is not necessarily the most complex character (in terms of difficulty), as it simply requires writing the same sixteen-stroke character 龍 lóng (lit. ""dragon"") four times in the space for one. Another 64-stroke character is /𠔻 (U+2053B) zhèng composed of 興 xīng/xìng (lit. ""flourish"") four times."
"Tucson,_Arizona","Interstate 10, which runs southeast to northwest through town, connects Tucson to Phoenix to the northwest on the way to its western terminus in Santa Monica, California, and to Las Cruces, New Mexico and El Paso, Texas toward its eastern terminus in Jacksonville, Florida. I-19 runs south from Tucson toward Nogales and the U.S.-Mexico border. I-19 is the only Interstate highway that uses ""kilometer posts"" instead of ""mileposts"", although the speed limits are marked in miles per hour instead of kilometers per hour. On Sentinel Peak (also known as ""'A' Mountain""), just west of downtown, there is a giant ""A"" in honor of the University of Arizona. Starting in about 1916, a yearly tradition developed for freshmen to whitewash the ""A"", which was visible for miles. However, at the beginning of the Iraq War, anti-war activists painted it black. This was followed by a paint scuffle where the ""A"" was painted various colors until the city council intervened. It is now red, white and blue except when it is white or another color decided by a biennial election. Because of the three-color paint scheme often used, the shape of the A can be vague and indistinguishable from the rest of the peak. The top of Sentinel Peak, which is accessible by road, offers an outstanding scenic view of the city looking eastward. A parking lot located near the summit of Sentinel Peak was formerly a popular place to watch sunsets or view the city lights at night. South Tucson is actually the name of an independent, incorporated town of 1 sq mi (2.6 km2), completely surrounded by the city of Tucson, sitting just south of downtown. South Tucson has a colorful, dynamic history. It was first incorporated in 1936, and later reincorporated in 1940. The population consists of about 83% Mexican-American and 10% Native American residents. South Tucson is widely known for its many Mexican restaurants and the architectural styles which include bright outdoor murals, many of which have been painted over due to city policy. At the end of the first decade of the 21st century, downtown Tucson underwent a revitalization effort by city planners and the business community. The primary project was Rio Nuevo, a large retail and community center that has been stalled in planning for more than ten years. Downtown is generally regarded as the area bordered by 17th Street to the south, I-10 to the west, and 6th Street to the north, and Toole Avenue and the Union Pacific (formerly Southern Pacific) railroad tracks, site of the historic train depot and ""Locomotive #1673"", built in 1900. Downtown is divided into the Presidio District, the Barrio Viejo, and the Congress Street Arts and Entertainment District. Some authorities include the 4th Avenue shopping district, which is set just northeast of the rest of downtown and connected by an underpass beneath the UPRR tracks. Tucson was probably first visited by Paleo-Indians, known to have been in southern Arizona about 12,000 years ago. Recent archaeological excavations near the Santa Cruz River have located a village site dating from 2100 BC.[citation needed] The floodplain of the Santa Cruz River was extensively farmed during the Early Agricultural period, circa 1200 BC to AD 150. These people constructed irrigation canals and grew corn, beans, and other crops while gathering wild plants and hunting. The Early Ceramic period occupation of Tucson saw the first extensive use of pottery vessels for cooking and storage. The groups designated as the Hohokam lived in the area from AD 600 to 1450 and are known for their vast irrigation canal systems and their red-on-brown pottery.[citation needed] East Tucson is relatively new compared to other parts of the city, developed between the 1950s and the 1970s,[citation needed] with developments such as Desert Palms Park. It is generally classified as the area of the city east of Swan Road, with above-average real estate values relative to the rest of the city. The area includes urban and suburban development near the Rincon Mountains. East Tucson includes Saguaro National Park East. Tucson's ""Restaurant Row"" is also located on the east side, along with a significant corporate and financial presence. Restaurant Row is sandwiched by three of Tucson's storied Neighborhoods: Harold Bell Wright Estates, named after the famous author's ranch which occupied some of that area prior to the depression; the Tucson Country Club (the third to bear the name Tucson Country Club), and the Dorado Country Club. Tucson's largest office building is 5151 East Broadway in east Tucson, completed in 1975. The first phases of Williams Centre, a mixed-use, master-planned development on Broadway near Craycroft Road, were opened in 1987. Park Place, a recently renovated shopping center, is also located along Broadway (west of Wilmot Road). Since 2009, the Tucson Festival of Books has been held annually over a two-day period in March at the University of Arizona. By 2010 it had become the fourth largest book festival in the United States, with 450 authors and 80,000 attendees. In addition to readings and lectures, it features a science fair, varied entertainment, food, and exhibitors ranging from local retailers and publishers to regional and national nonprofit organizations. In 2011, the Festival began presenting a Founder's Award; recipients include Elmore Leonard and R.L. Stine. The Tucson metro area is served by many local television stations and is the 68th largest designated market area (DMA) in the U.S. with 433,310 homes (0.39% of the total U.S.). It is limited to the three counties of southeastern Arizona (Pima, Santa Cruz, and Cochise) The major television networks serving Tucson are: KVOA 4 (NBC), KGUN 9 (ABC), KMSB-TV 11 (Fox), KOLD-TV 13 (CBS), KTTU 18 (My Network TV) and KWBA 58 (The CW). KUAT-TV 6 is a PBS affiliate run by the University of Arizona (as is sister station KUAS 27). Tucson's Sun Tran bus system serves greater Tucson with standard, express, regional shuttle, and on-demand shuttle bus service. It was awarded Best Transit System in 1988 and 2005. A 3.9-mile streetcar line, Sun Link, connects the University of Arizona campus with 4th Avenue, downtown, and the Mercado District west of Interstate 10 and the Santa Cruz River. Ten-minute headway passenger service began July 25, 2014. The streetcar utilizes Sun Tran's card payment and transfer system, connecting with the University of Arizona's CatTran shuttles, Amtrak, and Greyhound intercity bus service. Near the intersection of Craycroft and Ft. Lowell Roads are the remnants of the Historic Fort Lowell. This area has become one of Tucson's iconic neighborhoods. In 1891, the Fort was abandoned and much of the interior was stripped of their useful components and it quickly fell into ruin. In 1900, three of the officer buildings were purchased for use as a sanitarium. The sanitarium was then sold to Harvey Adkins in 1928. The Bolsius family Pete, Nan and Charles Bolsius purchased and renovated surviving adobe buildings of the Fort – transforming them into spectacular artistic southwestern architectural examples. Their woodwork, plaster treatment and sense of proportion drew on their Dutch heritage and New Mexican experience. Other artists and academics throughout the middle of the 20th century, including: Win Ellis, Jack Maul, Madame Cheruy, Giorgio Belloli, Charels Bode, Veronica Hughart, Edward and Rosamond Spicer, Hazel Larson Archer and Ruth Brown, renovated adobes, built homes and lived in the area. The artist colony attracted writers and poets including beat generation Alan Harrington and Jack Kerouac whose visit is documented in his iconic book On the Road. This rural pocket in the middle of the city is listed on the National Register of Historic Places. Each year in February the neighborhood celebrates its history in the City Landmark it owns and restored the San Pedro Chapel. To prevent further loss of groundwater, Tucson has been involved in water conservation and groundwater preservation efforts, shifting away from its reliance on a series of Tucson area wells in favor of conservation, consumption-based pricing for residential and commercial water use, and new wells in the more sustainable Avra Valley aquifer, northwest of the city. An allocation from the Central Arizona Project Aqueduct (CAP), which passes more than 300 mi (480 km) across the desert from the Colorado River, has been incorporated into the city's water supply, annually providing over 20 million gallons of ""recharged"" water which is pumped into the ground to replenish water pumped out. Since 2001, CAP water has allowed the city to remove or turn off over 80 wells. A combination of urban and suburban development, the West Side is generally defined as the area west of I-10. Western Tucson encompasses the banks of the Santa Cruz River and the foothills of the Tucson Mountains, and includes the International Wildlife Museum, Sentinel Peak, and the Marriott Starr Pass Resort & Spa, located in the wealthy enclave known as Starr Pass. Moving past the Tucson Mountains, travelers find themselves in the area commonly referred to as ""west of"" Tucson or ""Old West Tucson"". A large undulating plain extending south into the Altar Valley, rural residential development predominates, but here you will also find major attractions including Saguaro National Park West, the Arizona-Sonora Desert Museum, and the Old Tucson Studios movie set/theme park. Another popular event held in February, which is early spring in Tucson, is the Fiesta de los Vaqueros, or rodeo week, founded by winter visitor, Leighton Kramer. While at its heart the Fiesta is a sporting event, it includes what is billed as ""the world's largest non-mechanized parade"". The Rodeo Parade is a popular event as most schools give two rodeo days off instead of Presidents Day. The exception to this is Presidio High (a non-public charter school), which doesn't get either. Western wear is seen throughout the city as corporate dress codes are cast aside during the Fiesta. The Fiesta de los Vaqueros marks the beginning of the rodeo season in the United States. The University of Arizona Wildcats sports teams, most notably the men's basketball and women's softball teams have strong local interest. The men's basketball team, formerly coached by Hall of Fame head coach Lute Olson and currently coached by Sean Miller, has made 25 straight NCAA Tournaments and won the 1997 National Championship. Arizona's Softball team has reached the NCAA National Championship game 12 times and has won 8 times, most recently in 2007. The university's swim teams have gained international recognition, with swimmers coming from as far as Japan and Africa to train with the coach Frank Busch who has also worked with the U.S. Olympic swim team for a number of years. Both men and women's swim teams recently[when?] won the NCAA National Championships. At the airport, where records have been kept since 1930, the record maximum temperature was 117 °F (47 °C) on June 26, 1990, and the record minimum temperature was 16 °F (−9 °C) on January 4, 1949. There is an average of 145.0 days annually with highs of 90 °F (32 °C) or higher and an average of 16.9 days with lows reaching or below the freezing mark. Measurable precipitation falls on an average of 53 days. The wettest year was 1983 with 21.86 in (555 mm) of precipitation, and the driest year was 1953 with 5.34 in (136 mm). The most rainfall in one month was 7.93 in (201 mm) in August 1955. The most rainfall in 24 hours was 3.93 in (100 mm) on July 29, 1958. Snow at the airport averages only 1.1 in (2.8 cm) annually. The most snow received in one year was 8.3 in (21 cm) and the most snow in one month was 6.8 in (17 cm) in December 1971. Much of Tucson's economic development has been centered on the development of the University of Arizona, which is currently the second largest employer in the city. Davis-Monthan Air Force Base, located on the southeastern edge of the city, also provides many jobs for Tucson residents. Its presence, as well as the presence of the US Army Intelligence Center (Fort Huachuca, the largest employer in the region in nearby Sierra Vista), has led to the development of a significant number of high-tech industries, including government contractors, in the area. The city of Tucson is also a major hub for the Union Pacific Railroad's Sunset Route that links the Los Angeles ports with the South/Southeast regions of the country. Tucson has one daily newspaper, the morning Arizona Daily Star. Wick Communications publishes the daily legal paper The Daily Territorial, while Boulder, Colo.-based 10/13 Communications publishes Tucson Weekly (an ""alternative"" publication), Inside Tucson Business and the Explorer. TucsonSentinel.com is a nonprofit independent online news organization. Tucson Lifestyle Magazine, Lovin' Life News, DesertLeaf, and Zócalo Magazine are monthly publications covering arts, architecture, decor, fashion, entertainment, business, history, and other events. The Arizona Daily Wildcat is the University of Arizona's student newspaper, and the Aztec News is the Pima Community College student newspaper. The New Vision is the newspaper for the Roman Catholic Diocese of Tucson, and the Arizona Jewish Post is the newspaper of the Jewish Federation of Southern Arizona. The monsoon can begin any time from mid-June to late July, with an average start date around July 3. It typically continues through August and sometimes into September. During the monsoon, the humidity is much higher than the rest of the year. It begins with clouds building up from the south in the early afternoon followed by intense thunderstorms and rainfall, which can cause flash floods. The evening sky at this time of year is often pierced with dramatic lightning strikes. Large areas of the city do not have storm sewers, so monsoon rains flood the main thoroughfares, usually for no longer than a few hours. A few underpasses in Tucson have ""feet of water"" scales painted on their supports to discourage fording by automobiles during a rainstorm. Arizona traffic code Title 28-910, the so-called ""Stupid Motorist Law"", was instituted in 1995 to discourage people from entering flooded roadways. If the road is flooded and a barricade is in place, motorists who drive around the barricade can be charged up to $2000 for costs involved in rescuing them. Despite all warnings and precautions, however, three Tucson drivers have drowned between 2004 and 2010. Tucson is known for being a trailblazer in voluntary partial publicly financed campaigns. Since 1985, both mayoral and council candidates have been eligible to receive matching public funds from the city. To become eligible, council candidates must receive 200 donations of $10 or more (300 for a mayoral candidate). Candidates must then agree to spending limits equal to 33¢ for every registered Tucson voter, or $79,222 in 2005 (the corresponding figures for mayor are 64¢ per registered voter, or $142,271 in 2003). In return, candidates receive matching funds from the city at a 1:1 ratio of public money to private donations. The only other limitation is that candidates may not exceed 75% of the limit by the date of the primary. Many cities, such as San Francisco and New York City, have copied this system, albeit with more complex spending and matching formulas. Tucson's largest park, Reid Park, is located in midtown and includes Reid Park Zoo and Hi Corbett Field. Speedway Boulevard, a major east-west arterial road in central Tucson, was named the ""ugliest street in America"" by Life magazine in the early 1970s, quoting Tucson Mayor James Corbett. Despite this, Speedway Boulevard was awarded ""Street of the Year"" by Arizona Highways in the late 1990s. According to David Leighton, historical writer for the Arizona Daily Star newspaper, Speedway Boulevard derives its name from an old horse racetrack, known as ""The Harlem River Speedway,"" more commonly called ""The Speedway,"" in New York City. The street was called ""The Speedway,"" from 1904 to about 1906 before the word ""The"" was taken out. Catalina Highway stretches 25 miles (40 km) and the entire mountain range is one of Tucson's most popular vacation spots for cycling, hiking, rock climbing, camping, birding, and wintertime snowboarding and skiing. Near the top of Mt. Lemmon is the town of Summerhaven. In Summerhaven, visitors will find log houses and cabins, a general store, and various shops, as well as numerous hiking trails. Near Summerhaven is the road to Ski Valley which hosts a ski lift, several runs, a giftshop, and nearby restaurant. Tucson's primary electrical power source is a coal and natural gas power-plant managed by Tucson Electric Power that is situated within the city limits on the south-western boundary of Davis-Monthan Air-force base adjacent to Interstate-10. The air pollution generated has raised some concerns as the Sundt operating station has been online since 1962 as is exempt from many pollution standards and controls due to its age. Solar has been gaining ground in Tucson with its ideal over 300 days of sunshine climate. Federal, state, and even local utility credits and incentives have also enticed residents to equip homes with solar systems. Davis-Monthan AFB has a 3.3 Megawatt (MW) ground-mounted solar photovoltaic (PV) array and a 2.7 MW rooftop-mounted PV array, both of which are located in the Base Housing area. The base will soon have the largest solar-generating capacity in the United States Department of Defense after awarding a contract on September 10, 2010, to SunEdison to construct a 14.5 MW PV field on the northwestern side of the base. Tucson (/ˈtuːsɒn/ /tuːˈsɒn/) is a city and the county seat of Pima County, Arizona, United States, and home to the University of Arizona. The 2010 United States Census put the population at 520,116, while the 2013 estimated population of the entire Tucson metropolitan statistical area (MSA) was 996,544. The Tucson MSA forms part of the larger Tucson-Nogales combined statistical area (CSA), with a total population of 980,263 as of the 2010 Census. Tucson is the second-largest populated city in Arizona behind Phoenix, both of which anchor the Arizona Sun Corridor. The city is  located 108 miles (174 km) southeast of Phoenix and 60 mi (97 km) north of the U.S.-Mexico border. Tucson is the 33rd largest city and the 59th largest metropolitan area in the United States. Roughly 150 Tucson companies are involved in the design and manufacture of optics and optoelectronics systems, earning Tucson the nickname Optics Valley. The League of American Bicyclists gave Tucson a gold rating for bicycle friendliness in late April 2007. Tucson hosts the largest perimeter cycling event in the United States. The ride called ""El Tour de Tucson"" happens in November on the Saturday before Thanksgiving. El Tour de Tucson produced and promoted by Perimeter Bicycling has as many as 10,000 participants from all over the world, annually. Tucson is one of only nine cities in the U.S. to receive a gold rating or higher for cycling friendliness from the League of American Bicyclists. The city is known for its winter cycling opportunities. Both road and mountain biking are popular in and around Tucson with trail areas including Starr Pass and Fantasy Island. Arizona, south of the Gila River was legally bought from Mexico in the Gadsden Purchase on June 8, 1854. Tucson became a part of the United States of America, although the American military did not formally take over control until March 1856. In 1857 Tucson became a stage station on the San Antonio-San Diego Mail Line and in 1858 became 3rd division headquarters of the Butterfield Overland Mail until the line shut down in March 1861. The Overland Mail Corporation attempted to continue running, however following the Bascom Affair, devastating Apache attacks on the stations and coaches ended operations in August 1861.[citation needed] In general, Tucson and Pima County support the Democratic Party, as opposed the state's largest metropolitan area, Phoenix, which usually supports the Republican Party. Congressional redistricting in 2013, following the publication of the 2010 Census, divided the Tucson area into three Federal Congressional districts (the first, second and third of Arizona). The city center is in the 3rd District, represented by Raul Grijalva, a Democrat, since 2003, while the more affluent residential areas to the south and east are in the 2nd District, represented by Republican Martha McSally since 2015, and the exurbs north and west between Tucson and Phoenix in the 3rd District are represented by Democrat Ann Kirkpatrick since 2008. The United States Postal Service operates post offices in Tucson. The Tucson Main Post Office is located at 1501 South Cherrybell Stravenue. Tracks include Tucson Raceway Park and Rillito Downs. Tucson Raceway Park hosts NASCAR-sanctioned auto racing events and is one of only two asphalt short tracks in Arizona. Rillito Downs is an in-town destination on weekends in January and February each year. This historic track held the first organized quarter horse races in the world, and they are still racing there. The racetrack is threatened by development. The Moltacqua racetrack, was another historic horse racetrack located on what is now Sabino Canyon Road and Vactor Ranch Trail, but it no longer exists. Tucson is located 118 mi (190 km) southeast of Phoenix and 60 mi (97 km) north of the United States - Mexico border. The 2010 United States Census puts the city's population at 520,116 with a metropolitan area population at 980,263. In 2009, Tucson ranked as the 32nd largest city and 52nd largest metropolitan area in the United States. A major city in the Arizona Sun Corridor, Tucson is the largest city in southern Arizona, the second largest in the state after Phoenix. It is also the largest city in the area of the Gadsden Purchase. As of 2015, The Greater Tucson Metro area has exceeded a population of 1 million. By 1900, 7,531 people lived in the city. The population increased gradually to 13,913 in 1910. At about this time, the U.S. Veterans Administration had begun construction on the present Veterans Hospital. Many veterans who had been gassed in World War I and were in need of respiratory therapy began coming to Tucson after the war, due to the clean dry air. Over the following years the city continued to grow, with the population increasing to 20,292 in 1920 and 36,818 in 1940. In 2006 the population of Pima County, in which Tucson is located, passed one million while the City of Tucson's population was 535,000. Also on the north side is the suburban community of Catalina Foothills, located in the foothills of the Santa Catalina Mountains just north of the city limits. This community includes among the area's most expensive homes, sometimes multimillion-dollar estates. The Foothills area is generally defined as north of River Road, east of Oracle Road, and west of Sabino Creek. Some of the Tucson area's major resorts are located in the Catalina Foothills, including the Hacienda Del Sol, Westin La Paloma Resort, Loews Ventana Canyon Resort and Canyon Ranch Resort. La Encantada, an upscale outdoor shopping mall, is also in the Foothills. In an effort to conserve water, Tucson is recharging groundwater supplies by running part of its share of CAP water into various open portions of local rivers to seep into their aquifer. Additional study is scheduled to determine the amount of water that is lost through evaporation from the open areas, especially during the summer. The City of Tucson already provides reclaimed water to its inhabitants, but it is only used for ""applications such as irrigation, dust control, and industrial uses."" These resources have been in place for more than 27 years, and deliver to over 900 locations. For the past 25 years, the Tucson Folk Festival has taken place the first Saturday and Sunday of May in downtown Tucson's El Presidio Park. In addition to nationally known headline acts each evening, the Festival highlights over 100 local and regional musicians on five stages is one of the largest free festivals in the country. All stages are within easy walking distance. Organized by the Tucson Kitchen Musicians Association, volunteers make this festival possible. KXCI 91.3-FM, Arizona's only community radio station, is a major partner, broadcasting from the Plaza Stage throughout the weekend. In addition, there are numerous workshops, events for children, sing-alongs, and a popular singer/songwriter contest. Musicians typically play 30-minute sets, supported by professional audio staff volunteers. A variety of food and crafts are available at the festival, as well as local micro-brews. All proceeds from sales go to fund future festivals. Jesuit missionary Eusebio Francisco Kino visited the Santa Cruz River valley in 1692, and founded the Mission San Xavier del Bac in 1700 about 7 mi (11 km) upstream from the site of the settlement of Tucson. A separate Convento settlement was founded downstream along the Santa Cruz River, near the base of what is now ""A"" mountain. Hugo O'Conor, the founding father of the city of Tucson, Arizona authorized the construction of a military fort in that location, Presidio San Agustín del Tucsón, on August 20, 1775 (near the present downtown Pima County Courthouse). During the Spanish period of the presidio, attacks such as the Second Battle of Tucson were repeatedly mounted by Apaches. Eventually the town came to be called ""Tucson"" and became a part of Sonora after Mexico gained independence from Spain in 1821. The Tucson Padres played at Kino Veterans Memorial Stadium from 2011 to 2013. They served as the AAA affiliate of the San Diego Padres. The team, formerly known as the Portland Beavers, was temporarily relocated to Tucson from Portland while awaiting the building of a new stadium in Escondido. Legal issues derailed the plans to build the Escondido stadium, so they moved to El Paso, Texas for the 2014 season. Previously, the Tucson Sidewinders, a triple-A affiliate of the Arizona Diamondbacks, won the Pacific Coast League championship and unofficial AAA championship in 2006. The Sidewinders played in Tucson Electric Park and were in the Pacific Conference South of the PCL. The Sidewinders were sold in 2007 and moved to Reno, Nevada after the 2008 season. They now compete as the Reno Aces. At the University of Arizona, where records have been kept since 1894, the record maximum temperature was 115 °F (46 °C) on June 19, 1960, and July 28, 1995, and the record minimum temperature was 6 °F (−14 °C) on January 7, 1913. There are an average of 150.1 days annually with highs of 90 °F (32 °C) or higher and an average of 26.4 days with lows reaching or below the freezing mark. Average annual precipitation is 11.15 in (283 mm). There is an average of 49 days with measurable precipitation. The wettest year was 1905 with 24.17 in (614 mm) and the driest year was 1924 with 5.07 in (129 mm). The most precipitation in one month was 7.56 in (192 mm) in July 1984. The most precipitation in 24 hours was 4.16 in (106 mm) on October 1, 1983. Annual snowfall averages 0.7 in (1.8 cm). The most snow in one year was 7.2 in (18 cm) in 1987. The most snow in one month was 6.0 in (15 cm) in January 1898 and March 1922. The city's elevation is 2,643 ft (806 m) above sea level (as measured at the Tucson International Airport). Tucson is situated on an alluvial plain in the Sonoran desert, surrounded by five minor ranges of mountains: the Santa Catalina Mountains and the Tortolita Mountains to the north, the Santa Rita Mountains to the south, the Rincon Mountains to the east, and the Tucson Mountains to the west. The high point of the Santa Catalina Mountains is 9,157 ft (2,791 m) Mount Lemmon, the southernmost ski destination in the continental U.S., while the Tucson Mountains include 4,687 ft (1,429 m) Wasson Peak. The highest point in the area is Mount Wrightson, found in the Santa Rita Mountains at 9,453 ft (2,881 m) above sea level. The community of Casas Adobes is also on the Northwest Side, with the distinction of being Tucson's first suburb, established in the late 1940s. Casas Adobes is centered on the historic Casas Adobes Plaza (built in 1948). Casas Adobes is also home to Tohono Chul Park (a nature preserve) near the intersection of North Oracle Road and West Ina Road. The attempted assassination of Representative Gabrielle Giffords, and the murders of chief judge for the U.S. District Court for Arizona, John Roll and five other people on January 8, 2011, occurred at the La Toscana Village in Casas Adobes. The Foothills Mall is also located on the northwest side in Casas Adobes. Perhaps the biggest sustainability problem in Tucson, with its high desert climate, is potable water supply. The state manages all water in Arizona through its Arizona Department of Water Resources (ADWR). The primary consumer of water is Agriculture (including golf courses), which consumes about 69% of all water. Municipal (which includes residential use) accounts for about 25% of use. Energy consumption and availability is another sustainability issue. However, with over 300 days of full sun a year, Tucson has demonstrated its potential to be an ideal solar energy producer. The accomplished and awarded writers (poets, novelists, dramatists, nonfiction writers) who have lived in Tucson include Edward Abbey, Erskine Caldwell, Barbara Kingsolver and David Foster Wallace. Some were associated with the University of Arizona, but many were independent writers who chose to make Tucson their home. The city is particularly active in publishing and presenting contemporary innovative poetry in various ways. Examples are the Chax Press, a publisher of poetry books in trade and book arts editions, and the University of Arizona Poetry Center, which has a sizable poetry library and presents readings, conferences, and workshops. As one of the oldest parts of town, Central Tucson is anchored by the Broadway Village shopping center designed by local architect Josias Joesler at the intersection of Broadway Boulevard and Country Club Road. The 4th Avenue Shopping District between downtown and the University and the Lost Barrio just East of downtown, also have many unique and popular stores. Local retail business in Central Tucson is densely concentrated along Fourth Avenue and the Main Gate Square on University Boulevard near the UA campus. The El Con Mall is also located in the eastern part of midtown. Southeast Tucson continues to experience rapid residential development. The area includes Davis-Monthan Air Force Base. The area is considered to be south of Golf Links Road. It is the home of Santa Rita High School, Chuck Ford Park (Lakeside Park), Lakeside Lake, Lincoln Park (upper and lower), The Lakecrest Neighborhoods, and Pima Community College East Campus. The Atterbury Wash with its access to excellent bird watching is also located in the Southeast Tucson area. The suburban community of Rita Ranch houses many of the military families from Davis-Monthan, and is near the southeastern-most expansion of the current city limits. Close by Rita Ranch and also within the city limits lies Civano, a planned development meant to showcase ecologically sound building practices and lifestyles. The City of Tucson, Pima County, the State of Arizona, and the private sector have all made commitments to create a growing, healthy economy[citation needed] with advanced technology industry sectors as its foundation. Raytheon Missile Systems (formerly Hughes Aircraft Co.), Texas Instruments, IBM, Intuit Inc., Universal Avionics, Honeywell Aerospace, Sunquest Information Systems, Sanofi-Aventis, Ventana Medical Systems, Inc., and Bombardier Aerospace all have a significant presence in Tucson. Roughly 150 Tucson companies are involved in the design and manufacture of optics and optoelectronics systems, earning Tucson the nickname ""Optics Valley"". Tucson is commonly known as ""The Old Pueblo"". While the exact origin of this nickname is uncertain, it is commonly traced back to Mayor R. N. ""Bob"" Leatherwood. When rail service was established to the city on March 20, 1880, Leatherwood celebrated the fact by sending telegrams to various leaders, including the President of the United States and the Pope, announcing that the ""ancient and honorable pueblo"" of Tucson was now connected by rail to the outside world. The term became popular with newspaper writers who often abbreviated it as ""A. and H. Pueblo"". This in turn transformed into the current form of ""The Old Pueblo"". Cycling is popular in Tucson due to its flat terrain and dry climate. Tucson and Pima County maintain an extensive network of marked bike routes, signal crossings, on-street bike lanes, mountain-biking trails, and dedicated shared-use paths. The Loop is a network of seven linear parks comprising over 100 mi (160 km) of paved, vehicle-free trails that encircles the majority of the city with links to Marana and Oro Valley. The Tucson-Pima County Bicycle Advisory Committee (TPCBAC) serves in an advisory capacity to local governments on issues relating to bicycle recreation, transportation, and safety. Tucson was awarded a gold rating for bicycle-friendliness by the League of American Bicyclists in 2006. Both the council members and the mayor serve four-year terms; none face term limits. Council members are nominated by their wards via a ward-level primary held in September. The top vote-earners from each party then compete at-large for their ward's seat on the November ballot. In other words, on election day the whole city votes on all the council races up for that year. Council elections are severed: Wards 1, 2, and 4 (as well as the mayor) are up for election in the same year (most recently 2011), while Wards 3, 5, and 6 share another year (most recently 2013). Central Tucson is bicycle-friendly. To the east of the University of Arizona, Third Street is bike-only except for local traffic and passes by the historic homes of the Sam Hughes neighborhood. To the west, E. University Boulevard leads to the Fourth Avenue Shopping District. To the North, N. Mountain Avenue has a full bike-only lane for half of the 3.5 miles (5.6 km) to the Rillito River Park bike and walk multi-use path. To the south, N. Highland Avenue leads to the Barraza-Aviation Parkway bicycle path. The expansive area northwest of the city limits is diverse, ranging from the rural communities of Catalina and parts of the town of Marana, the small suburb of Picture Rocks, the affluent town of Oro Valley in the western foothills of the Santa Catalina Mountains, and residential areas in the northeastern foothills of the Tucson Mountains. Continental Ranch (Marana), Dove Mountain (Marana), and Rancho Vistoso (Oro Valley) are all masterplanned communities located in the Northwest, where thousands of residents live. The Procession, held at sundown, consists of a non-motorized parade through downtown Tucson featuring many floats, sculptures, and memorials, in which the community is encouraged to participate. The parade is followed by performances on an outdoor stage, culminating in the burning of an urn in which written prayers have been collected from participants and spectators. The event is organized and funded by the non-profit arts organization Many Mouths One Stomach, with the assistance of many volunteers and donations from the public and local businesses. Winters in Tucson are mild relative to other parts of the United States. Daytime highs in the winter range between 64 and 75 °F (18 and 24 °C), with overnight lows between 30 and 44 °F (−1 and 7 °C). Tucson typically averages one hard freeze per winter season, with temperatures dipping to the mid or low-20s (−7 to −4 °C), but this is typically limited to only a very few nights. Although rare, snow has been known to fall in Tucson, usually a light dusting that melts within a day. The most recent snowfall was on February 20, 2013 when 2.0 inches of snow blanketed the city, the largest snowfall since 1987. From 1877 to 1878, the area suffered a rash of stagecoach robberies. Most notable, however, were the two holdups committed by masked road-agent William Whitney Brazelton. Brazelton held up two stages in the summer of 1878 near Point of Mountain Station approximately 17 mi (27 km) northwest of Tucson. John Clum, of Tombstone, Arizona fame was one of the passengers. Brazelton was eventually tracked down and killed on Monday August 19, 1878, in a mesquite bosque along the Santa Cruz River 3 miles (5 km) south of Tucson by Pima County Sheriff Charles A. Shibell and his citizen's posse. Brazelton had been suspected of highway robbery not only in the Tucson area, but also in the Prescott region and Silver City, New Mexico area as well. Brazelton's crimes prompted John J. Valentine, Sr. of Wells, Fargo & Co. to send special agent and future Pima County sheriff Bob Paul to investigate. Fort Lowell, then east of Tucson, was established to help protect settlers from Apache attacks. In 1882, Frank Stilwell was implicated in the murder of Morgan Earp by Cowboy Pete Spence's wife, Marietta, at the coroner's inquest on Morgan Earp's shooting. The coroner's jury concluded that Spence, Stilwell, Frederick Bode, and Florentino ""Indian Charlie"" Cruz were the prime suspects in the assassination of Morgan Earp. :250 Deputy U.S. Marshal Wyatt Earp gathered a few trusted friends and accompanied Virgil Earp and his family as they traveled to Benson for a train ride to California. They found Stilwell lying in wait for Virgil in the Tucson station and killed him on the tracks. After killing Stilwell, Wyatt deputized others and rode on a vendetta, killing three more cowboys over the next few days before leaving the state. Tucson has a desert climate (Köppen BWh), with two major seasons, summer and winter; plus three minor seasons: fall, spring, and the monsoon. Tucson averages 11.8 inches (299.7 mm) of precipitation per year, more than most other locations with desert climates, but it still qualifies due to its high evapotranspiration; in other words, it experiences a high net loss of water. A similar scenario is seen in Alice Springs, Australia, which averages 11 inches (279.4 mm) a year, but has a desert climate. As of the census of 2010, there were 520,116 people, 229,762 households, and 112,455 families residing in the city. The population density was 2,500.1 inhabitants per square mile (965.3/km²). There were 209,609 housing units at an average density of 1,076.7 per square mile (415.7/km²). The racial makeup of the city was 69.7% White (down from 94.8% in 1970), 5.0% Black or African-American, 2.7% Native American, 2.9% Asian, 0.2% Pacific Islander, 16.9% from other races, and 3.8% from two or more races. Hispanic or Latino of any race were 41.6% of the population. Non-Hispanic Whites were 47.2% of the population in 2010, down from 72.8% in 1970."
Architecture,"The architecture of different parts of Asia developed along different lines from that of Europe; Buddhist, Hindu and Sikh architecture each having different characteristics. Buddhist architecture, in particular, showed great regional diversity. Hindu temple architecture, which developed around the 3rd century BCE, is governed by concepts laid down in the Shastras, and is concerned with expressing the macrocosm and the microcosm. In many Asian countries, pantheistic religion led to architectural forms that were designed specifically to enhance the natural landscape. On the difference between the ideals of architecture and mere construction, the renowned 20th-century architect Le Corbusier wrote: ""You employ stone, wood, and concrete, and with these materials you build houses and palaces: that is construction. Ingenuity is at work. But suddenly you touch my heart, you do me good. I am happy and I say: This is beautiful. That is Architecture"". Meanwhile, the Industrial Revolution laid open the door for mass production and consumption. Aesthetics became a criterion for the middle class as ornamented products, once within the province of expensive craftsmanship, became cheaper under machine production. In Renaissance Europe, from about 1400 onwards, there was a revival of Classical learning accompanied by the development of Renaissance Humanism which placed greater emphasis on the role of the individual in society than had been the case during the Medieval period. Buildings were ascribed to specific architects – Brunelleschi, Alberti, Michelangelo, Palladio – and the cult of the individual had begun. There was still no dividing line between artist, architect and engineer, or any of the related vocations, and the appellation was often one of regional preference. Environmental sustainability has become a mainstream issue, with profound effect on the architectural profession. Many developers, those who support the financing of buildings, have become educated to encourage the facilitation of environmentally sustainable design, rather than solutions based primarily on immediate cost. Major examples of this can be found in Passive solar building design, greener roof designs, biodegradable materials, and more attention to a structure's energy usage. This major shift in architecture has also changed architecture schools to focus more on the environment. Sustainability in architecture was pioneered by Frank Lloyd Wright, in the 1960s by Buckminster Fuller and in the 1970s by architects such as Ian McHarg and Sim Van der Ryn in the US and Brenda and Robert Vale in the UK and New Zealand. There has been an acceleration in the number of buildings which seek to meet green building sustainable design principles. Sustainable practices that were at the core of vernacular architecture increasingly provide inspiration for environmentally and socially sustainable contemporary techniques. The U.S. Green Building Council's LEED (Leadership in Energy and Environmental Design) rating system has been instrumental in this. To restrict the meaning of (architectural) formalism to art for art's sake is not only reactionary; it can also be a purposeless quest for perfection or originality which degrades form into a mere instrumentality"". The 19th-century English art critic, John Ruskin, in his Seven Lamps of Architecture, published 1849, was much narrower in his view of what constituted architecture. Architecture was the ""art which so disposes and adorns the edifices raised by men ... that the sight of them"" contributes ""to his mental health, power, and pleasure"". It is widely assumed that architectural success was the product of a process of trial and error, with progressively less trial and more replication as the results of the process proved increasingly satisfactory. What is termed vernacular architecture continues to be produced in many parts of the world. Indeed, vernacular buildings make up most of the built world that people experience every day. Early human settlements were mostly rural. Due to a surplus in production the economy began to expand resulting in urbanization thus creating urban areas which grew and evolved very rapidly in some cases, such as that of Çatal Höyük in Anatolia and Mohenjo Daro of the Indus Valley Civilization in modern-day Pakistan. When modern architecture was first practiced, it was an avant-garde movement with moral, philosophical, and aesthetic underpinnings. Immediately after World War I, pioneering modernist architects sought to develop a completely new style appropriate for a new post-war social and economic order, focused on meeting the needs of the middle and working classes. They rejected the architectural practice of the academic refinement of historical styles which served the rapidly declining aristocratic order. The approach of the Modernist architects was to reduce buildings to pure forms, removing historical references and ornament in favor of functionalist details. Buildings displayed their functional and structural elements, exposing steel beams and concrete surfaces instead of hiding them behind decorative forms. While the notion that structural and aesthetic considerations should be entirely subject to functionality was met with both popularity and skepticism, it had the effect of introducing the concept of ""function"" in place of Vitruvius' ""utility"". ""Function"" came to be seen as encompassing all criteria of the use, perception and enjoyment of a building, not only practical but also aesthetic, psychological and cultural. According to Vitruvius, the architect should strive to fulfill each of these three attributes as well as possible. Leon Battista Alberti, who elaborates on the ideas of Vitruvius in his treatise, De Re Aedificatoria, saw beauty primarily as a matter of proportion, although ornament also played a part. For Alberti, the rules of proportion were those that governed the idealised human figure, the Golden mean. The most important aspect of beauty was therefore an inherent part of an object, rather than something applied superficially; and was based on universal, recognisable truths. The notion of style in the arts was not developed until the 16th century, with the writing of Vasari: by the 18th century, his Lives of the Most Excellent Painters, Sculptors, and Architects had been translated into Italian, French, Spanish and English. Texts on architecture have been written since ancient time. These texts provided both general advice and specific formal prescriptions or canons. Some examples of canons are found in the writings of the 1st-century BCE Roman Architect Vitruvius. Some of the most important early examples of canonic architecture are religious. One such reaction to the cold aesthetic of modernism and Brutalism is the school of metaphoric architecture, which includes such things as biomorphism and zoomorphic architecture, both using nature as the primary source of inspiration and design. While it is considered by some to be merely an aspect of postmodernism, others consider it to be a school in its own right and a later development of expressionist architecture. The earliest surviving written work on the subject of architecture is De architectura, by the Roman architect Vitruvius in the early 1st century AD. According to Vitruvius, a good building should satisfy the three principles of firmitas, utilitas, venustas, commonly known by the original translation – firmness, commodity and delight. An equivalent in modern English would be: Many architects resisted modernism, finding it devoid of the decorative richness of historical styles. As the first generation of modernists began to die after WWII, a second generation of architects including Paul Rudolph, Marcel Breuer, and Eero Saarinen tried to expand the aesthetics of modernism with Brutalism, buildings with expressive sculptural facades made of unfinished concrete. But an even new younger postwar generation critiqued modernism and Brutalism for being too austere, standardized, monotone, and not taking into account the richness of human experience offered in historical buildings across time and in different places and cultures. In the early 19th century, Augustus Welby Northmore Pugin wrote Contrasts (1836) that, as the titled suggested, contrasted the modern, industrial world, which he disparaged, with an idealized image of neo-medieval world. Gothic architecture, Pugin believed, was the only ""true Christian form of architecture."" Beginning in the late 1950s and 1960s, architectural phenomenology emerged as an important movement in the early reaction against modernism, with architects like Charles Moore in the USA, Christian Norberg-Schulz in Norway, and Ernesto Nathan Rogers and Vittorio Gregotti in Italy, who collectively popularized an interest in a new contemporary architecture aimed at expanding human experience using historical buildings as models and precedents. Postmodernism produced a style that combined contemporary building technology and cheap materials, with the aesthetics of older pre-modern and non-modern styles, from high classical architecture to popular or vernacular regional building styles. Robert Venturi famously defined postmodern architecture as a ""decorated shed"" (an ordinary building which is functionally designed inside and embellished on the outside), and upheld it against modernist and brutalist ""ducks"" (buildings with unnecessarily expressive tectonic forms). Islamic architecture began in the 7th century CE, incorporating architectural forms from the ancient Middle East and Byzantium, but also developing features to suit the religious and social needs of the society. Examples can be found throughout the Middle East, North Africa, Spain and the Indian Sub-continent. The widespread application of the pointed arch was to influence European architecture of the Medieval period. Building first evolved out of the dynamics between needs (shelter, security, worship, etc.) and means (available building materials and attendant skills). As human cultures developed and knowledge began to be formalized through oral traditions and practices, building became a craft, and ""architecture"" is the name given to the most highly formalized and respected versions of that craft. The architecture and urbanism of the Classical civilizations such as the Greek and the Roman evolved from civic ideals rather than religious or empirical ones and new building types emerged. Architectural ""style"" developed in the form of the Classical orders. A revival of the Classical style in architecture was accompanied by a burgeoning of science and engineering which affected the proportions and structure of buildings. At this stage, it was still possible for an artist to design a bridge as the level of structural calculations involved was within the scope of the generalist. Architecture has to do with planning and designing form, space and ambience to reflect functional, technical, social, environmental and aesthetic considerations. It requires the creative manipulation and coordination of materials and technology, and of light and shadow. Often, conflicting requirements must be resolved. The practice of Architecture also encompasses the pragmatic aspects of realizing buildings and structures, including scheduling, cost estimation and construction administration. Documentation produced by architects, typically drawings, plans and technical specifications, defines the structure and/or behavior of a building or other kind of system that is to be or has been constructed. In Europe during the Medieval period, guilds were formed by craftsmen to organise their trades and written contracts have survived, particularly in relation to ecclesiastical buildings. The role of architect was usually one with that of master mason, or Magister lathomorum as they are sometimes described in contemporary documents. Architects such as Frank Lloyd Wright developed Organic architecture, in which the form was defined by its environment and purpose, with an aim to promote harmony between human habitation and the natural world with prime examples being Robie House and Fallingwater. Around the beginning of the 20th century, a general dissatisfaction with the emphasis on revivalist architecture and elaborate decoration gave rise to many new lines of thought that served as precursors to Modern Architecture. Notable among these is the Deutscher Werkbund, formed in 1907 to produce better quality machine made objects. The rise of the profession of industrial design is usually placed here. Following this lead, the Bauhaus school, founded in Weimar, Germany in 1919, redefined the architectural bounds prior set throughout history, viewing the creation of a building as the ultimate synthesis—the apex—of art, craft, and technology. In many ancient civilizations, such as those of Egypt and Mesopotamia, architecture and urbanism reflected the constant engagement with the divine and the supernatural, and many ancient cultures resorted to monumentality in architecture to represent symbolically the political power of the ruler, the ruling elite, or the state itself. Architects such as Mies van der Rohe, Philip Johnson and Marcel Breuer worked to create beauty based on the inherent qualities of building materials and modern construction techniques, trading traditional historic forms for simplified geometric forms, celebrating the new means and methods made possible by the Industrial Revolution, including steel-frame construction, which gave birth to high-rise superstructures. By mid-century, Modernism had morphed into the International Style, an aesthetic epitomized in many ways by the Twin Towers of New York's World Trade Center designed by Minoru Yamasaki. For Ruskin, the aesthetic was of overriding significance. His work goes on to state that a building is not truly a work of architecture unless it is in some way ""adorned"". For Ruskin, a well-constructed, well-proportioned, functional building needed string courses or rustication, at the very least. In the late 20th century a new concept was added to those included in the compass of both structure and function, the consideration of sustainability, hence sustainable architecture. To satisfy the contemporary ethos a building should be constructed in a manner which is environmentally friendly in terms of the production of its materials, its impact upon the natural and built environment of its surrounding area and the demands that it makes upon non-sustainable power sources for heating, cooling, water and waste management and lighting. The major architectural undertakings were the buildings of abbeys and cathedrals. From about 900 CE onwards, the movements of both clerics and tradesmen carried architectural knowledge across Europe, resulting in the pan-European styles Romanesque and Gothic. Architecture (Latin architectura, from the Greek ἀρχιτέκτων arkhitekton ""architect"", from ἀρχι- ""chief"" and τέκτων ""builder"") is both the process and the product of planning, designing, and constructing buildings and other physical structures. Architectural works, in the material form of buildings, are often perceived as cultural symbols and as works of art. Historical civilizations are often identified with their surviving architectural achievements. Nunzia Rondanini stated, ""Through its aesthetic dimension architecture goes beyond the functional aspects that it has in common with other human sciences. Through its own particular way of expressing values, architecture can stimulate and influence social life without presuming that, in and of itself, it will promote social development.' Early Asian writings on architecture include the Kao Gong Ji of China from the 7th–5th centuries BCE; the Shilpa Shastras of ancient India and Manjusri Vasthu Vidya Sastra of Sri Lanka. Since the 1980s, as the complexity of buildings began to increase (in terms of structural systems, services, energy and technologies), the field of architecture became multi-disciplinary with specializations for each project type, technological expertise or project delivery methods. In addition, there has been an increased separation of the 'design' architect [Notes 1] from the 'project' architect who ensures that the project meets the required standards and deals with matters of liability.[Notes 2] The preparatory processes for the design of any large building have become increasingly complicated, and require preliminary studies of such matters as durability, sustainability, quality, money, and compliance with local laws. A large structure can no longer be the design of one person but must be the work of many. Modernism and Postmodernism have been criticised by some members of the architectural profession who feel that successful architecture is not a personal, philosophical, or aesthetic pursuit by individualists; rather it has to consider everyday needs of people and use technology to create liveable environments, with the design process being informed by studies of behavioral, environmental, and social sciences. Among the philosophies that have influenced modern architects and their approach to building design are rationalism, empiricism, structuralism, poststructuralism, and phenomenology. Concurrently, the recent movements of New Urbanism, Metaphoric architecture and New Classical Architecture promote a sustainable approach towards construction, that appreciates and develops smart growth, architectural tradition and classical design. This in contrast to modernist and globally uniform architecture, as well as leaning against solitary housing estates and suburban sprawl. With the emerging knowledge in scientific fields and the rise of new materials and technology, architecture and engineering began to separate, and the architect began to concentrate on aesthetics and the humanist aspects, often at the expense of technical aspects of building design. There was also the rise of the ""gentleman architect"" who usually dealt with wealthy clients and concentrated predominantly on visual qualities derived usually from historical prototypes, typified by the many country houses of Great Britain that were created in the Neo Gothic or Scottish Baronial styles. Formal architectural training in the 19th century, for example at École des Beaux-Arts in France, gave much emphasis to the production of beautiful drawings and little to context and feasibility. Effective architects generally received their training in the offices of other architects, graduating to the role from draughtsmen or clerks. Vernacular architecture became increasingly ornamental. House builders could use current architectural design in their work by combining features found in pattern books and architectural journals."
Cotton,"Most cotton in the United States, Europe and Australia is harvested mechanically, either by a cotton picker, a machine that removes the cotton from the boll without damaging the cotton plant, or by a cotton stripper, which strips the entire boll off the plant. Cotton strippers are used in regions where it is too windy to grow picker varieties of cotton, and usually after application of a chemical defoliant or the natural defoliation that occurs after a freeze. Cotton is a perennial crop in the tropics, and without defoliation or freezing, the plant will continue to grow. In Iran (Persia), the history of cotton dates back to the Achaemenid era (5th century BC); however, there are few sources about the planting of cotton in pre-Islamic Iran. The planting of cotton was common in Merv, Ray and Pars of Iran. In Persian poets' poems, especially Ferdowsi's Shahname, there are references to cotton (""panbe"" in Persian). Marco Polo (13th century) refers to the major products of Persia, including cotton. John Chardin, a French traveler of the 17th century who visited the Safavid Persia, spoke approvingly of the vast cotton farms of Persia. Though known since antiquity the commercial growing of cotton in Egypt only started in 1820's, following a Frenchman, by the name of M. Jumel, propositioning the then ruler, Mohamed Ali Pasha, that he could earn a substantial income by growing an extra-long staple Maho (Barbadence) cotton, in Lower Egypt, for the French market. Mohamed Ali Pasha accepted the proposition and granted himself the monopoly on the sale and export of cotton in Egypt; and later dictated cotton should be grown in preference to other crops. By the time of the American Civil war annual exports had reached $16 million (120,000 bales), which rose to $56 million by 1864, primarily due to the loss of the Confederate supply on the world market. Exports continued to grow even after the reintroduction of US cotton, produced now by a paid workforce, and Egyptian exports reached 1.2 million bales a year by 1903. The era of manufactured fibers began with the development of rayon in France in the 1890s. Rayon is derived from a natural cellulose and cannot be considered synthetic, but requires extensive processing in a manufacturing process, and led the less expensive replacement of more naturally derived materials. A succession of new synthetic fibers were introduced by the chemicals industry in the following decades. Acetate in fiber form was developed in 1924. Nylon, the first fiber synthesized entirely from petrochemicals, was introduced as a sewing thread by DuPont in 1936, followed by DuPont's acrylic in 1944. Some garments were created from fabrics based on these fibers, such as women's hosiery from nylon, but it was not until the introduction of polyester into the fiber marketplace in the early 1950s that the market for cotton came under threat. The rapid uptake of polyester garments in the 1960s caused economic hardship in cotton-exporting economies, especially in Central American countries, such as Nicaragua, where cotton production had boomed tenfold between 1950 and 1965 with the advent of cheap chemical pesticides. Cotton production recovered in the 1970s, but crashed to pre-1960 levels in the early 1990s. Production capacity in Britain and the United States was improved by the invention of the cotton gin by the American Eli Whitney in 1793. Before the development of cotton gins, the cotton fibers had to be pulled from the seeds tediously by hand. By the late 1700s a number of crude ginning machines had been developed. However, to produce a bale of cotton required over 600 hours of human labor, making large-scale production uneconomical in the United States, even with the use of humans as slave labor. The gin that Whitney manufactured (the Holmes design) reduced the hours down to just a dozen or so per bale. Although Whitney patented his own design for a cotton gin, he manufactured a prior design from Henry Odgen Holmes, for which Holmes filed a patent in 1796. Improving technology and increasing control of world markets allowed British traders to develop a commercial chain in which raw cotton fibers were (at first) purchased from colonial plantations, processed into cotton cloth in the mills of Lancashire, and then exported on British ships to captive colonial markets in West Africa, India, and China (via Shanghai and Hong Kong). In addition to concerns over subsidies, the cotton industries of some countries are criticized for employing child labor and damaging workers' health by exposure to pesticides used in production. The Environmental Justice Foundation has campaigned against the prevalent use of forced child and adult labor in cotton production in Uzbekistan, the world's third largest cotton exporter. The international production and trade situation has led to ""fair trade"" cotton clothing and footwear, joining a rapidly growing market for organic clothing, fair fashion or ""ethical fashion"". The fair trade system was initiated in 2005 with producers from Cameroon, Mali and Senegal. India's cotton-processing sector gradually declined during British expansion in India and the establishment of colonial rule during the late 18th and early 19th centuries. This was largely due to aggressive colonialist mercantile policies of the British East India Company, which made cotton processing and manufacturing workshops in India uncompetitive. Indian markets were increasingly forced to supply only raw cotton and, by British-imposed law, to purchase manufactured textiles from Britain.[citation needed] While Brazil was fighting the US through the WTO's Dispute Settlement Mechanism against a heavily subsidized cotton industry, a group of four least-developed African countries – Benin, Burkina Faso, Chad, and Mali – also known as ""Cotton-4"" have been the leading protagonist for the reduction of US cotton subsidies through negotiations. The four introduced a ""Sectoral Initiative in Favour of Cotton"", presented by Burkina Faso's President Blaise Compaoré during the Trade Negotiations Committee on 10 June 2003. Beginning as a self-help program in the mid-1960s, the Cotton Research and Promotion Program (CRPP) was organized by U.S. cotton producers in response to cotton's steady decline in market share. At that time, producers voted to set up a per-bale assessment system to fund the program, with built-in safeguards to protect their investments. With the passage of the Cotton Research and Promotion Act of 1966, the program joined forces and began battling synthetic competitors and re-establishing markets for cotton. Today, the success of this program has made cotton the best-selling fiber in the U.S. and one of the best-selling fibers in the world.[citation needed] During the late medieval period, cotton became known as an imported fiber in northern Europe, without any knowledge of how it was derived, other than that it was a plant. Because Herodotus had written in his Histories, Book III, 106, that in India trees grew in the wild producing wool, it was assumed that the plant was a tree, rather than a shrub. This aspect is retained in the name for cotton in several Germanic languages, such as German Baumwolle, which translates as ""tree wool"" (Baum means ""tree""; Wolle means ""wool""). Noting its similarities to wool, people in the region could only imagine that cotton must be produced by plant-borne sheep. John Mandeville, writing in 1350, stated as fact the now-preposterous belief: ""There grew there [India] a wonderful tree which bore tiny lambs on the endes of its branches. These branches were so pliable that they bent down to allow the lambs to feed when they are hungrie  [sic]."" (See Vegetable Lamb of Tartary.) By the end of the 16th century, cotton was cultivated throughout the warmer regions in Asia and the Americas. The 25,000 cotton growers in the United States of America are heavily subsidized at the rate of $2 billion per year although China now provides the highest overall level of cotton sector support. The future of these subsidies is uncertain and has led to anticipatory expansion of cotton brokers' operations in Africa. Dunavant expanded in Africa by buying out local operations. This is only possible in former British colonies and Mozambique; former French colonies continue to maintain tight monopolies, inherited from their former colonialist masters, on cotton purchases at low fixed prices. Successful cultivation of cotton requires a long frost-free period, plenty of sunshine, and a moderate rainfall, usually from 600 to 1,200 mm (24 to 47 in). Soils usually need to be fairly heavy, although the level of nutrients does not need to be exceptional. In general, these conditions are met within the seasonally dry tropics and subtropics in the Northern and Southern hemispheres, but a large proportion of the cotton grown today is cultivated in areas with less rainfall that obtain the water from irrigation. Production of the crop for a given year usually starts soon after harvesting the preceding autumn. Cotton is naturally a perennial but is grown as an annual to help control pests. Planting time in spring in the Northern hemisphere varies from the beginning of February to the beginning of June. The area of the United States known as the South Plains is the largest contiguous cotton-growing region in the world. While dryland (non-irrigated) cotton is successfully grown in this region, consistent yields are only produced with heavy reliance on irrigation water drawn from the Ogallala Aquifer. Since cotton is somewhat salt and drought tolerant, this makes it an attractive crop for arid and semiarid regions. As water resources get tighter around the world, economies that rely on it face difficulties and conflict, as well as potential environmental problems. For example, improper cropping and irrigation practices have led to desertification in areas of Uzbekistan, where cotton is a major export. In the days of the Soviet Union, the Aral Sea was tapped for agricultural irrigation, largely of cotton, and now salination is widespread. Historically, in North America, one of the most economically destructive pests in cotton production has been the boll weevil. Due to the US Department of Agriculture's highly successful Boll Weevil Eradication Program (BWEP), this pest has been eliminated from cotton in most of the United States. This program, along with the introduction of genetically engineered Bt cotton (which contains a bacterial gene that codes for a plant-produced protein that is toxic to a number of pests such as cotton bollworm and pink bollworm), has allowed a reduction in the use of synthetic insecticides. A public genome sequencing effort of cotton was initiated in 2007 by a consortium of public researchers. They agreed on a strategy to sequence the genome of cultivated, tetraploid cotton. ""Tetraploid"" means that cultivated cotton actually has two separate genomes within its nucleus, referred to as the A and D genomes. The sequencing consortium first agreed to sequence the D-genome relative of cultivated cotton (G. raimondii, a wild Central American cotton species) because of its small size and limited number of repetitive elements. It is nearly one-third the number of bases of tetraploid cotton (AD), and each chromosome is only present once.[clarification needed] The A genome of G. arboreum would be sequenced next. Its genome is roughly twice the size of G. raimondii's. Part of the difference in size between the two genomes is the amplification of retrotransposons (GORGE). Once both diploid genomes are assembled, then research could begin sequencing the actual genomes of cultivated cotton varieties. This strategy is out of necessity; if one were to sequence the tetraploid genome without model diploid genomes, the euchromatic DNA sequences of the AD genomes would co-assemble and the repetitive elements of AD genomes would assembly independently into A and D sequences respectively. Then there would be no way to untangle the mess of AD sequences without comparing them to their diploid counterparts. GM cotton acreage in India grew at a rapid rate, increasing from 50,000 hectares in 2002 to 10.6 million hectares in 2011. The total cotton area in India was 12.1 million hectares in 2011, so GM cotton was grown on 88% of the cotton area. This made India the country with the largest area of GM cotton in the world. A long-term study on the economic impacts of Bt cotton in India, published in the Journal PNAS in 2012, showed that Bt cotton has increased yields, profits, and living standards of smallholder farmers. The U.S. GM cotton crop was 4.0 million hectares in 2011 the second largest area in the world, the Chinese GM cotton crop was third largest by area with 3.9 million hectares and Pakistan had the fourth largest GM cotton crop area of 2.6 million hectares in 2011. The initial introduction of GM cotton proved to be a success in Australia – the yields were equivalent to the non-transgenic varieties and the crop used much less pesticide to produce (85% reduction). The subsequent introduction of a second variety of GM cotton led to increases in GM cotton production until 95% of the Australian cotton crop was GM in 2009 making Australia the country with the fifth largest GM cotton crop in the world. Other GM cotton growing countries in 2011 were Argentina, Myanmar, Burkina Faso, Brazil, Mexico, Colombia, South Africa and Costa Rica. Organic cotton is generally understood as cotton from plants not genetically modified and that is certified to be grown without the use of any synthetic agricultural chemicals, such as fertilizers or pesticides. Its production also promotes and enhances biodiversity and biological cycles. In the United States, organic cotton plantations are required to enforce the National Organic Program (NOP). This institution determines the allowed practices for pest control, growing, fertilizing, and handling of organic crops. As of 2007, 265,517 bales of organic cotton were produced in 24 countries, and worldwide production was growing at a rate of more than 50% per year. The fiber is most often spun into yarn or thread and used to make a soft, breathable textile. The use of cotton for fabric is known to date to prehistoric times; fragments of cotton fabric dated from 5000 BC have been excavated in Mexico and the Indus Valley Civilization in Ancient India (modern-day Pakistan and some parts of India). Although cultivated since antiquity, it was the invention of the cotton gin that lowered the cost of production that led to its widespread use, and it is the most widely used natural fiber cloth in clothing today. But Bt cotton is ineffective against many cotton pests, however, such as plant bugs, stink bugs, and aphids; depending on circumstances it may still be desirable to use insecticides against these. A 2006 study done by Cornell researchers, the Center for Chinese Agricultural Policy and the Chinese Academy of Science on Bt cotton farming in China found that after seven years these secondary pests that were normally controlled by pesticide had increased, necessitating the use of pesticides at similar levels to non-Bt cotton and causing less profit for farmers because of the extra expense of GM seeds. However, a 2009 study by the Chinese Academy of Sciences, Stanford University and Rutgers University refuted this. They concluded that the GM cotton effectively controlled bollworm. The secondary pests were mostly miridae (plant bugs) whose increase was related to local temperature and rainfall and only continued to increase in half the villages studied. Moreover, the increase in insecticide use for the control of these secondary insects was far smaller than the reduction in total insecticide use due to Bt cotton adoption. A 2012 Chinese study concluded that Bt cotton halved the use of pesticides and doubled the level of ladybirds, lacewings and spiders. The International Service for the Acquisition of Agri-biotech Applications (ISAAA) said that, worldwide, GM cotton was planted on an area of 25 million hectares in 2011. This was 69% of the worldwide total area planted in cotton. The cottonseed which remains after the cotton is ginned is used to produce cottonseed oil, which, after refining, can be consumed by humans like any other vegetable oil. The cottonseed meal that is left generally is fed to ruminant livestock; the gossypol remaining in the meal is toxic to monogastric animals. Cottonseed hulls can be added to dairy cattle rations for roughage. During the American slavery period, cotton root bark was used in folk remedies as an abortifacient, that is, to induce a miscarriage. Gossypol was one of the many substances found in all parts of the cotton plant and it was described by the scientists as 'poisonous pigment'. It also appears to inhibit the development of sperm or even restrict the mobility of the sperm. Also, it is thought to interfere with the menstrual cycle by restricting the release of certain hormones. The public sector effort continues with the goal to create a high-quality, draft genome sequence from reads generated by all sources. The public-sector effort has generated Sanger reads of BACs, fosmids, and plasmids as well as 454 reads. These later types of reads will be instrumental in assembling an initial draft of the D genome. In 2010, two companies (Monsanto and Illumina), completed enough Illumina sequencing to cover the D genome of G. raimondii about 50x. They announced that they would donate their raw reads to the public. This public relations effort gave them some recognition for sequencing the cotton genome. Once the D genome is assembled from all of this raw material, it will undoubtedly assist in the assembly of the AD genomes of cultivated varieties of cotton, but a lot of hard work remains. The earliest evidence of cotton use in South Asia has been found at the site of Mehrgarh, Pakistan, where cotton threads have been found preserved in copper beads; these finds have been dated to Neolithic (between 6000 and 5000 BCE). Cotton cultivation in the region is dated to the Indus Valley Civilization, which covered parts of modern eastern Pakistan and northwestern India between 3300 and 1300 BCE The Indus cotton industry was well-developed and some methods used in cotton spinning and fabrication continued to be used until the industrialization of India. Between 2000 and 1000 BC cotton became widespread across much of India. For example, it has been found at the site of Hallus in Karnataka dating from around 1000 BC. By the 1840s, India was no longer capable of supplying the vast quantities of cotton fibers needed by mechanized British factories, while shipping bulky, low-price cotton from India to Britain was time-consuming and expensive. This, coupled with the emergence of American cotton as a superior type (due to the longer, stronger fibers of the two domesticated native American species, Gossypium hirsutum and Gossypium barbadense), encouraged British traders to purchase cotton from plantations in the United States and plantations in the Caribbean. By the mid-19th century, ""King Cotton"" had become the backbone of the southern American economy. In the United States, cultivating and harvesting cotton became the leading occupation of slaves. Cotton remained a key crop in the Southern economy after emancipation and the end of the Civil War in 1865. Across the South, sharecropping evolved, in which landless black and white farmers worked land owned by others in return for a share of the profits. Some farmers rented the land and bore the production costs themselves. Until mechanical cotton pickers were developed, cotton farmers needed additional labor to hand-pick cotton. Picking cotton was a source of income for families across the South. Rural and small town school systems had split vacations so children could work in the fields during ""cotton-picking."" Cotton is used to make a number of textile products. These include terrycloth for highly absorbent bath towels and robes; denim for blue jeans; cambric, popularly used in the manufacture of blue work shirts (from which we get the term ""blue-collar""); and corduroy, seersucker, and cotton twill. Socks, underwear, and most T-shirts are made from cotton. Bed sheets often are made from cotton. Cotton also is used to make yarn used in crochet and knitting. Fabric also can be made from recycled or recovered cotton that otherwise would be thrown away during the spinning, weaving, or cutting process. While many fabrics are made completely of cotton, some materials blend cotton with other fibers, including rayon and synthetic fibers such as polyester. It can either be used in knitted or woven fabrics, as it can be blended with elastine to make a stretchier thread for knitted fabrics, and apparel such as stretch jeans. The largest producers of cotton, currently (2009), are China and India, with annual production of about 34 million bales and 33.4 million bales, respectively; most of this production is consumed by their respective textile industries. The largest exporters of raw cotton are the United States, with sales of $4.9 billion, and Africa, with sales of $2.1 billion. The total international trade is estimated to be $12 billion. Africa's share of the cotton trade has doubled since 1980. Neither area has a significant domestic textile industry, textile manufacturing having moved to developing nations in Eastern and South Asia such as India and China. In Africa, cotton is grown by numerous small holders. Dunavant Enterprises, based in Memphis, Tennessee, is the leading cotton broker in Africa, with hundreds of purchasing agents. It operates cotton gins in Uganda, Mozambique, and Zambia. In Zambia, it often offers loans for seed and expenses to the 180,000 small farmers who grow cotton for it, as well as advice on farming methods. Cargill also purchases cotton in Africa for export. Cotton linters are fine, silky fibers which adhere to the seeds of the cotton plant after ginning. These curly fibers typically are less than 1⁄8 inch (3.2 mm) long. The term also may apply to the longer textile fiber staple lint as well as the shorter fuzzy fibers from some upland species. Linters are traditionally used in the manufacture of paper and as a raw material in the manufacture of cellulose. In the UK, linters are referred to as ""cotton wool"". This can also be a refined product (absorbent cotton in U.S. usage) which has medical, cosmetic and many other practical uses. The first medical use of cotton wool was by Sampson Gamgee at the Queen's Hospital (later the General Hospital) in Birmingham, England. The advent of the Industrial Revolution in Britain provided a great boost to cotton manufacture, as textiles emerged as Britain's leading export. In 1738, Lewis Paul and John Wyatt, of Birmingham, England, patented the roller spinning machine, as well as the flyer-and-bobbin system for drawing cotton to a more even thickness using two sets of rollers that traveled at different speeds. Later, the invention of the James Hargreaves' spinning jenny in 1764, Richard Arkwright's spinning frame in 1769 and Samuel Crompton's spinning mule in 1775 enabled British spinners to produce cotton yarn at much higher rates. From the late 18th century on, the British city of Manchester acquired the nickname ""Cottonopolis"" due to the cotton industry's omnipresence within the city, and Manchester's role as the heart of the global cotton trade. During the American Civil War, American cotton exports slumped due to a Union blockade on Southern ports, and also because of a strategic decision by the Confederate government to cut exports, hoping to force Britain to recognize the Confederacy or enter the war. This prompted the main purchasers of cotton, Britain and France, to turn to Egyptian cotton. British and French traders invested heavily in cotton plantations. The Egyptian government of Viceroy Isma'il took out substantial loans from European bankers and stock exchanges. After the American Civil War ended in 1865, British and French traders abandoned Egyptian cotton and returned to cheap American exports,[citation needed] sending Egypt into a deficit spiral that led to the country declaring bankruptcy in 1876, a key factor behind Egypt's occupation by the British Empire in 1882. Genetically modified (GM) cotton was developed to reduce the heavy reliance on pesticides. The bacterium Bacillus thuringiensis (Bt) naturally produces a chemical harmful only to a small fraction of insects, most notably the larvae of moths and butterflies, beetles, and flies, and harmless to other forms of life. The gene coding for Bt toxin has been inserted into cotton, causing cotton, called Bt cotton, to produce this natural insecticide in its tissues. In many regions, the main pests in commercial cotton are lepidopteran larvae, which are killed by the Bt protein in the transgenic cotton they eat. This eliminates the need to use large amounts of broad-spectrum insecticides to kill lepidopteran pests (some of which have developed pyrethroid resistance). This spares natural insect predators in the farm ecology and further contributes to noninsecticide pest management. Cotton lisle is a finely-spun, tightly twisted type of cotton that is noted for being strong and durable. Lisle is composed of two strands that have each been twisted an extra twist per inch than ordinary yarns and combined to create a single thread. The yarn is spun so that it is compact and solid. This cotton is used mainly for underwear, stockings, and gloves. Colors applied to this yarn are noted for being more brilliant than colors applied to softer yarn. This type of thread was first made in the city of Lisle, France (now Lille), hence its name."
States_of_Germany,"The Basic Law of the Federal Republic of Germany, the federal constitution, stipulates that the structure of each Federal State's government must ""conform to the principles of republican, democratic, and social government, based on the rule of law"" (Article 28). Most of the states are governed by a cabinet led by a Ministerpräsident (Minister-President), together with a unicameral legislative body known as the Landtag (State Diet). The states are parliamentary republics and the relationship between their legislative and executive branches mirrors that of the federal system: the legislatures are popularly elected for four or five years (depending on the state), and the Minister-President is then chosen by a majority vote among the Landtag's members. The Minister-President appoints a cabinet to run the state's agencies and to carry out the executive duties of the state's government. Municipalities (Gemeinden): Every rural district and every Amt is subdivided into municipalities, while every urban district is a municipality in its own right. There are (as of 6 March 2009[update]) 12,141 municipalities, which are the smallest administrative units in Germany. Cities and towns are municipalities as well, also having city rights or town rights (Stadtrechte). Nowadays, this is mostly just the right to be called a city or town. However, in former times there were many other privileges, including the right to impose local taxes or to allow industry only within city limits. Later, the constitution was amended to state that the citizens of the 16 states had successfully achieved the unity of Germany in free self-determination and that the Basic Law thus applied to the entire German people. Article 23, which had allowed ""any other parts of Germany"" to join, was rephrased. It had been used in 1957 to reintegrate the Saar Protectorate as the Saarland into the Federal Republic, and this was used as a model for German reunification in 1990. The amended article now defines the participation of the Federal Council and the 16 German states in matters concerning the European Union. During the Allied occupation of Germany after World War II, internal borders were redrawn by the Allied military governments. No single state comprised more than 30% of either population or territory; this was intended to prevent any one state from being as dominant within Germany as Prussia had been in the past. Initially, only seven of the pre-War states remained: Baden (in part), Bavaria (reduced in size), Bremen, Hamburg, Hesse (enlarged), Saxony, and Thuringia. The states with hyphenated names, such as Rhineland-Palatinate, North Rhine-Westphalia, and Saxony-Anhalt, owed their existence to the occupation powers and were created out of mergers of former Prussian provinces and smaller states. Former German territory that lie east of the Oder-Neisse Line fell under either Polish or Soviet administration but attempts were made at least symbolically not to abandon sovereignty well into the 1960s. However, no attempts were made to establish new states in these territories as they lay outside the jurisdiction of West Germany at that time. The debate on a new delimitation of the German territory started in 1919 as part of discussions about the new constitution. Hugo Preuss, the father of the Weimar Constitution, drafted a plan to divide the German Reich into 14 roughly equal-sized states. His proposal was turned down due to opposition of the states and concerns of the government. Article 18 of the constitution enabled a new delimitation of the German territory but set high hurdles: Three fifth of the votes handed in, and at least the majority of the population are necessary to decide on the alteration of territory. In fact, until 1933 there were only four changes in the configuration of the German states: The 7 Thuringian states were merged in 1920, whereby Coburg opted for Bavaria, Pyrmont joined Prussia in 1922, and Waldeck did so in 1929. Any later plans to break up the dominating Prussia into smaller states failed because political circumstances were not favorable to state reforms. Federalism is one of the entrenched constitutional principles of Germany. According to the German constitution (called Grundgesetz or in English Basic Law), some topics, such as foreign affairs and defense, are the exclusive responsibility of the federation (i.e., the federal level), while others fall under the shared authority of the states and the federation; the states retain residual legislative authority for all other areas, including ""culture"", which in Germany includes not only topics such as financial promotion of arts and sciences, but also most forms of education and job training. Though international relations including international treaties are primarily the responsibility of the federal level, the constituent states have certain limited powers in this area: in matters that affect them directly, the states defend their interests at the federal level through the Bundesrat (literally Federal Council, the upper house of the German Federal Parliament) and in areas where they have legislative authority they have limited powers to conclude international treaties ""with the consent of the federal government"". Paragraph 6 of Article 29 stated that if a petition was successful a referendum should be held within three years. Since the deadline passed on 5 May 1958 without anything happening the Hesse state government filed a constitutional complaint with the Federal Constitutional Court in October 1958. The complaint was dismissed in July 1961 on the grounds that Article 29 had made the new delimitation of the federal territory an exclusively federal matter. At the same time, the Court reaffirmed the requirement for a territorial revision as a binding order to the relevant constitutional bodies. Germany is a federal republic consisting of sixteen federal states (German: Bundesland, or Land).[a] Since today's Germany was formed from an earlier collection of several states, it has a federal constitution, and the constituent states retain a measure of sovereignty. With an emphasis on geographical conditions, Berlin and Hamburg are frequently called Stadtstaaten (city-states), as is the Free Hanseatic City of Bremen, which in fact includes the cities of Bremen and Bremerhaven. The remaining 13 states are called Flächenländer (literally: area states). In the Paris Agreements of 23 October 1954, France offered to establish an independent ""Saarland"", under the auspices of the Western European Union (WEU), but on 23 October 1955 in the Saar Statute referendum the Saar electorate rejected this plan by 67.7% to 32.3% (out of a 96.5% turnout: 423,434 against, 201,975 for) despite the public support of Federal German Chancellor Konrad Adenauer for the plan. The rejection of the plan by the Saarlanders was interpreted as support for the Saar to join the Federal Republic of Germany. Federalism has a long tradition in German history. The Holy Roman Empire comprised many petty states numbering more than 300 around 1796. The number of territories was greatly reduced during the Napoleonic Wars (1796–1814). After the Congress of Vienna (1815), 39 states formed the German Confederation. The Confederation was dissolved after the Austro-Prussian War and replaced by a North German Federation under Prussian hegemony; this war left Prussia dominant in Germany, and German nationalism would compel the remaining independent states to ally with Prussia in the Franco-Prussian War of 1870–71, and then to accede to the crowning of King Wilhelm of Prussia as German Emperor. The new German Empire included 25 states (three of them, Hanseatic cities) and the imperial territory of Alsace-Lorraine. The empire was dominated by Prussia, which controlled 65% of the territory and 62% of the population. After the territorial losses of the Treaty of Versailles, the remaining states continued as republics of a new German federation. These states were gradually de facto abolished and reduced to provinces under the Nazi regime via the Gleichschaltung process, as the states administratively were largely superseded by the Nazi Gau system. The municipalities have two major policy responsibilities. First, they administer programs authorized by the federal or state government. Such programs typically relate to youth, schools, public health, and social assistance. Second, Article 28(2) of the Basic Law guarantees the municipalities ""the right to regulate on their own responsibility all the affairs of the local community within the limits set by law."" Under this broad statement of competence, local governments can justify a wide range of activities. For instance, many municipalities develop and expand the economic infrastructure of their communities through the development of industrial trading estates. The Districts of Germany (Kreise) are administrative districts, and every state except the city-states of Berlin, Hamburg, and Bremen consists of ""rural districts"" (Landkreise), District-free Towns/Cities (Kreisfreie Städte, in Baden-Württemberg also called ""urban districts"", or Stadtkreise), cities that are districts in their own right, or local associations of a special kind (Kommunalverbände besonderer Art), see below. The state Free Hanseatic City of Bremen consists of two urban districts, while Berlin and Hamburg are states and urban districts at the same time. Local associations of a special kind are an amalgamation of one or more Landkreise with one or more Kreisfreie Städte to form a replacement of the aforementioned administrative entities at the district level. They are intended to implement simplification of administration at that level. Typically, a district-free city or town and its urban hinterland are grouped into such an association, or Kommunalverband besonderer Art. Such an organization requires the issuing of special laws by the governing state, since they are not covered by the normal administrative structure of the respective states. In 1952, following a referendum, Baden, Württemberg-Baden, and Württemberg-Hohenzollern merged into Baden-Württemberg. In 1957, the Saar Protectorate rejoined the Federal Republic as the Saarland. German reunification in 1990, in which the German Democratic Republic (East Germany) ascended into the Federal Republic, resulted in the addition of the re-established eastern states of Brandenburg, Mecklenburg-West Pomerania (in German Mecklenburg-Vorpommern), Saxony (Sachsen), Saxony-Anhalt (Sachsen-Anhalt), and Thuringia (Thüringen), as well as the reunification of West and East Berlin into Berlin and its establishment as a full and equal state. A regional referendum in 1996 to merge Berlin with surrounding Brandenburg as ""Berlin-Brandenburg"" failed to reach the necessary majority vote in Brandenburg, while a majority of Berliners voted in favour of the merger. A new delimitation of the federal territory has been discussed since the Federal Republic was founded in 1949 and even before. Committees and expert commissions advocated a reduction of the number of states; academics (Rutz, Miegel, Ottnad etc.) and politicians (Döring, Apel, and others) made proposals –  some of them far-reaching –  for redrawing boundaries but hardly anything came of these public discussions. Territorial reform is sometimes propagated by the richer states as a means to avoid or reduce fiscal transfers. In southwestern Germany, territorial revision seemed to be a top priority since the border between the French and American occupation zones was set along the Autobahn Karlsruhe-Stuttgart-Ulm (today the A8). Article 118 stated ""The division of the territory comprising Baden, Württemberg-Baden and Württemberg-Hohenzollern into Länder may be revised, without regard to the provisions of Article 29, by agreement between the Länder concerned. If no agreement is reached, the revision shall be effected by a federal law, which shall provide for an advisory referendum."" Since no agreement was reached, a referendum was held on 9 December 1951 in four different voting districts, three of which approved the merger (South Baden refused but was overruled as the result of total votes was decisive). On 25 April 1952, the three former states merged to form Baden-Württemberg. Upon its founding in 1949, West Germany had eleven states. These were reduced to nine in 1952 when three south-western states (South Baden, Württemberg-Hohenzollern, and Württemberg-Baden) merged to form Baden-Württemberg. From 1957, when the French-occupied Saar Protectorate was returned and formed into the Saarland, the Federal Republic consisted of ten states, which are referred to as the ""Old States"" today. West Berlin was under the sovereignty of the Western Allies and neither a Western German state nor part of one. However, it was in many ways de facto integrated with West Germany under a special status. A new delimitation of the federal territory keeps being debated in Germany, though ""Some scholars note that there are significant differences among the American states and regional governments in other federations without serious calls for territorial changes ..."", as political scientist Arthur B. Gunlicks remarks. He summarizes the main arguments for boundary reform in Germany: ""... the German system of dual federalism requires strong Länder that have the administrative and fiscal capacity to implement legislation and pay for it from own source revenues. Too many Länder also make coordination among them and with the federation more complicated ..."". But several proposals have failed so far; territorial reform remains a controversial topic in German politics and public perception. The creation of the Federal Republic of Germany in 1949 was through the unification of the western states (which were previously under American, British, and French administration) created in the aftermath of World War II. Initially, in 1949, the states of the Federal Republic were Baden, Bavaria (in German: Bayern), Bremen, Hamburg, Hesse (Hessen), Lower Saxony (Niedersachsen), North Rhine Westphalia (Nordrhein-Westfalen), Rhineland-Palatinate (Rheinland-Pfalz), Schleswig-Holstein, Württemberg-Baden, and Württemberg-Hohenzollern. West Berlin, while officially not part of the Federal Republic, was largely integrated and considered as a de facto state. The governments in Berlin, Bremen and Hamburg are designated by the term Senate. In the three free states of Bavaria, Saxony, and Thuringia the government is referred to as the State Government (Staatsregierung), and in the other ten states the term Land Government (Landesregierung) is used. Before January 1, 2000, Bavaria had a bicameral parliament, with a popularly elected Landtag, and a Senate made up of representatives of the state's major social and economic groups. The Senate was abolished following a referendum in 1998. The states of Berlin, Bremen, and Hamburg are governed slightly differently from the other states. In each of those cities, the executive branch consists of a Senate of approximately eight, selected by the state's parliament; the senators carry out duties equivalent to those of the ministers in the larger states. The equivalent of the Minister-President is the Senatspräsident (President of the Senate) in Bremen, the Erster Bürgermeister (First Mayor) in Hamburg, and the Regierender Bürgermeister (Governing Mayor) in Berlin. The parliament for Berlin is called the Abgeordnetenhaus (House of Representatives), while Bremen and Hamburg both have a Bürgerschaft. The parliaments in the remaining 13 states are referred to as Landtag (State Parliament). The use of the term Länder (Lands) dates back to the Weimar Constitution of 1919. Before this time, the constituent states of the German Empire were called Staaten (States). Today, it is very common to use the term Bundesland (Federal Land). However, this term is not used officially, neither by the constitution of 1919 nor by the Basic Law (Constitution) of 1949. Three Länder call themselves Freistaaten (Free States, which is the old-fashioned German expression for Republic), Bavaria (since 1919), Saxony (originally since 1919 and again since 1990), and Thuringia (since 1994). There is little continuity between the current states and their predecessors of the Weimar Republic with the exception of the three free states, and the two city-states of Hamburg and Bremen. After the Nazi Party seized power in January 1933, the Länder increasingly lost importance. They became administrative regions of a centralised country. Three changes are of particular note: on January 1, 1934, Mecklenburg-Schwerin was united with the neighbouring Mecklenburg-Strelitz; and, by the Greater Hamburg Act (Groß-Hamburg-Gesetz), from April 1, 1937, the area of the city-state was extended, while Lübeck lost its independence and became part of the Prussian province of Schleswig-Holstein. As the premiers did not come to an agreement on this question, the Parliamentary Council was supposed to address this issue. Its provisions are reflected in Article 29. There was a binding provision for a new delimitation of the federal territory: the Federal Territory must be revised ... (paragraph 1). Moreover, in territories or parts of territories whose affiliation with a Land had changed after 8 May 1945 without a referendum, people were allowed to petition for a revision of the current status within a year after the promulgation of the Basic Law (paragraph 2). If at least one tenth of those entitled to vote in Bundestag elections were in favour of a revision, the federal government had to include the proposal into its legislation. Then a referendum was required in each territory or part of a territory whose affiliation was to be changed (paragraph 3). The proposal should not take effect if within any of the affected territories a majority rejected the change. In this case, the bill had to be introduced again and after passing had to be confirmed by referendum in the Federal Republic as a whole (paragraph 4). The reorganization should be completed within three years after the Basic Law had come into force (paragraph 6). In his investiture address, given on 28 October 1969 in Bonn, Chancellor Willy Brandt proposed that the government would consider Article 29 of the Basic Law as a binding order. An expert commission was established, named after its chairman, the former Secretary of State Professor Werner Ernst. After two years of work, the experts delivered their report in 1973. It provided an alternative proposal for both northern Germany and central and southwestern Germany. In the north, either a single new state consisting of Schleswig-Holstein, Hamburg, Bremen and Lower Saxony should be created (solution A) or two new states, one in the northeast consisting of Schleswig-Holstein, Hamburg and the northern part of Lower Saxony (from Cuxhaven to Lüchow-Dannenberg) and one in the northwest consisting of Bremen and the rest of Lower Saxony (solution B). In the Center and South West either Rhineland-Palatinate (with the exception of the Germersheim district but including the Rhine-Neckar region) should be merged with Hesse and the Saarland (solution C), the district of Germersheim would then become part of Baden-Württemberg."
Gramophone_record,"Electric recording which developed during the time that early radio was becoming popular (1925) benefited from the microphones and amplifiers used in radio studios. The early electric recordings were reminiscent tonally of acoustic recordings, except there was more recorded bass and treble as well as delicate sounds and overtones cut on the records. This was in spite of some carbon microphones used, which had resonances that colored the recorded tone. The double button carbon microphone with stretched diaphragm was a marked improvement. Alternatively, the Wente style condenser microphone used with the Western Electric licensed recording method had a brilliant midrange and was prone to overloading from sibilants in speech, but generally it gave more accurate reproduction than carbon microphones. In the 1930s, record companies began issuing collections of 78 rpm records by one performer or of one type of music in specially assembled albums, typically with artwork on the front cover and liner notes on the back or inside cover. Most albums included three or four records, with two sides each, making six or eight tunes per album. When the 12-inch vinyl LP era began in 1949, the single record often had the same or similar number of tunes as a typical album of 78s, and was still often referred to as an ""album"". A further limitation of the gramophone record is that fidelity steadily declines as playback progresses; there is more vinyl per second available for fine reproduction of high frequencies at the large-diameter beginning of the groove than exist at the smaller-diameters close to the end of the side. At the start of a groove on an LP there are 510 mm of vinyl per second traveling past the stylus while the ending of the groove gives 200–210 mm of vinyl per second — less than half the linear resolution. Distortion towards the end of the side is likely to become more apparent as record wear increases.* The newly invented Western Electric moving coil or dynamic microphone was part of the Wide Range System. It had a flatter audio response than the old style Wente condenser type and didn't require electronics installed in the microphone housing. Signals fed to the cutting head were pre-emphasized in the treble region to help override noise in playback. Groove cuts in the vertical plane were employed rather than the usual lateral cuts. The chief advantage claimed was more grooves per inch that could be crowded together, resulting in longer playback time. Additionally, the problem of inner groove distortion, which plagued lateral cuts, could be avoided with the vertical cut system. Wax masters were made by flowing heated wax over a hot metal disc thus avoiding the microscopic irregularities of cast blocks of wax and the necessity of planing and polishing. From the mid-1950s through the 1960s, in the U.S. the common home record player or ""stereo"" (after the introduction of stereo recording) would typically have had these features: a three- or four-speed player (78, 45, 33 1⁄3, and sometimes 16 2⁄3 rpm); with changer, a tall spindle that would hold several records and automatically drop a new record on top of the previous one when it had finished playing, a combination cartridge with both 78 and microgroove styli and a way to flip between the two; and some kind of adapter for playing the 45s with their larger center hole. The adapter could be a small solid circle that fit onto the bottom of the spindle (meaning only one 45 could be played at a time) or a larger adaptor that fit over the entire spindle, permitting a stack of 45s to be played. New or ""virgin"" heavy/heavyweight (180–220 g) vinyl is commonly used for modern audiophile vinyl releases in all genres. Many collectors prefer to have heavyweight vinyl albums, which have been reported to have better sound than normal vinyl because of their higher tolerance against deformation caused by normal play. 180 g vinyl is more expensive to produce only because it uses more vinyl. Manufacturing processes are identical regardless of weight. In fact, pressing lightweight records requires more care. An exception is the propensity of 200 g pressings to be slightly more prone to non-fill, when the vinyl biscuit does not sufficiently fill a deep groove during pressing (percussion or vocal amplitude changes are the usual locations of these artifacts). This flaw causes a grinding or scratching sound at the non-fill point. German record company Odeon is often said to have pioneered the album in 1909 when it released the Nutcracker Suite by Tchaikovsky on 4 double-sided discs in a specially designed package. (It is not indicated what size the records are.) However, Deutsche Grammophon had produced an album for its complete recording of the opera Carmen in the previous year. The practice of issuing albums does not seem to have been widely taken up by other record companies for many years; however, HMV provided an album, with a pictorial cover, for the 1917 recording of The Mikado (Gilbert & Sullivan). In some ways similar to the laser turntable is the IRENE scanning machine for disc records, which images with microphotography in two dimensions, invented by a team of physicists at Lawrence Berkeley Laboratories. IRENE will retrieve the information from a laterally modulated monaural grooved sound source without touching the medium itself, but cannot read vertically modulated information. This excludes grooved recordings such as cylinders and some radio transcriptions that feature a hill-and-dale format of recording, and stereophonic or quadraphonic grooved recordings, which utilize a combination of the two as well as supersonic encoding for quadraphonic. Where old disc recordings are considered to be of artistic or historic interest, from before the era of tape or where no tape master exists, archivists play back the disc on suitable equipment and record the result, typically onto a digital format, which can be copied and manipulated to remove analog flaws without any further damage to the source recording. For example, Nimbus Records uses a specially built horn record player to transfer 78s. Anyone can do this using a standard record player with a suitable pickup, a phono-preamp (pre-amplifier) and a typical personal computer. However, for accurate transfer, professional archivists carefully choose the correct stylus shape and diameter, tracking weight, equalisation curve and other playback parameters and use high-quality analogue-to-digital converters. The complete technical disclosure of the Columbia LP by Peter C. Goldmark, Rene' Snepvangers and William S. Bachman in 1949 made it possible for a great variety of record companies to get into the business of making long playing records. The business grew quickly and interest spread in high fidelity sound and the do-it-yourself market for pickups, turntables, amplifier kits, loudspeaker enclosure plans, and AM/FM radio tuners. The LP record for longer works, 45 rpm for pop music, and FM radio became high fidelity program sources in demand. Radio listeners heard recordings broadcast and this in turn generated more record sales. The industry flourished. In 1877, Thomas Edison invented the phonograph. Unlike the phonautograph, it was capable of both recording and reproducing sound. Despite the similarity of name, there is no documentary evidence that Edison's phonograph was based on Scott's phonautograph. Edison first tried recording sound on a wax-impregnated paper tape, with the idea of creating a ""telephone repeater"" analogous to the telegraph repeater he had been working on. Although the visible results made him confident that sound could be physically recorded and reproduced, his notes do not indicate that he actually reproduced sound before his first experiment in which he used tinfoil as a recording medium several months later. The tinfoil was wrapped around a grooved metal cylinder and a sound-vibrated stylus indented the tinfoil while the cylinder was rotated. The recording could be played back immediately. The Scientific American article that introduced the tinfoil phonograph to the public mentioned Marey, Rosapelly and Barlow as well as Scott as creators of devices for recording but, importantly, not reproducing sound. Edison also invented variations of the phonograph that used tape and disc formats. Numerous applications for the phonograph were envisioned, but although it enjoyed a brief vogue as a startling novelty at public demonstrations, the tinfoil phonograph proved too crude to be put to any practical use. A decade later, Edison developed a greatly improved phonograph that used a hollow wax cylinder instead of a foil sheet. This proved to be both a better-sounding and far more useful and durable device. The wax phonograph cylinder created the recorded sound market at the end of the 1880s and dominated it through the early years of the 20th century. Stereophonic sound recording, which attempts to provide a more natural listening experience by reproducing the spatial locations of sound sources in the horizontal plane, was the natural extension to monophonic recording, and attracted various alternative engineering attempts. The ultimately dominant ""45/45"" stereophonic record system was invented by Alan Blumlein of EMI in 1931 and patented the same year. EMI cut the first stereo test discs using the system in 1933 (see Bell Labs Stereo Experiments of 1933) although the system was not exploited commercially until much later. The 45 rpm discs also came in a variety known as extended play (EP), which achieved up to 10–15 minutes play at the expense of attenuating (and possibly compressing) the sound to reduce the width required by the groove. EP discs were cheaper to produce, and were used in cases where unit sales were likely to be more limited or to reissue LP albums on the smaller format for those people who had only 45 rpm players. LP albums could be purchased 1 EP at a time, with four items per EP, or in a boxed set with 3 EPs or 12 items. The large center hole on 45s allows for easier handling by jukebox mechanisms. EPs were generally discontinued by the late 1950s in the U.S. as three- and four-speed record players replaced the individual 45 players. One indication of the decline of the 45 rpm EP is that the last Columbia Records reissue of Frank Sinatra songs on 45 rpm EP records, called Frank Sinatra (Columbia B-2641) was issued on December 7, 1959. The EP lasted considerably longer in Europe, and was a popular format during the 1960s for recordings by artists such as Serge Gainsbourg and the Beatles. There were important quality advances in recordings specifically made for radio broadcast. In the early 1930s Bell Telephone Laboratories and Western Electric announced the total reinvention of disc recording: the Western Electric Wide Range System, ""The New Voice of Action"". The intent of the new Western Electric system was to improve the overall quality of disc recording and playback. The recording speed was 33 1⁄3 rpm, originally used in the Western Electric/ERPI movie audio disc system implemented in the early Warner Brothers' Vitaphone ""talkies"" of 1927. Ultimately, the New Orthophonic curve was disclosed in a publication by R.C. Moyer of RCA Victor in 1953. He traced RCA Victor characteristics back to the Western Electric ""rubber line"" recorder in 1925 up to the early 1950s laying claim to long-held recording practices and reasons for major changes in the intervening years. The RCA Victor New Orthophonic curve was within the tolerances for the NAB/NARTB, Columbia LP, and AES curves. It eventually became the technical predecessor to the RIAA curve. Contrary to popular belief, if placed properly and prepared-for, drums could be effectively used and heard on even the earliest jazz and military band recordings. The loudest instruments such as the drums and trumpets were positioned the farthest away from the collecting horn. Lillian Hardin Armstrong, a member of King Oliver's Creole Jazz Band, which recorded at Gennett Records in 1923, remembered that at first Oliver and his young second trumpet, Louis Armstrong, stood next to each other and Oliver's horn could not be heard. ""They put Louis about fifteen feet over in the corner, looking all sad."" For fading instrumental parts in and out while recording, some performers were placed on a moveable platform, which could draw the performer(s) nearer or further away as required.[citation needed] Lateral-cut disc records were developed in the United States by Emile Berliner, who named his system the ""gramophone"", distinguishing it from Edison's wax cylinder ""phonograph"" and Columbia's wax cylinder ""graphophone"". Berliner's earliest discs, first marketed in 1889, but only in Europe, were 5 inches (13 cm) in diameter, and were played with a small hand-propelled machine. Both the records and the machine were adequate only for use as a toy or curiosity, due to the limited sound quality. In the United States in 1894, under the Berliner Gramophone trademark, Berliner started marketing records with somewhat more substantial entertainment value, along with somewhat more substantial gramophones to play them. Berliner's records had poor sound quality compared to wax cylinders, but his manufacturing associate Eldridge R. Johnson eventually improved the sound quality. Abandoning Berliner's ""Gramophone"" trademark for legal reasons, in 1901 Johnson's and Berliner's separate companies reorganized to form the Victor Talking Machine Company, whose products would come to dominate the market for many years. Emile Berliner moved his company to Montreal in 1900. The factory which became RCA Victor stills exists. There is a dedicated museum in Montreal for Berliner. The development of quadraphonic records was announced in 1971. These recorded four separate sound signals. This was achieved on the two stereo channels by electronic matrixing, where the additional channels were combined into the main signal. When the records were played, phase-detection circuits in the amplifiers were able to decode the signals into four separate channels. There were two main systems of matrixed quadraphonic records produced, confusingly named SQ (by CBS) and QS (by Sansui). They proved commercially unsuccessful, but were an important precursor to later surround-sound systems, as seen in SACD and home cinema today. Beginning in 1939, Dr. Peter Goldmark and his staff at Columbia Records and at CBS Laboratories undertook efforts to address problems of recording and playing back narrow grooves and developing an inexpensive, reliable consumer playback system. It took about eight years of study, except when it was suspended because of World War II. Finally, the 12-inch (30 cm) Long Play (LP) 33 1⁄3 rpm microgroove record album was introduced by the Columbia Record Company at a New York press conference on June 18, 1948. Eventually the 12-inch (300 mm) 33 1⁄3 rpm LP prevailed as the predominant format for musical albums, and 10-inch LPs were no longer issued. The last Columbia Records reissue of any Frank Sinatra songs on a 10-inch LP record was an album called Hall of Fame, CL 2600, issued on October 26, 1956, containing six songs, one each by Tony Bennett, Rosemary Clooney, Johnnie Ray, Frank Sinatra, Doris Day, and Frankie Laine. The 10-inch LP however had a longer life in the United Kingdom, where important early British rock and roll albums such as Lonnie Donegan's Lonnie Donegan Showcase and Billy Fury's The Sound of Fury were released in that form. The 7-inch (175 mm) 45 rpm disc or ""single"" established a significant niche for shorter duration discs, typically containing one item on each side. The 45 rpm discs typically emulated the playing time of the former 78 rpm discs, while the 12-inch LP discs eventually provided up to one half-hour of recorded material per side. Over the years a variety of record equalization practices emerged and there was no industry standard. For example, in Europe recordings for years required playback with a bass turnover setting of 250–300 Hz and a treble roll-off at 10,000 Hz ranging from 0 to −5 dB or more. In the US there were more varied practices and a tendency to use higher bass turnover frequencies such as 500 Hz as well as a greater treble rolloff like −8.5 dB and even more to record generally higher modulation levels on the record. Tonearm skating forces and other perturbations are also picked up by the stylus. This is a form of frequency multiplexing as the control signal (restoring force) used to keep the stylus in the groove is carried by the same mechanism as the sound itself. Subsonic frequencies below about 20 Hz in the audio signal are dominated by tracking effects, which is one form of unwanted rumble (""tracking noise"") and merges with audible frequencies in the deep bass range up to about 100 Hz. High fidelity sound equipment can reproduce tracking noise and rumble. During a quiet passage, woofer speaker cones can sometimes be seen to vibrate with the subsonic tracking of the stylus, at frequencies as low as just above 0.5 Hz (the frequency at which a 33 1⁄3 rpm record turns on the turntable; 5⁄9 Hz exactly on an ideal turntable). Another reason for very low frequency material can be a warped disk: its undulations produce frequencies of only a few hertz and present day amplifiers have large power bandwidths. For this reason, many stereo receivers contained a switchable subsonic filter. Some subsonic content is directly out of phase in each channel. If played back on a mono subwoofer system, the noise will cancel, significantly reducing the amount of rumble that is reproduced. ELPJ, a Japanese-based company, sells a laser turntable that uses a laser to read vinyl discs optically, without physical contact. The laser turntable eliminates record wear and the possibility of accidental scratches, which degrade the sound, but its expense limits use primarily to digital archiving of analog records, and the laser does not play back colored vinyl or picture discs. Various other laser-based turntables were tried during the 1990s, but while a laser reads the groove very accurately, since it does not touch the record, the dust that vinyl attracts due to static electric charge is not mechanically pushed out of the groove, worsening sound quality in casual use compared to conventional stylus playback. Also in the late 1970s, ""direct-to-disc"" records were produced, aimed at an audiophile niche market. These completely bypassed the use of magnetic tape in favor of a ""purist"" transcription directly to the master lacquer disc. Also during this period, half-speed mastered and ""original master"" records were released, using expensive state-of-the-art technology. A further late 1970s development was the Disco Eye-Cued system used mainly on Motown 12-inch singles released between 1978 and 1980. The introduction, drum-breaks, or choruses of a track were indicated by widely separated grooves, giving a visual cue to DJs mixing the records. The appearance of these records is similar to an LP, but they only contain one track each side. Many electronic dance music and hip hop releases today are still preferred on vinyl; however, digital copies are still widely available. This is because for disc jockeys (""DJs""), vinyl has an advantage over the CD: direct manipulation of the medium. DJ techniques such as slip-cueing, beatmatching, and scratching originated on turntables. With CDs or compact audio cassettes one normally has only indirect manipulation options, e.g., the play, stop, and pause buttons. With a record one can place the stylus a few grooves farther in or out, accelerate or decelerate the turntable, or even reverse its direction, provided the stylus, record player, and record itself are built to withstand it. However, many CDJ and DJ advances, such as DJ software and time-encoded vinyl, now have these capabilities and more. In 1931, RCA Victor launched the first commercially available vinyl long-playing record, marketed as program-transcription discs. These revolutionary discs were designed for playback at 33 1⁄3 rpm and pressed on a 30 cm diameter flexible plastic disc, with a duration of about ten minutes playing time per side. RCA Victor's early introduction of a long-play disc was a commercial failure for several reasons including the lack of affordable, reliable consumer playback equipment and consumer wariness during the Great Depression. Because of financial hardships that plagued the recording industry during that period (and RCA's own parched revenues), Victor's long-playing records were discontinued by early 1933. In January 1938, Milt Gabler started recording for his new label, Commodore Records, and to allow for longer continuous performances, he recorded some 12-inch records. Eddie Condon explained: ""Gabler realized that a jam session needs room for development."" The first two 12-inch recordings did not take advantage of the extra length: ""Carnegie Drag"" was 3:15; ""Carnegie Jump"", 2:41. But at the second session, on April 30, the two 12-inch recordings were longer: ""Embraceable You"" was 4:05; ""Serenade to a Shylock"", 4:32. Another way around the time limitation was to issue a selection on both sides of a single record. Vaudeville stars Gallagher and Shean recorded ""Mr. Gallagher and Mr. Shean"", written by Irving and Jack Kaufman, as two sides of a 10-inch 78 in 1922 for Cameo. An obvious workaround for longer recordings was to release a set of records. An early multi-record release was in 1903, when HMV in England made the first complete recording of an opera, Verdi's Ernani, on 40 single-sided discs. In 1940, Commodore released Eddie Condon and his Band's recording of ""A Good Man Is Hard to Find"" in four parts, issued on both sides of two 12-inch 78s. This limitation on the duration of recordings persisted from 1910 until the invention of the LP record, in 1948. In popular music, this time limitation of about 3:30 on a 10-inch 78 rpm record meant that singers usually did not release long pieces on record. One exception is Frank Sinatra's recording of Rodgers and Hammerstein's ""Soliloquy"", from Carousel, made on May 28, 1946. Because it ran 7:57, longer than both sides of a standard 78 rpm 10-inch record, it was released on Columbia's Masterwork label (the classical division) as two sides of a 12-inch record. The same was true of John Raitt's performance of the song on the original cast album of Carousel, which had been issued on a 78-rpm album set by American Decca in 1945. Vinyl pressings were made with stampers from master cuts that were electroplated in vacuo by means of gold sputtering. Audio response was claimed out to 8,000 Hz, later 13,000 Hz, using light weight pickups employing jeweled styli. Amplifiers and cutters both using negative feedback were employed thereby improving the range of frequencies cut and lowering distortion levels. Radio transcription producers such as World Broadcasting System and Associated Music Publishers (AMP) were the dominant licensees of the Western Electric wide range system and towards the end of the 1930s were responsible for two-thirds of the total radio transcription business. These recordings use a bass turnover of 300 Hz and a 10,000 Hz rolloff of −8.5 dB. In 1931, RCA Victor introduced their vinyl-based Victrolac compound as a material for some unusual-format and special-purpose records. By the end of the 1930s vinyl's advantages of light weight, relative unbreakability and low surface noise had made it the material of choice for prerecorded radio programming and other critical applications. When it came to ordinary 78 rpm records, however, the much higher cost of the raw material, as well as its vulnerability to the heavy pickups and crudely mass-produced steel needles still commonly used in home record players, made its general substitution for shellac impractical at that time. During the Second World War, the United States Armed Forces produced thousands of 12-inch vinyl 78 rpm V-Discs for use by the troops overseas. After the war, the wider use of vinyl became more practical as new record players with relatively lightweight crystal pickups and precision-ground styli made of sapphire or an exotic osmium alloy proliferated. In late 1945, RCA Victor began offering special transparent red vinyl De Luxe pressings of some classical 78s, at a de luxe price. Later, Decca Records introduced vinyl Deccalite 78s, while other record companies came up with vinyl concoctions such as Metrolite, Merco Plastic and Sav-o-flex, but these were mainly used to produce ""unbreakable"" children's records and special thin vinyl DJ pressings for shipment to radio stations. The phonautograph, patented by Léon Scott in 1857, used a vibrating diaphragm and stylus to graphically record sound waves as tracings on sheets of paper, purely for visual analysis and without any intent of playing them back. In the 2000s, these tracings were first scanned by audio engineers and digitally converted into audible sound. Phonautograms of singing and speech made by Scott in 1860 were played back as sound for the first time in 2008. Along with a tuning fork tone and unintelligible snippets recorded as early as 1857, these are the earliest known recordings of sound. In 1901, 10-inch disc records were introduced, followed in 1903 by 12-inch records. These could play for more than three and four minutes respectively, while contemporary cylinders could only play for about two minutes. In an attempt to head off the disc advantage, Edison introduced the Amberol cylinder in 1909, with a maximum playing time of 4½ minutes (at 160 rpm), which in turn were superseded by Blue Amberol Records, which had a playing surface made of celluloid, a plastic, which was far less fragile. Despite these improvements, during the 1910s discs decisively won this early format war, although Edison continued to produce new Blue Amberol cylinders for an ever-dwindling customer base until late in 1929. By 1919 the basic patents for the manufacture of lateral-cut disc records had expired, opening the field for countless companies to produce them. Analog disc records would dominate the home entertainment market until they were outsold by the digital compact disc in the late 1980s (which was in turn supplanted by digital audio recordings distributed via online music stores and Internet file sharing). Over time, fidelity, dynamic and noise levels improved to the point that it was harder to tell the difference between a live performance in the studio and the recorded version. This was especially true after the invention of the variable reluctance magnetic pickup cartridge by General Electric in the 1940s when high quality cuts were played on well-designed audio systems. The Capehart radio/phonographs of the era with large diameter electrodynamic loudspeakers, though not ideal, demonstrated this quite well with ""home recordings"" readily available in the music stores for the public to buy. During the first half of the 1920s, engineers at Western Electric, as well as independent inventors such as Orlando Marsh, developed technology for capturing sound with a microphone, amplifying it with vacuum tubes, then using the amplified signal to drive an electromagnetic recording head. Western Electric's innovations resulted in a greatly expanded and more even frequency response, creating a dramatically fuller, clearer and more natural-sounding recording. Distant or less strong sounds that were impossible to record by the old methods could now be captured. Volume was now limited only by the groove spacing on the record and the limitations of the intended playback device. Victor and Columbia licensed the new electrical system from Western Electric and began issuing electrically recorded discs in 1925. The first classical recording was of Chopin impromptus and Schubert's Litanei by Alfred Cortot for Victor. For collectable or nostalgia purposes, or for the benefit of higher-quality audio playback provided by the 78 rpm speed with newer vinyl records and their lightweight stylus pickups, a small number of 78 rpm records have been released since the major labels ceased production. One of the first attempts at this was in the 1950s, when inventor Ewing Dunbar Nunn founded the label Audiophile Records, which released, in addition to standard 33 1/3 rpm LPs, 78 rpm-mastered albums that were microgroove and pressed on vinyl (as opposed to traditional 78s, with their shellac composition and wider 3-mil sized grooves). This was done by the label mainly to take advantage of the wider audio frequency response that faster speeds like 78 rpm can provide for vinyl microgroove records, hence the label's name (obviously catering to the audiophiles of the 1950s ""hi-fi"" era, when stereo gear could provide a much wider range of audio than before). Also in the late 1950s, Bell Records released a few budget-priced 7"" microgrooved records at 78 rpm. The older 78 format continued to be mass-produced alongside the newer formats using new materials until about 1960 in the U.S., and in a few countries, such as India (where some Beatles recordings were issued on 78), into the 1960s. For example, Columbia Records' last reissue of Frank Sinatra songs on 78 rpm records was an album called Young at Heart, issued November 1, 1954. As late as the 1970s, some children's records were released at the 78 rpm speed. In the United Kingdom, the 78 rpm single lasted longer than in the United States and the 45 rpm took longer to become popular. The 78 rpm was overtaken in popularity by the 45 rpm in the late 1950s, as teenagers became increasingly affluent. The normal commercial disc is engraved with two sound-bearing concentric spiral grooves, one on each side, running from the outside edge towards the center. The last part of the spiral meets an earlier part to form a circle. The sound is encoded by fine variations in the edges of the groove that cause a stylus (needle) placed in it to vibrate at acoustic frequencies when the disc is rotated at the correct speed. Generally, the outer and inner parts of the groove bear no intended sound (an exception is Split Enz's Mental Notes). Towards the center, at the end of the groove, there is another wide-pitched section known as the lead-out. At the very end of this section the groove joins itself to form a complete circle, called the lock groove; when the stylus reaches this point, it circles repeatedly until lifted from the record. On some recordings (for example Sgt. Pepper's Lonely Hearts Club Band by The Beatles, Super Trouper by Abba and Atom Heart Mother by Pink Floyd), the sound continues on the lock groove, which gives a strange repeating effect. Automatic turntables rely on the position or angular velocity of the arm, as it reaches the wider spacing in the groove, to trigger a mechanism that lifts the arm off the record. Precisely because of this mechanism, most automatic turntables are incapable of playing any audio in the lock groove, since they will lift the arm before it reaches that groove. The term ""high fidelity"" was coined in the 1920s by some manufacturers of radio receivers and phonographs to differentiate their better-sounding products claimed as providing ""perfect"" sound reproduction. The term began to be used by some audio engineers and consumers through the 1930s and 1940s. After 1949 a variety of improvements in recording and playback technologies, especially stereo recordings, which became widely available in 1958, gave a boost to the ""hi-fi"" classification of products, leading to sales of individual components for the home such as amplifiers, loudspeakers, phonographs, and tape players. High Fidelity and Audio were two magazines that hi-fi consumers and engineers could read for reviews of playback equipment and recordings. For the first several decades of disc record manufacturing, sound was recorded directly on to the ""master disc"" at the recording studio. From about 1950 on (earlier for some large record companies, later for some small ones) it became usual to have the performance first recorded on audio tape, which could then be processed and/or edited, and then dubbed on to the master disc. A record cutter would engrave the grooves into the master disc. Early versions of these master discs were soft wax, and later a harder lacquer was used. The mastering process was originally something of an art as the operator had to manually allow for the changes in sound which affected how wide the space for the groove needed to be on each rotation. In spite of their flaws, such as the lack of portability, records still have enthusiastic supporters. Vinyl records continue to be manufactured and sold today, especially by independent rock bands and labels, although record sales are considered to be a niche market composed of audiophiles, collectors, and DJs. Old records and out-of-print recordings in particular are in much demand by collectors the world over. (See Record collecting.) Many popular new albums are given releases on vinyl records and older albums are also given reissues, sometimes on audiophile-grade vinyl. Delicate sounds and fine overtones were mostly lost, because it took a lot of sound energy to vibrate the recording horn diaphragm and cutting mechanism. There were acoustic limitations due to mechanical resonances in both the recording and playback system. Some pictures of acoustic recording sessions show horns wrapped with tape to help mute these resonances. Even an acoustic recording played back electrically on modern equipment sounds like it was recorded through a horn, notwithstanding a reduction in distortion because of the modern playback. Toward the end of the acoustic era, there were many fine examples of recordings made with horns. The lateral cut NAB curve was remarkably similar to the NBC Orthacoustic curve that evolved from practices within the National Broadcasting Company since the mid-1930s. Empirically, and not by any formula, it was learned that the bass end of the audio spectrum below 100 Hz could be boosted somewhat to override system hum and turntable rumble noises. Likewise at the treble end beginning at 1,000 Hz, if audio frequencies were boosted by 16 dB at 10,000 Hz the delicate sibilant sounds of speech and high overtones of musical instruments could survive the noise level of cellulose acetate, lacquer/aluminum, and vinyl disc media. When the record was played back using a complementary inverse curve, signal-to-noise ratio was improved and the programming sounded more lifelike. Vinyl records do not break easily, but the soft material is easily scratched. Vinyl readily acquires a static charge, attracting dust that is difficult to remove completely. Dust and scratches cause audio clicks and pops. In extreme cases, they can cause the needle to skip over a series of grooves, or worse yet, cause the needle to skip backwards, creating a ""locked groove"" that repeats over and over. This is the origin of the phrase ""like a broken record"" or ""like a scratched record"", which is often used to describe a person or thing that continually repeats itself. Locked grooves are not uncommon and were even heard occasionally in radio broadcasts. There is a theory that vinyl records can audibly represent higher frequencies than compact discs. According to Red Book specifications, the compact disc has a frequency response of 20 Hz up to 22,050 Hz, and most CD players measure flat within a fraction of a decibel from at least 20 Hz to 20 kHz at full output. Turntable rumble obscures the low-end limit of vinyl but the upper end can be, with some cartridges, reasonably flat within a few decibels to 30 kHz, with gentle roll-off. Carrier signals of Quad LPs popular in the 1970s were at 30 kHz to be out of the range of human hearing. The average human auditory system is sensitive to frequencies from 20 Hz to a maximum of around 20,000 Hz. The upper and lower frequency limits of human hearing vary per person. Original master discs are created by lathe-cutting: a lathe is used to cut a modulated groove into a blank record. The blank records for cutting used to be cooked up, as needed, by the cutting engineer, using what Robert K. Morrison describes as a ""metallic soap,"" containing lead litharge, ozokerite, barium sulfate, montan wax, stearin and paraffin, among other ingredients. Cut ""wax"" sound discs would be placed in a vacuum chamber and gold-sputtered to make them electrically conductive for use as mandrels in an electroforming bath, where pressing stamper parts were made. Later, the French company Pyral invented a ready-made blank disc having a thin nitro-cellulose lacquer coating (approximately 7 mils thickness on both sides) that was applied to an aluminum substrate. Lacquer cuts result in an immediately playable, or processable, master record. If vinyl pressings are wanted, the still-unplayed sound disc is used as a mandrel for electroforming nickel records that are used for manufacturing pressing stampers. The electroformed nickel records are mechanically separated from their respective mandrels. This is done with relative ease because no actual ""plating"" of the mandrel occurs in the type of electrodeposition known as electroforming, unlike with electroplating, in which the adhesion of the new phase of metal is chemical and relatively permanent. The one-molecule-thick coating of silver (that was sprayed onto the processed lacquer sound disc in order to make its surface electrically conductive) reverse-plates onto the nickel record's face. This negative impression disc (having ridges in place of grooves) is known as a nickel master, ""matrix"" or ""father."" The ""father"" is then used as a mandrel to electroform a positive disc known as a ""mother"". Many mothers can be grown on a single ""father"" before ridges deteriorate beyond effective use. The ""mothers"" are then used as mandrels for electroforming more negative discs known as ""sons"". Each ""mother"" can be used to make many ""sons"" before deteriorating. The ""sons"" are then converted into ""stampers"" by center-punching a spindle hole (which was lost from the lacquer sound disc during initial electroforming of the ""father""), and by custom-forming the target pressing profile. This allows them to be placed in the dies of the target (make and model) record press and, by center-roughing, to facilitate the adhesion of the label, which gets stuck onto the vinyl pressing without any glue. In this way, several million vinyl discs can be produced from a single lacquer sound disc. When only a few hundred discs are required, instead of electroforming a ""son"" (for each side), the ""father"" is removed of its silver and converted into a stamper. Production by this latter method, known as the ""two-step-process"" (as it does not entail creation of ""sons"" but does involve creation of ""mothers,"" which are used for test playing and kept as ""safeties"" for electroforming future ""sons"") is limited to a few hundred vinyl pressings. The pressing count can increase if the stamper holds out and the quality of the vinyl is high. The ""sons"" made during a ""three-step"" electroforming make better stampers since they don't require silver removal (which reduces some high fidelity because of etching erasing part of the smallest groove modulations) and also because they have a stronger metal structure than ""fathers"". A gramophone record (phonograph record in American English) or vinyl record, commonly known as a ""record"", is an analogue sound storage medium in the form of a flat polyvinyl chloride (previously shellac) disc with an inscribed, modulated spiral groove. The groove usually starts near the periphery and ends near the center of the disc. Phonograph records are generally described by their diameter in inches (12"", 10"", 7""), the rotational speed in rpm at which they are played (16 2⁄3, 33 1⁄3, 45, 78), and their time capacity resulting from a combination of those parameters (LP – long playing 33 1⁄3 rpm, SP – 78 rpm single, EP – 12-inch single or extended play, 33 or 45 rpm); their reproductive quality or level of fidelity (high-fidelity, orthophonic, full-range, etc.), and the number of audio channels provided (mono, stereo, quad, etc.). A different format, CD-4 (not to be confused with compact disc), by RCA, encoded the front-rear difference information on an ultrasonic carrier, which required a special wideband cartridge to capture it on carefully calibrated pickup arm/turntable combinations. CD-4 was even less successful than the two matrixed formats. (A further problem was that no cutting heads were available that could handle the HF information. That was remedied by cutting at half the speed. Later, the special half-speed cutting heads and equalization techniques were employed to get a wider frequency response in stereo with reduced distortion and greater headroom.) When auto-changing turntables were commonplace, records were typically pressed with a raised (or ridged) outer edge and a raised label area, allowing records to be stacked onto each other without the delicate grooves coming into contact, reducing the risk of damage. Auto-changers included a mechanism to support a stack of several records above the turntable itself, dropping them one at a time onto the active turntable to be played in order. Many longer sound recordings, such as complete operas, were interleaved across several 10-inch or 12-inch discs for use with auto-changing mechanisms, so that the first disk of a three-disk recording would carry sides 1 and 6 of the program, while the second disk would carry sides 2 and 5, and the third, sides 3 and 4, allowing sides 1, 2, and 3 to be played automatically; then the whole stack reversed to play sides 4, 5, and 6. Due to recording mastering and manufacturing limitations, both high and low frequencies were removed from the first recorded signals by various formulae. With low frequencies, the stylus must swing a long way from side to side, requiring the groove to be wide, taking up more space and limiting the playing time of the record. At high frequencies, hiss, pops, and ticks are significant. These problems can be reduced by using equalization to an agreed standard. During recording the amplitude of low frequencies is reduced, thus reducing the groove width required, and the amplitude at high frequencies is increased. The playback equipment boosts bass and cuts treble so as to restore the tonal balance in the original signal; this also reduces the high frequency noise. Thus more music will fit on the record, and noise is reduced. In 1968, Reprise planned to release a series of 78 rpm singles from their artists on their label at the time, called the Reprise Speed Series. Only one disc actually saw release, Randy Newman's I Think It's Going to Rain Today, a track from his self-titled debut album (with The Beehive State on the flipside). Reprise did not proceed further with the series due to a lack of sales for the single, and a lack of general interest in the concept. Guitarist & vocalist Leon Redbone released a promotional 78 rpm record in 1978 featuring two songs (Alabama Jubilee and Please Don't Talk About Me When I'm Gone) from his Champagne Charlie album. In 1980 Stiff Records in the United Kingdom issued a 78 by Joe ""King"" Carrasco containing the songs Buena (Spanish for ""good,"" with the alternate spelling ""Bueno"" on the label) and Tuff Enuff. Underground comic cartoonist and 78 rpm record collector Robert Crumb released three discs with his Cheap Suit Serenaders in the 1980s. The phonograph disc record was the primary medium used for music reproduction until late in the 20th century, replacing the phonograph cylinder record–with which it had co-existed from the late 1880s through to the 1920s–by the late 1920s. Records retained the largest market share even when new formats such as compact cassette were mass-marketed. By the late 1980s, digital media, in the form of the compact disc, had gained a larger market share, and the vinyl record left the mainstream in 1991. From the 1990s to the 2010s, records continued to be manufactured and sold on a much smaller scale, and were especially used by disc jockeys (DJ)s, released by artists in some genres, and listened to by a niche market of audiophiles. The phonograph record has made a niche resurgence in the early 21st century – 9.2 million records were sold in the U.S. in 2014, a 260% increase since 2009. Likewise, in the UK sales have increased five-fold from 2009 to 2014. West in 1930 and later P. G. A. H. Voigt (1940) showed that the early Wente-style condenser microphones contributed to a 4 to 6 dB midrange brilliance or pre-emphasis in the recording chain. This meant that the electrical recording characteristics of Western Electric licensees such as Columbia Records and Victor Talking Machine Company in the 1925 era had a higher amplitude in the midrange region. Brilliance such as this compensated for dullness in many early magnetic pickups having drooping midrange and treble response. As a result, this practice was the empirical beginning of using pre-emphasis above 1,000 Hz in 78 rpm and 33 1⁄3 rpm records. Breakage was very common in the shellac era. In the 1934 John O'Hara novel, Appointment in Samarra, the protagonist ""broke one of his most favorites, Whiteman's Lady of the Evening ... He wanted to cry but could not."" A poignant moment in J. D. Salinger's 1951 novel The Catcher in the Rye occurs after the adolescent protagonist buys a record for his younger sister but drops it and ""it broke into pieces ... I damn-near cried, it made me feel so terrible."" A sequence where a school teacher's collection of 78 rpm jazz records is smashed by a group of rebellious students is a key moment in the film Blackboard Jungle. In 2014 artist Jack White sold 40,000 copies of his second solo release, Lazaretto, on vinyl. The sales of the record beat the largest sales in one week on vinyl since 1991. The sales record was previously held by Pearl Jam's, Vitalogy, which sold 34,000 copies in one week in 1994. In 2014, the sale of vinyl records was the only physical music medium with increasing sales with relation to the previous year. Sales of other mediums including individual digital tracks, digital albums and compact discs have fallen, the latter having the greatest drop-in-sales rate. Vinyl records can be warped by heat, improper storage, exposure to sunlight, or manufacturing defects such as excessively tight plastic shrinkwrap on the album cover. A small degree of warp was common, and allowing for it was part of the art of turntable and tonearm design. ""wow"" (once-per-revolution pitch variation) could result from warp, or from a spindle hole that was not precisely centered. Standard practice for LPs was to place the LP in a paper or plastic inner cover. This, if placed within the outer cardboard cover so that the opening was entirely within the outer cover, was said to reduce ingress of dust onto the record surface. Singles, with rare exceptions, had simple paper covers with no inner cover. Evidence from the early technical literature concerning electrical recording suggests that it wasn't until the 1942–1949 period that there were serious efforts to standardize recording characteristics within an industry. Heretofore, electrical recording technology from company to company was considered a proprietary art all the way back to the 1925 Western Electric licensed method used by Columbia and Victor. For example, what Brunswick-Balke-Collender (Brunswick Corporation) did was different from the practices of Victor. In the 1990s Rhino Records issued a series of boxed sets of 78 rpm reissues of early rock and roll hits, intended for owners of vintage jukeboxes. This was a disaster because Rhino did not warn customers that their records were made of vinyl, and that the vintage 78 RPM juke boxes were designed with heavy tone arms and steel needles to play the hard shellac records of their time. This failure to warn customers gave the Rhino 78 records a bad reputation,[citation needed] as they were destroyed by old juke boxes and old record players but played very well on newer 78-capable turntables with modern lightweight tone arms and jewel needles. Through the 1960s, 1970s, and 1980s, various methods to improve the dynamic range of mass-produced records involved highly advanced disc cutting equipment. These techniques, marketed, to name two, as the CBS DisComputer and Teldec Direct Metal Mastering, were used to reduce inner-groove distortion. RCA Victor introduced another system to reduce dynamic range and achieve a groove with less surface noise under the commercial name of Dynagroove. Two main elements were combined: another disk material with less surface noise in the groove and dynamic compression for masking background noise. Sometimes this was called ""diaphragming"" the source material and not favoured by some music lovers for its unnatural side effects. Both elements were reflected in the brandname of Dynagroove, described elsewhere in more detail. It also used the earlier advanced method of forward-looking control on groove spacing with respect to volume of sound and position on the disk. Lower recorded volume used closer spacing; higher recorded volume used wider spacing, especially with lower frequencies. Also, the higher track density at lower volumes enabled disk recordings to end farther away from the disk center than usual, helping to reduce endtrack distortion even further. One early attempt at lengthening the playing time should be mentioned. At least one manufacturer in the early 1920s, World Records, produced records that played at a constant linear velocity, controlled by Noel Pemberton Billing's patented add-on governor device. As these were played from the outside to the inside, the rotational speed of the records increased as reproduction progressed. This action is similar (although in reverse) to that on the modern compact disc and the CLV version of its predecessor, the Philips Laser Disc. The commercial rivalry between RCA Victor and Columbia Records led to RCA Victor's introduction of what it had intended to be a competing vinyl format, the 7-inch (175 mm) 45 rpm disc. For a two-year period from 1948 to 1950, record companies and consumers faced uncertainty over which of these formats would ultimately prevail in what was known as the ""War of the Speeds"". (See also format war.) In 1949 Capitol and Decca adopted the new LP format and RCA gave in and issued its first LP in January 1950. The 45 rpm size was gaining in popularity, too, and Columbia issued its first 45s in February 1951. By 1954, 200 million 45s had been sold. In March 1949, as RCA released the 45, Columbia released several hundred 7 inch 33 1/3 rpm small spindle hole singles. This format was soon dropped as it became clear that the RCA 45 was the single of choice and the Columbia 12 inch LP would be the 'album' of choice. The first release of the 45 came in seven colors: black 47-xxxx popular series, yellow 47-xxxx juvenile series, green (teal) 48-xxxx country series, deep red 49-xxxx classical series, bright red (cerise) 50-xxxx blues/spiritual series, light blue 51-xxxx international series, dark blue 52-xxxx light classics. All colors were soon dropped in favor of black because of production problems. However, yellow and deep red were continued until about 1952. The first 45 rpm record created for sale was ""PeeWee the Piccolo"" RCA 47-0147 pressed in yellow translucent vinyl at the Sherman Avenue plant, Indianapolis Dec. 7, 1948, R.O. Price, plant manager. Broadcasters were faced with having to adapt daily to the varied recording characteristics of many sources: various makers of ""home recordings"" readily available to the public, European recordings, lateral-cut transcriptions, and vertical-cut transcriptions. Efforts were started in 1942 to standardize within the National Association of Broadcasters (NAB), later known as the National Association of Radio and Television Broadcasters (NARTB). The NAB, among other items, issued recording standards in 1949 for laterally and vertically cut records, principally transcriptions. A number of 78 rpm record producers as well as early LP makers also cut their records to the NAB/NARTB lateral standard. Under the direction of recording engineer C. Robert Fine, Mercury Records initiated a minimalist single microphone monaural recording technique in 1951. The first record, a Chicago Symphony Orchestra performance of Pictures at an Exhibition, conducted by Rafael Kubelik, was described as ""being in the living presence of the orchestra"" by The New York Times music critic. The series of records was then named Mercury Living Presence. In 1955, Mercury began three-channel stereo recordings, still based on the principle of the single microphone. The center (single) microphone was of paramount importance, with the two side mics adding depth and space. Record masters were cut directly from a three-track to two-track mixdown console, with all editing of the master tapes done on the original three-tracks. In 1961, Mercury enhanced this technique with three-microphone stereo recordings using 35 mm magnetic film instead of half-inch tape for recording. The greater thickness and width of 35 mm magnetic film prevented tape layer print-through and pre-echo and gained extended frequency range and transient response. The Mercury Living Presence recordings were remastered to CD in the 1990s by the original producer, Wilma Cozart Fine, using the same method of 3-to-2 mix directly to the master recorder. Groove recordings, first designed in the final quarter of the 19th century, held a predominant position for nearly a century—withstanding competition from reel-to-reel tape, the 8-track cartridge, and the compact cassette. In 1988, the compact disc surpassed the gramophone record in unit sales. Vinyl records experienced a sudden decline in popularity between 1988 and 1991, when the major label distributors restricted their return policies, which retailers had been relying on to maintain and swap out stocks of relatively unpopular titles. First the distributors began charging retailers more for new product if they returned unsold vinyl, and then they stopped providing any credit at all for returns. Retailers, fearing they would be stuck with anything they ordered, only ordered proven, popular titles that they knew would sell, and devoted more shelf space to CDs and cassettes. Record companies also deleted many vinyl titles from production and distribution, further undermining the availability of the format and leading to the closure of pressing plants. This rapid decline in the availability of records accelerated the format's decline in popularity, and is seen by some as a deliberate ploy to make consumers switch to CDs, which were more profitable for the record companies. In 1926 Joseph P. Maxwell and Henry C. Harrison from Bell Telephone Laboratories disclosed that the recording pattern of the Western Electric ""rubber line"" magnetic disc cutter had a constant velocity characteristic. This meant that as frequency increased in the treble, recording amplitude decreased. Conversely, in the bass as frequency decreased, recording amplitude increased. Therefore, it was necessary to attenuate the bass frequencies below about 250 Hz, the bass turnover point, in the amplified microphone signal fed to the recording head. Otherwise, bass modulation became excessive and overcutting took place into the next record groove. When played back electrically with a magnetic pickup having a smooth response in the bass region, a complementary boost in amplitude at the bass turnover point was necessary. G. H. Miller in 1934 reported that when complementary boost at the turnover point was used in radio broadcasts of records, the reproduction was more realistic and many of the musical instruments stood out in their true form. Terms such as ""long-play"" (LP) and ""extended-play"" (EP) describe multi-track records that play much longer than the single-item-per-side records, which typically do not go much past four minutes per side. An LP can play for up to 30 minutes per side, though most played for about 22 minutes per side, bringing the total playing time of a typical LP recording to about forty-five minutes. Many pre-1952 LPs, however, played for about 15 minutes per side. The 7-inch 45 rpm format normally contains one item per side but a 7-inch EP could achieve recording times of 10 to 15 minutes at the expense of attenuating and compressing the sound to reduce the width required by the groove. EP discs were generally used to make available tracks not on singles including tracks on LPs albums in a smaller, less expensive format for those who had only 45 rpm players. The large center hole on 7-inch 45 rpm records allows for easier handling by jukebox mechanisms. The term ""album"", originally used to mean a ""book"" with liner notes, holding several 78 rpm records each in its own ""page"" or sleeve, no longer has any relation to the physical format: a single LP record, or nowadays more typically a compact disc. The earliest disc records (1889–1894) were made of various materials including hard rubber. Around 1895, a shellac-based compound was introduced and became standard. Exact formulas for this compound varied by manufacturer and over the course of time, but it was typically composed of about one-third shellac and about two-thirds mineral filler, which meant finely pulverized rock, usually slate and limestone, with an admixture of cotton fibers to add tensile strength, carbon black for color (without this, it tended to be a ""dirty"" gray or brown color that most record companies considered unattractive), and a very small amount of a lubricant to facilitate mold release during manufacture. Some makers, notably Columbia Records, used a laminated construction with a core disc of coarser material or fiber. The production of shellac records continued until the end of the 78 rpm format (i.e., the late 1950s in most developed countries, but well into the 1960s in some other places), but increasingly less abrasive formulations were used during its declining years and very late examples in truly like-new condition can have as low noise levels as vinyl. In the 1890s, the recording formats of the earliest (toy) discs were mainly 12.5 cm (nominally five inches) in diameter; by the mid-1890s, the discs were usually 7 in (nominally 17.5 cm) in diameter. By 1910 the 10-inch (25.4 cm) record was by far the most popular standard, holding about three minutes of music or other entertainment on a side. From 1903 onwards, 12-inch records (30.5 cm) were also sold commercially, mostly of classical music or operatic selections, with four to five minutes of music per side. Victor, Brunswick and Columbia also issued 12-inch popular medleys, usually spotlighting a Broadway show score. However, other sizes did appear. Eight-inch discs with a 2-inch-diameter (51 mm) label became popular for about a decade in Britain, but they cannot be played in full on most modern record players because the tone arm cannot play far enough in toward the center without modification of the equipment. Some of Elvis Presley's early singles on Sun Records might have sold more copies on 78 than on 45. This is because the majority of those sales in 1954–55 were to the ""hillbilly"" market in the South and Southwestern United States, where replacing the family 78 rpm player with a new 45 rpm player was a luxury few could afford at the time. By the end of 1957, RCA Victor announced that 78s accounted for less than 10% of Presley's singles sales, essentially announcing the death throes of the 78 rpm format. The last Presley single released on 78 in the United States was RCA Victor 20-7410, I Got Stung/One Night (1958), while the last 78 in the UK was RCA 1194, A Mess Of Blues/Girl Of My Best Friend (1960). By about 1910,[note 1] bound collections of empty sleeves with a paperboard or leather cover, similar to a photograph album, were sold as record albums that customers could use to store their records (the term ""record album"" was printed on some covers). These albums came in both 10-inch and 12-inch sizes. The covers of these bound books were wider and taller than the records inside, allowing the record album to be placed on a shelf upright, like a book, suspending the fragile records above the shelf and protecting them. Early recordings were made entirely acoustically, the sound being collected by a horn and piped to a diaphragm, which vibrated the cutting stylus. Sensitivity and frequency range were poor, and frequency response was very irregular, giving acoustic recordings an instantly recognizable tonal quality. A singer practically had to put his or her face in the recording horn. Lower-pitched orchestral instruments such as cellos and double basses were often doubled (or replaced) by louder wind instruments, such as tubas. Standard violins in orchestral ensembles were commonly replaced by Stroh violins, which became popular with recording studios. Eventually, when it was more common for electric recordings to be played back electrically in the 1930s and 1940s, the overall tone was much like listening to a radio of the era. Magnetic pickups became more common and were better designed as time went on, making it possible to improve the damping of spurious resonances. Crystal pickups were also introduced as lower cost alternatives. The dynamic or moving coil microphone was introduced around 1930 and the velocity or ribbon microphone in 1932. Both of these high quality microphones became widespread in motion picture, radio, recording, and public address applications. RCA 45s were also adapted to the smaller spindle of an LP player with a plastic snap-in insert known as a ""spider"". These inserts, commissioned by RCA president David Sarnoff and invented by Thomas Hutchison, were prevalent starting in the 1960s, selling in the tens of millions per year during the 45 rpm heyday. In countries outside the U.S., 45s often had the smaller album-sized holes, e.g., Australia and New Zealand, or as in the United Kingdom, especially before the 1970s, the disc had a small hole within a circular central section held only by three or four lands so that it could be easily punched out if desired (typically for use in jukeboxes). In 1925, 78.26 rpm was chosen as the standard because of the introduction of the electrically powered synchronous turntable motor. This motor ran at 3600 rpm, such that a 46:1 gear ratio would produce 78.26 rpm. In parts of the world that used 50 Hz current, the standard was 77.92 rpm (3,000 rpm with a 77:2 ratio), which was also the speed at which a strobe disc with 77 lines would ""stand still"" in 50 Hz light (92 lines for 60 Hz). After World War II these records were retroactively known as 78s, to distinguish them from other newer disc record formats. Earlier they were just called records, or when there was a need to distinguish them from cylinders, disc records. The playing time of a phonograph record depended on the turntable speed and the groove spacing. At the beginning of the 20th century, the early discs played for two minutes, the same as early cylinder records. The 12-inch disc, introduced by Victor in 1903, increased the playing time to three and a half minutes. Because a 10-inch 78 rpm record could hold about three minutes of sound per side and the 10-inch size was the standard size for popular music, almost all popular recordings were limited to around three minutes in length. For example, when King Oliver's Creole Jazz Band, including Louis Armstrong on his first recordings, recorded 13 sides at Gennett Records in Richmond, Indiana, in 1923, one side was 2:09 and four sides were 2:52–2:59. Some recordings, such as books for the blind, were pressed at 16 2⁄3 rpm. Prestige Records released jazz records in this format in the late 1950s; for example, two of their Miles Davis albums were paired together in this format. Peter Goldmark, the man who developed the 33 1⁄3 rpm record, developed the Highway Hi-Fi 16 2⁄3 rpm record to be played in Chrysler automobiles, but poor performance of the system and weak implementation by Chrysler and Columbia led to the demise of the 16 2⁄3 rpm records. Subsequently, the 16 2⁄3 rpm speed was used for narrated publications for the blind and visually impaired, and were never widely commercially available, although it was common to see new turntable models with a 16 rpm speed setting produced as late as the 1970s. Electrical recording preceded electrical home reproduction because of the initial high cost of the new system. In 1925, the Victor company introduced the Victor Orthophonic Victrola, an acoustical record player that was specifically designed to play electrically recorded discs, as part of a line that also included electrically reproducing Electrolas. The acoustical Orthophonics ranged in price from US$95 to US$300, depending on cabinetry; by comparison, the cheapest Electrola cost US$650, the price of a new Ford automobile in an era when clerical jobs paid about $20 a week. As the playing of gramophone records causes gradual degradation of the recording, they are best preserved by transferring them onto other media and playing the records as rarely as possible. They need to be stored on edge, and do best under environmental conditions that most humans would find comfortable. The medium needs to be kept clean, but alcohol should only be used on PVC or optical media, not on 78s.[citation needed] The equipment for playback of certain formats (e.g., 16 and 78 rpm) is manufactured only in small quantities, leading to increased difficulty in finding equipment to play the recordings. Vinyl's lower surface noise level than shellac was not forgotten, nor was its durability. In the late 1930s, radio commercials and pre-recorded radio programs being sent to disc jockeys started being stamped in vinyl, so they would not break in the mail. In the mid-1940s, special DJ copies of records started being made of vinyl also, for the same reason. These were all 78 rpm. During and after World War II, when shellac supplies were extremely limited, some 78 rpm records were pressed in vinyl instead of shellac, particularly the six-minute 12-inch (30 cm) 78 rpm records produced by V-Disc for distribution to United States troops in World War II. In the 1940s, radio transcriptions, which were usually on 16-inch records, but sometimes 12-inch, were always made of vinyl, but cut at 33 1⁄3 rpm. Shorter transcriptions were often cut at 78 rpm. After World War II, two new competing formats came onto the market and gradually replaced the standard ""78"": the 33 1⁄3 rpm (often just referred to as the 33 rpm), and the 45 rpm (see above). The 33 1⁄3 rpm LP (for ""long-play"") format was developed by Columbia Records and marketed in June 1948. RCA Victor developed the 45 rpm format and marketed it in March 1949, each pursuing their own r&d in secret. Both types of new disc used narrower grooves, intended to be played with smaller stylus—typically 0.001 inches (25 µm) wide, compared to 0.003 inches (76 µm) for a 78—so the new records were sometimes called Microgroove. In the mid-1950s all record companies agreed to a common recording standard called RIAA equalization. Prior to the establishment of the standard each company used its own preferred standard, requiring discriminating listeners to use pre-amplifiers with multiple selectable equalization curves. The mid-1970s saw the introduction of dbx-encoded records, again for the audiophile niche market. These were completely incompatible with standard record playback preamplifiers, relying on the dbx compandor encoding/decoding scheme to greatly increase dynamic range (dbx encoded disks were recorded with the dynamic range compressed by a factor of two in dB: quiet sounds were meant to be played back at low gain and loud sounds were meant to be played back at high gain, via automatic gain control in the playback equipment; this reduced the effect of surface noise on quiet passages). A similar and very short-lived scheme involved using the CBS-developed ""CX"" noise reduction encoding/decoding scheme. Unwilling to accept and license Columbia's system, in February 1949 RCA Victor, in cooperation of its parent, the Radio Corporation of America, released the first 45 rpm single, 7 inches in diameter with a large center hole. The 45 rpm player included a changing mechanism that allowed multiple disks to be stacked, much as a conventional changer handled 78s. The short playing time of a single 45 rpm side meant that long works, such as symphonies, had to be released on multiple 45s instead of a single LP, but RCA claimed that the new high-speed changer rendered side breaks so brief as to be inaudible or inconsequential. Early 45 rpm records were made from either vinyl or polystyrene. They had a playing time of eight minutes. Flexible or so-called ""unbreakable"" records made of unusual materials were introduced by a number of manufacturers at various times during the 78 rpm era. In the UK, Nicole records, made of celluloid or a similar substance coated onto a cardboard core disc, were produced for a few years beginning in 1904, but they suffered from an exceptionally high level of surface noise. In the United States, Columbia Records introduced flexible, fiber-cored ""Marconi Velvet Tone Record"" pressings in 1907, but the advantages and longevity of their relatively noiseless surfaces depended on the scrupulous use of special gold-plated Marconi Needles and the product was not a success. Thin, flexible plastic records such as the German Phonycord and the British Filmophone and Goodson records appeared around 1930 but also did not last long. The contemporary French Pathé Cellodiscs, made of a very thin black plastic, which uncannily resembles the vinyl ""sound sheet"" magazine inserts of the 1965–1985 era, were similarly short-lived. In the US, Hit of the Week records, made of a patented translucent plastic called Durium coated on a heavy brown paper base, were introduced in early 1930. A new issue came out every week and they were sold at newsstands like a weekly magazine. Although inexpensive and commercially successful at first, they soon fell victim to the Great Depression and production in the US ended in 1932. Related Durium records continued to be made somewhat later in the UK and elsewhere, and as remarkably late as 1950 in Italy, where the name ""Durium"" survived far into the LP era as a trademark on ordinary vinyl records. Despite all these attempts at innovation, shellac compounds continued to be used for the overwhelming majority of commercial 78 rpm records during the lifetime of the format. The ""orange peel"" effect on vinyl records is caused by worn molds. Rather than having the proper mirror-like finish, the surface of the record will have a texture that looks like orange peel. This introduces noise into the record, particularly in the lower frequency range. With direct metal mastering (DMM), the master disc is cut on a copper-coated disc, which can also have a minor ""orange peel"" effect on the disc itself. As this ""orange peel"" originates in the master rather than being introduced in the pressing stage, there is no ill effect as there is no physical distortion of the groove. It was not unusual for electric recordings to be played back on acoustic phonographs. The Victor Orthophonic phonograph was a prime example where such playback was expected. In the Orthophonic, which benefited from telephone research, the mechanical pickup head was redesigned with lower resonance than the traditional mica type. Also, a folded horn with an exponential taper was constructed inside the cabinet to provide better impedance matching to the air. As a result, playback of an Orthophonic record sounded like it was coming from a radio."
"Punjab,_Pakistan","Maharaja Ranjit Singh's death in the summer of 1839 brought political chaos and the subsequent battles of succession and the bloody infighting between the factions at court weakened the state. Relationships with neighbouring British territories then broke down, starting the First Anglo-Sikh War; this led to a British official being resident in Lahore and the annexation in 1849 of territory south of the Satluj to British India. After the Second Anglo-Sikh War in 1849, the Sikh Empire became the last territory to be merged into British India. In Jhelum 35 British soldiers of HM XXIV regiment were killed by the local resistance during the Indian Rebellion of 1857.[citation needed] The onset of the southwest monsoon is anticipated to reach Punjab by May, but since the early 1970s the weather pattern has been irregular. The spring monsoon has either skipped over the area or has caused it to rain so hard that floods have resulted. June and July are oppressively hot. Although official estimates rarely place the temperature above 46 °C, newspaper sources claim that it reaches 51 °C and regularly carry reports about people who have succumbed to the heat. Heat records were broken in Multan in June 1993, when the mercury was reported to have risen to 54 °C. In August the oppressive heat is punctuated by the rainy season, referred to as barsat, which brings relief in its wake. The hardest part of the summer is then over, but cooler weather does not come until late October. The Government of Punjab is a provincial government in the federal structure of Pakistan, is based in Lahore, the capital of the Punjab Province. The Chief Minister of Punjab (CM) is elected by the Provincial Assembly of the Punjab to serve as the head of the provincial government in Punjab, Pakistan. The current Chief Minister is Shahbaz Sharif, who became the Chief Minister of Punjab as being restored after Governor's rule starting from 25 February 2009 to 30 March 2009. Thereafter got re-elected as a result of 11 May 2013 elections. The Provincial Assembly of the Punjab is a unicameral legislature of elected representatives of the province of Punjab, which is located in Lahore in eastern Pakistan. The Assembly was established under Article 106 of the Constitution of Pakistan as having a total of 371 seats, with 66 seats reserved for women and eight reserved for non-Muslims. In 1758, the general of the Hindu Maratha Empire, Raghunath Rao conquered Lahore and Attock. Timur Shah Durrani, the son and viceroy of Ahmad Shah Abdali, was driven out of Punjab. Lahore, Multan, Dera Ghazi Khan, Kashmir and other subahs on the south and eastern side of Peshawar were under the Maratha rule for the most part. In Punjab and Kashmir, the Marathas were now major players. The Third Battle of Panipat took place on 1761, Ahmad Shah Abdali invaded the Maratha territory of Punjab and captured remnants of the Maratha Empire in Punjab and Kashmir regions and re-consolidated control over them. Punjab's geography mostly consists of the alluvial plain of the Indus River and its four major tributaries in Pakistan, the Jhelum, Chenab, Ravi, and Sutlej rivers. There are several mountainous regions, including the Sulaiman Mountains in the southwest part of the province, and Margalla Hills, Salt Range, and Pothohar Plateau in the north. Agriculture is the chief source of income and employment in Punjab; wheat and cotton are the principal crops. Since independence, Punjab has become the seat of political and economic power; it remains the most industrialised province of Pakistan. It counts for 39.2% of large scale manufacturing and 70% of small scale manufacturing in the country. Its capital Lahore is a major regional cultural, historical, and economic centre. As of June 2012[update], Pakistan's electricity problems were so severe that violent riots were taking place across Punjab. According to protesters, load shedding was depriving the cities of electricity 20–22 hours a day, causing businesses to go bust and making living extremely hard. Gujranwala, Toba Tek Singh, Faisalabad, Sialkot, Bahawalnagar and communities across Khanewal District saw widespread rioting and violence on Sunday 17 June 2012, with the houses of several members of parliament being attacked as well as the offices of regional energy suppliers Fesco, Gepco and Mepco being ransacked or attacked. The fairs held at the shrines of Sufi saints are called urs. They generally mark the death anniversary of the saint. On these occasions devotees assemble in large numbers and pay homage to the memory of the saint. Soul inspiring music is played and devotees dance in ecstasy. The music on these occasions is essentially folk and appealing. It forms a part of the folk music through mystic messages. The most important urs are: urs of Data Ganj Buksh at Lahore, urs of Hazrat Sultan Bahu at Jhang, urs of Hazrat Shah Jewna at Jhang, urs of Hazrat Mian Mir at Lahore, urs of Baba Farid Ganj Shakar at Pakpattan, urs of Hazrat Bahaudin Zakria at Multan, urs of Sakhi Sarwar Sultan at Dera Ghazi Khan, urs of Shah Hussain at Lahore, urs of Hazrat Bulleh Shah at Kasur, urs of Hazrat Imam Bari (Bari Shah Latif) at Rawalpindi-Islamabad and urs of Shah Inayar Qadri (the murrshad of Bulleh Shah) in Lahore. For the popular taste however, light music, particularly Ghazals and folk songs, which have an appeal of their own, the names of Mehdi Hassan, Ghulam Ali, Nur Jehan, Malika Pukhraj, Farida Khanum, Roshen Ara Begum, and Nusrat Fateh Ali Khan are well-known. Folk songs and dances of the Punjab reflect a wide range of moods: the rains, sowing and harvesting seasons. Luddi, Bhangra and Sammi depict the joy of living. Love legends of Heer Ranjha, Mirza Sahiban, Sohni Mahenwal and Saiful Mulk are sung in different styles. The structure of a mosque is simple and it expresses openness. Calligraphic inscriptions from the Quran decorate mosques and mausoleums in Punjab. The inscriptions on bricks and tiles of the mausoleum of Shah Rukn-e-Alam (1320 AD) at Multan are outstanding specimens of architectural calligraphy. The earliest existing building in South Asia with enamelled tile-work is the tomb of Shah Yusuf Gardezi (1150 AD) at Multan. A specimen of the sixteenth century tile-work at Lahore is the tomb of Sheikh Musa Ahangar, with its brilliant blue dome. The tile-work of Emperor Shah Jahan is of a richer and more elaborate nature. The pictured wall of Lahore Fort is the last line in the tile-work in the entire world. Due to its location, the Punjab region came under constant attacks and influence from the west and witnessed centuries of foreign invasions by the Greeks, Kushans, Scythians, Turks, and Afghans. The city of Taxila, founded by son of Taksh the son Bharat who was the brother of Ram. It was reputed to house the oldest university in the world,[citation needed] Takshashila University. One of the teachers was the great Vedic thinker and politician Chanakya. Taxila was a great centre of learning and intellectual discussion during the Maurya Empire. It is a UN World Heritage site, valued for its archaeological and religious history. Among the Punjabi poets, the names of Sultan Bahu, Bulleh Shah, Mian Muhammad Baksh, and Waris Shah and folk singers like Inayat Hussain Bhatti and Tufail Niazi, Alam Lohar, Sain Marna, Mansoor Malangi, Allah Ditta Lona wala, Talib Hussain Dard, Attaullah Khan Essa Khailwi, Gamoo Tahliwala, Mamzoo Gha-lla, Akbar Jat, Arif Lohar, Ahmad Nawaz Cheena and Hamid Ali Bela are well-known. In the composition of classical ragas, there are such masters as Malika-i-Mauseequi (Queen of Music) Roshan Ara Begum, Ustad Amanat Ali Khan, Salamat Ali Khan and Ustad Fateh Ali Khan. Alam Lohar has made significant contributions to folklore and Punjabi literature, by being a very influential Punjabi folk singer from 1930 until 1979. The capital and largest city is Lahore which was the historical capital of the wider Punjab region. Other important cities include Faisalabad, Rawalpindi, Gujranwala, Sargodha, Multan, Sialkot, Bahawalpur, Gujrat, Sheikhupura, Jhelum and Sahiwal. Undivided Punjab is home to six rivers, of which five flow through Pakistani Punjab. From west to east, these are: the Indus, Jhelum, Beas, Chenab, Ravi and Sutlej. Nearly 60% of Pakistan's population lives in the Punjab. It is the nation's only province that touches every other province; it also surrounds the federal enclave of the national capital city at Islamabad. In the acronym P-A-K-I-S-T-A-N, the P is for Punjab. The province is home to several historical sites, including the Shalimar Gardens, the Lahore Fort, the Badshahi Mosque, the Rohtas Fort and the ruins of the ancient city of Harrapa. The Anarkali Market and Jahangir's Tomb are prominent in the city of Lahore as is the Lahore Museum, while the ancient city of Taxila in the northwest was once a major centre of Buddhist and Hindu influence. Several important Sikh shrines are in the province, including the birthplace of the first Guru, Guru Nanak. (born at Nankana Sahib). There are a few famous hill stations, including Murree, Bhurban, Patriata and Fort Munro. Exhibitions and annual horse shows in all districts and a national horse and cattle show at Lahore are held with the official patronage. The national horse and cattle show at Lahore is the biggest festival where sports, exhibitions, and livestock competitions are held. It not only encourages and patronises agricultural products and livestock through the exhibitions of agricultural products and cattle but is also a colourful documentary on the rich cultural heritage of the province with its strong rural roots. The Punjabis followed a diverse plethora of faiths, mainly comprising Hinduism[citation needed] , when the Muslim Umayyad army led by Muhammad bin Qasim conquered Sindh and Southern Punjab in 712, by defeating Raja Dahir. The Umayyad Caliphate was the second Islamic caliphate established after the death of Muhammad. It was ruled by the Umayyad dynasty, whose name derives from Umayya ibn Abd Shams, the great-grandfather of the first Umayyad caliph. Although the Umayyad family originally came from the city of Mecca, their capital was Damascus. Muhammad bin Qasim was the first to bring message of Islam to the population of Punjab.[citation needed] Punjab was part of different Muslim Empires consisting of Afghans and Turkic peoples in co-operation with local Punjabi tribes and others.[citation needed] In the 11th century, during the reign of Mahmud of Ghazni, the province became an important centre with Lahore as its second capital[citation needed] of the Ghaznavid Empire based out of Afghanistan. The Punjab region became predominantly Muslim due to missionary Sufi saints whose dargahs dot the landscape of Punjab region. Punjab during Mahabharata times was known as Panchanada. Punjab was part of the Indus Valley Civilization, more than 4000 years ago. The main site in Punjab was the city of Harrapa. The Indus Valley Civilization spanned much of what is today Pakistan and eventually evolved into the Indo-Aryan civilisation. The Vedic civilisation flourished along the length of the Indus River. This civilisation shaped subsequent cultures in South Asia and Afghanistan. Although the archaeological site at Harappa was partially damaged in 1857 when engineers constructing the Lahore-Multan railroad used brick from the Harappa ruins for track ballast, an abundance of artefacts have nevertheless been found. Punjab was part of the great ancient empires including the Gandhara Mahajanapadas, Achaemenids, Macedonians, Mauryas, Kushans, Guptas, and Hindu Shahi. It also comprised the Gujar empire for a period of time, otherwise known as the Gurjara-Pratihara empire. Agriculture flourished and trading cities (such as Multan and Lahore) grew in wealth. The northwestern part of the South Asia, including Punjab, was repeatedly invaded or conquered by various foreign empires, such as those of Tamerlane, Alexander the Great and Genghis Khan. Having conquered Drangiana, Arachosia, Gedrosia and Seistan in ten days, Alexander crossed the Hindu Kush and was thus fully informed of the magnificence of the country and its riches in gold, gems and pearls. However, Alexander had to encounter and reduce the tribes on the border of Punjab before entering the luxuriant plains. Having taken a northeasterly direction, he marched against the Aspii (mountaineers), who offered vigorous resistance, but were subdued.[citation needed] Alexander then marched through Ghazni, blockaded Magassa, and then marched to Ora and Bazira. Turning to the northeast, Alexander marched to Pucela, the capital of the district now known as Pakhli. He entered Western Punjab, where the ancient city of Nysa (at the site of modern-day Mong) was situated. A coalition was formed against Alexander by the Cathians, the people of Multan, who were very skilful in war. Alexander invested many troops, eventually killing seventeen thousand Cathians in this battle, and the city of Sagala (present-day Sialkot) was razed to the ground. Alexander left Punjab in 326 B.C. and took his army to the heartlands of his empire.[citation needed] Punjab (Urdu, Punjabi: پنجاب, panj-āb, ""five waters"":  listen (help·info)), also spelled Panjab, is the most populous of the four provinces of Pakistan. It has an area of 205,344 square kilometres (79,284 square miles) and a population of 91.379.615 in 2011, approximately 56% of the country's total population. Its provincial capital and largest city is Lahore. Punjab is bordered by the Indian states of Jammu and Kashmir to the northeast and Punjab and Rajasthan to the east. In Pakistan it is bordered by Sindh to the south, Balochistān and Khyber Pakhtunkhwa to the west, and Islamabad and Azad Kashmir to the north. Punjab witnessed major battles between the armies of India and Pakistan in the wars of 1965 and 1971. Since the 1990s Punjab hosted several key sites of Pakistan's nuclear program such as Kahuta. It also hosts major military bases such as at Sargodha and Rawalpindi. The peace process between India and Pakistan, which began in earnest in 2004, has helped pacify the situation. Trade and people-to-people contacts through the Wagah border are now starting to become common. Indian Sikh pilgrims visit holy sites such as Nankana Sahib. There are 48 departments in Punjab government. Each Department is headed by a Provincial Minister (Politician) and a Provincial Secretary (A civil servant of usually BPS-20 or BPS-21). All Ministers report to the Chief Minister, who is the Chief Executive. All Secretaries report to the Chief Secretary of Punjab, who is usually a BPS-22 Civil Servant. The Chief Secretary in turn reports to the Chief Minister. In addition to these departments, there are several Autonomous Bodies and Attached Departments that report directly to either the Secretaries or the Chief Secretary. Despite its tropical wet and dry climate, extensive irrigation makes it a rich agricultural region. Its canal-irrigation system established by the British is the largest in the world. Wheat and cotton are the largest crops. Other crops include rice, sugarcane, millet, corn, oilseeds, pulses, vegetables, and fruits such as kinoo. Livestock and poultry production are also important. Despite past animosities, the rural masses in Punjab's farms continue to use the Hindu calendar for planting and harvesting. In the mid-fifteenth century, the religion of Sikhism was born. During the Mughal empire, many Hindus increasingly adopted Sikhism. These became a formidable military force against the Mughals and later against the Afghan Empire. After fighting Ahmad Shah Durrani in the later eighteenth century, the Sikhs took control of Punjab and managed to establish the Sikh Empire under Maharaja Ranjit Singh, which lasted from 1799 to 1849. The capital of Ranjit Singh's empire was Lahore, and the empire also extended into Afghanistan and Kashmir. Bhangi Misl was the fist Sikh band to conquer Lahore and other towns of Punjab. Syed Ahmad Barelvi a Muslim, waged jihad and attempted to create an Islamic state with strict enforcement of Islamic law. Syed Ahmad Barelvi in 1821 with many supporters and spent two years organising popular and material support for his Punjab campaign. He carefully developed a network of people through the length and breadth of India to collect funds and encourage volunteers, travelling widely throughout India attracting a following among pious Muslims. In December 1826 Sayyid Ahmad and his followers clashed with Sikh troops at Akora Khattak, but with no decisive result. In a major battle near the town of Balakot in 1831, Sayyid Ahmad and Shah Ismail Shaheed with volunteer Muslims were defeated by the professional Sikh Army. The major and native language spoken in the Punjab is Punjabi (which is written in a Shahmukhi script in Pakistan) and Punjabis comprise the largest ethnic group in country. Punjabi is the provincial language of Punjab. There is not a single district in the province where Punjabi language is mother-tongue of less than 89% of population. The language is not given any official recognition in the Constitution of Pakistan at the national level. Punjabis themselves are a heterogeneous group comprising different tribes, clans (Urdu: برادری‎) and communities. In Pakistani Punjab these tribes have more to do with traditional occupations such as blacksmiths or artisans as opposed to rigid social stratifications. Punjabi dialects spoken in the province include Majhi (Standard), Saraiki and Hindko. Saraiki is mostly spoken in south Punjab, and Pashto, spoken in some parts of north west Punjab, especially in Attock District and Mianwali District. Despite the lack of a coastline, Punjab is the most industrialised province of Pakistan; its manufacturing industries produce textiles, sports goods, heavy machinery, electrical appliances, surgical instruments, vehicles, auto parts, metals, sugar mill plants, aircraft, cement, agricultural machinery, bicycles and rickshaws, floor coverings, and processed foods. In 2003, the province manufactured 90% of the paper and paper boards, 71% of the fertilizers, 69% of the sugar and 40% of the cement of Pakistan. Punjab is Pakistan's second largest province in terms of land area at 205,344 km2 (79,284 sq mi), after Balochistan, and is located at the north western edge of the geologic Indian plate in South Asia. The province is bordered by Kashmir (Azad Kashmir, Pakistan and Jammu and Kashmir, India) to the northeast, the Indian states of Punjab and Rajasthan to the east, the Pakistani province of Sindh to the south, the province of Balochistan to the southwest, the province of Khyber Pakhtunkhwa to the west, and the Islamabad Capital Territory to the north. Punjab has the largest economy in Pakistan, contributing most to the national GDP. The province's economy has quadrupled since 1972. Its share of Pakistan's GDP was 54.7% in 2000 and 59% as of 2010. It is especially dominant in the service and agriculture sectors of Pakistan's economy. With its contribution ranging from 52.1% to 64.5% in the Service Sector and 56.1% to 61.5% in the agriculture sector. It is also major manpower contributor because it has largest pool of professionals and highly skilled (technically trained) manpower in Pakistan. It is also dominant in the manufacturing sector, though the dominance is not as huge, with historical contributions raging from a low of 44% to a high of 52.6%. In 2007, Punjab achieved a growth rate of 7.8% and during the period 2002–03 to 2007–08, its economy grew at a rate of between 7% to 8% per year. and during 2008–09 grew at 6% against the total GDP growth of Pakistan at 4%."
Labour_Party_(UK),"The Labour Party's origins lie in the late 19th century, when it became apparent that there was a need for a new political party to represent the interests and needs of the urban proletariat, a demographic which had increased in number and had recently been given franchise. Some members of the trades union movement became interested in moving into the political field, and after further extensions of the voting franchise in 1867 and 1885, the Liberal Party endorsed some trade-union sponsored candidates. The first Lib–Lab candidate to stand was George Odger in the Southwark by-election of 1870. In addition, several small socialist groups had formed around this time, with the intention of linking the movement to political policies. Among these were the Independent Labour Party, the intellectual and largely middle-class Fabian Society, the Marxist Social Democratic Federation and the Scottish Labour Party. A perceived turning point was when Blair controversially allied himself with US President George W. Bush in supporting the Iraq War, which caused him to lose much of his political support. The UN Secretary-General, among many, considered the war illegal. The Iraq War was deeply unpopular in most western countries, with Western governments divided in their support and under pressure from worldwide popular protests. The decisions that led up to the Iraq war and its subsequent conduct are currently the subject of Sir John Chilcot's Iraq Inquiry. Wilson's government was responsible for a number of sweeping social and educational reforms under the leadership of Home Secretary Roy Jenkins such as the abolishment of the death penalty in 1964, the legalisation of abortion and homosexuality (initially only for men aged 21 or over, and only in England and Wales) in 1967 and the abolition of theatre censorship in 1968. Comprehensive education was expanded and the Open University created. However Wilson's government had inherited a large trade deficit that led to a currency crisis and ultimately a doomed attempt to stave off devaluation of the pound. Labour went on to lose the 1970 general election to the Conservatives under Edward Heath. ""New Labour"" was first termed as an alternative branding for the Labour Party, dating from a conference slogan first used by the Labour Party in 1994, which was later seen in a draft manifesto published by the party in 1996, called New Labour, New Life For Britain. It was a continuation of the trend that had begun under the leadership of Neil Kinnock. ""New Labour"" as a name has no official status, but remains in common use to distinguish modernisers from those holding to more traditional positions, normally referred to as ""Old Labour"". Blair announced in September 2006 that he would quit as leader within the year, though he had been under pressure to quit earlier than May 2007 in order to get a new leader in place before the May elections which were expected to be disastrous for Labour. In the event, the party did lose power in Scotland to a minority Scottish National Party government at the 2007 elections and, shortly after this, Blair resigned as Prime Minister and was replaced by his Chancellor, Gordon Brown. Although the party experienced a brief rise in the polls after this, its popularity soon slumped to its lowest level since the days of Michael Foot. During May 2008, Labour suffered heavy defeats in the London mayoral election, local elections and the loss in the Crewe and Nantwich by-election, culminating in the party registering its worst ever opinion poll result since records began in 1943, of 23%, with many citing Brown's leadership as a key factor. Membership of the party also reached a low ebb, falling to 156,205 by the end of 2009: over 40 per cent of the 405,000 peak reached in 1997 and thought to be the lowest total since the party was founded. Kinnock then resigned as leader and was replaced by John Smith. Smith's leadership once again saw the re-emergence of tension between those on the party's left and those identified as ""modernisers"", both of whom advocated radical revisions of the party's stance albeit in different ways. At the 1993 conference, Smith successfully changed the party rules and lessened the influence of the trade unions on the selection of candidates to stand for Parliament by introducing a one member, one vote system called ""OMOV"" — but only barely, after a barnstorming speech by John Prescott which required Smith to compromise on other individual negotiations. As the economic situation worsened MacDonald agreed to form a ""National Government"" with the Conservatives and the Liberals. On 24 August 1931 MacDonald submitted the resignation of his ministers and led a small number of his senior colleagues in forming the National Government together with the other parties. This caused great anger among those within the Labour Party who felt betrayed by MacDonald's actions: he and his supporters were promptly expelled from the Labour Party and formed a separate National Labour Organisation. The remaining Labour Party MPs (led again by Arthur Henderson) and a few Liberals went into opposition. The ensuing 1931 general election resulted in overwhelming victory for the National Government and disaster for the Labour Party which won only 52 seats, 225 fewer than in 1929. In 1899, a Doncaster member of the Amalgamated Society of Railway Servants, Thomas R. Steels, proposed in his union branch that the Trade Union Congress call a special conference to bring together all left-wing organisations and form them into a single body that would sponsor Parliamentary candidates. The motion was passed at all stages by the TUC, and the proposed conference was held at the Memorial Hall on Farringdon Street on 26 and 27 February 1900. The meeting was attended by a broad spectrum of working-class and left-wing organisations — trades unions represented about one third of the membership of the TUC delegates. Support for the LRC was boosted by the 1901 Taff Vale Case, a dispute between strikers and a railway company that ended with the union being ordered to pay £23,000 damages for a strike. The judgement effectively made strikes illegal since employers could recoup the cost of lost business from the unions. The apparent acquiescence of the Conservative Government of Arthur Balfour to industrial and business interests (traditionally the allies of the Liberal Party in opposition to the Conservative's landed interests) intensified support for the LRC against a government that appeared to have little concern for the industrial proletariat and its problems. The party's performance held up in local elections in 2012 with Labour consolidating its position in the North and Midlands, while also regaining some ground in Southern England. In Wales the party enjoyed good successes, regaining control of most Welsh Councils lost in 2008, including the capital city, Cardiff. In Scotland, Labour's held overall control of Glasgow City Council despite some predictions to the contrary, and also enjoyed a +3.26 swing across Scotland. In London, results were mixed for the party; Ken Livingstone lost the election for Mayor of London, but the party gained its highest ever representation in the Greater London Authority in the concurrent assembly election. The Communist Party of Great Britain was refused affiliation to the Labour Party between 1921 and 1923. Meanwhile, the Liberal Party declined rapidly, and the party also suffered a catastrophic split which allowed the Labour Party to gain much of the Liberals' support. With the Liberals thus in disarray, Labour won 142 seats in 1922, making it the second largest political group in the House of Commons and the official opposition to the Conservative government. After the election the now-rehabilitated Ramsay MacDonald was voted the first official leader of the Labour Party. The 1910 election saw 42 Labour MPs elected to the House of Commons, a significant victory since, a year before the election, the House of Lords had passed the Osborne judgment ruling that Trades Unions in the United Kingdom could no longer donate money to fund the election campaigns and wages of Labour MPs. The governing Liberals were unwilling to repeal this judicial decision with primary legislation. The height of Liberal compromise was to introduce a wage for Members of Parliament to remove the need to involve the Trade Unions. By 1913, faced with the opposition of the largest Trades Unions, the Liberal government passed the Trade Disputes Act to allow Trade Unions to fund Labour MPs once more. The Black Wednesday economic disaster in September 1992 left the Conservative government's reputation for monetary excellence in tatters, and by the end of that year Labour had a comfortable lead over the Tories in the opinion polls. Although the recession was declared over in April 1993 and a period of strong and sustained economic growth followed, coupled with a relatively swift fall in unemployment, the Labour lead in the opinion polls remained strong. However, Smith died from a heart attack in May 1994. After a debate, the 129 delegates passed Hardie's motion to establish ""a distinct Labour group in Parliament, who shall have their own whips, and agree upon their policy, which must embrace a readiness to cooperate with any party which for the time being may be engaged in promoting legislation in the direct interests of labour."" This created an association called the Labour Representation Committee (LRC), meant to coordinate attempts to support MPs sponsored by trade unions and represent the working-class population. It had no single leader, and in the absence of one, the Independent Labour Party nominee Ramsay MacDonald was elected as Secretary. He had the difficult task of keeping the various strands of opinions in the LRC united. The October 1900 ""Khaki election"" came too soon for the new party to campaign effectively; total expenses for the election only came to £33. Only 15 candidatures were sponsored, but two were successful; Keir Hardie in Merthyr Tydfil and Richard Bell in Derby. The nationalist parties, in turn, demanded devolution to their respective constituent countries in return for their supporting the government. When referendums for Scottish and Welsh devolution were held in March 1979 Welsh devolution was rejected outright while the Scottish referendum returned a narrow majority in favour without reaching the required threshold of 40% support. When the Labour government duly refused to push ahead with setting up the proposed Scottish Assembly, the SNP withdrew its support for the government: this finally brought the government down as it triggered a vote of confidence in Callaghan's government that was lost by a single vote on 28 March 1979, necessitating a general election. Labour improved its performance in 1987, gaining 20 seats and so reducing the Conservative majority from 143 to 102. They were now firmly re-established as the second political party in Britain as the Alliance had once again failed to make a breakthrough with seats. A merger of the SDP and Liberals formed the Liberal Democrats. Following the 1987 election, the National Executive Committee resumed disciplinary action against members of Militant, who remained in the party, leading to further expulsions of their activists and the two MPs who supported the group. Fear of advances by the nationalist parties, particularly in Scotland, led to the suppression of a report from Scottish Office economist Gavin McCrone that suggested that an independent Scotland would be 'chronically in surplus'. By 1977 by-election losses and defections to the breakaway Scottish Labour Party left Callaghan heading a minority government, forced to trade with smaller parties in order to govern. An arrangement negotiated in 1977 with Liberal leader David Steel, known as the Lib-Lab Pact, ended after one year. Deals were then forged with various small parties including the Scottish National Party and the Welsh nationalist Plaid Cymru, prolonging the life of the government. The party's decision-making bodies on a national level formally include the National Executive Committee (NEC), Labour Party Conference and National Policy Forum (NPF)—although in practice the Parliamentary leadership has the final say on policy. The 2008 Labour Party Conference was the first at which affiliated trade unions and Constituency Labour Parties did not have the right to submit motions on contemporary issues that would previously have been debated. Labour Party conferences now include more ""keynote"" addresses, guest speakers and question-and-answer sessions, while specific discussion of policy now takes place in the National Policy Forum. On 1 March 2014, at a special conference the party reformed internal Labour election procedures, including replacing the electoral college system for selecting new leaders with a ""one member, one vote"" system following the recommendation of a review by former general-secretary Ray Collins. Mass membership would be encouraged by allowing ""registered supporters"" to join at a low cost, as well as full membership. Members from the trade unions would also have to explicitly ""opt in"" rather than ""opt out"" of paying a political levy to Labour. After its defeat in the 1979 general election the Labour Party underwent a period of internal rivalry between the left represented by Tony Benn, and the right represented by Denis Healey. The election of Michael Foot as leader in 1980, and the leftist policies he espoused, such as unilateral nuclear disarmament, leaving the European Economic Community (EEC) and NATO, closer governmental influence in the banking system, the creation of a national minimum wage and a ban on fox hunting led in 1981 to four former cabinet ministers from the right of the Labour Party (Shirley Williams, William Rodgers, Roy Jenkins and David Owen) forming the Social Democratic Party. Benn was only narrowly defeated by Healey in a bitterly fought deputy leadership election in 1981 after the introduction of an electoral college intended to widen the voting franchise to elect the leader and their deputy. By 1982, the National Executive Committee had concluded that the entryist Militant tendency group were in contravention of the party's constitution. The Militant newspaper's five member editorial board were expelled on 22 February 1983. Historically within the party, differentiation was made between the ""soft left"" and the ""hard left"", with the former embracing more moderately social democratic views while the hard left subscribed to a strongly socialist, even Marxist, ideology. Members on the hard left were often disparaged as the ""loony left,"" particularly in the popular media. The term ""hard left"" was sometimes used in the 1980s to describe Trotskyist groups such as the Militant tendency, Socialist Organiser and Socialist Action. In more recent times, Members of Parliament in the Socialist Campaign Group and the Labour Representation Committee are seen as constituting a hard left in contrast to a soft left represented by organisations such as Compass and the magazine Tribune. Foot resigned and was replaced as leader by Neil Kinnock, with Roy Hattersley as his deputy. The new leadership progressively dropped unpopular policies. The miners strike of 1984–85 over coal mine closures, for which miners' leader Arthur Scargill was blamed, and the Wapping dispute led to clashes with the left of the party, and negative coverage in most of the press. Tabloid vilification of the so-called loony left continued to taint the parliamentary party by association from the activities of 'extra-parliamentary' militants in local government. The 1923 general election was fought on the Conservatives' protectionist proposals but, although they got the most votes and remained the largest party, they lost their majority in parliament, necessitating the formation of a government supporting free trade. Thus, with the acquiescence of Asquith's Liberals, Ramsay MacDonald became the first ever Labour Prime Minister in January 1924, forming the first Labour government, despite Labour only having 191 MPs (less than a third of the House of Commons). Labour runs a minority government in the Welsh Assembly under Carwyn Jones, is the largest opposition party in the Scottish Parliament and has twenty MEPs in the European Parliament, sitting in the Socialists and Democrats Group. The party also organises in Northern Ireland, but does not contest elections to the Northern Ireland Assembly. The Labour Party is a full member of the Party of European Socialists and Progressive Alliance, and holds observer status in the Socialist International. In September 2015, Jeremy Corbyn was elected Leader of the Labour Party. In their first meeting after the election the group's Members of Parliament decided to adopt the name ""The Labour Party"" formally (15 February 1906). Keir Hardie, who had taken a leading role in getting the party established, was elected as Chairman of the Parliamentary Labour Party (in effect, the Leader), although only by one vote over David Shackleton after several ballots. In the party's early years the Independent Labour Party (ILP) provided much of its activist base as the party did not have individual membership until 1918 but operated as a conglomerate of affiliated bodies. The Fabian Society provided much of the intellectual stimulus for the party. One of the first acts of the new Liberal Government was to reverse the Taff Vale judgement. The Labour Party is considered to be left of centre. It was initially formed as a means for the trade union movement to establish political representation for itself at Westminster. It only gained a 'socialist' commitment with the original party constitution of 1918. That 'socialist' element, the original Clause IV, was seen by its strongest advocates as a straightforward commitment to the ""common ownership"", or nationalisation, of the ""means of production, distribution and exchange"". Although about a third of British industry was taken into public ownership after the Second World War, and remained so until the 1980s, the right of the party were questioning the validity of expanding on this objective by the late 1950s. Influenced by Anthony Crosland's book, The Future of Socialism (1956), the circle around party leader Hugh Gaitskell felt that the commitment was no longer necessary. While an attempt to remove Clause IV from the party constitution in 1959 failed, Tony Blair, and the 'modernisers' saw the issue as putting off potential voters, and were successful thirty-five years later, with only limited opposition from senior figures in the party. For many years Labour held to a policy of not allowing residents of Northern Ireland to apply for membership, instead supporting the Social Democratic and Labour Party (SDLP) which informally takes the Labour whip in the House of Commons. The 2003 Labour Party Conference accepted legal advice that the party could not continue to prohibit residents of the province joining, and whilst the National Executive has established a regional constituency party it has not yet agreed to contest elections there. In December 2015 a meeting of the members of the Labour Party in Northern Ireland decided unanimously to contest the elections for the Northern Ireland Assembly held in May 2016. Labour went on to win the 1950 general election, but with a much reduced majority of five seats. Soon afterwards, defence became a divisive issue within the party, especially defence spending (which reached a peak of 14% of GDP in 1951 during the Korean War), straining public finances and forcing savings elsewhere. The Chancellor of the Exchequer, Hugh Gaitskell, introduced charges for NHS dentures and spectacles, causing Bevan, along with Harold Wilson (then President of the Board of Trade), to resign over the dilution of the principle of free treatment on which the NHS had been established. Callaghan had been widely expected to call a general election in the autumn of 1978 when most opinion polls showed Labour to have a narrow lead. However he decided to extend his wage restraint policy for another year hoping that the economy would be in a better shape for a 1979 election. But during the winter of 1978–79 there were widespread strikes among lorry drivers, railway workers, car workers and local government and hospital workers in favour of higher pay-rises that caused significant disruption to everyday life. These events came to be dubbed the ""Winter of Discontent"". Finance proved a major problem for the Labour Party during this period; a ""cash for peerages"" scandal under Blair resulted in the drying up of many major sources of donations. Declining party membership, partially due to the reduction of activists' influence upon policy-making under the reforms of Neil Kinnock and Blair, also contributed to financial problems. Between January and March 2008, the Labour Party received just over £3 million in donations and were £17 million in debt; compared to the Conservatives' £6 million in donations and £12 million in debt. Clement Attlee's proved one of the most radical British governments of the 20th century, enacting Keynesian economic policies, presiding over a policy of nationalising major industries and utilities including the Bank of England, coal mining, the steel industry, electricity, gas, and inland transport (including railways, road haulage and canals). It developed and implemented the ""cradle to grave"" welfare state conceived by the economist William Beveridge. To this day, the party considers the 1948 creation of Britain's publicly funded National Health Service (NHS) under health minister Aneurin Bevan its proudest achievement. Attlee's government also began the process of dismantling the British Empire when it granted independence to India and Pakistan in 1947, followed by Burma (Myanmar) and Ceylon (Sri Lanka) the following year. At a secret meeting in January 1947, Attlee and six cabinet ministers, including Foreign Secretary Ernest Bevin, decided to proceed with the development of Britain's nuclear weapons programme, in opposition to the pacifist and anti-nuclear stances of a large element inside the Labour Party. The party was a member of the Labour and Socialist International between 1923 and 1940. Since 1951 the party has been a member of the Socialist International, which was founded thanks to the efforts of the Clement Attlee leadership. However, in February 2013, the Labour Party NEC decided to downgrade participation to observer membership status, ""in view of ethical concerns, and to develop international co-operation through new networks"". Labour was a founding member of the Progressive Alliance international founded in co-operation with the Social Democratic Party of Germany and other social-democratic parties on 22 May 2013. The ""yo yo"" in the opinion polls continued into 1992, though after November 1990 any Labour lead in the polls was rarely sufficient for a majority. Major resisted Kinnock's calls for a general election throughout 1991. Kinnock campaigned on the theme ""It's Time for a Change"", urging voters to elect a new government after more than a decade of unbroken Conservative rule. However, the Conservatives themselves had undergone a dramatic change in the change of leader from Thatcher to Major, at least in terms of style if not substance. From the outset, it was clearly a well-received change, as Labour's 14-point lead in the November 1990 ""Poll of Polls"" was replaced by an 8% Tory lead a month later. The 2015 General Election resulted in a net loss of seats throughout Great Britain, with Labour representation falling to 232 seats in the House of Commons. The Party lost 40 of its 41 seats in Scotland in the face of record breaking swings to the Scottish National Party. The scale of the decline in Labour's support was much greater than what had occurred at the 2011 elections for the Scottish parliament. Though Labour gained more than 20 seats in England and Wales, mostly from the Liberal Democrats but also from the Conservative Party, it lost more seats to Conservative challengers, including that of Ed Balls, for net losses overall. The government collapsed after only nine months when the Liberals voted for a Select Committee inquiry into the Campbell Case, a vote which MacDonald had declared to be a vote of confidence. The ensuing 1924 general election saw the publication, four days before polling day, of the Zinoviev letter, in which Moscow talked about a Communist revolution in Britain. The letter had little impact on the Labour vote—which held up. It was the collapse of the Liberal party that led to the Conservative landslide. The Conservatives were returned to power although Labour increased its vote from 30.7% to a third of the popular vote, most Conservative gains being at the expense of the Liberals. However many Labourites for years blamed their defeat on foul play (the Zinoviev Letter), thereby according to A. J. P. Taylor misunderstanding the political forces at work and delaying needed reforms in the party. After losing the 1970 general election, Labour returned to opposition, but retained Harold Wilson as Leader. Heath's government soon ran into trouble over Northern Ireland and a dispute with miners in 1973 which led to the ""three-day week"". The 1970s proved a difficult time to be in government for both the Conservatives and Labour due to the 1973 oil crisis which caused high inflation and a global recession. The Labour Party was returned to power again under Wilson a few weeks after the February 1974 general election, forming a minority government with the support of the Ulster Unionists. The Conservatives were unable to form a government alone as they had fewer seats despite receiving more votes numerically. It was the first general election since 1924 in which both main parties had received less than 40% of the popular vote and the first of six successive general elections in which Labour failed to reach 40% of the popular vote. In a bid to gain a majority, a second election was soon called for October 1974 in which Labour, still with Harold Wilson as leader, won a majority of three, gaining just 18 seats taking its total to 319. Labour has long been identified with red, a political colour traditionally affiliated with socialism and the labour movement. The party conference in 1931 passed a motion ""That this conference adopts Party Colours, which should be uniform throughout the country, colours to be red and gold"". Since the party's inception, the red flag has been Labour's official symbol; the flag has been associated with socialism and revolution ever since the 1789 French Revolution and the revolutions of 1848. The red rose, a symbol of social democracy, was adopted as the party symbol in 1986 as part of a rebranding exercise and is now incorporated into the party logo. As it was founded by the unions to represent the interests of working-class people, Labour's link with the unions has always been a defining characteristic of the party. In recent years this link has come under increasing strain, with the RMT being expelled from the party in 2004 for allowing its branches in Scotland to affiliate to the left-wing Scottish Socialist Party. Other unions have also faced calls from members to reduce financial support for the Party and seek more effective political representation for their views on privatisation, public spending cuts and the anti-trade union laws. Unison and GMB have both threatened to withdraw funding from constituency MPs and Dave Prentis of UNISON has warned that the union will write ""no more blank cheques"" and is dissatisfied with ""feeding the hand that bites us"". Union funding was redesigned in 2013 after the Falkirk candidate-selection controversy. From the late-1980s onwards, the party adopted free market policies, leading many observers to describe the Labour Party as social democratic or the Third Way, rather than democratic socialist. Other commentators go further and argue that traditional social democratic parties across Europe, including the British Labour Party, have been so deeply transformed in recent years that it is no longer possible to describe them ideologically as 'social democratic', and claim that this ideological shift has put new strains on the party's traditional relationship with the trade unions. In the 2010 general election on 6 May that year, Labour with 29.0% of the vote won the second largest number of seats (258). The Conservatives with 36.5% of the vote won the largest number of seats (307), but no party had an overall majority, meaning that Labour could still remain in power if they managed to form a coalition with at least one smaller party. However, the Labour Party would have had to form a coalition with more than one other smaller party to gain an overall majority; anything less would result in a minority government. On 10 May 2010, after talks to form a coalition with the Liberal Democrats broke down, Brown announced his intention to stand down as Leader before the Labour Party Conference but a day later resigned as both Prime Minister and party leader. Harriet Harman became the Leader of the Opposition and acting Leader of the Labour Party following the resignation of Gordon Brown on 11 May 2010, pending a leadership election subsequently won by Ed Miliband. Miliband emphasised ""responsible capitalism"" and greater state intervention to change the balance of the UK economy away from financial services. Tackling vested interests and opening up closed circles in British society were also themes he returned to a number of times. Miliband also argued for greater regulation on banks and the energy companies."
Geology,"In the laboratory, stratigraphers analyze samples of stratigraphic sections that can be returned from the field, such as those from drill cores. Stratigraphers also analyze data from geophysical surveys that show the locations of stratigraphic units in the subsurface. Geophysical data and well logs can be combined to produce a better view of the subsurface, and stratigraphers often use computer programs to do this in three dimensions. Stratigraphers can then use these data to reconstruct ancient processes occurring on the surface of the Earth, interpret past environments, and locate areas for water, coal, and hydrocarbon extraction. The following four timelines show the geologic time scale. The first shows the entire time from the formation of the Earth to the present, but this compresses the most recent eon. Therefore, the second scale shows the most recent eon with an expanded scale. The second scale compresses the most recent era, so the most recent era is expanded in the third scale. Since the Quaternary is a very short period with short epochs, it is further expanded in the fourth scale. The second, third, and fourth timelines are therefore each subsections of their preceding timeline as indicated by asterisks. The Holocene (the latest epoch) is too small to be shown clearly on the third timeline on the right, another reason for expanding the fourth scale. The Pleistocene (P) epoch. Q stands for the Quaternary period. The principle of cross-cutting relationships pertains to the formation of faults and the age of the sequences through which they cut. Faults are younger than the rocks they cut; accordingly, if a fault is found that penetrates some formations but not those on top of it, then the formations that were cut are older than the fault, and the ones that are not cut must be younger than the fault. Finding the key bed in these situations may help determine whether the fault is a normal fault or a thrust fault. Some modern scholars, such as Fielding H. Garrison, are of the opinion that the origin of the science of geology can be traced to Persia after the Muslim conquests had come to an end. Abu al-Rayhan al-Biruni (973–1048 CE) was one of the earliest Persian geologists, whose works included the earliest writings on the geology of India, hypothesizing that the Indian subcontinent was once a sea. Drawing from Greek and Indian scientific literature that were not destroyed by the Muslim conquests, the Persian scholar Ibn Sina (Avicenna, 981–1037) proposed detailed explanations for the formation of mountains, the origin of earthquakes, and other topics central to modern geology, which provided an essential foundation for the later development of the science. In China, the polymath Shen Kuo (1031–1095) formulated a hypothesis for the process of land formation: based on his observation of fossil animal shells in a geological stratum in a mountain hundreds of miles from the ocean, he inferred that the land was formed by erosion of the mountains and by deposition of silt. Petrologists can also use fluid inclusion data and perform high temperature and pressure physical experiments to understand the temperatures and pressures at which different mineral phases appear, and how they change through igneous and metamorphic processes. This research can be extrapolated to the field to understand metamorphic processes and the conditions of crystallization of igneous rocks. This work can also help to explain processes that occur within the Earth, such as subduction and magma chamber evolution. All of these processes do not necessarily occur in a single environment, and do not necessarily occur in a single order. The Hawaiian Islands, for example, consist almost entirely of layered basaltic lava flows. The sedimentary sequences of the mid-continental United States and the Grand Canyon in the southwestern United States contain almost-undeformed stacks of sedimentary rocks that have remained in place since Cambrian time. Other areas are much more geologically complex. In the southwestern United States, sedimentary, volcanic, and intrusive rocks have been metamorphosed, faulted, foliated, and folded. Even older rocks, such as the Acasta gneiss of the Slave craton in northwestern Canada, the oldest known rock in the world have been metamorphosed to the point where their origin is undiscernable without laboratory analysis. In addition, these processes can occur in stages. In many places, the Grand Canyon in the southwestern United States being a very visible example, the lower rock units were metamorphosed and deformed, and then deformation ended and the upper, undeformed units were deposited. Although any amount of rock emplacement and rock deformation can occur, and they can occur any number of times, these concepts provide a guide to understanding the geological history of an area. In the laboratory, biostratigraphers analyze rock samples from outcrop and drill cores for the fossils found in them. These fossils help scientists to date the core and to understand the depositional environment in which the rock units formed. Geochronologists precisely date rocks within the stratigraphic section in order to provide better absolute bounds on the timing and rates of deposition. Magnetic stratigraphers look for signs of magnetic reversals in igneous rock units within the drill cores. Other scientists perform stable isotope studies on the rocks to gain information about past climate. James Hutton is often viewed as the first modern geologist. In 1785 he presented a paper entitled Theory of the Earth to the Royal Society of Edinburgh. In his paper, he explained his theory that the Earth must be much older than had previously been supposed in order to allow enough time for mountains to be eroded and for sediments to form new rocks at the bottom of the sea, which in turn were raised up to become dry land. Hutton published a two-volume version of his ideas in 1795 (Vol. 1, Vol. 2). The first geological map of the U.S. was produced in 1809 by William Maclure. In 1807, Maclure commenced the self-imposed task of making a geological survey of the United States. Almost every state in the Union was traversed and mapped by him, the Allegheny Mountains being crossed and recrossed some 50 times. The results of his unaided labours were submitted to the American Philosophical Society in a memoir entitled Observations on the Geology of the United States explanatory of a Geological Map, and published in the Society's Transactions, together with the nation's first geological map. This antedates William Smith's geological map of England by six years, although it was constructed using a different classification of rocks. The development of plate tectonics provided a physical basis for many observations of the solid Earth. Long linear regions of geologic features could be explained as plate boundaries. Mid-ocean ridges, high regions on the seafloor where hydrothermal vents and volcanoes exist, were explained as divergent boundaries, where two plates move apart. Arcs of volcanoes and earthquakes were explained as convergent boundaries, where one plate subducts under another. Transform boundaries, such as the San Andreas fault system, resulted in widespread powerful earthquakes. Plate tectonics also provided a mechanism for Alfred Wegener's theory of continental drift, in which the continents move across the surface of the Earth over geologic time. They also provided a driving force for crustal deformation, and a new setting for the observations of structural geology. The power of the theory of plate tectonics lies in its ability to combine all of these observations into a single theory of how the lithosphere moves over the convecting mantle. The principle of inclusions and components states that, with sedimentary rocks, if inclusions (or clasts) are found in a formation, then the inclusions must be older than the formation that contains them. For example, in sedimentary rocks, it is common for gravel from an older formation to be ripped up and included in a newer layer. A similar situation with igneous rocks occurs when xenoliths are found. These foreign bodies are picked up as magma or lava flows, and are incorporated, later to cool in the matrix. As a result, xenoliths are older than the rock which contains them. When rock units are placed under horizontal compression, they shorten and become thicker. Because rock units, other than muds, do not significantly change in volume, this is accomplished in two primary ways: through faulting and folding. In the shallow crust, where brittle deformation can occur, thrust faults form, which cause deeper rock to move on top of shallower rock. Because deeper rock is often older, as noted by the principle of superposition, this can result in older rocks moving on top of younger ones. Movement along faults can result in folding, either because the faults are not planar or because rock layers are dragged along, forming drag folds as slip occurs along the fault. Deeper in the Earth, rocks behave plastically, and fold instead of faulting. These folds can either be those where the material in the center of the fold buckles upwards, creating ""antiforms"", or where it buckles downwards, creating ""synforms"". If the tops of the rock units within the folds remain pointing upwards, they are called anticlines and synclines, respectively. If some of the units in the fold are facing downward, the structure is called an overturned anticline or syncline, and if all of the rock units are overturned or the correct up-direction is unknown, they are simply called by the most general terms, antiforms and synforms. In the 1960s, a series of discoveries, the most important of which was seafloor spreading, showed that the Earth's lithosphere, which includes the crust and rigid uppermost portion of the upper mantle, is separated into a number of tectonic plates that move across the plastically deforming, solid, upper mantle, which is called the asthenosphere. There is an intimate coupling between the movement of the plates on the surface and the convection of the mantle: oceanic plate motions and mantle convection currents always move in the same direction, because the oceanic lithosphere is the rigid upper thermal boundary layer of the convecting mantle. This coupling between rigid plates moving on the surface of the Earth and the convecting mantle is called plate tectonics. There are three major types of rock: igneous, sedimentary, and metamorphic. The rock cycle is an important concept in geology which illustrates the relationships between these three types of rock, and magma. When a rock crystallizes from melt (magma and/or lava), it is an igneous rock. This rock can be weathered and eroded, and then redeposited and lithified into a sedimentary rock, or be turned into a metamorphic rock due to heat and pressure that change the mineral content of the rock which gives it a characteristic fabric. The sedimentary rock can then be subsequently turned into a metamorphic rock due to heat and pressure and is then weathered, eroded, deposited, and lithified, ultimately becoming a sedimentary rock. Sedimentary rock may also be re-eroded and redeposited, and metamorphic rock may also undergo additional metamorphism. All three types of rocks may be re-melted; when this happens, a new magma is formed, from which an igneous rock may once again crystallize. Among the most well-known experiments in structural geology are those involving orogenic wedges, which are zones in which mountains are built along convergent tectonic plate boundaries. In the analog versions of these experiments, horizontal layers of sand are pulled along a lower surface into a back stop, which results in realistic-looking patterns of faulting and the growth of a critically tapered (all angles remain the same) orogenic wedge. Numerical models work in the same way as these analog models, though they are often more sophisticated and can include patterns of erosion and uplift in the mountain belt. This helps to show the relationship between erosion and the shape of the mountain range. These studies can also give useful information about pathways for metamorphism through pressure, temperature, space, and time. The addition of new rock units, both depositionally and intrusively, often occurs during deformation. Faulting and other deformational processes result in the creation of topographic gradients, causing material on the rock unit that is increasing in elevation to be eroded by hillslopes and channels. These sediments are deposited on the rock unit that is going down. Continual motion along the fault maintains the topographic gradient in spite of the movement of sediment, and continues to create accommodation space for the material to deposit. Deformational events are often also associated with volcanism and igneous activity. Volcanic ashes and lavas accumulate on the surface, and igneous intrusions enter from below. Dikes, long, planar igneous intrusions, enter along cracks, and therefore often form in large numbers in areas that are being actively deformed. This can result in the emplacement of dike swarms, such as those that are observable across the Canadian shield, or rings of dikes around the lava tube of a volcano. At the beginning of the 20th century, important advancement in geological science was facilitated by the ability to obtain accurate absolute dates to geologic events using radioactive isotopes and other methods. This changed the understanding of geologic time. Previously, geologists could only use fossils and stratigraphic correlation to date sections of rock relative to one another. With isotopic dates it became possible to assign absolute ages to rock units, and these absolute dates could be applied to fossil sequences in which there was datable material, converting the old relative ages into new absolute ages. Seismologists can use the arrival times of seismic waves in reverse to image the interior of the Earth. Early advances in this field showed the existence of a liquid outer core (where shear waves were not able to propagate) and a dense solid inner core. These advances led to the development of a layered model of the Earth, with a crust and lithosphere on top, the mantle below (separated within itself by seismic discontinuities at 410 and 660 kilometers), and the outer core and inner core below that. More recently, seismologists have been able to create detailed images of wave speeds inside the earth in the same way a doctor images a body in a CT scan. These images have led to a much more detailed view of the interior of the Earth, and have replaced the simplified layered model with a much more dynamic model. Sir Charles Lyell first published his famous book, Principles of Geology, in 1830. This book, which influenced the thought of Charles Darwin, successfully promoted the doctrine of uniformitarianism. This theory states that slow geological processes have occurred throughout the Earth's history and are still occurring today. In contrast, catastrophism is the theory that Earth's features formed in single, catastrophic events and remained unchanged thereafter. Though Hutton believed in uniformitarianism, the idea was not widely accepted at the time. The principle of faunal succession is based on the appearance of fossils in sedimentary rocks. As organisms exist at the same time period throughout the world, their presence or (sometimes) absence may be used to provide a relative age of the formations in which they are found. Based on principles laid out by William Smith almost a hundred years before the publication of Charles Darwin's theory of evolution, the principles of succession were developed independently of evolutionary thought. The principle becomes quite complex, however, given the uncertainties of fossilization, the localization of fossil types due to lateral changes in habitat (facies change in sedimentary strata), and that not all fossils may be found globally at the same time. In addition to identifying rocks in the field, petrologists identify rock samples in the laboratory. Two of the primary methods for identifying rocks in the laboratory are through optical microscopy and by using an electron microprobe. In an optical mineralogy analysis, thin sections of rock samples are analyzed through a petrographic microscope, where the minerals can be identified through their different properties in plane-polarized and cross-polarized light, including their birefringence, pleochroism, twinning, and interference properties with a conoscopic lens. In the electron microprobe, individual locations are analyzed for their exact chemical compositions and variation in composition within individual crystals. Stable and radioactive isotope studies provide insight into the geochemical evolution of rock units. Structural geologists use microscopic analysis of oriented thin sections of geologic samples to observe the fabric within the rocks which gives information about strain within the crystalline structure of the rocks. They also plot and combine measurements of geological structures in order to better understand the orientations of faults and folds in order to reconstruct the history of rock deformation in the area. In addition, they perform analog and numerical experiments of rock deformation in large and small settings. For many geologic applications, isotope ratios of radioactive elements are measured in minerals that give the amount of time that has passed since a rock passed through its particular closure temperature, the point at which different radiometric isotopes stop diffusing into and out of the crystal lattice. These are used in geochronologic and thermochronologic studies. Common methods include uranium-lead dating, potassium-argon dating, argon-argon dating and uranium-thorium dating. These methods are used for a variety of applications. Dating of lava and volcanic ash layers found within a stratigraphic sequence can provide absolute age data for sedimentary rock units which do not contain radioactive isotopes and calibrate relative dating techniques. These methods can also be used to determine ages of pluton emplacement. Thermochemical techniques can be used to determine temperature profiles within the crust, the uplift of mountain ranges, and paleotopography. Geologists use a number of field, laboratory, and numerical modeling methods to decipher Earth history and understand the processes that occur on and inside the Earth. In typical geological investigations, geologists use primary information related to petrology (the study of rocks), stratigraphy (the study of sedimentary layers), and structural geology (the study of positions of rock units and their deformation). In many cases, geologists also study modern soils, rivers, landscapes, and glaciers; investigate past and current life and biogeochemical pathways, and use geophysical methods to investigate the subsurface. Extension causes the rock units as a whole to become longer and thinner. This is primarily accomplished through normal faulting and through the ductile stretching and thinning. Normal faults drop rock units that are higher below those that are lower. This typically results in younger units being placed below older units. Stretching of units can result in their thinning; in fact, there is a location within the Maria Fold and Thrust Belt in which the entire sedimentary sequence of the Grand Canyon can be seen over a length of less than a meter. Rocks at the depth to be ductilely stretched are often also metamorphosed. These stretched rocks can also pinch into lenses, known as boudins, after the French word for ""sausage"", because of their visual similarity."
Royal_assent,"When the act is assented to by the sovereign in person, or by empowered Royal Commissioners, royal assent is considered given at the moment when the assent is declared in the presence of both houses jointly assembled. When the procedure created by the Royal Assent Act 1967 is followed, assent is considered granted when the presiding officers of both houses, having received the letters patent from the king or queen signifying the assent, have notified their respective house of the grant of royal assent. Thus, if each presiding officer makes the announcement at a different time (for instance because one house is not sitting on a certain date), assent is regarded as effective when the second announcement is made. This is important because, under British Law, unless there is any provision to the contrary, an act takes effect on the date on which it receives royal assent and that date is not regarded as being the date when the letters patent are signed, or when they are delivered to the presiding officers of each house, but the date on which both houses have been formally acquainted of the assent. Royal assent is the method by which a country's constitutional monarch (possibly through a delegated official) formally approves an act of that nation's parliament, thus making it a law or letting it be promulgated as law. In the vast majority of contemporary monarchies, this act is considered to be little more than a formality; even in those nations which still permit their ruler to withhold the royal assent (such as the United Kingdom, Norway, and Liechtenstein), the monarch almost never does so, save in a dire political emergency or upon the advice of their government. While the power to withhold royal assent was once exercised often in European monarchies, it is exceedingly rare in the modern, democratic political atmosphere that has developed there since the 18th century. Originally, legislative power was exercised by the sovereign acting on the advice of the Curia Regis, or Royal Council, in which important magnates and clerics participated and which evolved into parliament. The so-called Model Parliament included bishops, abbots, earls, barons, and two knights from each shire and two burgesses from each borough among its members. In 1265, the Earl of Leicester irregularly called a full parliament without royal authorisation. The body eventually came to be divided into two branches: bishops, abbots, earls, and barons formed the House of Lords, while the shire and borough representatives formed the House of Commons. The King would seek the advice and consent of both houses before making any law. During Henry VI's reign, it became regular practice for the two houses to originate legislation in the form of bills, which would not become law unless the sovereign's assent was obtained, as the sovereign was, and still remains, the enactor of laws. Hence, all acts include the clause ""Be it enacted by the Queen's (King's) most Excellent Majesty, by and with the advice and consent of the Lords Spiritual and Temporal, and Commons, in this present Parliament assembled, and by the authority of the same, as follows..."". The Parliament Acts 1911 and 1949 provide a second potential preamble if the House of Lords were to be excluded from the process. In Commonwealth realms other than the UK, royal assent is granted or withheld either by the realm's sovereign or, more frequently, by the representative of the sovereign, the governor-general. In federated realms, assent in each state, province, or territory is granted or withheld by the representatives of the sovereign. In Australia, this is the governors of the states, administrators of the territories, or the governor-general in the Australian Capital Territory. For Canada, this is the lieutenant governors of the provinces. A lieutenant governor may defer assent to the governor general, and the governor general may defer assent to federal bills to the sovereign. During the rule of the succeeding Hanoverian dynasty, power was gradually exercised more by parliament and the government. The first Hanoverian monarch, George I, relied on his ministers to a greater extent than did previous monarchs. Later Hanoverian monarchs attempted to restore royal control over legislation: George III and George IV both openly opposed Catholic Emancipation and asserted that to grant assent to a Catholic emancipation bill would violate the Coronation Oath, which required the sovereign to preserve and protect the established Church of England from Papal domination and would grant rights to individuals who were in league with a foreign power which did not recognise their legitimacy. However, George IV reluctantly granted his assent upon the advice of his ministers. Thus, as the concept of ministerial responsibility has evolved, the power to withhold royal assent has fallen into disuse, both in the United Kingdom and in the other Commonwealth realms. Instead, the monarch directly grants royal assent by Order in Council. Assent is granted or refused on the advice of the Lord Chancellor. A recent example when assent was refused (or, more correctly, when the Lord Chancellor declined to present the law for assent) was in 2007, concerning reforms to the constitution of the Chief Pleas of Sark. (A revised version of the proposed reforms was subsequently given assent.) In 2011, campaigners against a law that sought to reduce the number of senators in the states of Jersey petitioned the Privy Council to advise the Queen to refuse royal assent. An Order in Council of 13 July 2011 established new rules for the consideration of petitions against granting royal assent. In the United Kingdom, a bill is presented for royal assent after it has passed all the required stages in both the House of Commons and the House of Lords. Under the Parliament Acts 1911 and 1949, the House of Commons may, under certain circumstances, direct that a bill be presented for assent despite lack of passage by the House of Lords. Officially, assent is granted by the sovereign or by Lords Commissioners authorised to act by letters patent. It may be granted in parliament or outside parliament; in the latter case, each house must be separately notified before the bill takes effect. When granting assent by commission, the sovereign authorises three or more (normally five) lords who are Privy Counsellors to grant assent in his or her name. The Lords Commissioners, as the monarch's representatives are known, wear scarlet parliamentary robes and sit on a bench between the throne and the Woolsack. The Lords Reading Clerk reads the commission aloud; the senior commissioner then states, ""My Lords, in obedience to Her Majesty's Commands, and by virtue of the Commission which has been now read, We do declare and notify to you, the Lords Spiritual and Temporal and Commons in Parliament assembled, that Her Majesty has given Her Royal Assent to the several Acts in the Commission mentioned."" The government, consisting of the monarch and the ministers, will then usually approve the proposal and the sovereign and one of the ministers signs the proposal with the addition of an enacting clause, thereafter notifying the States General that ""The King assents to the proposal."" It has happened in exceptional circumstances that the government does not approve a law that has been passed in parliament. In such a case, neither the monarch nor a minister will sign the bill, notifying the States General that ""The King will keep the proposal under advisement."" A law that has received royal assent will be published in the State Magazine, with the original being kept in the archives of the King's Offices. No provision within the constitution grants the monarch an ability to veto legislation directly; however, no provision prohibits the sovereign from withholding royal assent, which effectively constitutes a veto. When the Spanish media asked King Juan Carlos if he would endorse the bill legalising same-sex marriages, he answered ""Soy el Rey de España y no el de Bélgica"" (""I am the King of Spain and not that of Belgium"")—a reference to King Baudouin I of Belgium, who had refused to sign the Belgian law legalising abortion. The King gave royal assent to Law 13/2005 on 1 July 2005; the law was gazetted in the Boletín Oficial del Estado on 2 July and came into effect on 3 July 2005. Likewise, in 2010, King Juan Carlos gave royal assent to a law permitting abortion on demand. Royal assent is sometimes associated with elaborate ceremonies. In the United Kingdom, for instance, the sovereign may appear personally in the House of Lords or may appoint Lords Commissioners, who announce that royal assent has been granted at a ceremony held at the Palace of Westminster. However, royal assent is usually granted less ceremonially by letters patent. In other nations, such as Australia, the governor-general merely signs the bill. In Canada, the governor general may give assent either in person at a ceremony held in the Senate or by a written declaration notifying parliament of his or her agreement to the bill. The power of parliament to pass bills was often thwarted by monarchs. Charles I dissolved parliament in 1629, after it passed motions critical of and bills seeking to restrict his arbitrary exercise of power. During the eleven years of personal rule that followed, Charles performed legally dubious actions, such as raising taxes without parliament's approval. After the English Civil War, it was accepted that parliament should be summoned to meet regularly, but it was still commonplace for monarchs to refuse royal assent to bills. In 1678, Charles II withheld his assent from a bill ""for preserving the Peace of the Kingdom by raising the Militia, and continuing them in Duty for Two and Forty Days,"" suggesting that he, not parliament, should control the militia. The last Stuart monarch, Anne, similarly withheld on 11 March 1708, on the advice of her ministers, her assent from a bill for the settling of Militia in Scotland. No monarch has since withheld royal assent on a bill passed by the British parliament. Measures, which were the means by which the National Assembly for Wales passed legislation between 2006 and 2011, were assented to by the Queen by means of an Order in Council. Section 102 of the Government of Wales Act 2006 required the Clerk to the Assembly to present measures passed by the assembly after a four-week period during which the Counsel General for Wales or the Attorney General could refer the proposed measure to the Supreme Court for a decision as to whether the measure was within the assembly's legislative competence. If the Spanish monarch ever refused in conscience to grant royal assent, a procedure similar to the Belgian handling of King Baudouin's objection would not be possible under the current constitution. If the sovereign were ever declared incapable of discharging royal authority, his or her powers would not be transferred to the Cabinet, pending the parliamentary appointment of a regency. Instead, the constitution mandates the next person of age in the line of succession would immediately become regent. Therefore, had Juan Carlos followed the Belgian example in 2005 or 2010, a declaration of incapacity would have transferred power to Felipe, then the heir apparent. The constitution of Jordan grants its monarch the right to withhold assent to laws passed by its parliament. Article 93 of that document gives the Jordanian sovereign six months to sign or veto any legislation sent to him from the National Assembly; if he vetoes it within that timeframe, the assembly may override his veto by a two-thirds vote of both houses; otherwise, the law does not go into effect (but it may be reconsidered in the next session of the assembly). If the monarch fails to act within six months of the bill being presented to him, it becomes law without his signature. After the House of Representatives has debated the law, it either approves it and sends it to the Senate with the text ""The Second Chamber of the States General sends the following approved proposal of law to the First Chamber"", or it rejects it and returns it to the government with the text ""The Second Chamber of the States General has rejected the accompanying proposal of law."" If the upper house then approves the law, it sends it back to the government with the text ""To the King, The States General have accepted the proposal of law as it is offered here."" In Belgium, the sanction royale has the same legal effect as royal assent; the Belgian constitution requires a theoretically possible refusal of royal sanction to be countersigned—as any other act of the monarch—by a minister responsible before the House of Representatives. The monarch promulgates the law, meaning that he or she formally orders that the law be officially published and executed. In 1990, when King Baudouin advised his cabinet he could not, in conscience, sign a bill decriminalising abortion (a refusal patently not covered by a responsible minister), the Council of Ministers, at the King's own request, declared Baudouin incapable of exercising his powers. In accordance with the Belgian constitution, upon the declaration of the sovereign's incapacity, the Council of Ministers assumed the powers of the head of state until parliament could rule on the King's incapacity and appoint a regent. The bill was then assented to by all members of the Council of Ministers ""on behalf of the Belgian People"". In a joint meeting, both houses of parliament declared the King capable of exercising his powers again the next day. Under the Royal Assent Act 1967, royal assent can be granted by the sovereign in writing, by means of letters patent, that are presented to the presiding officer of each house of parliament. Then, the presiding officer makes a formal, but simple statement to the house, acquainting each house that royal assent has been granted to the acts mentioned. Thus, unlike the granting of royal assent by the monarch in person or by Royal Commissioners, the method created by the Royal Assent Act 1967 does not require both houses to meet jointly for the purpose of receiving the notice of royal assent. The standard text of the letters patent is set out in The Crown Office (Forms and Proclamations Rules) Order 1992, with minor amendments in 2000. In practice this remains the standard method, a fact that is belied by the wording of the letters patent for the appointment of the Royal Commissioners and by the wording of the letters patent for the granting of royal assent in writing under the 1967 Act (""... And forasmuch as We cannot at this time be present in the Higher House of Our said Parliament being the accustomed place for giving Our Royal Assent...""). In Canada, the traditional ceremony for granting assent in parliament was regularly used until the 21st century, long after it had been discontinued in the United Kingdom and other Commonwealth realms. One result, conceived as part of a string of royal duties intended to demonstrate Canada's status as an independent kingdom, was that King George VI personally assented to nine bills of the Canadian parliament during the 1939 royal tour of Canada—85 years after his great-grandmother, Queen Victoria, had last granted royal assent personally in the United Kingdom. Under the Royal Assent Act 2002, however, the alternative practice of granting assent in writing, with each house being notified separately ( the Speaker of the Senate or a representative reads to the senators the letters from the governor general regarding the written declaration of Royal Assent), was brought into force. As the act also provides, royal assent is to be signified—by the governor general, or, more often, by a deputy, usually a Justice of the Supreme Court, at least twice each calendar year: for the first appropriation measure and for at least one other act, usually the first non-appropriation measure passed. However, the act provides that a grant of royal assent is not rendered invalid by a failure to employ the traditional ceremony where required. Royal assent is the final stage in the legislative process for acts of the Scottish parliament. The process is governed by sections 28, 32, and 33 of the Scotland Act 1998. After a bill has been passed, the Presiding Officer of the Scottish Parliament submits it to the monarch for royal assent after a four-week period, during which the Advocate General for Scotland, the Lord Advocate, the Attorney General or the Secretary of State for Scotland may refer the bill to the Supreme Court of the United Kingdom (prior to 1 October 2009, the Judicial Committee of the Privy Council) for review of its legality. Royal assent is signified by letters patent under the Great Seal of Scotland in the following form which is set out in The Scottish Parliament (Letters Patent and Proclamations) Order 1999 (SI 1999/737) and of which notice is published in the London, Edinburgh, and Belfast Gazettes: While royal assent has not been withheld in the United Kingdom since 1708, it has often been withheld in British colonies and former colonies by governors acting on royal instructions. In the United States Declaration of Independence, colonists complained that George III ""has refused his Assent to Laws, the most wholesome and necessary for the public good [and] has forbidden his Governors to pass Laws of immediate and pressing importance, unless suspended in their operation till his Assent should be obtained; and when so suspended, he has utterly neglected to attend to them."" Even after colonies such as Canada, Australia, New Zealand, the Union of South Africa, and Newfoundland were granted responsible government, the British government continued to sometimes advise governors-general on the granting of assent; assent was also occasionally reserved to allow the British government to examine a bill before advising the governor-general. Since 1993, the Sodor and Man Diocesan Synod has had power to enact measures making provision ""with respect to any matter concerning the Church of England in the Island"". If approved by Tynwald, a measure ""shall have the force and effect of an Act of Tynwald upon the Royal Assent thereto being announced to Tynwald"". Between 1979 and 1993, the Synod had similar powers, but limited to the extension to the Isle of Man of measures of the General Synod. Before 1994, royal assent was granted by Order in Council, as for a bill, but the power to grant royal assent to measures has now been delegated to the lieutenant governor. A Measure does not require promulgation. During the 1960s, the ceremony of assenting by commission was discontinued and is now only employed once a year, at the end of the annual parliamentary session. In 1960, the Gentleman Usher of the Black Rod arrived to summon the House of Commons during a heated debate and several members protested against the disruption by refusing to attend the ceremony. The debacle was repeated in 1965; this time, when the Speaker left the chair to go to the House of Lords, some members continued to make speeches. As a result, the Royal Assent Act 1967 was passed, creating an additional form for the granting of royal assent. As the attorney-general explained, ""there has been a good deal of resentment not only at the loss of Parliamentary time that has been involved but at the breaking of the thread of a possibly eloquent speech and the disruption of a debate that may be caused."" The granting of assent by the monarch in person, or by commission, is still possible, but this third form is used on a day-to-day basis. In Australia, a technical issue arose with the royal assent in both 1976 and 2001. In 1976, a bill originating in the House of Representatives was mistakenly submitted to the Governor-General and assented to. However, it was later discovered that it had not been passed by each house. The error arose because two bills of the same title had originated from the house. The Governor-General revoked the first assent, before assenting to the bill which had actually passed. The same procedure was followed to correct a similar error which arose in 2001. Since the Balfour Declaration of 1926 and the Statute of Westminster 1931, the all Commonwealth realms have been sovereign kingdoms, the monarch and governors-general acting solely on the advice of the local ministers who generally maintain the support of the legislature and are the ones who secure the passage of bills. They, therefore, are unlikely to advise the sovereign or his or her representative to withhold assent. The power to withhold the royal assent was exercised by Alberta's lieutenant governor, John C. Bowen, in 1937, in respect of three bills passed in the legislature dominated by William Aberhart's Social Credit party. Two bills sought to put banks under the authority of the province, thereby interfering with the federal government's powers. The third, the Accurate News and Information Bill, purported to force newspapers to print government rebuttals to stories to which the provincial cabinet objected. The unconstitutionality of all three bills was later confirmed by the Supreme Court of Canada and by the Judicial Committee of the Privy Council. Independently of the method used to signify royal assent, it is the responsibility of the Clerk of the Parliaments, once the assent has been duly notified to both houses, not only to endorse the act in the name of the monarch with the formal Norman French formula, but to certify that assent has been granted. The clerk signs one authentic copy of the bill and inserts the date (in English) on which the assent was notified to the two houses after the title of the act. When an act is published, the signature of the clerk is omitted, as is the Norman French formula, should the endorsement have been made in writing. However, the date on which the assent was notified is printed in brackets. Articles 41 and 68 of the constitution empower the sovereign to withhold royal assent from bills adopted by the Legislative Assembly. In 2010, the kingdom moved towards greater democracy, with King George Tupou V saying that he would be guided by his prime minister in the exercising of his powers. Nonetheless, this does not preclude an independent royal decision to exercise a right of veto. In November 2011, the assembly adopted an Arms and Ammunitions (Amendment) Bill, which reduced the possible criminal sentences for the illicit possession of firearms. The bill was adopted by ten votes to eight. Two members of the assembly had recently been charged with the illicit possession of firearms. The Prime Minister, Lord Tuʻivakanō, voted in favour of the amendment. Members of the opposition denounced the bill and asked the King to veto it, which he did in December. If the Governor General of Canada is unable to give assent, it can be done by either the Deputy of the Governor General of Canada—the Chief Justice of Canada—or another justice of the Supreme Court of Canada. It is not actually necessary for the governor general to sign a bill passed by a legislature, the signature being merely an attestation. In each case, the parliament must be apprised of the granting of assent before the bill is considered to have become law. Two methods are available: the sovereign's representatives may grant assent in the presence of both houses of parliament; alternatively, each house may be notified separately, usually by the speaker of that house. However, though both houses must be notified on the same day, notice to the House of Commons while it is not in session may be given by way of publishing a special issue of the Journals of the House of Commons, whereas the Senate must be sitting and the governor general's letter read aloud by the speaker. Special procedures apply to legislation passed by Tynwald, the legislature of the Isle of Man. Before the lordship of the Island was purchased by the British Crown in 1765 (the Revestment), the assent of the Lord of Mann to a bill was signified by letter to the governor. After 1765, royal assent was at first signified by letter from the Secretary of State to the governor; but, during the British Regency, the practice began of granting royal assent by Order in Council, which continues to this day, though limited to exceptional cases since 1981. A new device for granting assent was created during the reign of King Henry VIII. In 1542, Henry sought to execute his fifth wife, Catherine Howard, whom he accused of committing adultery; the execution was to be authorised not after a trial but by a bill of attainder, to which he would have to personally assent after listening to the entire text. Henry decided that ""the repetition of so grievous a Story and the recital of so infamous a crime"" in his presence ""might reopen a Wound already closing in the Royal Bosom"". Therefore, parliament inserted a clause into the Act of Attainder, providing that assent granted by Commissioners ""is and ever was and ever shall be, as good"" as assent granted by the sovereign personally. The procedure was used only five times during the 16th century, but more often during the 17th and 18th centuries, especially when George III's health began to deteriorate. Queen Victoria became the last monarch to personally grant assent in 1854. Royal assent is not sufficient to give legal effect to an Act of Tynwald. By ancient custom, an act did not come into force until it had been promulgated at an open-air sitting of Tynwald, usually held on Tynwald Hill at St John's on St John's Day (24 June), but, since the adoption of the Gregorian calendar in 1753, on 5 July (or on the following Monday if 5 July is a Saturday or Sunday). Promulgation originally consisted of the reading of the Act in English and Manx; but, after 1865 the reading of the title of the act and a summary of each section were sufficient. This was reduced in 1895 to the titles and a memorandum of the object and purport of the act, and, since 1988, only the short title and a summary of the long title have been read. Before the reign of Henry VIII, the sovereign always granted his or her assent in person. The sovereign, wearing the Imperial State Crown, would be seated on the throne in the Lords chamber, surrounded by heralds and members of the royal court—a scene that nowadays is repeated only at the annual State Opening of Parliament. The Commons, led by their speaker, would listen from the Bar of the Lords, just outside the chamber. The Clerk of the Parliaments presented the bills awaiting assent to the monarch, save that supply bills were traditionally brought up by the speaker. The Clerk of the Crown, standing on the sovereign's right, then read aloud the titles of the bills (in earlier times, the entire text of the bills). The Clerk of the Parliaments, standing on the sovereign's left, responded by stating the appropriate Norman French formula. The Royal Assent ceremony takes place in the Senate, as the sovereign is traditionally barred from the House of Commons. On the day of the event, the Speaker of the Senate will read to the chamber a notice from the secretary to the governor general indicating when the viceroy or a deputy thereof will arrive. The Senate thereafter cannot adjourn until after the ceremony. The speaker moves to sit beside the throne, the Mace Bearer, with mace in hand, stands adjacent to him or her, and the governor general enters to take the speaker's chair. The Usher of the Black Rod is then commanded by the speaker to summon the Members of Parliament, who follow Black Rod back to the Senate, the Sergeant-at-Arms carrying the mace of the House of Commons. In the Senate, those from the commons stand behind the bar, while Black Rod proceeds to stand next to the governor general, who then nods his or her head to signify Royal Assent to the presented bills (which do not include appropriations bills). Once the list of bills is complete, the Clerk of the Senate states: ""in Her Majesty's name, His [or Her] Excellency the Governor General [or the deputy] doth assent to these bills."" If there are any appropriation bills to receive Royal Assent, the Speaker of the House of Commons will read their titles and the Senate clerk repeats them to the governor general, who nods his or her head to communicate Royal Assent. When these bills have all been assented to, the Clerk of the Senate recites ""in Her Majesty's name, His [or Her] Excellency the Governor General [or the deputy] thanks her loyal subjects, accepts their benevolence and assents to these bills. The governor general or his or her deputy then depart parliament. In Australia, the formal ceremony of granting assent in parliament has not been regularly used since the early 20th century. Now, the bill is sent to the governor-general's residence by the house in which it originated. The governor-general then signs the bill, sending messages to the President of the Senate and the Speaker of the House of Representatives, who notify their respective houses of the governor-general's action. A similar practice is followed in New Zealand, where the governor-general has not personally granted the Royal Assent in parliament since 1875. Under modern constitutional conventions, the sovereign acts on the advice of his or her ministers. Since these ministers most often maintain the support of parliament and are the ones who obtain the passage of bills, it is highly improbable that they would advise the sovereign to withhold assent. An exception is sometimes stated to be if bills are not passed in good faith, though it is difficult to make an interpretation on what this circumstance might constitute. Hence, in modern practice, royal assent is always granted; a refusal to do so would be appropriate only in an emergency requiring the use of the monarch's reserve powers. Title IV of the 1978 Spanish constitution invests the Consentimiento Real (Royal Assent) and promulgation (publication) of laws with the monarch of Spain, while Title III, The Cortes Generales, Chapter 2, Drafting of Bills, outlines the method by which bills are passed. According to Article 91, within fifteen days of passage of a bill by the Cortes Generales, the sovereign shall give his or her assent and publish the new law. Article 92 invests the monarch with the right to call for a referendum, on the advice of the president of the government (commonly referred to in English as the prime minister) and the authorisation of the cortes. Articles 77–79 of the Norwegian Constitution specifically grant the monarch of Norway the right to withhold royal assent from any bill passed by the Storting. Should the sovereign ever choose to exercise this privilege, Article 79 provides a means by which his veto may be over-ridden: ""If a Bill has been passed unaltered by two sessions of the Storting, constituted after two separate successive elections and separated from each other by at least two intervening sessions of the Storting, without a divergent Bill having been passed by any Storting in the period between the first and last adoption, and it is then submitted to the King with a petition that His Majesty shall not refuse his assent to a Bill which, after the most mature deliberation, the Storting considers to be beneficial, it shall become law even if the Royal Assent is not accorded before the Storting goes into recess."" The Clerk of the Parliaments, an official of the House of Lords, traditionally states a formula in Anglo-Norman Law French, indicating the sovereign's decision. The granting of royal assent to a supply bill is indicated with the words ""La Reyne remercie ses bons sujets, accepte leur benevolence, et ainsi le veult"", translated as ""The Queen thanks her good subjects, accepts their bounty, and wills it so."" For other public or private bills, the formula is simply ""La Reyne le veult"" (""the Queen wills it""). For personal bills, the phrase is ""Soit fait comme il est désiré"" (""let it be as it is desired""). The appropriate formula for withholding assent is the euphemistic ""La Reyne s'avisera"" (""the Queen will consider it""). When the sovereign is male, Le Roy is substituted for La Reyne."
1973_oil_crisis,"In 2004, declassified documents revealed that the U.S. was so distraught by the rise in oil prices and being challenged by under-developed countries that they briefly considered military action to forcibly seize Middle Eastern oilfields in late 1973. Although no explicit plan was mentioned, a conversation between U.S. Secretary of Defense James Schlesinger and British Ambassador to the United States Lord Cromer revealed Schlesinger had told him that ""it was no longer obvious to him that the U.S. could not use force."" British Prime Minister Edward Heath was so worried by this prospect that he ordered a British intelligence estimate of U.S. intentions, which concluded America ""might consider it could not tolerate a situation in which the U.S. and its allies were at the mercy of a small group of unreasonable countries,"" and that they would prefer a rapid operation to seize oilfields in Saudi Arabia and Kuwait, and possibly Abu Dhabi in military action was decided upon. Although the Soviet response to such an act would likely not involve force, intelligence warned ""the American occupation would need to last 10 years as the West developed alternative energy sources, and would result in the ‘total alienation’ of the Arabs and much of the rest of the Third World."" The embargo was not uniform across Europe. Of the nine members of the European Economic Community (EEC), the Netherlands faced a complete embargo, the UK and France received almost uninterrupted supplies (having refused to allow America to use their airfields and embargoed arms and supplies to both the Arabs and the Israelis), while the other six faced partial cutbacks. The UK had traditionally been an ally of Israel, and Harold Wilson's government supported the Israelis during the Six-Day War. His successor, Ted Heath, reversed this policy in 1970, calling for Israel to withdraw to its pre-1967 borders. OPEC soon lost its preeminent position, and in 1981, its production was surpassed by that of other countries. Additionally, its own member nations were divided. Saudi Arabia, trying to recover market share, increased production, pushing prices down, shrinking or eliminating profits for high-cost producers. The world price, which had peaked during the 1979 energy crisis at nearly $40 per barrel, decreased during the 1980s to less than $10 per barrel. Adjusted for inflation, oil briefly fell back to pre-1973 levels. This ""sale"" price was a windfall for oil-importing nations, both developing and developed. To help reduce consumption, in 1974 a national maximum speed limit of 55 mph (about 88 km/h) was imposed through the Emergency Highway Energy Conservation Act. Development of the Strategic Petroleum Reserve began in 1975, and in 1977 the cabinet-level Department of Energy was created, followed by the National Energy Act of 1978.[citation needed] On November 28, 1995, Bill Clinton signed the National Highway Designation Act, ending the federal 55 mph (89 km/h) speed limit, allowing states to restore their prior maximum speed limit. In 1973, Nixon named William E. Simon as the first Administrator of the Federal Energy Office, a short-term organization created to coordinate the response to the embargo. Simon allocated states the same amount of domestic oil for 1974 that each had consumed in 1972, which worked for states whose populations were not increasing. In other states, lines at gasoline stations were common. The American Automobile Association reported that in the last week of February 1974, 20% of American gasoline stations had no fuel. This contributed to the ""Oil Shock"". After 1971, OPEC was slow to readjust prices to reflect this depreciation. From 1947 to 1967, the dollar price of oil had risen by less than two percent per year. Until the oil shock, the price had also remained fairly stable versus other currencies and commodities. OPEC ministers had not developed institutional mechanisms to update prices in sync with changing market conditions, so their real incomes lagged. The substantial price increases of 1973–1974 largely returned their prices and corresponding incomes to Bretton Woods levels in terms of commodities such as gold. Despite being relatively unaffected by the embargo, the UK nonetheless faced an oil crisis of its own - a series of strikes by coal miners and railroad workers over the winter of 1973–74 became a major factor in the change of government. Heath asked the British to heat only one room in their houses over the winter. The UK, Germany, Italy, Switzerland and Norway banned flying, driving and boating on Sundays. Sweden rationed gasoline and heating oil. The Netherlands imposed prison sentences for those who used more than their ration of electricity. An increase in imported cars into North America forced General Motors, Ford and Chrysler to introduce smaller and fuel-efficient models for domestic sales. The Dodge Omni / Plymouth Horizon from Chrysler, the Ford Fiesta and the Chevrolet Chevette all had four-cylinder engines and room for at least four passengers by the late 1970s. By 1985, the average American vehicle moved 17.4 miles per gallon, compared to 13.5 in 1970. The improvements stayed even though the price of a barrel of oil remained constant at $12 from 1974 to 1979. Sales of large sedans for most makes (except Chrysler products) recovered within two model years of the 1973 crisis. The Cadillac DeVille and Fleetwood, Buick Electra, Oldsmobile 98, Lincoln Continental, Mercury Marquis, and various other luxury oriented sedans became popular again in the mid-1970s. The only full-size models that did not recover were lower price models such as the Chevrolet Bel Air, and Ford Galaxie 500. Slightly smaller, mid-size models such as the Oldsmobile Cutlass, Chevrolet Monte Carlo, Ford Thunderbird and various other models sold well. In response to American aid to Israel, on October 16, 1973, OPEC raised the posted price of oil by 70%, to $5.11 a barrel. The following day, oil ministers agreed to the embargo, a cut in production by five percent from September's output and to continue to cut production in five percent monthly increments until their economic and political objectives were met. On October 19, Nixon requested Congress to appropriate $2.2 billion in emergency aid to Israel, including $1.5 billion in outright grants. George Lenczowski notes, ""Military supplies did not exhaust Nixon's eagerness to prevent Israel's collapse...This [$2.2 billion] decision triggered a collective OPEC response."" Libya immediately announced it would embargo oil shipments to the United States. Saudi Arabia and the other Arab oil-producing states joined the embargo on October 20, 1973. At their Kuwait meeting, OAPEC proclaimed the embargo that curbed exports to various countries and blocked all oil deliveries to the US as a ""principal hostile country"". The USSR's invasion of Afghanistan was only one sign of insecurity in the region, also marked by increased American weapons sales, technology, and outright military presence. Saudi Arabia and Iran became increasingly dependent on American security assurances to manage both external and internal threats, including increased military competition between them over increased oil revenues. Both states were competing for preeminence in the Persian Gulf and using increased revenues to fund expanded militaries. By 1979, Saudi arms purchases from the US exceeded five times Israel's. Another motive for the large scale purchase of arms from the US by Saudi Arabia was the failure of the Shah during January 1979 to maintain control of Iran, a non-Arabic but largely Shiite Muslim nation, which fell to a theocratic Islamist government under the Ayatollah Ruhollah Khomeini in the wake of the 1979 Iranian Revolution. Saudi Arabia, on the other hand, is an Arab, largely Sunni Muslim nation headed by a near absolutist monarchy. In the wake of the Iranian revolution the Saudis were forced to deal with the prospect of internal destabilization via the radicalism of Islamism, a reality which would quickly be revealed in the seizure of the Grand Mosque in Mecca by Wahhabi extremists during November 1979 and a Shiite revolt in the oil rich Al-Hasa region of Saudi Arabia in December of the same year. In November 2010, Wikileaks leaked confidential diplomatic cables pertaining to the United States and its allies which revealed that the late Saudi King Abdullah urged the United States to attack Iran in order to destroy its potential nuclear weapons program, describing Iran as ""a snake whose head should be cut off without any procrastination."" Although lacking historical connections to the Middle East, Japan was the country most dependent on Arab oil. 71% of its imported oil came from the Middle East in 1970. On November 7, 1973, the Saudi and Kuwaiti governments declared Japan a ""nonfriendly"" country to encourage it to change its noninvolvement policy. It received a 5% production cut in December, causing a panic. On November 22, Japan issued a statement ""asserting that Israel should withdraw from all of the 1967 territories, advocating Palestinian self-determination, and threatening to reconsider its policy toward Israel if Israel refused to accept these preconditions"". By December 25, Japan was considered an Arab-friendly state. Federal safety standards, such as NHTSA Federal Motor Vehicle Safety Standard 215 (pertaining to safety bumpers), and compacts like the 1974 Mustang I were a prelude to the DOT ""downsize"" revision of vehicle categories. By 1977, GM's full-sized cars reflected the crisis. By 1979, virtually all ""full-size"" American cars had shrunk, featuring smaller engines and smaller outside dimensions. Chrysler ended production of their full-sized luxury sedans at the end of the 1981 model year, moving instead to a full front-wheel drive lineup for 1982 (except for the M-body Dodge Diplomat/Plymouth Gran Fury and Chrysler New Yorker Fifth Avenue sedans). The crisis reduced the demand for large cars. Japanese imports, primarily the Toyota Corona, the Toyota Corolla, the Datsun B210, the Datsun 510, the Honda Civic, the Mitsubishi Galant (a captive import from Chrysler sold as the Dodge Colt), the Subaru DL, and later the Honda Accord all had four cylinder engines that were more fuel efficient than the typical American V8 and six cylinder engines. Japanese imports became mass-market leaders with unibody construction and front-wheel drive, which became de facto standards. The energy crisis led to greater interest in renewable energy, nuclear power and domestic fossil fuels. There is criticism that American energy policies since the crisis have been dominated by crisis-mentality thinking, promoting expensive quick fixes and single-shot solutions that ignore market and technology realities. Instead of providing stable rules that support basic research while leaving plenty of scope for entrepreneurship and innovation, congresses and presidents have repeatedly backed policies which promise solutions that are politically expedient, but whose prospects are doubtful. Compact trucks were introduced, such as the Toyota Hilux and the Datsun Truck, followed by the Mazda Truck (sold as the Ford Courier), and the Isuzu-built Chevrolet LUV. Mitsubishi rebranded its Forte as the Dodge D-50 a few years after the oil crisis. Mazda, Mitsubishi and Isuzu had joint partnerships with Ford, Chrysler, and GM, respectively. Later the American makers introduced their domestic replacements (Ford Ranger, Dodge Dakota and the Chevrolet S10/GMC S-15), ending their captive import policy. On October 6, 1973, Syria and Egypt, with support from other Arab nations, launched a surprise attack on Israel, on Yom Kippur. This renewal of hostilities in the Arab–Israeli conflict released the underlying economic pressure on oil prices. At the time, Iran was the world's second-largest oil exporter and a close US ally. Weeks later, the Shah of Iran said in an interview: ""Of course [the price of oil] is going to rise... Certainly! And how!... You've [Western nations] increased the price of the wheat you sell us by 300 percent, and the same for sugar and cement... You buy our crude oil and sell it back to us, refined as petrochemicals, at a hundred times the price you've paid us... It's only fair that, from now on, you should pay more for oil. Let's say ten times more."" The 1973 oil crisis began in October 1973 when the members of the Organization of Arab Petroleum Exporting Countries (OAPEC, consisting of the Arab members of OPEC plus Egypt and Syria) proclaimed an oil embargo. By the end of the embargo in March 1974, the price of oil had risen from US$3 per barrel to nearly $12 globally; US prices were significantly higher. The embargo caused an oil crisis, or ""shock"", with many short- and long-term effects on global politics and the global economy. It was later called the ""first oil shock"", followed by the 1979 oil crisis, termed the ""second oil shock."" The crisis had a major impact on international relations and created a rift within NATO. Some European nations and Japan sought to disassociate themselves from United States foreign policy in the Middle East to avoid being targeted by the boycott. Arab oil producers linked any future policy changes to peace between the belligerents. To address this, the Nixon Administration began multilateral negotiations with the combatants. They arranged for Israel to pull back from the Sinai Peninsula and the Golan Heights. By January 18, 1974, US Secretary of State Henry Kissinger had negotiated an Israeli troop withdrawal from parts of the Sinai Peninsula. The promise of a negotiated settlement between Israel and Syria was enough to convince Arab oil producers to lift the embargo in March 1974. Price controls exacerbated the crisis in the US. The system limited the price of ""old oil"" (that which had already been discovered) while allowing newly discovered oil to be sold at a higher price to encourage investment. Predictably, old oil was withdrawn from the market, creating greater scarcity. The rule also discouraged development of alternative energies. The rule had been intended to promote oil exploration. Scarcity was addressed by rationing (as in many countries). Motorists faced long lines at gas stations beginning in summer 1972 and increasing by summer 1973. On August 15, 1971, the United States unilaterally pulled out of the Bretton Woods Accord. The US abandoned the Gold Exchange Standard whereby the value of the dollar had been pegged to the price of gold and all other currencies were pegged to the dollar, whose value was left to ""float"" (rise and fall according to market demand). Shortly thereafter, Britain followed, floating the pound sterling. The other industrialized nations followed suit with their respective currencies. Anticipating that currency values would fluctuate unpredictably for a time, the industrialized nations increased their reserves (by expanding their money supplies) in amounts far greater than before. The result was a depreciation of the dollar and other industrialized nations' currencies. Because oil was priced in dollars, oil producers' real income decreased. In September 1971, OPEC issued a joint communiqué stating that, from then on, they would price oil in terms of a fixed amount of gold. Some buyers lamented the small size of the first Japanese compacts, and both Toyota and Nissan (then known as Datsun) introduced larger cars such as the Toyota Corona Mark II, the Toyota Cressida, the Mazda 616 and Datsun 810, which added passenger space and amenities such as air conditioning, power steering, AM-FM radios, and even power windows and central locking without increasing the price of the vehicle. A decade after the 1973 oil crisis, Honda, Toyota and Nissan, affected by the 1981 voluntary export restraints, opened US assembly plants and established their luxury divisions (Acura, Lexus and Infiniti, respectively) to distinguish themselves from their mass-market brands. Some of the income was dispensed in the form of aid to other underdeveloped nations whose economies had been caught between higher oil prices and lower prices for their own export commodities, amid shrinking Western demand. Much went for arms purchases that exacerbated political tensions, particularly in the Middle East. Saudi Arabia spent over 100 billion dollars in the ensuing decades for helping spread its fundamentalist interpretation of Islam, known as Wahhabism, throughout the world, via religious charities such al-Haramain Foundation, which often also distributed funds to violent Sunni extremist groups such as Al-Qaeda and the Taliban. In the United States, scholars argue that there already existed a negotiated settlement based on equality between both parties prior to 1973. The possibility that the Middle East could become another superpower confrontation with the USSR was of more concern to the US than oil. Further, interest groups and government agencies more worried about energy were no match for Kissinger's dominance. In the US production, distribution and price disruptions ""have been held responsible for recessions, periods of excessive inflation, reduced productivity, and lower economic growth."" The embargo had a negative influence on the US economy by causing immediate demands to address the threats to U.S. energy security. On an international level, the price increases changed competitive positions in many industries, such as automobiles. Macroeconomic problems consisted of both inflationary and deflationary impacts. The embargo left oil companies searching for new ways to increase oil supplies, even in rugged terrain such as the Arctic. Finding oil and developing new fields usually required five to ten years before significant production."
Premier_League,"The Premier League sends representatives to UEFA's European Club Association, the number of clubs and the clubs themselves chosen according to UEFA coefficients. For the 2012–13 season the Premier League has 10 representatives in the Association: Arsenal, Aston Villa, Chelsea, Everton, Fulham, Liverpool, Manchester City, Manchester United, Newcastle United and Tottenham Hotspur. The European Club Association is responsible for electing three members to UEFA's Club Competitions Committee, which is involved in the operations of UEFA competitions such as the Champions League and UEFA Europa League. One significant feature of the Premier League in the mid-2000s was the dominance of the so-called ""Big Four"" clubs: Arsenal, Chelsea, Liverpool and Manchester United. During this decade, and particularly from 2002 to 2009, they dominated the top four spots, which came with UEFA Champions League qualification, taking all top four places in 5 out of 6 seasons from 2003–04 to 2008–09 inclusive, with Arsenal going as far as winning the league without losing a single game in 2003–04, the only time it has ever happened in the Premier League. In May 2008 Kevin Keegan stated that ""Big Four"" dominance threatened the division, ""This league is in danger of becoming one of the most boring but great leagues in the world."" Premier League chief executive Richard Scudamore said in defence: ""There are a lot of different tussles that go on in the Premier League depending on whether you're at the top, in the middle or at the bottom that make it interesting."" The TV rights agreement between the Premier League and Sky has faced accusations of being a cartel, and a number of court cases have arisen as a result. An investigation by the Office of Fair Trading in 2002 found BSkyB to be dominant within the pay TV sports market, but concluded that there were insufficient grounds for the claim that BSkyB had abused its dominant position. In July 1999 the Premier League's method of selling rights collectively for all member clubs was investigated by the UK Restrictive Practices Court, who concluded that the agreement was not contrary to the public interest. Due to insistence by the International Federation of Association Football (FIFA), the international governing body of football, that domestic leagues reduce the number of games clubs played, the number of clubs was reduced to 20 in 1995 when four teams were relegated from the league and only two teams promoted. On 8 June 2006, FIFA requested that all major European leagues, including Italy's Serie A and Spain's La Liga be reduced to 18 teams by the start of the 2007–08 season. The Premier League responded by announcing their intention to resist such a reduction. Ultimately, the 2007–08 season kicked off again with 20 teams. Participation in the Premier League by some Scottish or Irish clubs has sometimes been discussed, but without result. The idea came closest to reality in 1998, when Wimbledon received Premier League approval to relocate to Dublin, Ireland, but the move was blocked by the Football Association of Ireland. Additionally, the media occasionally discusses the idea that Scotland's two biggest teams, Celtic and Rangers, should or will take part in the Premier League, but nothing has come of these discussions. Despite significant European success during the 1970s and early 1980s, the late '80s had marked a low point for English football. Stadiums were crumbling, supporters endured poor facilities, hooliganism was rife, and English clubs were banned from European competition for five years following the Heysel Stadium disaster in 1985. The Football League First Division, which had been the top level of English football since 1888, was well behind leagues such as Italy's Serie A and Spain's La Liga in attendances and revenues, and several top English players had moved abroad. As of the 2015–16 season, Premier League football has been played in 53 stadiums since the formation of the Premier League in 1992. The Hillsborough disaster in 1989 and the subsequent Taylor Report saw a recommendation that standing terraces should be abolished; as a result all stadiums in the Premier League are all-seater. Since the formation of the Premier League, football grounds in England have seen constant improvements to capacity and facilities, with some clubs moving to new-build stadiums. Nine stadiums that have seen Premier League football have now been demolished. The stadiums for the 2010–11 season show a large disparity in capacity: Old Trafford, the home of Manchester United has a capacity of 75,957 with Bloomfield Road, the home of Blackpool, having a capacity of 16,220. The combined total capacity of the Premier League in the 2010–11 season is 770,477 with an average capacity of 38,523. There are 20 clubs in the Premier League. During the course of a season (from August to May) each club plays the others twice (a double round-robin system), once at their home stadium and once at that of their opponents, for a total of 38 games. Teams receive three points for a win and one point for a draw. No points are awarded for a loss. Teams are ranked by total points, then goal difference, and then goals scored. If still equal, teams are deemed to occupy the same position. If there is a tie for the championship, for relegation, or for qualification to other competitions, a play-off match at a neutral venue decides rank. The three lowest placed teams are relegated into the Football League Championship, and the top two teams from the Championship, together with the winner of play-offs involving the third to sixth placed Championship clubs, are promoted in their place. Managers in the Premier League are involved in the day-to-day running of the team, including the training, team selection, and player acquisition. Their influence varies from club-to-club and is related to the ownership of the club and the relationship of the manager with fans. Managers are required to have a UEFA Pro Licence which is the final coaching qualification available, and follows the completion of the UEFA 'B' and 'A' Licences. The UEFA Pro Licence is required by every person who wishes to manage a club in the Premier League on a permanent basis (i.e. more than 12 weeks – the amount of time an unqualified caretaker manager is allowed to take control). Caretaker appointments are managers that fill the gap between a managerial departure and a new appointment. Several caretaker managers have gone on to secure a permanent managerial post after performing well as a caretaker; examples include Paul Hart at Portsmouth and David Pleat at Tottenham Hotspur. Its main body is solid sterling silver and silver gilt, while its plinth is made of malachite, a semi-precious stone. The plinth has a silver band around its circumference, upon which the names of the title-winning clubs are listed. Malachite's green colour is also representative of the green field of play. The design of the trophy is based on the heraldry of Three Lions that is associated with English football. Two of the lions are found above the handles on either side of the trophy – the third is symbolised by the captain of the title winning team as he raises the trophy, and its gold crown, above his head at the end of the season. The ribbons that drape the handles are presented in the team colours of the league champions that year. However, by the turn of the 1990s the downward trend was starting to reverse; England had been successful in the 1990 FIFA World Cup, reaching the semi-finals. UEFA, European football's governing body, lifted the five-year ban on English clubs playing in European competitions in 1990 (resulting in Manchester United lifting the UEFA Cup Winners' Cup in 1991) and the Taylor Report on stadium safety standards, which proposed expensive upgrades to create all-seater stadiums in the aftermath of the Hillsborough disaster, was published in January of that year. At the close of the 1991 season, a proposal was tabled for the establishment of a new league that would bring more money into the game overall. The Founder Members Agreement, signed on 17 July 1991 by the game's top-flight clubs, established the basic principles for setting up the FA Premier League. The newly formed top division would have commercial independence from The Football Association and the Football League, giving the FA Premier League licence to negotiate its own broadcast and sponsorship agreements. The argument given at the time was that the extra income would allow English clubs to compete with teams across Europe. The Premier League has the highest revenue of any football league in the world, with total club revenues of €2.48 billion in 2009–10. In 2013–14, due to improved television revenues and cost controls, the Premier League had net profits in excess of £78 million, exceeding all other football leagues. In 2010 the Premier League was awarded the Queen's Award for Enterprise in the International Trade category for its outstanding contribution to international trade and the value it brings to English football and the United Kingdom's broadcasting industry. The first Sky television rights agreement was worth £304 million over five seasons. The next contract, negotiated to start from the 1997–98 season, rose to £670 million over four seasons. The third contract was a £1.024 billion deal with BSkyB for the three seasons from 2001–02 to 2003–04. The league brought in £320 million from the sale of its international rights for the three-year period from 2004–05 to 2006–07. It sold the rights itself on a territory-by-territory basis. Sky's monopoly was broken from August 2006 when Setanta Sports was awarded rights to show two out of the six packages of matches available. This occurred following an insistence by the European Commission that exclusive rights should not be sold to one television company. Sky and Setanta paid a total of £1.7 billion, a two-thirds increase which took many commentators by surprise as it had been widely assumed that the value of the rights had levelled off following many years of rapid growth. Setanta also hold rights to a live 3 pm match solely for Irish viewers. The BBC has retained the rights to show highlights for the same three seasons (on Match of the Day) for £171.6 million, a 63 per cent increase on the £105 million it paid for the previous three-year period. Sky and BT have agreed to jointly pay £84.3 million for delayed television rights to 242 games (that is the right to broadcast them in full on television and over the internet) in most cases for a period of 50 hours after 10 pm on matchday. Overseas television rights fetched £625 million, nearly double the previous contract. The total raised from these deals is more than £2.7 billion, giving Premier League clubs an average media income from league games of around £40 million-a-year from 2007 to 2010. Stadium attendances are a significant source of regular income for Premier League clubs. For the 2009–10 season, average attendances across the league clubs were 34,215 for Premier League matches with a total aggregate attendance figure of 13,001,616. This represents an increase of 13,089 from the average attendance of 21,126 recorded in the league's first season (1992–93). However, during the 1992–93 season the capacities of most stadiums were reduced as clubs replaced terraces with seats in order to meet the Taylor Report's 1994–95 deadline for all-seater stadiums. The Premier League's record average attendance of 36,144 was set during the 2007–08 season. This record was then beaten in the 2013–14 season recording an average attendance of 36,695 with a total attendance of just under 14 million, the highest average in England's top flight since 1950. The Golden Boot is awarded to the top Premier League scorer at the end of each season. Former Blackburn Rovers and Newcastle United striker Alan Shearer holds the record for most Premier League goals with 260. Twenty-four players have reached the 100-goal mark. Since the first Premier League season in 1992–93, 14 different players from 10 different clubs have won or shared the top scorers title. Thierry Henry won his fourth overall scoring title by scoring 27 goals in the 2005–06 season. Andrew Cole and Alan Shearer hold the record for most goals in a season (34) – for Newcastle and Blackburn respectively. Ryan Giggs of Manchester United holds the record for scoring goals in consecutive seasons, having scored in the first 21 seasons of the league. Players may only be transferred during transfer windows that are set by the Football Association. The two transfer windows run from the last day of the season to 31 August and from 31 December to 31 January. Player registrations cannot be exchanged outside these windows except under specific licence from the FA, usually on an emergency basis. As of the 2010–11 season, the Premier League introduced new rules mandating that each club must register a maximum 25-man squad of players aged over 21, with the squad list only allowed to be changed in transfer windows or in exceptional circumstances. This was to enable the 'home grown' rule to be enacted, whereby the League would also from 2010 require at least 8 of the named 25 man squad to be made up of 'home-grown players'. The years following 2009 marked a shift in the structure of the ""Big Four"" with Tottenham Hotspur and Manchester City both breaking into the top four. In the 2009–10 season, Tottenham finished fourth and became the first team to break the top four since Everton in 2005. Criticism of the gap between an elite group of ""super clubs"" and the majority of the Premier League has continued, nevertheless, due to their increasing ability to spend more than the other Premier League clubs. Manchester City won the title in the 2011–12 season, becoming the first club outside the ""Big Four"" to win since 1994–95. That season also saw two of the Big Four (Chelsea and Liverpool) finish outside the top four places for the first time since 1994–95. In response to concerns that clubs were increasingly passing over young English players in favour of foreign players, in 1999, the Home Office tightened its rules for granting work permits to players from countries outside of the European Union. A non-EU player applying for the permit must have played for his country in at least 75 per cent of its competitive 'A' team matches for which he was available for selection during the previous two years, and his country must have averaged at least 70th place in the official FIFA world rankings over the previous two years. If a player does not meet those criteria, the club wishing to sign him may appeal. Television has played a major role in the history of the Premier League. The League's decision to assign broadcasting rights to BSkyB in 1992 was at the time a radical decision, but one that has paid off. At the time pay television was an almost untested proposition in the UK market, as was charging fans to watch live televised football. However, a combination of Sky's strategy, the quality of Premier League football and the public's appetite for the game has seen the value of the Premier League's TV rights soar. At the inception of the Premier League in 1992–93, just eleven players named in the starting line-ups for the first round of matches hailed from outside of the United Kingdom or Ireland. By 2000–01, the number of foreign players participating in the Premier League was 36 per cent of the total. In the 2004–05 season the figure had increased to 45 per cent. On 26 December 1999, Chelsea became the first Premier League side to field an entirely foreign starting line-up, and on 14 February 2005 Arsenal were the first to name a completely foreign 16-man squad for a match. By 2009, under 40% of the players in the Premier League were English. The competition formed as the FA Premier League on 20 February 1992 following the decision of clubs in the Football League First Division to break away from the Football League, which was originally founded in 1888, and take advantage of a lucrative television rights deal. The deal was worth £1 billion a year domestically as of 2013–14, with BSkyB and BT Group securing the domestic rights to broadcast 116 and 38 games respectively. The league generates €2.2 billion per year in domestic and international television rights. In 2014/15, teams were apportioned revenues of £1.6 billion. The Premier League is the most-watched football league in the world, broadcast in 212 territories to 643 million homes and a potential TV audience of 4.7 billion people. In the 2014–15 season, the average Premier League match attendance exceeded 36,000, second highest of any professional football league behind the Bundesliga's 43,500. Most stadium occupancies are near capacity. The Premier League rank second in the UEFA coefficients of leagues based on performances in European competitions over the past five seasons. The record transfer fee for a Premier League player has risen steadily over the lifetime of the competition. Prior to the start of the first Premier League season Alan Shearer became the first British player to command a transfer fee of more than £3 million. The record rose steadily in the Premier League's first few seasons, until Alan Shearer made a record breaking £15 million move to Newcastle United in 1996. The three highest transfer in the sport's history had a Premier League club on the selling end, with Tottenham Hotspur selling Gareth Bale to Real Madrid for £85 million in 2013, Manchester United's sale of Cristiano Ronaldo to Real Madrid for £80 million in 2009, and Liverpool selling Luis Suárez to Barcelona for £75 million in 2014. The Football Association Premier League Ltd (FAPL) is operated as a corporation and is owned by the 20 member clubs. Each club is a shareholder, with one vote each on issues such as rule changes and contracts. The clubs elect a chairman, chief executive, and board of directors to oversee the daily operations of the league. The current chairman is Sir Dave Richards, who was appointed in April 1999, and the chief executive is Richard Scudamore, appointed in November 1999. The former chairman and chief executive, John Quinton and Peter Leaver, were forced to resign in March 1999 after awarding consultancy contracts to former Sky executives Sam Chisholm and David Chance. The Football Association is not directly involved in the day-to-day operations of the Premier League, but has veto power as a special shareholder during the election of the chairman and chief executive and when new rules are adopted by the league. There has been an increasing gulf between the Premier League and the Football League. Since its split with the Football League, many established clubs in the Premier League have managed to distance themselves from their counterparts in lower leagues. Owing in large part to the disparity in revenue from television rights between the leagues, many newly promoted teams have found it difficult to avoid relegation in their first season in the Premier League. In every season except 2001–02 and 2011–12, at least one Premier League newcomer has been relegated back to the Football League. In 1997–98 all three promoted clubs were relegated at the end of the season. The team placed fifth in the Premier League automatically qualifies for the UEFA Europa League, and the sixth and seventh-placed teams can also qualify, depending on the winners of the two domestic cup competitions i.e. the FA Cup and the Capital One Cup (League Cup). Two Europa League places are reserved for the winners of each tournament; if the winner of either the FA Cup or League Cup qualifies for the Champions League, then that place will go to the next-best placed finisher in the Premier League. A further place in the UEFA Europa League is also available via the Fair Play initiative. If the Premier League has one of the three highest Fair Play rankings in Europe, the highest ranked team in the Premier League Fair Play standings which has not already qualified for Europe will automatically qualify for the UEFA Europa League first qualifying round. An exception to the usual European qualification system happened in 2005, after Liverpool won the Champions League the year before, but did not finish in a Champions League qualification place in the Premier League that season. UEFA gave special dispensation for Liverpool to enter the Champions League, giving England five qualifiers. UEFA subsequently ruled that the defending champions qualify for the competition the following year regardless of their domestic league placing. However, for those leagues with four entrants in the Champions League, this meant that if the Champions League winner finished outside the top four in its domestic league, it would qualify at the expense of the fourth-placed team in the league. No association can have more than four entrants in the Champions League. This occurred in 2012, when Chelsea – who had won the Champions League the previous year, but finished sixth in the league – qualified for the Champions League in place of Tottenham Hotspur, who went into the Europa League. Television money had also become much more important; the Football League received £6.3 million for a two-year agreement in 1986, but when that deal was renewed in 1988, the price rose to £44 million over four years. The 1988 negotiations were the first signs of a breakaway league; ten clubs threatened to leave and form a ""super league"", but were eventually persuaded to stay. As stadiums improved and match attendance and revenues rose, the country's top teams again considered leaving the Football League in order to capitalise on the growing influx of money being pumped into the sport. Between the 1992–93 season and the 2012–13 season, Premier League clubs had won the UEFA Champions League four times (as well as supplying five of the runners-up), behind Spain's La Liga with six wins, and Italy's Serie A with five wins, and ahead of, among others, Germany's Bundesliga with three wins (see table here). The FIFA Club World Cup (or the FIFA Club World Championship, as it was originally called) has been won by Premier league clubs once (Manchester United in 2008), and they have also been runners-up twice, behind Brazil's Brasileirão with four wins, and Spain's La Liga and Italy's Serie A with two wins each (see table here). The league held its first season in 1992–93 and was originally composed of 22 clubs. The first ever Premier League goal was scored by Brian Deane of Sheffield United in a 2–1 win against Manchester United. The 22 inaugural members of the new Premier League were Arsenal, Aston Villa, Blackburn Rovers, Chelsea, Coventry City, Crystal Palace, Everton, Ipswich Town, Leeds United, Liverpool, Manchester City, Manchester United, Middlesbrough, Norwich City, Nottingham Forest, Oldham Athletic, Queens Park Rangers, Sheffield United, Sheffield Wednesday, Southampton, Tottenham Hotspur, and Wimbledon. Luton Town, Notts County and West Ham United were the three teams relegated from the old first division at the end of the 1991–92 season, and did not take part in the inaugural Premier League season. In 2011, a Welsh club participated in the Premier League for the first time after Swansea City gained promotion. The first Premier League match to be played outside England was Swansea City's home match at the Liberty Stadium against Wigan Athletic on 20 August 2011. In 2012–13, Swansea qualified for the Europa League by winning the League Cup. The number of Welsh clubs in the Premier League increased to two for the first time in 2013–14, as Cardiff City gained promotion, but Cardiff City was relegated after its maiden season. The Premier League is a corporation in which the 20 member clubs act as shareholders. Seasons run from August to May. Teams play 38 matches each (playing each team in the league twice, home and away), totalling 380 matches in the season. Most games are played on Saturday and Sunday afternoons; others during weekday evenings. It is currently sponsored by Barclays Bank and thus officially known as the Barclays Premier League and is colloquially known as the Premiership. Outside the UK it is commonly referred to as the English Premier League (EPL). The BBC's highlights package on Saturday and Sunday nights, as well as other evenings when fixtures justify, will run until 2016. Television rights alone for the period 2010 to 2013 have been purchased for £1.782 billion. On 22 June 2009, due to troubles encountered by Setanta Sports after it failed to meet a final deadline over a £30 million payment to the Premier League, ESPN was awarded two packages of UK rights containing a total of 46 matches that were available for the 2009–10 season as well as a package of 23 matches per season from 2010–11 to 2012–13. On 13 June 2012, the Premier League announced that BT had been awarded 38 games a season for the 2013–14 through 2015–16 seasons at £246 million-a-year. The remaining 116 games were retained by Sky who paid £760 million-a-year. The total domestic rights have raised £3.018 billion, an increase of 70.2% over the 2010–11 to 2012–13 rights. The value of the licensing deal rose by another 70.2% in 2015, when Sky and BT paid a total of £5.136 billion to renew their contracts with the Premier League for another three years up to the 2018–19 season. The Premier League sells its television rights on a collective basis. This is in contrast to some other European Leagues, including La Liga, in which each club sells its rights individually, leading to a much higher share of the total income going to the top few clubs. The money is divided into three parts: half is divided equally between the clubs; one quarter is awarded on a merit basis based on final league position, the top club getting twenty times as much as the bottom club, and equal steps all the way down the table; the final quarter is paid out as facilities fees for games that are shown on television, with the top clubs generally receiving the largest shares of this. The income from overseas rights is divided equally between the twenty clubs. The Premier League is broadcast in the United States through NBC Sports. Premier League viewership has increased rapidly, with NBC and NBCSN averaging a record 479,000 viewers in the 2014–15 season, up 118% from 2012–13 when coverage still aired on Fox Soccer and ESPN/ESPN2 (220,000 viewers), and NBC Sports has been widely praised for its coverage. NBC Sports reached a six-year extension with the Premier League in 2015 to broadcast the league through the 2021–22 season in a deal valued at $1 billion (£640 million). In 1992, the First Division clubs resigned from the Football League en masse and on 27 May 1992 the FA Premier League was formed as a limited company working out of an office at the Football Association's then headquarters in Lancaster Gate. This meant a break-up of the 104-year-old Football League that had operated until then with four divisions; the Premier League would operate with a single division and the Football League with three. There was no change in competition format; the same number of teams competed in the top flight, and promotion and relegation between the Premier League and the new First Division remained the same as the old First and Second Divisions with three teams relegated from the league and three promoted. The Premier League is particularly popular in Asia, where it is the most widely distributed sports programme. In Australia, Fox Sports broadcasts almost all of the season's 380 matches live, and Foxtel gives subscribers the option of selecting which Saturday 3pm match to watch. In India, the matches are broadcast live on STAR Sports. In China, the broadcast rights were awarded to Super Sports in a six-year agreement that began in the 2013–14 season. As of the 2013–14 season, Canadian broadcast rights to the Premier League are jointly owned by Sportsnet and TSN, with both rival networks holding rights to 190 matches per season. The managing director of London Weekend Television (LWT), Greg Dyke, met with the representatives of the ""big five"" football clubs in England in 1990. The meeting was to pave the way for a break away from The Football League. Dyke believed that it would be more lucrative for LWT if only the larger clubs in the country were featured on national television and wanted to establish whether the clubs would be interested in a larger share of television rights money. The five clubs decided it was a good idea and decided to press ahead with it; however, the league would have no credibility without the backing of The Football Association and so David Dein of Arsenal held talks to see whether the FA were receptive to the idea. The FA did not enjoy an amicable relationship with the Football League at the time and considered it as a way to weaken the Football League's position. The Premier League distributes a portion of its television revenue to clubs that are relegated from the league in the form of ""parachute payments"". Starting with the 2013–14 season, these payments are in excess of £60 million over four seasons. Though designed to help teams adjust to the loss of television revenues (the average Premier League team receives £55 million while the average Football League Championship club receives £2 million), critics maintain that the payments actually widen the gap between teams that have reached the Premier League and those that have not, leading to the common occurrence of teams ""bouncing back"" soon after their relegation. For some clubs who have failed to win immediate promotion back to the Premier League, financial problems, including in some cases administration or even liquidation have followed. Further relegations down the footballing ladder have ensued for several clubs unable to cope with the gap."
Airport,"On runways, green lights indicate the beginning of the runway for landing, while red lights indicate the end of the runway. Runway edge lighting consists of white lights spaced out on both sides of the runway, indicating the edge. Some airports have more complicated lighting on the runways including lights that run down the centerline of the runway and lights that help indicate the approach (an approach lighting system, or ALS). Low-traffic airports may use pilot controlled lighting to save electricity and staffing costs. Airports may also contain premium and VIP services. The premium and VIP services may include express check-in and dedicated check-in counters. These services are usually reserved for First and Business class passengers, premium frequent flyers, and members of the airline's clubs. Premium services may sometimes be open to passengers who are members of a different airline's frequent flyer program. This can sometimes be part of a reciprocal deal, as when multiple airlines are part of the same alliance, or as a ploy to attract premium customers away from rival airlines. Most major airports provide commercial outlets for products and services. Most of these companies, many of which are internationally known brands, are located within the departure areas. These include clothing boutiques and restaurants. Prices charged for items sold at these outlets are generally higher than those outside the airport. However, some airports now regulate costs to keep them comparable to ""street prices"". This term is misleading as prices often match the manufacturers' suggested retail price (MSRP) but are almost never discounted.[citation needed] The first lighting used on an airport was during the latter part of the 1920s; in the 1930s approach lighting came into use. These indicated the proper direction and angle of descent. The colours and flash intervals of these lights became standardized under the International Civil Aviation Organization (ICAO). In the 1940s, the slope-line approach system was introduced. This consisted of two rows of lights that formed a funnel indicating an aircraft's position on the glideslope. Additional lights indicated incorrect altitude and direction. The distances passengers need to move within a large airport can be substantial. It is common for airports to provide moving walkways and buses. The Hartsfield–Jackson Atlanta International Airport has a tram that takes people through the concourses and baggage claim. Major airports with more than one terminal offer inter-terminal transportation, such as Mexico City International Airport, where the domestic building of Terminal 1 is connected by Aerotrén to Terminal 2, on the other side of the airport. Most of the world's airports are owned by local, regional, or national government bodies who then lease the airport to private corporations who oversee the airport's operation. For example, in the United Kingdom the state-owned British Airports Authority originally operated eight of the nation's major commercial airports - it was subsequently privatized in the late 1980s, and following its takeover by the Spanish Ferrovial consortium in 2006, has been further divested and downsized to operating just five. Germany's Frankfurt Airport is managed by the quasi-private firm Fraport. While in India GMR Group operates, through joint ventures, Indira Gandhi International Airport and Rajiv Gandhi International Airport. Bengaluru International Airport and Chhatrapati Shivaji International Airport are controlled by GVK Group. The rest of India's airports are managed by the Airports Authority of India. At extremely large airports, a circuit is in place but not usually used. Rather, aircraft (usually only commercial with long routes) request approach clearance while they are still hours away from the airport, often before they even take off from their departure point. Large airports have a frequency called Clearance Delivery which is used by departing aircraft specifically for this purpose. This then allows aircraft to take the most direct approach path to the runway and land without worrying about interference from other aircraft. While this system keeps the airspace free and is simpler for pilots, it requires detailed knowledge of how aircraft are planning to use the airport ahead of time and is therefore only possible with large commercial airliners on pre-scheduled flights. The system has recently become so advanced that controllers can predict whether an aircraft will be delayed on landing before it even takes off; that aircraft can then be delayed on the ground, rather than wasting expensive fuel waiting in the air. Most airports welcome filming on site, although it must be agreed in advance and may be subject to a fee. Landside, filming can take place in all public areas. However airside, filming is heavily restricted, the only airside locations where filming is permitted are the Departure Lounge and some outside areas. To film in an airside location, all visitors must go through security, the same as passengers, and be accompanied by a full airside pass holder and have their passport with them at all times. Filming can not be undertaken in Security, at Immigration/Customs, or in Baggage Reclaim. Tower Control controls aircraft on the runway and in the controlled airspace immediately surrounding the airport. Tower controllers may use radar to locate an aircraft's position in three-dimensional space, or they may rely on pilot position reports and visual observation. They coordinate the sequencing of aircraft in the traffic pattern and direct aircraft on how to safely join and leave the circuit. Aircraft which are only passing through the airspace must also contact Tower Control in order to be sure that they remain clear of other traffic. At all airports the use of a traffic pattern (often called a traffic circuit outside the U.S.) is possible. They may help to assure smooth traffic flow between departing and arriving aircraft. There is no technical need within modern aviation for performing this pattern, provided there is no queue. And due to the so-called SLOT-times, the overall traffic planning tend to assure landing queues are avoided. If for instance an aircraft approaches runway 17 (which has a heading of approx. 170 degrees) from the north (coming from 360/0 degrees heading towards 180 degrees), the aircraft will land as fast as possible by just turning 10 degrees and follow the glidepath, without orbit the runway for visual reasons, whenever this is possible. For smaller piston engined airplanes at smaller airfields without ILS equipment, things are very differently though. Ground Control is responsible for directing all ground traffic in designated ""movement areas"", except the traffic on runways. This includes planes, baggage trains, snowplows, grass cutters, fuel trucks, stair trucks, airline food trucks, conveyor belt vehicles and other vehicles. Ground Control will instruct these vehicles on which taxiways to use, which runway they will use (in the case of planes), where they will park, and when it is safe to cross runways. When a plane is ready to takeoff it will stop short of the runway, at which point it will be turned over to Tower Control. After a plane has landed, it will depart the runway and be returned to Ground Control. Generally, this pattern is a circuit consisting of five ""legs"" that form a rectangle (two legs and the runway form one side, with the remaining legs forming three more sides). Each leg is named (see diagram), and ATC directs pilots on how to join and leave the circuit. Traffic patterns are flown at one specific altitude, usually 800 or 1,000 ft (244 or 305 m) above ground level (AGL). Standard traffic patterns are left-handed, meaning all turns are made to the left. One of the main reason for this is that pilots sit on the left side of the airplane, and a Left-hand patterns improves their visibility of the airport and pattern. Right-handed patterns do exist, usually because of obstacles such as a mountain, or to reduce noise for local residents. The predetermined circuit helps traffic flow smoothly because all pilots know what to expect, and helps reduce the chance of a mid-air collision. An airport is an aerodrome with facilities for flights to take off and land. Airports often have facilities to store and maintain aircraft, and a control tower. An airport consists of a landing area, which comprises an aerially accessible open space including at least one operationally active surface such as a runway for a plane to take off or a helipad, and often includes adjacent utility buildings such as control towers, hangars  and terminals. Larger airports may have fixed base operator services, airport aprons, air traffic control centres, passenger facilities such as restaurants and lounges, and emergency services. Airport construction boomed during the 1960s with the increase in jet aircraft traffic. Runways were extended out to 3,000 m (9,800 ft). The fields were constructed out of reinforced concrete using a slip-form machine that produces a continual slab with no disruptions along the length. The early 1960s also saw the introduction of jet bridge systems to modern airport terminals, an innovation which eliminated outdoor passenger boarding. These systems became commonplace in the United States by the 1970s. Following the war, some of these military airfields added civil facilities for handling passenger traffic. One of the earliest such fields was Paris – Le Bourget Airport at Le Bourget, near Paris. The first airport to operate scheduled international commercial services was Hounslow Heath Aerodrome in August 1919, but it was closed and supplanted by Croydon Airport in March 1920. In 1922, the first permanent airport and commercial terminal solely for commercial aviation was opened at Flughafen Devau near what was then Königsberg, East Prussia. The airports of this era used a paved ""apron"", which permitted night flying as well as landing heavier aircraft. There are a number of aids available to pilots, though not all airports are equipped with them. A visual approach slope indicator (VASI) helps pilots fly the approach for landing. Some airports are equipped with a VHF omnidirectional range (VOR) to help pilots find the direction to the airport. VORs are often accompanied by a distance measuring equipment (DME) to determine the distance to the VOR. VORs are also located off airports, where they serve to provide airways for aircraft to navigate upon. In poor weather, pilots will use an instrument landing system (ILS) to find the runway and fly the correct approach, even if they cannot see the ground. The number of instrument approaches based on the use of the Global Positioning System (GPS) is rapidly increasing and may eventually be the primary means for instrument landings. An airbase, sometimes referred to as an air station or airfield, provides basing and support of military aircraft. Some airbases, known as military airports, provide facilities similar to their civilian counterparts. For example, RAF Brize Norton in the UK has a terminal which caters to passengers for the Royal Air Force's scheduled TriStar flights to the Falkland Islands. Some airbases are co-located with civilian airports, sharing the same ATC facilities, runways, taxiways and emergency services, but with separate terminals, parking areas and hangars. Bardufoss Airport , Bardufoss Air Station in Norway and Pune Airport in India are examples of this. The majority of the world's airports are non-towered, with no air traffic control presence. However, at particularly busy airports, or airports with other special requirements, there is an air traffic control (ATC) system whereby controllers (usually ground-based) direct aircraft movements via radio or other communications links. This coordinated oversight facilitates safety and speed in complex operations where traffic moves in all three dimensions. Air traffic control responsibilities at airports are usually divided into at least two main areas: ground and tower, though a single controller may work both stations. The busiest airports also have clearance delivery, apron control, and other specialized ATC stations. Airports have played major roles in films and television programs due to their very nature as a transport and international hub, and sometimes because of distinctive architectural features of particular airports. One such example of this is The Terminal, a film about a man who becomes permanently grounded in an airport terminal and must survive only on the food and shelter provided by the airport. They are also one of the major elements in movies such as The V.I.P.s, Airplane!, Airport (1970), Die Hard 2, Soul Plane, Jackie Brown, Get Shorty, Home Alone, Liar Liar, Passenger 57, Final Destination (2000), Unaccompanied Minors, Catch Me If You Can, Rendition and The Langoliers. They have also played important parts in television series like Lost, The Amazing Race, America's Next Top Model, Cycle 10 which have significant parts of their story set within airports. In other programmes and films, airports are merely indicative of journeys, e.g. Good Will Hunting. Airports are divided into landside and airside areas. Landside areas include parking lots, public transportation train stations and access roads. Airside areas include all areas accessible to aircraft, including runways, taxiways and aprons. Access from landside areas to airside areas is tightly controlled at most airports. Passengers on commercial flights access airside areas through terminals, where they can purchase tickets, clear security check, or claim luggage and board aircraft through gates. The waiting areas which provide passenger access to aircraft are typically called concourses, although this term is often used interchangeably with terminal. The majority of the world's airports are non-towered, with no air traffic control presence. Busy airports have air traffic control (ATC) system. All airports use a traffic pattern to assure smooth traffic flow between departing and arriving aircraft. There are a number of aids available to pilots, though not all airports are equipped with them. Many airports have lighting that help guide planes using the runways and taxiways at night or in rain, snow, or fog. In the U.S. and Canada, the vast majority of airports, large and small, will either have some form of automated airport weather station, a human observer or a combination of the two. Air safety is an important concern in the operation of an airport, and airports often have their own safety services. The title of ""world's oldest airport"" is disputed, but College Park Airport in Maryland, US, established in 1909 by Wilbur Wright, is generally agreed to be the world's oldest continually operating airfield, although it serves only general aviation traffic. Bisbee-Douglas International Airport in Arizona was declared ""the first international airport of the Americas"" by US president Franklin D. Roosevelt in 1943. Pearson Field Airport in Vancouver, Washington had a dirigible land in 1905 and planes in 1911 and is still in use. Bremen Airport opened in 1913 and remains in use, although it served as an American military field between 1945 and 1949. Amsterdam Airport Schiphol opened on September 16, 1916 as a military airfield, but only accepted civil aircraft from December 17, 1920, allowing Sydney Airport in Sydney, Australia—which started operations in January 1920—to claim to be one of the world's oldest continually operating commercial airports. Minneapolis-Saint Paul International Airport in Minneapolis-Saint Paul, Minnesota, opened in 1920 and has been in continuous commercial service since. It serves about 35,000,000 passengers each year and continues to expand, recently opening a new 11,000 foot (3,355 meter) runway. Of the airports constructed during this early period in aviation, it is one of the largest and busiest that is still currently operating. Rome Ciampino Airport, opened 1916, is also a contender, as well as the Don Mueang International Airport near Bangkok,Thailand, which opened in 1914. Increased aircraft traffic during World War I led to the construction of landing fields. Aircraft had to approach these from certain directions and this led to the development of aids for directing the approach and landing slope. Many large airports are located near railway trunk routes for seamless connection of multimodal transport, for instance Frankfurt Airport, Amsterdam Airport Schiphol, London Heathrow Airport, London Gatwick Airport and London Stansted Airport. It is also common to connect an airport and a city with rapid transit, light rail lines or other non-road public transport systems. Some examples of this would include the AirTrain JFK at John F. Kennedy International Airport in New York, Link Light Rail that runs from the heart of downtown Seattle to Seattle–Tacoma International Airport, and the Silver Line T at Boston's Logan International Airport by the Massachusetts Bay Transportation Authority (MBTA). Such a connection lowers risk of missed flights due to traffic congestion. Large airports usually have access also through controlled-access highways ('freeways' or 'motorways') from which motor vehicles enter either the departure loop or the arrival loop. Hazards to aircraft include debris, nesting birds, and reduced friction levels due to environmental conditions such as ice, snow, or rain. Part of runway maintenance is airfield rubber removal which helps maintain friction levels. The fields must be kept clear of debris using cleaning equipment so that loose material does not become a projectile and enter an engine duct (see foreign object damage). In adverse weather conditions, ice and snow clearing equipment can be used to improve traction on the landing strip. For waiting aircraft, equipment is used to spray special deicing fluids on the wings. Many ground crew at the airport work at the aircraft. A tow tractor pulls the aircraft to one of the airbridges, The ground power unit is plugged in. It keeps the electricity running in the plane when it stands at the terminal. The engines are not working, therefore they do not generate the electricity, as they do in flight. The passengers disembark using the airbridge. Mobile stairs can give the ground crew more access to the aircraft's cabin. There is a cleaning service to clean the aircraft after the aircraft lands. Flight catering provides the food and drinks on flights. A toilet waste truck removes the human waste from the tank which holds the waste from the toilets in the aircraft. A water truck fills the water tanks of the aircraft. A fuel transfer vehicle transfers aviation fuel from fuel tanks underground, to the aircraft tanks. A tractor and its dollies bring in luggage from the terminal to the aircraft. They also carry luggage to the terminal if the aircraft has landed, and is being unloaded. Hi-loaders lift the heavy luggage containers to the gate of the cargo hold. The ground crew push the luggage containers into the hold. If it has landed, they rise, the ground crew push the luggage container on the hi-loader, which carries it down. The luggage container is then pushed on one of the tractors dollies. The conveyor, which is a conveyor belt on a truck, brings in the awkwardly shaped, or late luggage. The airbridge is used again by the new passengers to embark the aircraft. The tow tractor pushes the aircraft away from the terminal to a taxi area. The aircraft should be off of the airport and in the air in 90 minutes. The airport charges the airline for the time the aircraft spends at the airport."
Comprehensive_school,"There is some controversy about comprehensive schools. As a rule of thumb those supporting The Left Party, the Social Democratic Party of Germany and Alliance '90/The Greens are in favour of comprehensive schools, while those supporting the Christian Democratic Union and the Free Democratic Party are opposed to them. In these schools children could be selected on the basis of curriculum aptitude related to the school's specialism even though the schools do take quotas from each quartile of the attainment range to ensure they were not selective by attainment. A problem with this is whether the quotas should be taken from a normal distribution or from the specific distribution of attainment in the immediate catchment area. In the selective school system, which survives in several parts of the United Kingdom, admission is dependent on selection criteria, most commonly a cognitive test or tests. Although comprehensive schools were introduced to England and Wales in 1965, there are 164 selective grammar schools that are still in operation.[citation needed] (though this is a small number compared to approximately 3500 state secondary schools in England). Most comprehensives are secondary schools for children between the ages of 11 to 16, but in a few areas there are comprehensive middle schools, and in some places the secondary level is divided into two, for students aged 11 to 14 and those aged 14 to 18, roughly corresponding to the US middle school (or junior high school) and high school, respectively. With the advent of key stages in the National Curriculum some local authorities reverted from the Middle School system to 11–16 and 11–18 schools so that the transition between schools corresponds to the end of one key stage and the start of another. Education in Northern Ireland differs slightly from systems used elsewhere in the United Kingdom, but it is more similar to that used in England and Wales than it is to Scotland. A comprehensive school is a state school that does not select its intake on the basis of academic achievement or aptitude. This is in contrast to the selective school system, where admission is restricted on the basis of selection criteria. The term is commonly used in relation to England and Wales, where comprehensive schools were introduced on an experimental basis in the 1940s and became more widespread from 1965. About 90% of British secondary school pupils now attend comprehensive schools. They correspond broadly to the public high school in the United States and Canada and to the German Gesamtschule.[citation needed] Comprehensive schools are primarily about providing an entitlement curriculum to all children, without selection whether due to financial considerations or attainment. A consequence of that is a wider ranging curriculum, including practical subjects such as design and technology and vocational learning, which were less common or non-existent in grammar schools. Providing post-16 education cost-effectively becomes more challenging for smaller comprehensive schools, because of the number of courses needed to cover a broader curriculum with comparatively fewer students. This is why schools have tended to get larger and also why many local authorities have organised secondary education into 11–16 schools, with the post-16 provision provided by Sixth Form colleges and Further Education Colleges. Comprehensive schools do not select their intake on the basis of academic achievement or aptitude, but there are demographic reasons why the attainment profiles of different schools vary considerably. In addition, government initiatives such as the City Technology Colleges and Specialist schools programmes have made the comprehensive ideal less certain. Germany has a comprehensive school known as the Gesamtschule. While some German schools such as the Gymnasium and the Realschule have rather strict entrance requirements, the Gesamtschule does not have such requirements. They offer college preparatory classes for the students who are doing well, general education classes for average students, and remedial courses for those who aren't doing that well. In most cases students attending a Gesamtschule may graduate with the Hauptschulabschluss, the Realschulabschluss or the Abitur depending on how well they did in school. Scotland has a very different educational system from England and Wales, though also based on comprehensive education. It has different ages of transfer, different examinations and a different philosophy of choice and provision. All publicly funded primary and secondary schools are comprehensive. The Scottish Government has rejected plans for specialist schools as of 2005. By 1975 the majority of local authorities in England and Wales had abandoned the 11-plus examination and moved to a comprehensive system. Over that 10-year period many secondary modern schools and grammar schools were amalgamated to form large neighbourhood comprehensives, whilst a number of new schools were built to accommodate a growing school population. By the mid-1970s the system had been almost fully implemented, with virtually no secondary modern schools remaining. Many grammar schools were either closed or changed to comprehensive status. Some local authorities, including Sandwell and Dudley in the West Midlands, changed all of its state secondary schools to comprehensive schools during the 1970s. Since the 1988 Education Reform Act, parents have a right to choose which school their child should go to or whether to not send them to school at all and to home educate them instead. The concept of ""school choice"" introduces the idea of competition between state schools, a fundamental change to the original ""neighbourhood comprehensive"" model, and is partly intended as a means by which schools that are perceived to be inferior are forced either to improve or, if hardly anyone wants to go there, to close down. Government policy is currently promoting 'specialisation' whereby parents choose a secondary school appropriate for their child's interests and skills. Most initiatives focus on parental choice and information, implementing a pseudo-market incentive to encourage better schools. This logic has underpinned the controversial league tables of school performance. Comprehensive schools have been accused of grade inflation after a study revealed that Gymnasium senior students of average mathematical ability found themselves at the very bottom of their class and had an average grade of ""Five"", which means ""Failed"". Gesamtschule senior students of average mathematical ability found themselves in the upper half of their class and had an average grade of ""Three Plus"". When a central Abitur examination was established in the State of North Rhine-Westphalia, it was revealed that Gesamtschule students did worse than could be predicted by their grades or class rank. Barbara Sommer (Christian Democratic Union), Education Minister of North Rhine-Westphalia, commented that: Looking at the performance gap between comprehensives and the Gymnasium [at the Abitur central examination] [...] it is difficult to understand why the Social Democratic Party of Germany wants to do away with the Gymnasium. [...] The comprehensives do not help students achieve [...] I am sick and tired of the comprehensive schools blaming their problems on the social class origins of their students. What kind of attitude is this to blame their own students? She also called the Abitur awarded by the Gymnasium the true Abitur and the Abitur awarded by the Gesamtschule ""Abitur light"". As a reaction, Sigrid Beer (Alliance '90/The Greens) stated that comprehensives were structurally discriminated against by the government, which favoured the Gymnasiums. She also said that many of the students awarded the Abitur by the comprehensives came from ""underprivileged groups"" and sneering at their performance was a ""piece of impudence"". Gesamtschulen might put bright working class students at risk according to several studies. It could be shown that an achievement gap opens between working class students attending a comprehensive and their middle class peers. Also working class students attending a Gymnasium or a Realschule outperform students from similar backgrounds attending a comprehensive. However it is not students attending a comprehensive, but students attending a Hauptschule, who perform the poorest. The ""Mittelschule"" is a school in some States of Germany that offers regular classes and remedial classes but no college preparatory classes. In some States of Germany, the Hauptschule does not exist, and any student who has not been accepted by another school has to attend the Mittelschule. Students may be awarded the Hauptschulabschluss or the Mittlere Reife but not the Abitur. The first comprehensives were set up after the Second World War. In 1946, for example, Walworth School was one of five 'experimental' comprehensive schools set up by the London County Council Another early comprehensive school was Holyhead County School in Anglesey in 1949. Other early examples of comprehensive schools included Woodlands Boys School in Coventry (opened in 1954) and Tividale Comprehensive School in Tipton. Gibraltar opened its first comprehensive school in 1972. Between the ages of 12 and 16 two comprehensive schools cater for girls and boys separately. Students may also continue into the sixth form to complete their A-levels. The largest expansion of comprehensive schools in 1965 resulted from a policy decision taken in 1965 by Anthony Crosland, Secretary of State for Education in the 1964–1970 Labour government. The policy decision was implemented by Circular 10/65, an instruction to local education authorities to plan for conversion. Students sat the 11+ examination in their last year of primary education and were sent to one of a secondary modern, secondary technical or grammar school depending on their perceived ability. Secondary technical schools were never widely implemented and for 20 years there was a virtual bipartite system which saw fierce competition for the available grammar school places, which varied between 15% and 25% of total secondary places, depending on location.[citation needed] In 1976 the future Labour prime minister James Callaghan launched what became known as the 'great debate' on the education system. He went on to list the areas he felt needed closest scrutiny: the case for a core curriculum, the validity and use of informal teaching methods, the role of school inspection and the future of the examination system. Comprehensive school remains the most common type of state secondary school in England, and the only type in Wales. They account for around 90% of pupils, or 64% if one does not count schools with low-level selection. This figure varies by region. Comprehensive schools were introduced into Ireland in 1966 by an initiative by Patrick Hillery, Minister for Education, to give a broader range of education compared to that of the vocational school system, which was then the only system of schools completely controlled by the state. Until then, education in Ireland was largely dominated by religious persuasion, particularly the voluntary secondary school system was a particular realisation of this. The comprehensive school system is still relatively small and to an extent has been superseded by the community school concept. The Irish word for a comprehensive school is a 'scoil chuimsitheach.' In principle, comprehensive schools were conceived as ""neighbourhood"" schools for all students in a specified catchment area. Current education reforms with Academies Programme, Free Schools and University Technical Colleges will no doubt have some impact on the comprehensive ideal but it is too early to say to what degree. Finland has used comprehensive schools since the 1970s, in the sense that everyone is expected to complete the nine grades of peruskoulu, from the age 7 to 16. The division to lower comprehensive school (grades 1–6, ala-aste, alakoulu) and upper comprehensive school (grades 7–9, yläaste, yläkoulu) has been discontinued. The introduction of the community school model in the 1970s controversially removed the denominational basis of the schools, but religious interests were invited to be represented on the Boards of Management. Community schools are divided into two models, the community school vested in the Minister for Education and the community college vested in the local Education and Training Board. Community colleges tended to be amalgamations of unviable local schools under the umbrella of a new community school model, but community schools have tended to be entirely new foundations. In Ireland comprehensive schools were an earlier model of state schools, introduced in the late 1960s and largely replaced by the secular community model of the 1970s. The comprehensive model generally incorporated older schools that were under Roman Catholic or Protestant ownership, and the various denominations still manage the school as patrons or trustees. The state owns the school property, which is vested in the trustees in perpetuity. The model was adopted to make state schools more acceptable to a largely conservative society of the time. Starting in 2010/2011, Hauptschulen were merged with Realschulen and Gesamtschulen to form a new type of comprehensive school in the German States of Berlin and Hamburg, called Stadtteilschule in Hamburg and Sekundarschule in Berlin (see: Education in Berlin, Education in Hamburg). The percentage of students attending a Gesamtschule varies by Bundesland. In the State of Brandenburg more than 50% of all students attended a Gesamtschule in 2007, while in the State of Bavaria less than 1% did. According to a study done by Helmut Fend (who had always been a fierce proponent of comprehensive schools) revealed that comprehensive schools do not help working class students. He compared alumni of the tripartite system to alumni of comprehensive schools. While working class alumni of comprehensive schools were awarded better school diplomas at age 35, they held similar occupational positions as working class alumni of the tripartite system and were as unlikely to graduate from college. In 1970 Margaret Thatcher became Secretary of State for Education of the new Conservative government. She ended the compulsion on local authorities to convert, however, many local authorities were so far down the path that it would have been prohibitively expensive to attempt to reverse the process, and more comprehensive schools were established under Mrs Thatcher than any other education secretary."
United_States_Army,"The Vietnam War is often regarded as a low point for the U.S. Army due to the use of drafted personnel, the unpopularity of the war with the American public, and frustrating restrictions placed on the military by American political leaders. While American forces had been stationed in the Republic of Vietnam since 1959, in intelligence & advising/training roles, they did not deploy in large numbers until 1965, after the Gulf of Tonkin Incident. American forces effectively established and maintained control of the ""traditional"" battlefield, however they struggled to counter the guerrilla hit and run tactics of the communist Viet Cong and the North Vietnamese Army. On a tactical level, American soldiers (and the U.S. military as a whole) did not lose a sizable battle. The army has relied heavily on tents to provide the various facilities needed while on deployment. The most common tent uses for the military are as temporary barracks (sleeping quarters), DFAC buildings (dining facilities), forward operating bases (FOBs), after action review (AAR), tactical operations center (TOC), morale, welfare, and recreation (MWR) facilities, and security checkpoints. Furthermore, most of these tents are set up and operated through the support of Natick Soldier Systems Center. During the Cold War, American troops and their allies fought Communist forces in Korea and Vietnam. The Korean War began in 1950, when the Soviets walked out of a U.N. Security meeting, removing their possible veto. Under a United Nations umbrella, hundreds of thousands of U.S. troops fought to prevent the takeover of South Korea by North Korea, and later, to invade the northern nation. After repeated advances and retreats by both sides, and the PRC People's Volunteer Army's entry into the war, the Korean Armistice Agreement returned the peninsula to the status quo in 1953. The War of 1812, the second and last American war against the United Kingdom, was less successful for the U.S. than the Revolution and Northwest Indian War against natives had been, though it ended on a high note for Americans as well. After the taking control of Lake Erie in 1813, the Americans were able to seize parts of western Upper Canada, burn York and defeat Tecumseh, which caused his Indian Confederacy to collapse. Following ending victories in the province of Upper Canada, which dubbed the U.S. Army ""Regulars, by God!"", British troops were able to capture and burn Washington. The regular army, however, proved they were professional and capable of defeating the British army during the invasions of Plattsburgh and Baltimore, prompting British agreement on the previously rejected terms of a status quo ante bellum. Two weeks after a treaty was signed (but not ratified), Andrew Jackson defeated the British in the Battle of New Orleans and became a national hero. Per the treaty both sides returned to the status quo with no victor. Collective training at the unit level takes place at the unit's assigned station, but the most intensive training at higher echelons is conducted at the three combat training centers (CTC); the National Training Center (NTC) at Fort Irwin, California, the Joint Readiness Training Center (JRTC) at Fort Polk, Louisiana, and the Joint Multinational Training Center (JMRC) at the Hohenfels Training Area in Hohenfels, Germany. ARFORGEN is the Army Force Generation process approved in 2006 to meet the need to continuously replenish forces for deployment, at unit level, and for other echelons as required by the mission. Individual-level replenishment still requires training at a unit level, which is conducted at the continental US (CONUS) replacement center at Fort Bliss, in New Mexico and Texas, before their individual deployment. The United States Army is made up of three components: the active component, the Regular Army; and two reserve components, the Army National Guard and the Army Reserve. Both reserve components are primarily composed of part-time soldiers who train once a month, known as battle assemblies or unit training assemblies (UTAs), and conduct two to three weeks of annual training each year. Both the Regular Army and the Army Reserve are organized under Title 10 of the United States Code, while the National Guard is organized under Title 32. While the Army National Guard is organized, trained and equipped as a component of the U.S. Army, when it is not in federal service it is under the command of individual state and territorial governors; the District of Columbia National Guard, however, reports to the U.S. President, not the district's mayor, even when not federalized. Any or all of the National Guard can be federalized by presidential order and against the governor's wishes. The Total Force Policy was adopted by Chief of Staff of the Army General Creighton Abrams in the aftermath of the Vietnam War and involves treating the three components of the army – the Regular Army, the Army National Guard and the Army Reserve as a single force. Believing that no U.S. president should be able to take the United States (and more specifically the U.S. Army) to war without the support of the American people, General Abrams intertwined the structure of the three components of the army in such a way as to make extended operations impossible, without the involvement of both the Army National Guard and the Army Reserve. The U.S. Army black beret (having been permanently replaced with the patrol cap) is no longer worn with the new ACU for garrison duty. After years of complaints that it wasn't suited well for most work conditions, Army Chief of Staff General Martin Dempsey eliminated it for wear with the ACU in June 2011. Soldiers still wear berets who are currently in a unit in jump status, whether the wearer is parachute-qualified, or not (maroon beret), Members of the 75th Ranger Regiment and the Airborne and Ranger Training Brigade (tan beret), and Special Forces (rifle green beret) and may wear it with the Army Service Uniform for non-ceremonial functions. Unit commanders may still direct the wear of patrol caps in these units in training environments or motor pools. The army is led by a civilian Secretary of the Army, who has the statutory authority to conduct all the affairs of the army under the authority, direction and control of the Secretary of Defense. The Chief of Staff of the Army, who is the highest-ranked military officer in the army, serves as the principal military adviser and executive agent for the Secretary of the Army, i.e., its service chief; and as a member of the Joint Chiefs of Staff, a body composed of the service chiefs from each of the four military services belonging to the Department of Defense who advise the President of the United States, the Secretary of Defense, and the National Security Council on operational military matters, under the guidance of the Chairman and Vice Chairman of the Joint Chiefs of Staff. In 1986, the Goldwater–Nichols Act mandated that operational control of the services follows a chain of command from the President to the Secretary of Defense directly to the unified combatant commanders, who have control of all armed forces units in their geographic or function area of responsibility. Thus, the secretaries of the military departments (and their respective service chiefs underneath them) only have the responsibility to organize, train and equip their service components. The army provides trained forces to the combatant commanders for use as directed by the Secretary of Defense. For the first two years Confederate forces did well in set battles but lost control of the border states. The Confederates had the advantage of defending a very large country in an area where disease caused twice as many deaths as combat. The Union pursued a strategy of seizing the coastline, blockading the ports, and taking control of the river systems. By 1863 the Confederacy was being strangled. Its eastern armies fought well, but the western armies were defeated one after another until the Union forces captured New Orleans in 1862 along with the Tennessee River. In the famous Vicksburg Campaign of 1862–63, Ulysses Grant seized the Mississippi River and cut off the Southwest. Grant took command of Union forces in 1864 and after a series of battles with very heavy casualties, he had Lee under siege in Richmond as William T. Sherman captured Atlanta and marched through Georgia and the Carolinas. The Confederate capital was abandoned in April 1865 and Lee subsequently surrendered his army at Appomattox Court House; all other Confederate armies surrendered within a few months. The end of World War II set the stage for the East–West confrontation known as the Cold War. With the outbreak of the Korean War, concerns over the defense of Western Europe rose. Two corps, V and VII, were reactivated under Seventh United States Army in 1950 and American strength in Europe rose from one division to four. Hundreds of thousands of U.S. troops remained stationed in West Germany, with others in Belgium, the Netherlands and the United Kingdom, until the 1990s in anticipation of a possible Soviet attack. The American Civil War was the costliest war for the U.S. in terms of casualties. After most slave states, located in the southern U.S., formed the Confederate States, C.S. troops led by former U.S. Army officers, mobilized a very large fraction of Southern white manpower. Forces of the United States (the ""Union"" or ""the North"") formed the Union Army consisting of a small body of regular army units and a large body of volunteer units raised from every state, north and south, except South Carolina.[citation needed] As a uniformed military service, the Army is part of the Department of the Army, which is one of the three military departments of the Department of Defense. The U.S. Army is headed by a civilian senior appointed civil servant, the Secretary of the Army (SECARMY), and by a chief military officer, the Chief of Staff of the Army (CSA) who is also a member of the Joint Chiefs of Staff. In the fiscal year 2016, the projected end strength for the Regular Army (USA) was 475,000 soldiers; the Army National Guard (ARNG) had 342,000 soldiers, and the United States Army Reserve (USAR) had 198,000 soldiers; the combined-component strength of the U.S. Army was 1,015,000 soldiers. As a branch of the armed forces, the mission of the U.S. Army is ""to fight and win our Nation's wars, by providing prompt, sustained, land dominance, across the full range of military operations and the spectrum of conflict, in support of combatant commanders."" The service participates in conflicts worldwide and is the major ground-based offensive and defensive force. The Pentagon bought 25,000 MRAP vehicles since 2007 in 25 variants through rapid acquisition with no long-term plans for the platforms. The Army plans to divest 7,456 vehicles and retain 8,585. Of the total number of vehicles the Army will keep, 5,036 will be put in storage, 1,073 will be used for training, and the remainder will be spread across the active force. The Oshkosh M-ATV will be kept the most at 5,681 vehicles, as it is smaller and lighter than other MRAPs for off-road mobility. The other most retained vehicle will be the Navistar MaxxPro Dash with 2,633 vehicles, plus 301 Maxxpro ambulances. Thousands of other MRAPs like the Cougar, BAE Caiman, and larger MaxxPros will be disposed of. The army is also changing its base unit from divisions to brigades. Division lineage will be retained, but the divisional headquarters will be able to command any brigade, not just brigades that carry their divisional lineage. The central part of this plan is that each brigade will be modular, i.e., all brigades of the same type will be exactly the same, and thus any brigade can be commanded by any division. As specified before the 2013 end-strength re-definitions, the three major types of ground combat brigades are: Many units are supplemented with a variety of specialized weapons, including the M249 SAW (Squad Automatic Weapon), to provide suppressive fire at the fire-team level. Indirect fire is provided by the M203 grenade launcher. The M1014 Joint Service Combat Shotgun or the Mossberg 590 Shotgun are used for door breaching and close-quarters combat. The M14EBR is used by designated marksmen. Snipers use the M107 Long Range Sniper Rifle, the M2010 Enhanced Sniper Rifle, and the M110 Semi-Automatic Sniper Rifle. The army's major campaign against the Indians was fought in Florida against Seminoles. It took long wars (1818–58) to finally defeat the Seminoles and move them to Oklahoma. The usual strategy in Indian wars was to seize control of the Indians winter food supply, but that was no use in Florida where there was no winter. The second strategy was to form alliances with other Indian tribes, but that too was useless because the Seminoles had destroyed all the other Indians when they entered Florida in the late eighteenth century. By 1989 Germany was nearing reunification and the Cold War was coming to a close. Army leadership reacted by starting to plan for a reduction in strength. By November 1989 Pentagon briefers were laying out plans to reduce army end strength by 23%, from 750,000 to 580,000. A number of incentives such as early retirement were used. In 1990 Iraq invaded its smaller neighbor, Kuwait, and U.S. land forces, quickly deployed to assure the protection of Saudi Arabia. In January 1991 Operation Desert Storm commenced, a U.S.-led coalition which deployed over 500,000 troops, the bulk of them from U.S. Army formations, to drive out Iraqi forces. The campaign ended in total victory, as Western coalition forces routed the Iraqi Army, organized along Soviet lines, in just one hundred hours. Following their basic and advanced training at the individual-level, soldiers may choose to continue their training and apply for an ""additional skill identifier"" (ASI). The ASI allows the army to take a wide ranging MOS and focus it into a more specific MOS. For example, a combat medic, whose duties are to provide pre-hospital emergency treatment, may receive ASI training to become a cardiovascular specialist, a dialysis specialist, or even a licensed practical nurse. For commissioned officers, ASI training includes pre-commissioning training either at USMA, or via ROTC, or by completing OCS. After commissioning, officers undergo branch specific training at the Basic Officer Leaders Course, (formerly called Officer Basic Course), which varies in time and location according their future assignments. Further career development is available through the Army Correspondence Course Program. The task of organizing the U.S. Army commenced in 1775. In the first one hundred years of its existence, the United States Army was maintained as a small peacetime force to man permanent forts and perform other non-wartime duties such as engineering and construction works. During times of war, the U.S. Army was augmented by the much larger United States Volunteers which were raised independently by various state governments. States also maintained full-time militias which could also be called into the service of the army. By the twentieth century, the U.S. Army had mobilized the U.S. Volunteers on four separate occasions during each of the major wars of the nineteenth century. During World War I, the ""National Army"" was organized to fight the conflict, replacing the concept of U.S. Volunteers. It was demobilized at the end of World War I, and was replaced by the Regular Army, the Organized Reserve Corps, and the State Militias. In the 1920s and 1930s, the ""career"" soldiers were known as the ""Regular Army"" with the ""Enlisted Reserve Corps"" and ""Officer Reserve Corps"" augmented to fill vacancies when needed. During the 1960s the Department of Defense continued to scrutinize the reserve forces and to question the number of divisions and brigades as well as the redundancy of maintaining two reserve components, the Army National Guard and the Army Reserve. In 1967 Secretary of Defense Robert McNamara decided that 15 combat divisions in the Army National Guard were unnecessary and cut the number to 8 divisions (1 mechanized infantry, 2 armored, and 5 infantry), but increased the number of brigades from 7 to 18 (1 airborne, 1 armored, 2 mechanized infantry, and 14 infantry). The loss of the divisions did not set well with the states. Their objections included the inadequate maneuver element mix for those that remained and the end to the practice of rotating divisional commands among the states that supported them. Under the proposal, the remaining division commanders were to reside in the state of the division base. No reduction, however, in total Army National Guard strength was to take place, which convinced the governors to accept the plan. The states reorganized their forces accordingly between 1 December 1967 and 1 May 1968. The United States Army (USA) is the largest branch of the United States Armed Forces and performs land-based military operations. It is one of the seven uniformed services of the United States and is designated as the Army of the United States in the United States Constitution, Article 2, Section 2, Clause 1 and United States Code, Title 10, Subtitle B, Chapter 301, Section 3001. As the largest and senior branch of the U.S. military, the modern U.S. Army has its roots in the Continental Army, which was formed (14 June 1775) to fight the American Revolutionary War (1775–83)—before the U.S. was established as a country. After the Revolutionary War, the Congress of the Confederation created the United States Army on 3 June 1784, to replace the disbanded Continental Army. The United States Army considers itself descended from the Continental Army, and dates its institutional inception from the origin of that armed force in 1775. The Continental Army was created on 14 June 1775 by the Continental Congress as a unified army for the colonies to fight Great Britain, with George Washington appointed as its commander. The army was initially led by men who had served in the British Army or colonial militias and who brought much of British military heritage with them. As the Revolutionary War progressed, French aid, resources, and military thinking influenced the new army. A number of European soldiers came on their own to help, such as Friedrich Wilhelm von Steuben, who taught the army Prussian tactics and organizational skills. On September 11, 2001, 53 Army civilians (47 employees and six contractors) and 22 soldiers were among the 125 victims killed in the Pentagon in a terrorist attack when American Airlines Flight 77 commandeered by five Al-Qaeda hijackers slammed into the western side of the building, as part of the September 11 attacks. Lieutenant General Timothy Maude was the highest-ranking military official killed at the Pentagon, and the most senior U.S. Army officer killed by foreign action since the death of Lieutenant General Simon B. Buckner, Jr. on June 18, 1945, in the Battle of Okinawa during World War II. The army employs various individual weapons to provide light firepower at short ranges. The most common weapons used by the army are the compact variant of the M16 rifle, the M4 carbine, as well as the 7.62×51mm variant of the FN SCAR for Army Rangers. The primary sidearm in the U.S. Army is the 9 mm M9 pistol; the M11 pistol is also used. Both handguns are to be replaced through the Modular Handgun System program. Soldiers are also equiped with various hand grenades, such as the M67 fragmentation grenade and M18 smoke grenade. Currently, the army is divided into the Regular Army, the Army Reserve, and the Army National Guard. The army is also divided into major branches such as Air Defense Artillery, Infantry, Aviation, Signal Corps, Corps of Engineers, and Armor. Before 1903 members of the National Guard were considered state soldiers unless federalized (i.e., activated) by the President. Since the Militia Act of 1903 all National Guard soldiers have held dual status: as National Guardsmen under the authority of the governor of their state or territory and, when activated, as a reserve of the U.S. Army under the authority of the President. In response to the September 11 attacks, and as part of the Global War on Terror, U.S. and NATO forces invaded Afghanistan in October 2001, displacing the Taliban government. The U.S. Army also led the combined U.S. and allied invasion of Iraq in 2003. It served as the primary source for ground forces with its ability to sustain short and long-term deployment operations. In the following years the mission changed from conflict between regular militaries to counterinsurgency, resulting in the deaths of more than 4,000 U.S service members (as of March 2008) and injuries to thousands more. 23,813 insurgents were killed in Iraq between 2003–2011. The army's most common vehicle is the High Mobility Multipurpose Wheeled Vehicle (HMMWV), commonly called the Humvee, which is capable of serving as a cargo/troop carrier, weapons platform, and ambulance, among many other roles. While they operate a wide variety of combat support vehicles, one of the most common types centers on the family of HEMTT vehicles. The M1A2 Abrams is the army's main battle tank, while the M2A3 Bradley is the standard infantry fighting vehicle. Other vehicles include the Stryker, and the M113 armored personnel carrier, and multiple types of Mine Resistant Ambush Protected (MRAP) vehicles. The U.S. Army currently consists of 10 active divisions as well as several independent units. The force is in the process of contracting after several years of growth. In June 2013, the Army announced plans to downsize to 32 active combat brigade teams by 2015 to match a reduction in active duty strength to 490,000 soldiers. Army Chief of Staff Raymond Odierno has projected that by 2018 the Army will eventually shrink to ""450,000 in the active component, 335,000 in the National Guard and 195,000 in U.S. Army Reserve."" The United States joined World War II in December 1941 after the Japanese attack on Pearl Harbor. On the European front, U.S. Army troops formed a significant portion of the forces that captured North Africa and Sicily, and later fought in Italy. On D-Day, June 6, 1944, and in the subsequent liberation of Europe and defeat of Nazi Germany, millions of U.S. Army troops played a central role. In the Pacific War, U.S. Army soldiers participated alongside the United States Marine Corps in capturing the Pacific Islands from Japanese control. Following the Axis surrenders in May (Germany) and August (Japan) of 1945, army troops were deployed to Japan and Germany to occupy the two defeated nations. Two years after World War II, the Army Air Forces separated from the army to become the United States Air Force in September 1947 after decades of attempting to separate. Also, in 1948, the army was desegregated by order of President Harry S. Truman. Starting in 1910, the army began acquiring fixed-wing aircraft. In 1910, Mexico was having a civil war, peasant rebels fighting government soldiers. The army was deployed to American towns near the border to ensure safety to lives and property. In 1916, Pancho Villa, a major rebel leader, attacked Columbus, New Mexico, prompting a U.S. intervention in Mexico until 7 February 1917. They fought the rebels and the Mexican federal troops until 1918. The United States joined World War I in 1917 on the side of Britain, France, Russia, Italy and other allies. U.S. troops were sent to the Western Front and were involved in the last offensives that ended the war. With the armistice in November 1918, the army once again decreased its forces. After the war, though, the Continental Army was quickly given land certificates and disbanded in a reflection of the republican distrust of standing armies. State militias became the new nation's sole ground army, with the exception of a regiment to guard the Western Frontier and one battery of artillery guarding West Point's arsenal. However, because of continuing conflict with Native Americans, it was soon realized that it was necessary to field a trained standing army. The Regular Army was at first very small, and after General St. Clair's defeat at the Battle of the Wabash, the Regular Army was reorganized as the Legion of the United States, which was established in 1791 and renamed the ""United States Army"" in 1796. Training in the U.S. Army is generally divided into two categories – individual and collective. Basic training consists of 10 weeks for most recruits followed by Advanced Individualized Training (AIT) where they receive training for their military occupational specialties (MOS). Some individuals MOSs range anywhere from 14–20 weeks of One Station Unit Training (OSUT), which combines Basic Training and AIT. The length of AIT school varies by the MOS The length of time spent in AIT depends on the MOS of the soldier, and some highly technical MOS training may require many months (e.g., foreign language translators). Depending on the needs of the army, Basic Combat Training for combat arms soldiers is conducted at a number of locations, but two of the longest-running are the Armor School and the Infantry School, both at Fort Benning, Georgia."
Harvard_University,"Harvard's 209-acre (85 ha) main campus is centered on Harvard Yard in Cambridge, about 3 miles (5 km) west-northwest of the State House in downtown Boston, and extends into the surrounding Harvard Square neighborhood. Harvard Yard itself contains the central administrative offices and main libraries of the university, academic buildings including Sever Hall and University Hall, Memorial Church, and the majority of the freshman dormitories. Sophomore, junior, and senior undergraduates live in twelve residential Houses, nine of which are south of Harvard Yard along or near the Charles River. The other three are located in a residential neighborhood half a mile northwest of the Yard at the Quadrangle (commonly referred to as the Quad), which formerly housed Radcliffe College students until Radcliffe merged its residential system with Harvard. Each residential house contains rooms for undergraduates, House masters, and resident tutors, as well as a dining hall and library. The facilities were made possible by a gift from Yale University alumnus Edward Harkness. For the 2012–13 school year annual tuition was $38,000, with a total cost of attendance of $57,000. Beginning 2007, families with incomes below $60,000 pay nothing for their children to attend, including room and board. Families with incomes between $60,000 to $80,000 pay only a few thousand dollars per year, and families earning between $120,000 and $180,000 pay no more than 10% of their annual incomes. In 2009, Harvard offered grants totaling $414 million across all eleven divisions;[further explanation needed] $340 million came from institutional funds, $35 million from federal support, and $39 million from other outside support. Grants total 88% of Harvard's aid for undergraduate students, with aid also provided by loans (8%) and work-study (4%). The four-year, full-time undergraduate program comprises a minority of enrollments at the university and emphasizes instruction with an ""arts and sciences focus"". Between 1978 and 2008, entering students were required to complete a core curriculum of seven classes outside of their concentration. Since 2008, undergraduate students have been required to complete courses in eight General Education categories: Aesthetic and Interpretive Understanding, Culture and Belief, Empirical and Mathematical Reasoning, Ethical Reasoning, Science of Living Systems, Science of the Physical Universe, Societies of the World, and United States in the World. Harvard offers a comprehensive doctoral graduate program and there is a high level of coexistence between graduate and undergraduate degrees. The Carnegie Foundation for the Advancement of Teaching, The New York Times, and some students have criticized Harvard for its reliance on teaching fellows for some aspects of undergraduate education; they consider this to adversely affect the quality of education. Other: Civil rights leader W. E. B. Du Bois; philosopher Henry David Thoreau; authors Ralph Waldo Emerson and William S. Burroughs; educators Werner Baer, Harlan Hanson; poets Wallace Stevens, T. S. Eliot and E. E. Cummings; conductor Leonard Bernstein; cellist Yo Yo Ma; pianist and composer Charlie Albright; composer John Alden Carpenter; comedian, television show host and writer Conan O'Brien; actors Tatyana Ali, Nestor Carbonell, Matt Damon, Fred Gwynne, Hill Harper, Rashida Jones, Tommy Lee Jones, Ashley Judd, Jack Lemmon, Natalie Portman, Mira Sorvino, Elisabeth Shue, and Scottie Thompson; film directors Darren Aronofsky, Terrence Malick, Mira Nair, and Whit Stillman; architect Philip Johnson; musicians Rivers Cuomo, Tom Morello, and Gram Parsons; musician, producer and composer Ryan Leslie; serial killer Ted Kaczynski; programmer and activist Richard Stallman; NFL quarterback Ryan Fitzpatrick; NFL center Matt Birk; NBA player Jeremy Lin; US Ski Team skier Ryan Max Riley; physician Sachin H. Jain; physicist J. Robert Oppenheimer; computer pioneer and inventor An Wang; Tibetologist George de Roerich; and Marshall Admiral Isoroku Yamamoto. Charles W. Eliot, president 1869–1909, eliminated the favored position of Christianity from the curriculum while opening it to student self-direction. While Eliot was the most crucial figure in the secularization of American higher education, he was motivated not by a desire to secularize education, but by Transcendentalist Unitarian convictions. Derived from William Ellery Channing and Ralph Waldo Emerson, these convictions were focused on the dignity and worth of human nature, the right and ability of each person to perceive truth, and the indwelling God in each person. Harvard's 2,400 professors, lecturers, and instructors instruct 7,200 undergraduates and 14,000 graduate students. The school color is crimson, which is also the name of the Harvard sports teams and the daily newspaper, The Harvard Crimson. The color was unofficially adopted (in preference to magenta) by an 1875 vote of the student body, although the association with some form of red can be traced back to 1858, when Charles William Eliot, a young graduate student who would later become Harvard's 21st and longest-serving president (1869–1909), bought red bandanas for his crew so they could more easily be distinguished by spectators at a regatta. Older than The Game by 23 years, the Harvard-Yale Regatta was the original source of the athletic rivalry between the two schools. It is held annually in June on the Thames River in eastern Connecticut. The Harvard crew is typically considered to be one of the top teams in the country in rowing. Today, Harvard fields top teams in several other sports, such as the Harvard Crimson men's ice hockey team (with a strong rivalry against Cornell), squash, and even recently won NCAA titles in Men's and Women's Fencing. Harvard also won the Intercollegiate Sailing Association National Championships in 2003. Throughout the 18th century, Enlightenment ideas of the power of reason and free will became widespread among Congregationalist ministers, putting those ministers and their congregations in tension with more traditionalist, Calvinist parties.:1–4 When the Hollis Professor of Divinity David Tappan died in 1803 and the president of Harvard Joseph Willard died a year later, in 1804, a struggle broke out over their replacements. Henry Ware was elected to the chair in 1805, and the liberal Samuel Webber was appointed to the presidency of Harvard two years later, which signaled the changing of the tide from the dominance of traditional ideas at Harvard to the dominance of liberal, Arminian ideas (defined by traditionalists as Unitarian ideas).:4–5:24 Women remained segregated at Radcliffe, though more and more took Harvard classes. Nonetheless, Harvard's undergraduate population remained predominantly male, with about four men attending Harvard College for every woman studying at Radcliffe. Following the merger of Harvard and Radcliffe admissions in 1977, the proportion of female undergraduates steadily increased, mirroring a trend throughout higher education in the United States. Harvard's graduate schools, which had accepted females and other groups in greater numbers even before the college, also became more diverse in the post-World War II period. Undergraduate admission to Harvard is characterized by the Carnegie Foundation as ""more selective, lower transfer-in"". Harvard College accepted 5.3% of applicants for the class of 2019, a record low and the second lowest acceptance rate among all national universities. Harvard College ended its early admissions program in 2007 as the program was believed to disadvantage low-income and under-represented minority applicants applying to selective universities, yet for the class of 2016 an Early Action program was reintroduced. During the divestment from South Africa movement in the late 1980s, student activists erected a symbolic ""shantytown"" on Harvard Yard and blockaded a speech given by South African Vice Consul Duke Kent-Brown. The Harvard Management Company repeatedly refused to divest, stating that ""operating expenses must not be subject to financially unrealistic strictures or carping by the unsophisticated or by special interest groups."" However, the university did eventually reduce its South African holdings by $230 million (out of $400 million) in response to the pressure. Established originally by the Massachusetts legislature and soon thereafter named for John Harvard (its first benefactor), Harvard is the United States' oldest institution of higher learning, and the Harvard Corporation (formally, the President and Fellows of Harvard College) is its first chartered corporation. Although never formally affiliated with any denomination, the early College primarily trained Congregationalist and Unitarian clergy. Its curriculum and student body were gradually secularized during the 18th century, and by the 19th century Harvard had emerged as the central cultural establishment among Boston elites. Following the American Civil War, President Charles W. Eliot's long tenure (1869–1909) transformed the college and affiliated professional schools into a modern research university; Harvard was a founding member of the Association of American Universities in 1900. James Bryant Conant led the university through the Great Depression and World War II and began to reform the curriculum and liberalize admissions after the war. The undergraduate college became coeducational after its 1977 merger with Radcliffe College. In the early years the College trained many Puritan ministers.[citation needed] (A 1643 publication said the school's purpose was ""to advance learning and perpetuate it to posterity, dreading to leave an illiterate ministry to the churches when our present ministers shall lie in the dust"".) It offered a classic curriculum on the English university model—​​many leaders in the colony had attended the University of Cambridge—​​but conformed Puritanism. It was never affiliated with any particular denomination, but many of its earliest graduates went on to become clergymen in Congregational and Unitarian churches. Harvard has several athletic facilities, such as the Lavietes Pavilion, a multi-purpose arena and home to the Harvard basketball teams. The Malkin Athletic Center, known as the ""MAC"", serves both as the university's primary recreation facility and as a satellite location for several varsity sports. The five-story building includes two cardio rooms, an Olympic-size swimming pool, a smaller pool for aquaerobics and other activities, a mezzanine, where all types of classes are held, an indoor cycling studio, three weight rooms, and a three-court gym floor to play basketball. The MAC offers personal trainers and specialty classes. It is home to Harvard volleyball, fencing and wrestling. The offices of several of the school's varsity coaches are also in the MAC. The University is organized into eleven separate academic units—ten faculties and the Radcliffe Institute for Advanced Study—with campuses throughout the Boston metropolitan area: its 209-acre (85 ha) main campus is centered on Harvard Yard in Cambridge, approximately 3 miles (5 km) northwest of Boston; the business school and athletics facilities, including Harvard Stadium, are located across the Charles River in the Allston neighborhood of Boston and the medical, dental, and public health schools are in the Longwood Medical Area. Harvard's $37.6 billion financial endowment is the largest of any academic institution. James Bryant Conant (president, 1933–1953) reinvigorated creative scholarship to guarantee its preeminence among research institutions. He saw higher education as a vehicle of opportunity for the talented rather than an entitlement for the wealthy, so Conant devised programs to identify, recruit, and support talented youth. In 1943, he asked the faculty make a definitive statement about what general education ought to be, at the secondary as well as the college level. The resulting Report, published in 1945, was one of the most influential manifestos in the history of American education in the 20th century. The Harvard Business School and many of the university's athletics facilities, including Harvard Stadium, are located on a 358-acre (145 ha) campus opposite the Cambridge campus in Allston. The John W. Weeks Bridge is a pedestrian bridge over the Charles River connecting both campuses. The Harvard Medical School, Harvard School of Dental Medicine, and the Harvard School of Public Health are located on a 21-acre (8.5 ha) campus in the Longwood Medical and Academic Area approximately 3.3 miles (5.3 km) southwest of downtown Boston and 3.3 miles (5.3 km) south of the Cambridge campus. Politics: U.N. Secretary General Ban Ki-moon; American political leaders John Hancock, John Adams, John Quincy Adams, Rutherford B. Hayes, Theodore Roosevelt, Franklin D. Roosevelt, John F. Kennedy, Al Gore, George W. Bush and Barack Obama; Chilean President Sebastián Piñera; Colombian President Juan Manuel Santos; Costa Rican President José María Figueres; Mexican Presidents Felipe Calderón, Carlos Salinas de Gortari and Miguel de la Madrid; Mongolian President Tsakhiagiin Elbegdorj; Peruvian President Alejandro Toledo; Taiwanese President Ma Ying-jeou; Canadian Governor General David Lloyd Johnston; Indian Member of Parliament Jayant Sinha; Albanian Prime Minister Fan S. Noli; Canadian Prime Ministers Mackenzie King and Pierre Trudeau; Greek Prime Minister Antonis Samaras; Israeli Prime Minister Benjamin Netanyahu; former Pakistani Prime Minister Benazir Bhutto; U. S. Secretary of Housing and Urban Development Shaun Donovan; Canadian political leader Michael Ignatieff; Pakistani Members of Provincial Assembly Murtaza Bhutto and Sanam Bhutto; Bangladesh Minister of Finance Abul Maal Abdul Muhith; President of Puntland Abdiweli Mohamed Ali; U.S. Ambassador to the European Union Anthony Luzzatto Gardner. The Harvard University Library System is centered in Widener Library in Harvard Yard and comprises nearly 80 individual libraries holding over 18 million volumes. According to the American Library Association, this makes it the largest academic library in the United States, and one of the largest in the world. Cabot Science Library, Lamont Library, and Widener Library are three of the most popular libraries for undergraduates to use, with easy access and central locations. There are rare books, manuscripts and other special collections throughout Harvard's libraries; Houghton Library, the Arthur and Elizabeth Schlesinger Library on the History of Women in America, and the Harvard University Archives consist principally of rare and unique materials. America's oldest collection of maps, gazetteers, and atlases both old and new is stored in Pusey Library and open to the public. The largest collection of East-Asian language material outside of East Asia is held in the Harvard-Yenching Library. Harvard has purchased tracts of land in Allston, a walk across the Charles River from Cambridge, with the intent of major expansion southward. The university now owns approximately fifty percent more land in Allston than in Cambridge. Proposals to connect the Cambridge campus with the new Allston campus include new and enlarged bridges, a shuttle service and/or a tram. Plans also call for sinking part of Storrow Drive (at Harvard's expense) for replacement with park land and pedestrian access to the Charles River, as well as the construction of bike paths, and buildings throughout the Allston campus. The institution asserts that such expansion will benefit not only the school, but surrounding community, pointing to such features as the enhanced transit infrastructure, possible shuttles open to the public, and park space which will also be publicly accessible. The Harvard Crimson competes in 42 intercollegiate sports in the NCAA Division I Ivy League. Harvard has an intense athletic rivalry with Yale University culminating in The Game, although the Harvard–Yale Regatta predates the football game. This rivalry, though, is put aside every two years when the Harvard and Yale Track and Field teams come together to compete against a combined Oxford University and Cambridge University team, a competition that is the oldest continuous international amateur competition in the world. Harvard's faculty includes scholars such as biologist E. O. Wilson, cognitive scientist Steven Pinker, physicists Lisa Randall and Roy Glauber, chemists Elias Corey, Dudley R. Herschbach and George M. Whitesides, computer scientists Michael O. Rabin and Leslie Valiant, Shakespeare scholar Stephen Greenblatt, writer Louis Menand, critic Helen Vendler, historians Henry Louis Gates, Jr. and Niall Ferguson, economists Amartya Sen, N. Gregory Mankiw, Robert Barro, Stephen A. Marglin, Don M. Wilson III and Martin Feldstein, political philosophers Harvey Mansfield, Baroness Shirley Williams and Michael Sandel, Fields Medalist mathematician Shing-Tung Yau, political scientists Robert Putnam, Joseph Nye, and Stanley Hoffmann, scholar/composers Robert Levin and Bernard Rands, astrophysicist Alyssa A. Goodman, and legal scholars Alan Dershowitz and Lawrence Lessig. Harvard was formed in 1636 by vote of the Great and General Court of the Massachusetts Bay Colony. It was initially called ""New College"" or ""the college at New Towne"". In 1638, the college became home for North America's first known printing press, carried by the ship John of London. In 1639, the college was renamed Harvard College after deceased clergyman John Harvard, who was an alumnus of the University of Cambridge. He had left the school £779 and his library of some 400 books. The charter creating the Harvard Corporation was granted in 1650. Harvard is a large, highly residential research university. The nominal cost of attendance is high, but the University's large endowment allows it to offer generous financial aid packages. It operates several arts, cultural, and scientific museums, alongside the Harvard Library, which is the world's largest academic and private library system, comprising 79 individual libraries with over 18 million volumes. Harvard's alumni include eight U.S. presidents, several foreign heads of state, 62 living billionaires, 335 Rhodes Scholars, and 242 Marshall Scholars. To date, some 150 Nobel laureates, 18 Fields Medalists and 13 Turing Award winners have been affiliated as students, faculty, or staff. Harvard's academic programs operate on a semester calendar beginning in early September and ending in mid-May. Undergraduates typically take four half-courses per term and must maintain a four-course rate average to be considered full-time. In many concentrations, students can elect to pursue a basic program or an honors-eligible program requiring a senior thesis and/or advanced course work. Students graduating in the top 4–5% of the class are awarded degrees summa cum laude, students in the next 15% of the class are awarded magna cum laude, and the next 30% of the class are awarded cum laude. Harvard has chapters of academic honor societies such as Phi Beta Kappa and various committees and departments also award several hundred named prizes annually. Harvard, along with other universities, has been accused of grade inflation, although there is evidence that the quality of the student body and its motivation have also increased. Harvard College reduced the number of students who receive Latin honors from 90% in 2004 to 60% in 2005. Moreover, the honors of ""John Harvard Scholar"" and ""Harvard College Scholar"" will now be given only to the top 5 percent and the next 5 percent of each class. In 1846, the natural history lectures of Louis Agassiz were acclaimed both in New York and on the campus at Harvard College. Agassiz's approach was distinctly idealist and posited Americans' ""participation in the Divine Nature"" and the possibility of understanding ""intellectual existences"". Agassiz's perspective on science combined observation with intuition and the assumption that a person can grasp the ""divine plan"" in all phenomena. When it came to explaining life-forms, Agassiz resorted to matters of shape based on a presumed archetype for his evidence. This dual view of knowledge was in concert with the teachings of Common Sense Realism derived from Scottish philosophers Thomas Reid and Dugald Stewart, whose works were part of the Harvard curriculum at the time. The popularity of Agassiz's efforts to ""soar with Plato"" probably also derived from other writings to which Harvard students were exposed, including Platonic treatises by Ralph Cudworth, John Norrisand, in a Romantic vein, Samuel Coleridge. The library records at Harvard reveal that the writings of Plato and his early modern and Romantic followers were almost as regularly read during the 19th century as those of the ""official philosophy"" of the more empirical and more deistic Scottish school. Harvard has been highly ranked by many university rankings. In particular, it has consistently topped the Academic Ranking of World Universities (ARWU) since 2003, and the THE World Reputation Rankings since 2011, when the first time such league tables were published. When the QS and Times were published in partnership as the THE-QS World University Rankings during 2004-2009, Harvard had also been regarded the first in every year. The University's undergraduate program has been continuously among the top two in the U.S. News & World Report. In 2014, Harvard topped the University Ranking by Academic Performance (URAP). It was ranked 8th on the 2013-2014 PayScale College Salary Report and 14th on the 2013 PayScale College Education Value Rankings. From a poll done by The Princeton Review, Harvard is the second most commonly named ""dream college"", both for students and parents in 2013, and was the first nominated by parents in 2009. In 2011, the Mines ParisTech : Professional Ranking World Universities ranked Harvard 1st university in the world in terms of number of alumni holding CEO position in Fortune Global 500 companies. Harvard operates several arts, cultural, and scientific museums. The Harvard Art Museums comprises three museums. The Arthur M. Sackler Museum includes collections of ancient, Asian, Islamic and later Indian art, the Busch-Reisinger Museum, formerly the Germanic Museum, covers central and northern European art, and the Fogg Museum of Art, covers Western art from the Middle Ages to the present emphasizing Italian early Renaissance, British pre-Raphaelite, and 19th-century French art. The Harvard Museum of Natural History includes the Harvard Mineralogical Museum, Harvard University Herbaria featuring the Blaschka Glass Flowers exhibit, and the Museum of Comparative Zoology. Other museums include the Carpenter Center for the Visual Arts, designed by Le Corbusier, housing the film archive, the Peabody Museum of Archaeology and Ethnology, specializing in the cultural history and civilizations of the Western Hemisphere, and the Semitic Museum featuring artifacts from excavations in the Middle East. Harvard's athletic rivalry with Yale is intense in every sport in which they meet, coming to a climax each fall in the annual football meeting, which dates back to 1875 and is usually called simply ""The Game"". While Harvard's football team is no longer one of the country's best as it often was a century ago during football's early days (it won the Rose Bowl in 1920), both it and Yale have influenced the way the game is played. In 1903, Harvard Stadium introduced a new era into football with the first-ever permanent reinforced concrete stadium of its kind in the country. The stadium's structure actually played a role in the evolution of the college game. Seeking to reduce the alarming number of deaths and serious injuries in the sport, Walter Camp (former captain of the Yale football team), suggested widening the field to open up the game. But the stadium was too narrow to accommodate a wider playing surface. So, other steps had to be taken. Camp would instead support revolutionary new rules for the 1906 season. These included legalizing the forward pass, perhaps the most significant rule change in the sport's history. Harvard has the largest university endowment in the world. As of September 2011[update], it had nearly regained the loss suffered during the 2008 recession. It was worth $32 billion in 2011, up from $28 billion in September 2010 and $26 billion in 2009. It suffered about 30% loss in 2008-09. In December 2008, Harvard announced that its endowment had lost 22% (approximately $8 billion) from July to October 2008, necessitating budget cuts. Later reports suggest the loss was actually more than double that figure, a reduction of nearly 50% of its endowment in the first four months alone. Forbes in March 2009 estimated the loss to be in the range of $12 billion. One of the most visible results of Harvard's attempt to re-balance its budget was their halting of construction of the $1.2 billion Allston Science Complex that had been scheduled to be completed by 2011, resulting in protests from local residents. As of 2012[update], Harvard University had a total financial aid reserve of $159 million for students, and a Pell Grant reserve of $4.093 million available for disbursement."
Madrasa,"Much of the study in the madrasah college centred on examining whether certain opinions of law were orthodox. This scholarly process of ""determining orthodoxy began with a question which the Muslim layman, called in that capacity mustaftī, presented to a jurisconsult, called mufti, soliciting from him a response, called fatwa, a legal opinion (the religious law of Islam covers civil as well as religious matters). The mufti (professor of legal opinions) took this question, studied it, researched it intensively in the sacred scriptures, in order to find a solution to it. This process of scholarly research was called ijtihād, literally, the exertion of one's efforts to the utmost limit."" The first institute of madrasa education was at the estate of Hazrat Zaid bin Arkam near a hill called Safa, where Hazrat Muhammad was the teacher and the students were some of his followers.[citation needed] After Hijrah (migration) the madrasa of ""Suffa"" was established in Madina on the east side of the Al-Masjid an-Nabawi mosque. Hazrat 'Ubada bin Samit was appointed there by Hazrat Muhammad as teacher and among the students.[citation needed] In the curriculum of the madrasa, there were teachings of The Qur'an,The Hadith, fara'iz, tajweed, genealogy, treatises of first aid, etc. There were also trainings of horse-riding, art of war, handwriting and calligraphy, athletics and martial arts. The first part of madrasa based education is estimated from the first day of ""nabuwwat"" to the first portion of the ""Umaiya"" caliphate.[citation needed] The Arabic term ijāzat al-tadrīs was awarded to Islamic scholars who were qualified to teach. According to Makdisi, the Latin title licentia docendi 'licence to teach' in the European university may have been a translation of the Arabic, but the underlying concept was very different. A significant difference between the ijāzat al-tadrīs and the licentia docendi was that the former was awarded by the individual scholar-teacher, while the latter was awarded by the chief official of the university, who represented the collective faculty, rather than the individual scholar-teacher. The first Madressa established in North America, Al-Rashid Islamic Institute, was established in Cornwall, Ontario in 1983 and has graduates who are Hafiz (Quran) and Ulama. The seminary was established by Mazhar Alam under the direction of his teacher the leading Indian Tablighi scholar Muhammad Zakariya Kandhlawi and focuses on the traditional Hanafi school of thought and shuns Salafist / Wahabi teachings. Due to its proximity to the US border city of Messina the school has historically had a high ratio of US students. Their most prominent graduate Shaykh Muhammad Alshareef completed his Hifz in the early 1990s then went on to deviate from his traditional roots and form the Salafist organization the AlMaghrib Institute. However, the classification of madaris as ""universities"" is disputed on the question of understanding of each institution on its own terms. In madaris, the ijāzahs were only issued in one field, the Islamic religious law of sharīʻah, and in no other field of learning. Other academic subjects, including the natural sciences, philosophy and literary studies, were only treated ""ancillary"" to the study of the Sharia. For example, a natural science like astronomy was only studied (if at all) to supply religious needs, like the time for prayer. This is why Ptolemaic astronomy was considered adequate, and is still taught in some modern day madaris. The Islamic law undergraduate degree from al-Azhar, the most prestigious madrasa, was traditionally granted without final examinations, but on the basis of the students' attentive attendance to courses. In contrast to the medieval doctorate which was granted by the collective authority of the faculty, the Islamic degree was not granted by the teacher to the pupil based on any formal criteria, but remained a ""personal matter, the sole prerogative of the person bestowing it; no one could force him to give one"". ""The first Ottoman Medrese was created in İznik in 1331 and most Ottoman medreses followed the traditions of Sunni Islam."" ""When an Ottoman sultan established a new medrese, he would invite scholars from the Islamic world—for example, Murad II brought scholars from Persia, such as ʻAlāʼ al-Dīn and Fakhr al-Dīn who helped enhance the reputation of the Ottoman medrese"". This reveals that the Islamic world was interconnected in the early modern period as they travelled around to other Islamic states exchanging knowledge. This sense that the Ottoman Empire was becoming modernised through globalization is also recognised by Hamadeh who says: ""Change in the eighteenth century as the beginning of a long and unilinear march toward westernisation reflects the two centuries of reformation in sovereign identity."" İnalcık also mentions that while scholars from for example Persia travelled to the Ottomans in order to share their knowledge, Ottomans travelled as well to receive education from scholars of these Islamic lands, such as Egypt, Persia and Turkestan. Hence, this reveals that similar to today's modern world, individuals from the early modern society travelled abroad to receive education and share knowledge and that the world was more interconnected than it seems. Also, it reveals how the system of ""schooling"" was also similar to today's modern world where students travel abroad to different countries for studies. Examples of Ottoman madaris are the ones built by Mehmed the Conqueror. He built eight madaris that were built ""on either side of the mosque where there were eight higher madaris for specialised studies and eight lower medreses, which prepared students for these."" The fact that they were built around, or near mosques reveals the religious impulses behind madrasa building and it reveals the interconnectedness between institutions of learning and religion. The students who completed their education in the lower medreses became known as danismends. This reveals that similar to the education system today, the Ottomans' educational system involved different kinds of schools attached to different kinds of levels. For example, there were lower madaris and specialised ones, and for one to get into the specialised area meant that he had to complete the classes in the lower one in order to adequately prepare himself for higher learning. al-Qarawīyīn University in Fez, Morocco is recognised by many historians as the oldest degree-granting university in the world, having been founded in 859 by Fatima al-Fihri. While the madrasa college could also issue degrees at all levels, the jāmiʻahs (such as al-Qarawīyīn and al-Azhar University) differed in the sense that they were larger institutions, more universal in terms of their complete source of studies, had individual faculties for different subjects, and could house a number of mosques, madaris, and other institutions within them. Such an institution has thus been described as an ""Islamic university"". Madaris were largely centred on the study of fiqh (Islamic jurisprudence). The ijāzat al-tadrīs wa-al-iftāʼ (""licence to teach and issue legal opinions"") in the medieval Islamic legal education system had its origins in the 9th century after the formation of the madhāhib (schools of jurisprudence). George Makdisi considers the ijāzah to be the origin of the European doctorate. However, in an earlier article, he considered the ijāzah to be of ""fundamental difference"" to the medieval doctorate, since the former was awarded by an individual teacher-scholar not obliged to follow any formal criteria, whereas the latter was conferred on the student by the collective authority of the faculty. To obtain an ijāzah, a student ""had to study in a guild school of law, usually four years for the basic undergraduate course"" and ten or more years for a post-graduate course. The ""doctorate was obtained after an oral examination to determine the originality of the candidate's theses"", and to test the student's ""ability to defend them against all objections, in disputations set up for the purpose."" These were scholarly exercises practised throughout the student's ""career as a graduate student of law."" After students completed their post-graduate education, they were awarded ijazas giving them the status of faqīh 'scholar of jurisprudence', muftī 'scholar competent in issuing fatwās', and mudarris 'teacher'. During the rule of the Fatimid and Mamluk dynasties and their successor states in the medieval Middle East, many of the ruling elite founded madaris through a religious endowment known as the waqf. Not only was the madrasa a potent symbol of status but it was an effective means of transmitting wealth and status to their descendants. Especially during the Mamlūk period, when only former slaves could assume power, the sons of the ruling Mamlūk elite were unable to inherit. Guaranteed positions within the new madaris thus allowed them to maintain status. Madaris built in this period include the Mosque-Madrasah of Sultan Ḥasan in Cairo. Madrasa (Arabic: مدرسة‎, madrasah, pl. مدارس, madāris, Turkish: Medrese) is the Arabic word for any type of educational institution, whether secular or religious (of any religion). The word is variously transliterated madrasah, madarasaa, medresa, madrassa, madraza, medrese, etc. In the West, the word usually refers to a specific type of religious school or college for the study of the Islamic religion, though this may not be the only subject studied. Not all students in madaris are Muslims; there is also a modern curriculum. People of all ages attend, and many often move on to becoming imams.[citation needed] The certificate of an ʻālim, for example, requires approximately twelve years of study.[citation needed] A good number of the ḥuffāẓ (plural of ḥāfiẓ) are the product of the madaris. The madaris also resemble colleges, where people take evening classes and reside in dormitories. An important function of the madaris is to admit orphans and poor children in order to provide them with education and training. Madaris may enroll female students; however, they study separately from the men.[citation needed] At the beginning of the Caliphate or Islamic Empire, the reliance on courts initially confined sponsorship and scholarly activities to major centres. Within several centuries, the development of Muslim educational institutions such as the madrasah and masjid eventually introduced such activities to provincial towns and dispersed them across the Islamic legal schools and Sufi orders. In addition to religious subjects, they also taught the ""rational sciences,"" as varied as mathematics, astronomy, astrology, geography, alchemy, philosophy, magic, and occultism, depending on the curriculum of the specific institution in question. The madaris, however, were not centres of advanced scientific study; scientific advances in Islam were usually carried out by scholars working under the patronage of royal courts. During this time,[when?] the Caliphate experienced a growth in literacy, having the highest literacy rate of the Middle Ages, comparable to classical Athens' literacy in antiquity but on a much larger scale. The emergence of the maktab and madrasa institutions played a fundamental role in the relatively high literacy rates of the medieval Islamic world. During its formative period, the term madrasah referred to a higher education institution, whose curriculum initially included only the ""religious sciences"", whilst philosophy and the secular sciences were often excluded. The curriculum slowly began to diversify, with many later madaris teaching both the religious and the ""secular sciences"", such as logic, mathematics and philosophy. Some madaris further extended their curriculum to history, politics, ethics, music, metaphysics, medicine, astronomy and chemistry. The curriculum of a madrasah was usually set by its founder, but most generally taught both the religious sciences and the physical sciences. Madaris were established throughout the Islamic world, examples being the 9th century University of al-Qarawiyyin, the 10th century al-Azhar University (the most famous), the 11th century Niẓāmīyah, as well as 75 madaris in Cairo, 51 in Damascus and up to 44 in Aleppo between 1155 and 1260. Many more were also established in the Andalusian cities of Córdoba, Seville, Toledo, Granada (Madrasah of Granada), Murcia, Almería, Valencia and Cádiz during the Caliphate of Córdoba. In the medieval Islamic world, an elementary school was known as a maktab, which dates back to at least the 10th century. Like madaris (which referred to higher education), a maktab was often attached to an endowed mosque. In the 11th century, the famous Persian Islamic philosopher and teacher Ibn Sīnā (known as Avicenna in the West), in one of his books, wrote a chapter about the maktab entitled ""The Role of the Teacher in the Training and Upbringing of Children,"" as a guide to teachers working at maktab schools. He wrote that children can learn better if taught in classes instead of individual tuition from private tutors, and he gave a number of reasons for why this is the case, citing the value of competition and emulation among pupils, as well as the usefulness of group discussions and debates. Ibn Sīnā described the curriculum of a maktab school in some detail, describing the curricula for two stages of education in a maktab school. In India the majority of these schools follow the Hanafi school of thought. The religious establishment forms part of the mainly two large divisions within the country, namely the Deobandis, who dominate in numbers (of whom the Darul Uloom Deoband constitutes one of the biggest madaris) and the Barelvis, who also make up a sizeable portion (Sufi-oriented). Some notable establishments include: Al Jamiatul Ashrafia, Mubarakpur, Manzar Islam Bareilly, Jamia Nizamdina New Delhi, Jamia Nayeemia Muradabad which is one of the largest learning centres for the Barelvis. The HR[clarification needed] ministry of the government of India has recently[when?] declared that a Central Madrasa Board would be set up. This will enhance the education system of madaris in India. Though the madaris impart Quranic education mainly, efforts are on to include Mathematics, Computers and science in the curriculum. In July 2015, the state government of Maharashtra created a stir de-recognised madrasa education, receiving critisicm from several political parties with the NCP accusing the ruling BJP of creating Hindu-Muslim friction in the state, and Kamal Farooqui of the All India Muslim Personal Law Board saying it was ""ill-designed""  Prior to the 12th century, women accounted for less than one percent of the world’s Islamic scholars. However, al-Sakhawi and Mohammad Akram Nadwi have since found evidence of over 8,000 female scholars since the 15th century. al-Sakhawi devotes an entire volume of his 12-volume biographical dictionary al-Ḍawʾ al-lāmiʻ to female scholars, giving information on 1,075 of them. More recently, the scholar Mohammad Akram Nadwi, currently a researcher from the Oxford Centre for Islamic Studies, has written 40 volumes on the muḥaddithāt (the women scholars of ḥadīth), and found at least 8,000 of them. According to the Sunni scholar Ibn ʻAsākir in the 12th century, there were opportunities for female education in the medieval Islamic world, writing that women could study, earn ijazahs (academic degrees), and qualify as scholars and teachers. This was especially the case for learned and scholarly families, who wanted to ensure the highest possible education for both their sons and daughters. Ibn ʻAsakir had himself studied under 80 different female teachers in his time. Female education in the Islamic world was inspired by Muhammad's wives, such as Khadijah, a successful businesswoman. According to a hadith attributed to Muhammad, he praised the women of Medina because of their desire for religious knowledge: Western commentators post-9/11 often perceive madaris as places of radical revivalism with a connotation of anti-Americanism and radical extremism, frequently associated in the Western press with Wahhabi attitudes toward non-Muslims. In Arabic the word madrasa simply means ""school"" and does not imply a political or religious affiliation, radical or otherwise. Madaris have varied curricula, and are not all religious. Some madaris in India, for example, have a secularised identity. Although early madaris were founded primarily to gain ""knowledge of God"" they also taught subjects such as mathematics and poetry. For example, in the Ottoman Empire, ""Madrasahs had seven categories of sciences that were taught, such as: styles of writing, oral sciences like the Arabic language, grammar, rhetoric, and history and intellectual sciences, such as logic."" This is similar to the Western world, in which universities began as institutions of the Catholic church. Al-Azhar University, founded in Cairo, Egypt in 975 by the Ismaʻīlī Shīʻī Fatimid dynasty as a jāmiʻah, had individual faculties for a theological seminary, Islamic law and jurisprudence, Arabic grammar, Islamic astronomy, early Islamic philosophy and logic in Islamic philosophy. The postgraduate doctorate in law was only obtained after ""an oral examination to determine the originality of the candidate's theses"", and to test the student's ""ability to defend them against all objections, in disputations set up for the purpose."" ‘Abd al-Laṭīf al-Baghdādī also delivered lectures on Islamic medicine at al-Azhar, while Maimonides delivered lectures on medicine and astronomy there during the time of Saladin. Another early jāmiʻah was the Niẓāmīyah of Baghdād (founded 1091), which has been called the ""largest university of the Medieval world."" Mustansiriya University, established by the ʻAbbāsid caliph al-Mustanṣir in 1233, in addition to teaching the religious subjects, offered courses dealing with philosophy, mathematics and the natural sciences. From around 750, during the Abbasid Caliphate, women “became renowned for their brains as well as their beauty”. In particular, many well known women of the time were trained from childhood in music, dancing and poetry. Mahbuba was one of these. Another feminine figure to be remembered for her achievements was Tawaddud, ""a slave girl who was said to have been bought at great cost by Hārūn al-Rashīd because she had passed her examinations by the most eminent scholars in astronomy, medicine, law, philosophy, music, history, Arabic grammar, literature, theology and chess"". Moreover, among the most prominent feminine figures was Shuhda who was known as ""the Scholar"" or ""the Pride of Women"" during the 12th century in Baghdad. Despite the recognition of women's aptitudes during the Abbasid dynasty, all these came to an end in Iraq with the sack of Baghdad in 1258. There is disagreement whether madaris ever became universities. Scholars like Arnold H. Green and Seyyed Hossein Nasr have argued that starting in the 10th century, some medieval Islamic madaris indeed became universities. George Makdisi and others, however, argue that the European university has no parallel in the medieval Islamic world. Darleen Pryds questions this view, pointing out that madaris and European universities in the Mediterranean region shared similar foundations by princely patrons and were intended to provide loyal administrators to further the rulers' agenda. Other scholars regard the university as uniquely European in origin and characteristics. In 2004, madaris were mainstreamed in 16 Regions nationwide, primarily in Muslim-majority areas in Mindanao under the auspices of the Department of Education (DepEd). The DepEd adopted Department Order No. 51, which instituted Arabic-language and Islamic Values instruction for Muslim children in state schools, and authorised implementation of the Standard Madrasa Curriculum (SMC) in private-run madaris. While there are state-recognised Islamic schools, such as Ibn Siena Integrated School in the Islamic City of Marawi, Sarang Bangun LC in Zamboanga and SMIE in Jolo, their Islamic studies programmes initially varied in application and content. In Singapore, madrasahs are private schools which are overseen by Majlis Ugama Islam Singapura (MUIS, English: Islamic Religious Council of Singapore). There are six Madrasahs in Singapore, catering to students from Primary 1 to Secondary 4. Four Madrasahs are coeducational and two are for girls. Students take a range of Islamic Studies subjects in addition to mainstream MOE curriculum subjects and sit for the PSLE and GCE 'O' Levels like their peers. In 2009, MUIS introduced the ""Joint Madrasah System"" (JMS), a joint collaboration of Madrasah Al-Irsyad Al-Islamiah primary school and secondary schools Madrasah Aljunied Al-Islamiah (offering the ukhrawi, or religious stream) and Madrasah Al-Arabiah Al-Islamiah (offering the academic stream). The JMS aims to introduce the International Baccalaureate (IB) programme into the Madrasah Al-Arabiah Al-Islamiah by 2019. Students attending a madrasah are required to wear the traditional Malay attire, including the songkok for boys and tudong for girls, in contrast to mainstream government schools which ban religious headgear as Singapore is officially a secular state. For students who wish to attend a mainstream school, they may opt to take classes on weekends at the madrasah instead of enrolling full-time. Although Ottoman madaris had a number of different branches of study, such as calligraphic sciences, oral sciences, and intellectual sciences, they primarily served the function of an Islamic centre for spiritual learning. ""The goal of all knowledge and in particular, of the spiritual sciences is knowledge of God."" Religion, for the most part, determines the significance and importance of each science. As İnalcık mentions: ""Those which aid religion are good and sciences like astrology are bad."" However, even though mathematics, or studies in logic were part of the madrasa's curriculum, they were all centred around religion. Even mathematics had a religious impulse behind its teachings. ""The Ulema of the Ottoman medreses held the view that hostility to logic and mathematics was futile since these accustomed the mind to correct thinking and thus helped to reveal divine truths"" – key word being ""divine"". İnalcık also mentions that even philosophy was only allowed to be studied so that it helped to confirm the doctrines of Islam."" Hence, madaris – schools were basically religious centres for religious teachings and learning in the Ottoman world. Although scholars such as Goffman have argued that the Ottomans were highly tolerant and lived in a pluralistic society, it seems that schools that were the main centres for learning were in fact heftily religious and were not religiously pluralistic, but centred around Islam. Similarly, in Europe ""Jewish children learned the Hebrew letters and texts of basic prayers at home, and then attended a school organised by the synagogue to study the Torah."" Wiesner-Hanks also says that Protestants also wanted to teach ""proper religious values."" This shows that in the early modern period, Ottomans and Europeans were similar in their ideas about how schools should be managed and what they should be primarily focused on. Thus, Ottoman madaris were very similar to present day schools in the sense that they offered a wide range of studies; however, these studies, in their ultimate objective, aimed to further solidify and consolidate Islamic practices and theories. As with any other country during the Early Modern Period, such as Italy and Spain in Europe, the Ottoman social life was interconnected with the medrese. Medreses were built in as part of a Mosque complex where many programmes, such as aid to the poor through soup kitchens, were held under the infrastructure of a mosque, which reveals the interconnectedness of religion and social life during this period. ""The mosques to which medreses were attached, dominated the social life in Ottoman cities."" Social life was not dominated by religion only in the Muslim world of the Ottoman Empire; it was also quite similar to the social life of Europe during this period. As Goffman says: ""Just as mosques dominated social life for the Ottomans, churches and synagogues dominated life for the Christians and Jews as well."" Hence, social life and the medrese were closely linked, since medreses taught many curricula, such as religion, which highly governed social life in terms of establishing orthodoxy. ""They tried moving their developing state toward Islamic orthodoxy."" Overall, the fact that mosques contained medreses comes to show the relevance of education to religion in the sense that education took place within the framework of religion and religion established social life by trying to create a common religious orthodoxy. Hence, medreses were simply part of the social life of society as students came to learn the fundamentals of their societal values and beliefs. In Southeast Asia, Muslim students have a choice of attending a secular government or an Islamic school. Madaris or Islamic schools are known as Sekolah Agama (Malay: religious school) in Malaysia and Indonesia, โรงเรียนศาสนาอิสลาม (Thai: school of Islam) in Thailand and madaris in the Philippines. In countries where Islam is not the majority or state religion, Islamic schools are found in regions such as southern Thailand (near the Thai-Malaysian border) and the southern Philippines in Mindanao, where a significant Muslim population can be found. Nevertheless, Makdisi has asserted that the European university borrowed many of its features from the Islamic madrasa, including the concepts of a degree and doctorate. Makdisi and Hugh Goddard have also highlighted other terms and concepts now used in modern universities which most likely have Islamic origins, including ""the fact that we still talk of professors holding the 'Chair' of their subject"" being based on the ""traditional Islamic pattern of teaching where the professor sits on a chair and the students sit around him"", the term 'academic circles' being derived from the way in which Islamic students ""sat in a circle around their professor"", and terms such as ""having 'fellows', 'reading' a subject, and obtaining 'degrees', can all be traced back"" to the Islamic concepts of aṣḥāb ('companions, as of Muhammad'), qirāʼah ('reading aloud the Qur'an') and ijāzah ('licence [to teach]') respectively. Makdisi has listed eighteen such parallels in terminology which can be traced back to their roots in Islamic education. Some of the practices now common in modern universities which Makdisi and Goddard trace back to an Islamic root include ""practices such as delivering inaugural lectures, wearing academic robes, obtaining doctorates by defending a thesis, and even the idea of academic freedom are also modelled on Islamic custom."" The Islamic scholarly system of fatwá and ijmāʻ, meaning opinion and consensus respectively, formed the basis of the ""scholarly system the West has practised in university scholarship from the Middle Ages down to the present day."" According to Makdisi and Goddard, ""the idea of academic freedom"" in universities was also ""modelled on Islamic custom"" as practised in the medieval Madrasa system from the 9th century. Islamic influence was ""certainly discernible in the foundation of the first deliberately planned university"" in Europe, the University of Naples Federico II founded by Frederick II, Holy Roman Emperor in 1224. However, all of these facets of medieval university life are considered by standard scholarship to be independent medieval European developments with no tracable Islamic influence. Generally, some reviewers have pointed out the strong inclination of Makdisi of overstating his case by simply resting on ""the accumulation of close parallels"", but all the while failing to point to convincing channels of transmission between the Muslim and Christian world. Norman Daniel points out that the Arab equivalent of the Latin disputation, the taliqa, was reserved for the ruler's court, not the madrasa, and that the actual differences between Islamic fiqh and medieval European civil law were profound. The taliqa only reached Islamic Spain, the only likely point of transmission, after the establishment of the first medieval universities. In fact, there is no Latin translation of the taliqa and, most importantly, no evidence of Latin scholars ever showing awareness of Arab influence on the Latin method of disputation, something they would have certainly found noteworthy. Rather, it was the medieval reception of the Greek Organon which set the scholastic sic et non in motion. Daniel concludes that resemblances in method had more to with the two religions having ""common problems: to reconcile the conflicting statements of their own authorities, and to safeguard the data of revelation from the impact of Greek philosophy""; thus Christian scholasticism and similar Arab concepts should be viewed in terms of a parallel occurrence, not of the transmission of ideas from one to the other, a view shared by Hugh Kennedy. As Muslim institutions of higher learning, the madrasa had the legal designation of waqf. In central and eastern Islamic lands, the view that the madrasa, as a charitable endowment, will remain under the control of the donor (and their descendent), resulted in a ""spurt"" of establishment of madaris in the 11th and 12th centuries. However, in Western Islamic lands, where the Maliki views prohibited donors from controlling their endowment, madaris were not as popular. Unlike the corporate designation of Western institutions of higher learning, the waqf designation seemed to have led to the exclusion of non-orthodox religious subjects such a philosophy and natural science from the curricula. The madrasa of al-Qarawīyīn, one of the two surviving madaris that predate the founding of the earliest medieval universities and are thus claimed to be the ""first universities"" by some authors, has acquired official university status as late as 1947. The other, al-Azhar, did acquire this status in name and essence only in the course of numerous reforms during the 19th and 20th century, notably the one of 1961 which introduced non-religious subjects to its curriculum, such as economics, engineering, medicine, and agriculture. It should also be noted that many medieval universities were run for centuries as Christian cathedral schools or monastic schools prior to their formal establishment as universitas scholarium; evidence of these immediate forerunners of the university dates back to the 6th century AD, thus well preceding the earliest madaris. George Makdisi, who has published most extensively on the topic concludes in his comparison between the two institutions: The term ""Islamic education"" means education in the light of Islam itself, which is rooted in the teachings of the Quran - holy book of Muslims. Islamic education and Muslim education are not the same. Because Islamic education has epistemological integration which is founded on Tawhid - Oneness or monotheism. For details Read ""A Qur’anic Methodology for Integrating Knowledge and Education: Implications for Malaysia’s Islamic Education Strategy"" written Tareq M Zayed  and ""Knowledge of Shariah and Knowledge to Manage ‘Self’ and ‘System’: Integration of Islamic Epistemology with the Knowledge and Education"" authored by Tareq M Zayed Ibn Sīnā refers to the secondary education stage of maktab schooling as a period of specialisation when pupils should begin to acquire manual skills, regardless of their social status. He writes that children after the age of 14 should be allowed to choose and specialise in subjects they have an interest in, whether it was reading, manual skills, literature, preaching, medicine, geometry, trade and commerce, craftsmanship, or any other subject or profession they would be interested in pursuing for a future career. He wrote that this was a transitional stage and that there needs to be flexibility regarding the age in which pupils graduate, as the student's emotional development and chosen subjects need to be taken into account. Today, the system of Arabic and Islamic education has grown and further integrated with Kerala government administration. In 2005, an estimated 6,000 Muslim Arabic teachers taught in Kerala government schools, with over 500,000 Muslim students. State-appointed committees, not private mosques or religious scholars outside the government, determine the curriculum and accreditation of new schools and colleges. Primary education in Arabic and Islamic studies is available to Kerala Muslims almost entirely in after-school madrasa programs - sharply unlike full-time madaris common in north India, which may replace formal schooling. Arabic colleges (over eleven of which exist within the state-run University of Calicut and the Kannur University) provide B.A. and Masters' level degrees. At all levels, instruction is co-educational, with many women instructors and professors. Islamic education boards are independently run by the following organizations, accredited by the Kerala state government: Samastha Kerala Islamic Education Board, Kerala Nadvathul Mujahideen, Jamaat-e-Islami Hind, and Jamiat Ulema-e-Hind. However, in English, the term madrasah usually refers to the specifically Islamic institutions. A typical Islamic school usually offers two courses of study: a ḥifẓ course teaching memorization of the Qur'an (the person who commits the entire Qurʼan to memory is called a ḥāfiẓ); and an ʻālim course leading the candidate to become an accepted scholar in the community. A regular curriculum includes courses in Arabic, tafsir (Qur'anic interpretation), sharīʻah (Islamic law), hadiths (recorded sayings and deeds of Muhammad), mantiq (logic), and Muslim history. In the Ottoman Empire, during the Early Modern Period, the study of hadiths was introduced by Süleyman I. Depending on the educational demands, some madaris also offer additional advanced courses in Arabic literature, English and other foreign languages, as well as science and world history. Ottoman madaris along with religious teachings also taught ""styles of writing, grammary, syntax, poetry, composition, natural sciences, political sciences, and etiquette."" Medievalist specialists who define the university as a legally autonomous corporation disagree with the term ""university"" for the Islamic madaris and jāmi‘ahs because the medieval university (from Latin universitas) was structurally different, being a legally autonomous corporation rather than a waqf institution like the madrasa and jāmiʻah. Despite the many similarities, medieval specialists have coined the term ""Islamic college"" for madrasa and jāmiʻah to differentiate them from the legally autonomous corporations that the medieval European universities were. In a sense, the madrasa resembles a university college in that it has most of the features of a university, but lacks the corporate element. Toby Huff summarises the difference as follows: The word madrasah derives from the triconsonantal Semitic root د-ر-س D-R-S 'to learn, study', through the wazn (form/stem) مفعل(ة)‎; mafʻal(ah), meaning ""a place where something is done"". Therefore, madrasah literally means ""a place where learning and studying take place"". The word is also present as a loanword with the same innocuous meaning in many Arabic-influenced languages, such as: Urdu, Bengali, Hindi, Persian, Turkish, Azeri, Kurdish, Indonesian, Malay and Bosnian / Croatian. In the Arabic language, the word مدرسة madrasah simply means the same as school does in the English language, whether that is private, public or parochial school, as well as for any primary or secondary school whether Muslim, non-Muslim, or secular. Unlike the use of the word school in British English, the word madrasah more closely resembles the term school in American English, in that it can refer to a university-level or post-graduate school as well as to a primary or secondary school. For example, in the Ottoman Empire during the Early Modern Period, madaris had lower schools and specialised schools where the students became known as danişmends. The usual Arabic word for a university, however, is جامعة (jāmiʻah). The Hebrew cognate midrasha also connotes the meaning of a place of learning; the related term midrash literally refers to study or learning, but has acquired mystical and religious connotations."
Great_Plains,"To allow for agricultural development of the Great Plains and house a growing population, the US passed the Homestead Acts of 1862: it allowed a settler to claim up to 160 acres (65 ha) of land, provided that he lived on it for a period of five years and cultivated it. The provisions were expanded under the Kinkaid Act of 1904 to include a homestead of an entire section. Hundreds of thousands of people claimed such homesteads, sometimes building sod houses out of the very turf of their land. Many of them were not skilled dryland farmers and failures were frequent. Much of the Plains were settled during relatively wet years. Government experts did not understand how farmers should cultivate the prairies and gave advice counter to what would have worked[citation needed]. Germans from Russia who had previously farmed, under similar circumstances, in what is now Ukraine were marginally more successful than other homesteaders. The Dominion Lands Act of 1871 served a similar function for establishing homesteads on the prairies in Canada. The North American Environmental Atlas, produced by the Commission for Environmental Cooperation, a NAFTA agency composed of the geographical agencies of the Mexican, American, and Canadian governments uses the ""Great Plains"" as an ecoregion synonymous with predominant prairies and grasslands rather than as physiographic region defined by topography. The Great Plains ecoregion includes five sub-regions: Temperate Prairies, West-Central Semi-Arid Prairies, South-Central Semi-Arid Prairies, Texas Louisiana Coastal Plains, and Tamaulipus-Texas Semi-Arid Plain, which overlap or expand upon other Great Plains designations. Much of the Great Plains became open range, or rangeland where cattle roamed free, hosting ranching operations where anyone was theoretically free to run cattle. In the spring and fall, ranchers held roundups where their cowboys branded new calves, treated animals and sorted the cattle for sale. Such ranching began in Texas and gradually moved northward. In 1866-95, cowboys herded 10 million cattle north to rail heads such as Dodge City, Kansas and Ogallala, Nebraska; from there, cattle were shipped eastward. The 100th meridian roughly corresponds with the line that divides the Great Plains into an area that receive 20 inches (510 millimetres) or more of rainfall per year and an area that receives less than 20 in (510 mm). In this context, the High Plains, as well as Southern Alberta, south-western Saskatchewan and Eastern Montana are mainly semi hot steppe land and are generally characterised by rangeland or marginal farmland. The region (especially the High Plains) is periodically subjected to extended periods of drought; high winds in the region may then generate devastating dust storms. The eastern Great Plains near the eastern boundary falls in the humid subtropical climate zone in the southern areas, and the northern and central areas fall in the humid continental climate. From the 1950s on, many areas of the Great Plains have become productive crop-growing areas because of extensive irrigation on large landholdings. The United States is a major exporter of agricultural products. The southern portion of the Great Plains lies over the Ogallala Aquifer, a huge underground layer of water-bearing strata dating from the last ice age. Center pivot irrigation is used extensively in drier sections of the Great Plains, resulting in aquifer depletion at a rate that is greater than the ground's ability to recharge. The railroads opened up the Great Plains for settlement, for now it was possible to ship wheat and other crops at low cost to the urban markets in the East, and Europe. Homestead land was free for American settlers. Railroads sold their land at cheap rates to immigrants in expectation they would generate traffic as soon as farms were established. Immigrants poured in, especially from Germany and Scandinavia. On the plains, very few single men attempted to operate a farm or ranch by themselves; they clearly understood the need for a hard-working wife, and numerous children, to handle the many chores, including child-rearing, feeding and clothing the family, managing the housework, feeding the hired hands, and, especially after the 1930s, handling paperwork and financial details. During the early years of settlement, farm women played an integral role in assuring family survival by working outdoors. After approximately one generation, women increasingly left the fields, thus redefining their roles within the family. New technology including sewing and washing machines encouraged women to turn to domestic roles. The scientific housekeeping movement, promoted across the land by the media and government extension agents, as well as county fairs which featured achievements in home cookery and canning, advice columns for women regarding farm bookkeeping, and home economics courses in the schools. The Great Plains is the broad expanse of flat land (a plain), much of it covered in prairie, steppe and grassland, that lies west of the Mississippi River tallgrass prairie states and east of the Rocky Mountains in the United States and Canada. This area covers parts, but not all, of the states of Colorado, Kansas, Montana, Nebraska, New Mexico, North Dakota, Oklahoma, South Dakota, Texas, and Wyoming, and the Canadian provinces of Alberta, Manitoba and Saskatchewan. The region is known for supporting extensive cattle ranching and dry farming. The term ""Great Plains"", for the region west of about the 96th or 98th meridian and east of the Rocky Mountains, was not generally used before the early 20th century. Nevin Fenneman's 1916 study, Physiographic Subdivision of the United States, brought the term Great Plains into more widespread usage. Before that the region was almost invariably called the High Plains, in contrast to the lower Prairie Plains of the Midwestern states. Today the term ""High Plains"" is used for a subregion of the Great Plains. With the arrival of Francisco Vázquez de Coronado, a Spanish conquistador, the first recorded history of encounter between Europeans and Native Americans in the Great Plains occurred in Texas, Kansas and Nebraska from 1540-1542. In that same time period, Hernando de Soto crossed a west-northwest direction in what is now Oklahoma and Texas. Today this is known as the De Soto Trail. The Spanish thought the Great Plains were the location of the mythological Quivira and Cíbola, a place said to be rich in gold. During the Cenozoic era, specifically about 25 million years ago during the Miocene and Pliocene epochs, the continental climate became favorable to the evolution of grasslands. Existing forest biomes declined and grasslands became much more widespread. The grasslands provided a new niche for mammals, including many ungulates and glires, that switched from browsing diets to grazing diets. Traditionally, the spread of grasslands and the development of grazers have been strongly linked. However, an examination of mammalian teeth suggests that it is the open, gritty habitat and not the grass itself which is linked to diet changes in mammals, giving rise to the ""grit, not grass"" hypothesis. The rural Plains have lost a third of their population since 1920. Several hundred thousand square miles (several hundred thousand square kilometers) of the Great Plains have fewer than 6 inhabitants per square mile (2.3 inhabitants per square kilometer)—the density standard Frederick Jackson Turner used to declare the American frontier ""closed"" in 1893. Many have fewer than 2 inhabitants per square mile (0.77 inhabitants per square kilometer). There are more than 6,000 ghost towns in the state of Kansas alone, according to Kansas historian Daniel Fitzgerald. This problem is often exacerbated by the consolidation of farms and the difficulty of attracting modern industry to the region. In addition, the smaller school-age population has forced the consolidation of school districts and the closure of high schools in some communities. The continuing population loss has led some to suggest that the current use of the drier parts of the Great Plains is not sustainable, and there has been a proposal - the ""Buffalo Commons"" - to return approximately 139,000 square miles (360,000 km2) of these drier parts to native prairie land. After 1870, the new railroads across the Plains brought hunters who killed off almost all the bison for their hides. The railroads offered attractive packages of land and transportation to European farmers, who rushed to settle the land. They (and Americans as well) also took advantage of the homestead laws to obtain free farms. Land speculators and local boosters identified many potential towns, and those reached by the railroad had a chance, while the others became ghost towns. In Kansas, for example, nearly 5000 towns were mapped out, but by 1970 only 617 were actually operating. In the mid-20th century, closeness to an interstate exchange determined whether a town would flourish or struggle for business. Although the eastern image of farm life in the prairies emphasized the isolation of the lonely farmer and wife, plains residents created busy social lives for themselves. They often sponsored activities that combined work, food and entertainment such as barn raisings, corn huskings, quilting bees, Grange meetings, church activities and school functions. Women organized shared meals and potluck events, as well as extended visits between families. The Grange was a nationwide farmers' organization, they reserved high offices for women, and gave them a voice in public affairs."
Symbiosis,"Amensalism is the type of relationship that exists where one species is inhibited or completely obliterated and one is unaffected. This type of symbiosis is relatively uncommon in rudimentary reference texts, but is omnipresent in the natural world.[citation needed] There are two types of amensalism, competition and antibiosis. Competition is where a larger or stronger organisms deprives a smaller or weaker one from a resource. Antibiosis occurs when one organism is damaged or killed by another through a chemical secretion. An example of competition is a sapling growing under the shadow of a mature tree. The mature tree can begin to rob the sapling of necessary sunlight and, if the mature tree is very large, it can take up rainwater and deplete soil nutrients. Throughout the process the mature tree is unaffected. Indeed, if the sapling dies, the mature tree gains nutrients from the decaying sapling. Note that these nutrients become available because of the sapling's decomposition, rather than from the living sapling, which would be a case of parasitism.[citation needed] An example of antibiosis is Juglans nigra (black walnut), secreting juglone, a substance which destroys many herbaceous plants within its root zone. Endosymbiosis is any symbiotic relationship in which one symbiont lives within the tissues of the other, either within the cells or extracellularly. Examples include diverse microbiomes, rhizobia, nitrogen-fixing bacteria that live in root nodules on legume roots; actinomycete nitrogen-fixing bacteria called Frankia, which live in alder tree root nodules; single-celled algae inside reef-building corals; and bacterial endosymbionts that provide essential nutrients to about 10%–15% of insects. Adaptation of the endosymbiont to the host's lifestyle leads to many changes in the endosymbiont–the foremost being drastic reduction in its genome size. This is due to many genes being lost during the process of metabolism, and DNA repair and recombination. While important genes participating in the DNA to RNA transcription, protein translation and DNA/RNA replication are retained. That is, a decrease in genome size is due to loss of protein coding genes and not due to lessening of inter-genic regions or open reading frame (ORF) size. Thus, species that are naturally evolving and contain reduced sizes of genes can be accounted for an increased number of noticeable differences between them, thereby leading to changes in their evolutionary rates. As the endosymbiotic bacteria related with these insects are passed on to the offspring strictly via vertical genetic transmission, intracellular bacteria goes through many hurdles during the process, resulting in the decrease in effective population sizes when compared to the free living bacteria. This incapability of the endosymbiotic bacteria to reinstate its wild type phenotype via a recombination process is called as Muller's ratchet phenomenon. Muller's ratchet phenomenon together with less effective population sizes has led to an accretion of deleterious mutations in the non-essential genes of the intracellular bacteria. This could have been due to lack of selection mechanisms prevailing in the rich environment of the host. During mutualistic symbioses, the host cell lacks some of the nutrients, which are provided by the endosymbiont. As a result, the host favors endosymbiont's growth processes within itself by producing some specialized cells. These cells affect the genetic composition of the host in order to regulate the increasing population of the endosymbionts and ensuring that these genetic changes are passed onto the offspring via vertical transmission (heredity). Commensal relationships may involve one organism using another for transportation (phoresy) or for housing (inquilinism), or it may also involve one organism using something another created, after its death (metabiosis). Examples of metabiosis are hermit crabs using gastropod shells to protect their bodies and spiders building their webs on plants. Amensalism is an interaction where an organism inflicts harm to another organism without any costs or benefits received by the other. A clear case of amensalism is where sheep or cattle trample grass. Whilst the presence of the grass causes negligible detrimental effects to the animal's hoof, the grass suffers from being crushed. Amensalism is often used to describe strongly asymmetrical competitive interactions, such as has been observed between the Spanish ibex and weevils of the genus Timarcha which feed upon the same type of shrub. Whilst the presence of the weevil has almost no influence on food availability, the presence of ibex has an enormous detrimental effect on weevil numbers, as they consume significant quantities of plant matter and incidentally ingest the weevils upon it. The definition of symbiosis has varied among scientists. Some believe symbiosis should only refer to persistent mutualisms, while others believe it should apply to any type of persistent biological interaction (in other words mutualistic, commensalistic, or parasitic). After 130 years of debate, current biology and ecology textbooks now use the latter ""de Bary"" definition or an even broader definition (where symbiosis means all species interactions), with the restrictive definition no longer used (in other words, symbiosis means mutualism). Symbiosis (from Greek σύν ""together"" and βίωσις ""living"") is close and often long-term interaction between two different biological species. In 1877 Albert Bernhard Frank used the word symbiosis (which previously had been used to depict people living together in community) to describe the mutualistic relationship in lichens. In 1879, the German mycologist Heinrich Anton de Bary defined it as ""the living together of unlike organisms."" Symbiosis played a major role in the co-evolution of flowering plants and the animals that pollinate them. Many plants that are pollinated by insects, bats, or birds have highly specialized flowers modified to promote pollination by a specific pollinator that is also correspondingly adapted. The first flowering plants in the fossil record had relatively simple flowers. Adaptive speciation quickly gave rise to many diverse groups of plants, and, at the same time, corresponding speciation occurred in certain insect groups. Some groups of plants developed nectar and large sticky pollen, while insects evolved more specialized morphologies to access and collect these rich food sources. In some taxa of plants and insects the relationship has become dependent, where the plant species can only be pollinated by one species of insect. Synnecrosis is a rare type of symbiosis in which the interaction between species is detrimental to both organisms involved. It is a short-lived condition, as the interaction eventually causes death. Because of this, evolution selects against synnecrosis and it is uncommon in nature. An example of this is the relationship between some species of bees and victims of the bee sting. Species of bees who die after stinging their prey inflict pain on themselves (albeit to protect the hive) as well as on the victim. This term is rarely used. Commensalism describes a relationship between two living organisms where one benefits and the other is not significantly harmed or helped. It is derived from the English word commensal used of human social interaction. The word derives from the medieval Latin word, formed from com- and mensa, meaning ""sharing a table"". An example of mutual symbiosis is the relationship between the ocellaris clownfish that dwell among the tentacles of Ritteri sea anemones. The territorial fish protects the anemone from anemone-eating fish, and in turn the stinging tentacles of the anemone protect the clownfish from its predators. A special mucus on the clownfish protects it from the stinging tentacles. A large percentage of herbivores have mutualistic gut flora that help them digest plant matter, which is more difficult to digest than animal prey. This gut flora is made up of cellulose-digesting protozoans or bacteria living in the herbivores' intestines. Coral reefs are the result of mutualisms between coral organisms and various types of algae that live inside them. Most land plants and land ecosystems rely on mutualisms between the plants, which fix carbon from the air, and mycorrhyzal fungi, which help in extracting water and minerals from the ground. One of the most spectacular examples of obligate mutualism is between the siboglinid tube worms and symbiotic bacteria that live at hydrothermal vents and cold seeps. The worm has no digestive tract and is wholly reliant on its internal symbionts for nutrition. The bacteria oxidize either hydrogen sulfide or methane, which the host supplies to them. These worms were discovered in the late 1980s at the hydrothermal vents near the Galapagos Islands and have since been found at deep-sea hydrothermal vents and cold seeps in all of the world's oceans. While historically, symbiosis has received less attention than other interactions such as predation or competition, it is increasingly recognized as an important selective force behind evolution, with many species having a long history of interdependent co-evolution. In fact, the evolution of all eukaryotes (plants, animals, fungi, and protists) is believed under the endosymbiotic theory to have resulted from a symbiosis between various sorts of bacteria. This theory is supported by certain organelles dividing independently of the cell, and the observation that some organelles seem to have their own nucleic acid. A further example is the goby fish, which sometimes lives together with a shrimp. The shrimp digs and cleans up a burrow in the sand in which both the shrimp and the goby fish live. The shrimp is almost blind, leaving it vulnerable to predators when outside its burrow. In case of danger the goby fish touches the shrimp with its tail to warn it. When that happens both the shrimp and goby fish quickly retreat into the burrow. Different species of gobies (Elacatinus spp.) also exhibit mutualistic behavior through cleaning up ectoparasites in other fish. Symbiotic relationships include those associations in which one organism lives on another (ectosymbiosis, such as mistletoe), or where one partner lives inside the other (endosymbiosis, such as lactobacilli and other bacteria in humans or Symbiodinium in corals). Symbiosis is also classified by physical attachment of the organisms; symbiosis in which the organisms have bodily union is called conjunctive symbiosis, and symbiosis in which they are not in union is called disjunctive symbiosis. Another non-obligate symbiosis is known from encrusting bryozoans and hermit crabs that live in a close relationship. The bryozoan colony (Acanthodesia commensale) develops a cirumrotatory growth and offers the crab (Pseudopagurus granulimanus) a helicospiral-tubular extension of its living chamber that initially was situated within a gastropod shell. A parasitic relationship is one in which one member of the association benefits while the other is harmed. This is also known as antagonistic or antipathetic symbiosis. Parasitic symbioses take many forms, from endoparasites that live within the host's body to ectoparasites that live on its surface. In addition, parasites may be necrotrophic, which is to say they kill their host, or biotrophic, meaning they rely on their host's surviving. Biotrophic parasitism is an extremely successful mode of life. Depending on the definition used, as many as half of all animals have at least one parasitic phase in their life cycles, and it is also frequent in plants and fungi. Moreover, almost all free-living animals are host to one or more parasite taxa. An example of a biotrophic relationship would be a tick feeding on the blood of its host. Some symbiotic relationships are obligate, meaning that both symbionts entirely depend on each other for survival. For example, many lichens consist of fungal and photosynthetic symbionts that cannot live on their own. Others are facultative (optional): they can, but do not have to live with the other organism. The biologist Lynn Margulis, famous for her work on endosymbiosis, contends that symbiosis is a major driving force behind evolution. She considers Darwin's notion of evolution, driven by competition, to be incomplete and claims that evolution is strongly based on co-operation, interaction, and mutual dependence among organisms. According to Margulis and Dorion Sagan, ""Life did not take over the globe by combat, but by networking."" Mutualism or interspecies reciprocal altruism is a relationship between individuals of different species where both individuals benefit. In general, only lifelong interactions involving close physical and biochemical contact can properly be considered symbiotic. Mutualistic relationships may be either obligate for both species, obligate for one but facultative for the other, or facultative for both. Many biologists restrict the definition of symbiosis to close mutualist relationships. Ectosymbiosis, also referred to as exosymbiosis, is any symbiotic relationship in which the symbiont lives on the body surface of the host, including the inner surface of the digestive tract or the ducts of exocrine glands. Examples of this include ectoparasites such as lice, commensal ectosymbionts such as the barnacles that attach themselves to the jaw of baleen whales, and mutualist ectosymbionts such as cleaner fish."
Humanism,"Religious humanism is an integration of humanist ethical philosophy with religious rituals and beliefs that centre on human needs, interests, and abilities. Though practitioners of religious humanism did not officially organise under the name of ""humanism"" until the late 19th and early 20th centuries, non-theistic religions paired with human-centred ethical philosophy have a long history. The Cult of Reason (French: Culte de la Raison) was a religion based on deism devised during the French Revolution by Jacques Hébert, Pierre Gaspard Chaumette and their supporters. In 1793 during the French Revolution, the cathedral Notre Dame de Paris was turned into a ""Temple to Reason"" and for a time Lady Liberty replaced the Virgin Mary on several altars. In the 1850s, Auguste Comte, the Father of Sociology, founded Positivism, a ""religion of humanity"". One of the earliest forerunners of contemporary chartered humanist organisations was the Humanistic Religious Association formed in 1853 in London. This early group was democratically organised, with male and female members participating in the election of the leadership and promoted knowledge of the sciences, philosophy, and the arts. The Ethical Culture movement was founded in 1876. The movement's founder, Felix Adler, a former member of the Free Religious Association, conceived of Ethical Culture as a new religion that would retain the ethical message at the heart of all religions. Ethical Culture was religious in the sense of playing a defining role in people's lives and addressing issues of ultimate concern. In the 6th century BCE, Taoist teacher Lao Tzu espoused a series of naturalistic concepts with some elements of humanistic philosophy. The Silver Rule of Confucianism from Analects XV.24, is an example of ethical philosophy based on human values rather than the supernatural. Humanistic thought is also contained in other Confucian classics, e.g., as recorded in Zuo Zhuan, Ji Liang says, ""People is the zhu (master, lord, dominance, owner or origin) of gods. So, to sage kings, people first, gods second""; Neishi Guo says, ""Gods, clever, righteous and wholehearted, comply with human."" Taoist and Confucian secularism contain elements of moral thought devoid of religious authority or deism however they only partly resembled our modern concept of secularism. In the high Renaissance, in fact, there was a hope that more direct knowledge of the wisdom of antiquity, including the writings of the Church fathers, the earliest known Greek texts of the Christian Gospels, and in some cases even the Jewish Kabbalah, would initiate a harmonious new era of universal agreement. With this end in view, Renaissance Church authorities afforded humanists what in retrospect appears a remarkable degree of freedom of thought. One humanist, the Greek Orthodox Platonist Gemistus Pletho (1355–1452), based in Mystras, Greece (but in contact with humanists in Florence, Venice, and Rome) taught a Christianised version of pagan polytheism. The words of the comic playwright P. Terentius Afer reverberated across the Roman world of the mid-2nd century BCE and beyond. Terence, an African and a former slave, was well placed to preach the message of universalism, of the essential unity of the human race, that had come down in philosophical form from the Greeks, but needed the pragmatic muscles of Rome in order to become a practical reality. The influence of Terence's felicitous phrase on Roman thinking about human rights can hardly be overestimated. Two hundred years later Seneca ended his seminal exposition of the unity of humankind with a clarion-call: The ad fontes principle also had many applications. The re-discovery of ancient manuscripts brought a more profound and accurate knowledge of ancient philosophical schools such as Epicureanism, and Neoplatonism, whose Pagan wisdom the humanists, like the Church fathers of old, tended, at least initially, to consider as deriving from divine revelation and thus adaptable to a life of Christian virtue. The line from a drama of Terence, Homo sum, humani nihil a me alienum puto (or with nil for nihil), meaning ""I am a human being, I think nothing human alien to me"", known since antiquity through the endorsement of Saint Augustine, gained renewed currency as epitomising the humanist attitude. The statement, in a play modeled or borrowed from a (now lost) Greek comedy by Menander, may have originated in a lighthearted vein – as a comic rationale for an old man's meddling – but it quickly became a proverb and throughout the ages was quoted with a deeper meaning, by Cicero and Saint Augustine, to name a few, and most notably by Seneca. Richard Bauman writes: Early humanists saw no conflict between reason and their Christian faith (see Christian Humanism). They inveighed against the abuses of the Church, but not against the Church itself, much less against religion. For them, the word ""secular"" carried no connotations of disbelief – that would come later, in the nineteenth century. In the Renaissance to be secular meant simply to be in the world rather than in a monastery. Petrarch frequently admitted that his brother Gherardo's life as a Carthusian monk was superior to his own (although Petrarch himself was in Minor Orders and was employed by the Church all his life). He hoped that he could do some good by winning earthly glory and praising virtue, inferior though that might be to a life devoted solely to prayer. By embracing a non-theistic philosophic base, however, the methods of the humanists, combined with their eloquence, would ultimately have a corrosive effect on established authority. In 1808 Bavarian educational commissioner Friedrich Immanuel Niethammer coined the term Humanismus to describe the new classical curriculum he planned to offer in German secondary schools, and by 1836 the word ""humanism"" had been absorbed into the English language in this sense. The coinage gained universal acceptance in 1856, when German historian and philologist Georg Voigt used humanism to describe Renaissance humanism, the movement that flourished in the Italian Renaissance to revive classical learning, a use which won wide acceptance among historians in many nations, especially Italy. Polemics about humanism have sometimes assumed paradoxical twists and turns. Early 20th century critics such as Ezra Pound, T. E. Hulme, and T. S. Eliot considered humanism to be sentimental ""slop"" (Hulme)[citation needed] or ""an old bitch gone in the teeth"" (Pound) and wanted to go back to a more manly, authoritarian society such as (they believed) existed in the Middle Ages. Postmodern critics who are self-described anti-humanists, such as Jean-François Lyotard and Michel Foucault, have asserted that humanism posits an overarching and excessively abstract notion of humanity or universal human nature, which can then be used as a pretext for imperialism and domination of those deemed somehow less than human. ""Humanism fabricates the human as much as it fabricates the nonhuman animal"", suggests Timothy Laurie, turning the human into what he calls ""a placeholder for a range of attributes that have been considered most virtuous among humans (e.g. rationality, altruism), rather than most commonplace (e.g. hunger, anger)"". Nevertheless, philosopher Kate Soper notes that by faulting humanism for falling short of its own benevolent ideals, anti-humanism thus frequently ""secretes a humanist rhetoric"". Raymond B. Bragg, the associate editor of The New Humanist, sought to consolidate the input of Leon Milton Birkhead, Charles Francis Potter, and several members of the Western Unitarian Conference. Bragg asked Roy Wood Sellars to draft a document based on this information which resulted in the publication of the Humanist Manifesto in 1933. Potter's book and the Manifesto became the cornerstones of modern humanism, the latter declaring a new religion by saying, ""any religion that can hope to be a synthesising and dynamic force for today must be shaped for the needs of this age. To establish such a religion is a major necessity of the present."" It then presented 15 theses of humanism as foundational principles for this new religion. After 1517, when the new invention of printing made these texts widely available, the Dutch humanist Erasmus, who had studied Greek at the Venetian printing house of Aldus Manutius, began a philological analysis of the Gospels in the spirit of Valla, comparing the Greek originals with their Latin translations with a view to correcting errors and discrepancies in the latter. Erasmus, along with the French humanist Jacques Lefèvre d'Étaples, began issuing new translations, laying the groundwork for the Protestant Reformation. Henceforth Renaissance humanism, particularly in the German North, became concerned with religion, while Italian and French humanism concentrated increasingly on scholarship and philology addressed to a narrow audience of specialists, studiously avoiding topics that might offend despotic rulers or which might be seen as corrosive of faith. After the Reformation, critical examination of the Bible did not resume until the advent of the so-called Higher criticism of the 19th-century German Tübingen school. Better acquaintance with Greek and Roman technical writings also influenced the development of European science (see the history of science in the Renaissance). This was despite what A. C. Crombie (viewing the Renaissance in the 19th-century manner as a chapter in the heroic March of Progress) calls ""a backwards-looking admiration for antiquity"", in which Platonism stood in opposition to the Aristotelian concentration on the observable properties of the physical world. But Renaissance humanists, who considered themselves as restoring the glory and nobility of antiquity, had no interest in scientific innovation. However, by the mid-to-late 16th century, even the universities, though still dominated by Scholasticism, began to demand that Aristotle be read in accurate texts edited according to the principles of Renaissance philology, thus setting the stage for Galileo's quarrels with the outmoded habits of Scholasticism. At about the same time, the word ""humanism"" as a philosophy centred on humankind (as opposed to institutionalised religion) was also being used in Germany by the so-called Left Hegelians, Arnold Ruge, and Karl Marx, who were critical of the close involvement of the church in the repressive German government. There has been a persistent confusion between the several uses of the terms: philanthropic humanists look to what they consider their antecedents in critical thinking and human-centered philosophy among the Greek philosophers and the great figures of Renaissance history; and scholarly humanists stress the linguistic and cultural disciplines needed to understand and interpret these philosophers and artists. The humanists' close study of Latin literary texts soon enabled them to discern historical differences in the writing styles of different periods. By analogy with what they saw as decline of Latin, they applied the principle of ad fontes, or back to the sources, across broad areas of learning, seeking out manuscripts of Patristic literature as well as pagan authors. In 1439, while employed in Naples at the court of Alfonso V of Aragon (at the time engaged in a dispute with the Papal States) the humanist Lorenzo Valla used stylistic textual analysis, now called philology, to prove that the Donation of Constantine, which purported to confer temporal powers on the Pope of Rome, was an 8th-century forgery. For the next 70 years, however, neither Valla nor any of his contemporaries thought to apply the techniques of philology to other controversial manuscripts in this way. Instead, after the fall of the Byzantine Empire to the Turks in 1453, which brought a flood of Greek Orthodox refugees to Italy, humanist scholars increasingly turned to the study of Neoplatonism and Hermeticism, hoping to bridge the differences between the Greek and Roman Churches, and even between Christianity itself and the non-Christian world. The refugees brought with them Greek manuscripts, not only of Plato and Aristotle, but also of the Christian Gospels, previously unavailable in the Latin West. Gellius says that in his day humanitas is commonly used as a synonym for philanthropy – or kindness and benevolence toward one's fellow human being. Gellius maintains that this common usage is wrong, and that model writers of Latin, such as Cicero and others, used the word only to mean what we might call ""humane"" or ""polite"" learning, or the Greek equivalent Paideia. Gellius became a favorite author in the Italian Renaissance, and, in fifteenth-century Italy, teachers and scholars of philosophy, poetry, and rhetoric were called and called themselves ""humanists"". Modern scholars, however, point out that Cicero (106 – 43 BCE), who was most responsible for defining and popularizing the term humanitas, in fact frequently used the word in both senses, as did his near contemporaries. For Cicero, a lawyer, what most distinguished humans from brutes was speech, which, allied to reason, could (and should) enable them to settle disputes and live together in concord and harmony under the rule of law. Thus humanitas included two meanings from the outset and these continue in the modern derivative, humanism, which even today can refer to both humanitarian benevolence and to scholarship. During the French Revolution, and soon after, in Germany (by the Left Hegelians), humanism began to refer to an ethical philosophy centered on humankind, without attention to the transcendent or supernatural. The designation Religious Humanism refers to organized groups that sprang up during the late-nineteenth and early twentieth centuries. It is similar to Protestantism, although centered on human needs, interests, and abilities rather than the supernatural. In the Anglophone world, such modern, organized forms of humanism, which are rooted in the 18th-century Enlightenment, have to a considerable extent more or less detached themselves from the historic connection of humanism with classical learning and the liberal arts. In his book, Humanism (1997), Tony Davies calls these critics ""humanist anti-humanists"". Critics of antihumanism, most notably Jürgen Habermas, counter that while antihumanists may highlight humanism's failure to fulfil its emancipatory ideal, they do not offer an alternative emancipatory project of their own. Others, like the German philosopher Heidegger considered themselves humanists on the model of the ancient Greeks, but thought humanism applied only to the German ""race"" and specifically to the Nazis and thus, in Davies' words, were anti-humanist humanists. Such a reading of Heidegger's thought is itself deeply controversial; Heidegger includes his own views and critique of Humanism in Letter On Humanism. Davies acknowledges that after the horrific experiences of the wars of the 20th century ""it should no longer be possible to formulate phrases like 'the destiny of man' or the 'triumph of human reason' without an instant consciousness of the folly and brutality they drag behind them"". For ""it is almost impossible to think of a crime that has not been committed in the name of human reason"". Yet, he continues, ""it would be unwise to simply abandon the ground occupied by the historical humanisms. For one thing humanism remains on many occasions the only available alternative to bigotry and persecution. The freedom to speak and write, to organise and campaign in defence of individual or collective interests, to protest and disobey: all these can only be articulated in humanist terms."" Humanists reacted against this utilitarian approach and the narrow pedantry associated with it. They sought to create a citizenry (frequently including women) able to speak and write with eloquence and clarity and thus capable of engaging the civic life of their communities and persuading others to virtuous and prudent actions. This was to be accomplished through the study of the studia humanitatis, today known as the humanities: grammar, rhetoric, history, poetry and moral philosophy. As a program to revive the cultural – and particularly the literary – legacy and moral philosophy of classical antiquity, Humanism was a pervasive cultural mode and not the program of a few isolated geniuses like Rabelais or Erasmus as is still sometimes popularly believed. But in the mid-18th century, during the French Enlightenment, a more ideological use of the term had come into use. In 1765, the author of an anonymous article in a French Enlightenment periodical spoke of ""The general love of humanity ... a virtue hitherto quite nameless among us, and which we will venture to call 'humanism', for the time has come to create a word for such a beautiful and necessary thing"". The latter part of the 18th and the early 19th centuries saw the creation of numerous grass-roots ""philanthropic"" and benevolent societies dedicated to human betterment and the spreading of knowledge (some Christian, some not). After the French Revolution, the idea that human virtue could be created by human reason alone independently from traditional religious institutions, attributed by opponents of the Revolution to Enlightenment philosophes such as Rousseau, was violently attacked by influential religious and political conservatives, such as Edmund Burke and Joseph de Maistre, as a deification or idolatry of humanity. Humanism began to acquire a negative sense. The Oxford English Dictionary records the use of the word ""humanism"" by an English clergyman in 1812 to indicate those who believe in the ""mere humanity"" (as opposed to the divine nature) of Christ, i.e., Unitarians and Deists. In this polarised atmosphere, in which established ecclesiastical bodies tended to circle the wagons and reflexively oppose political and social reforms like extending the franchise, universal schooling, and the like, liberal reformers and radicals embraced the idea of Humanism as an alternative religion of humanity. The anarchist Proudhon (best known for declaring that ""property is theft"") used the word ""humanism"" to describe a ""culte, déification de l’humanité"" (""worship, deification of humanity"") and Ernest Renan in L’avenir de la science: pensées de 1848 (""The Future of Knowledge: Thoughts on 1848"") (1848–49), states: ""It is my deep conviction that pure humanism will be the religion of the future, that is, the cult of all that pertains to humanity—all of life, sanctified and raised to the level of a moral value."" Renaissance humanism was an intellectual movement in Europe of the later Middle Ages and the Early Modern period. The 19th-century German historian Georg Voigt (1827–91) identified Petrarch as the first Renaissance humanist. Paul Johnson agrees that Petrarch was ""the first to put into words the notion that the centuries between the fall of Rome and the present had been the age of Darkness"". According to Petrarch, what was needed to remedy this situation was the careful study and imitation of the great classical authors. For Petrarch and Boccaccio, the greatest master was Cicero, whose prose became the model for both learned (Latin) and vernacular (Italian) prose. Just as artist and inventor Leonardo da Vinci – partaking of the zeitgeist though not himself a humanist – advocated study of human anatomy, nature, and weather to enrich Renaissance works of art, so Spanish-born humanist Juan Luis Vives (c. 1493–1540) advocated observation, craft, and practical techniques to improve the formal teaching of Aristotelian philosophy at the universities, helping to free them from the grip of Medieval Scholasticism. Thus, the stage was set for the adoption of an approach to natural philosophy, based on empirical observations and experimentation of the physical universe, making possible the advent of the age of scientific inquiry that followed the Renaissance. 6th-century BCE pre-Socratic Greek philosophers Thales of Miletus and Xenophanes of Colophon were the first in the region to attempt to explain the world in terms of human reason rather than myth and tradition, thus can be said to be the first Greek humanists. Thales questioned the notion of anthropomorphic gods and Xenophanes refused to recognise the gods of his time and reserved the divine for the principle of unity in the universe. These Ionian Greeks were the first thinkers to assert that nature is available to be studied separately from the supernatural realm. Anaxagoras brought philosophy and the spirit of rational inquiry from Ionia to Athens. Pericles, the leader of Athens during the period of its greatest glory was an admirer of Anaxagoras. Other influential pre-Socratics or rational philosophers include Protagoras (like Anaxagoras a friend of Pericles), known for his famous dictum ""man is the measure of all things"" and Democritus, who proposed that matter was composed of atoms. Little of the written work of these early philosophers survives and they are known mainly from fragments and quotations in other writers, principally Plato and Aristotle. The historian Thucydides, noted for his scientific and rational approach to history, is also much admired by later humanists. In the 3rd century BCE, Epicurus became known for his concise phrasing of the problem of evil, lack of belief in the afterlife, and human-centred approaches to achieving eudaimonia. He was also the first Greek philosopher to admit women to his school as a rule. Contemporary humanism entails a qualified optimism about the capacity of people, but it does not involve believing that human nature is purely good or that all people can live up to the Humanist ideals without help. If anything, there is recognition that living up to one's potential is hard work and requires the help of others. The ultimate goal is human flourishing; making life better for all humans, and as the most conscious species, also promoting concern for the welfare of other sentient beings and the planet as a whole. The focus is on doing good and living well in the here and now, and leaving the world a better place for those who come after. In 1925, the English mathematician and philosopher Alfred North Whitehead cautioned: ""The prophecy of Francis Bacon has now been fulfilled; and man, who at times dreamt of himself as a little lower than the angels, has submitted to become the servant and the minister of nature. It still remains to be seen whether the same actor can play both parts"". Humanistic psychology is a psychological perspective which rose to prominence in the mid-20th century in response to Sigmund Freud's psychoanalytic theory and B. F. Skinner's Behaviorism. The approach emphasizes an individual's inherent drive towards self-actualization and creativity. Psychologists Carl Rogers and Abraham Maslow introduced a positive, humanistic psychology in response to what they viewed as the overly pessimistic view of psychoanalysis in the early 1960s. Other sources include the philosophies of existentialism and phenomenology. Active in the early 1920s, F.C.S. Schiller labelled his work ""humanism"" but for Schiller the term referred to the pragmatist philosophy he shared with William James. In 1929, Charles Francis Potter founded the First Humanist Society of New York whose advisory board included Julian Huxley, John Dewey, Albert Einstein and Thomas Mann. Potter was a minister from the Unitarian tradition and in 1930 he and his wife, Clara Cook Potter, published Humanism: A New Religion. Throughout the 1930s, Potter was an advocate of such liberal causes as, women’s rights, access to birth control, ""civil divorce laws"", and an end to capital punishment. Renaissance humanism was an activity of cultural and educational reform engaged in by civic and ecclesiastical chancellors, book collectors, educators, and writers, who by the late fifteenth century began to be referred to as umanisti – ""humanists"". It developed during the fourteenth and the beginning of the fifteenth centuries, and was a response to the challenge of scholastic university education, which was then dominated by Aristotelian philosophy and logic. Scholasticism focused on preparing men to be doctors, lawyers or professional theologians, and was taught from approved textbooks in logic, natural philosophy, medicine, law and theology. There were important centres of humanism at Florence, Naples, Rome, Venice, Mantua, Ferrara, and Urbino. In China, Yellow Emperor is regarded as the humanistic primogenitor.[citation needed] Sage kings such as Yao and Shun are humanistic figures as recorded.[citation needed] King Wu of Zhou has the famous saying: ""Humanity is the Ling (efficacious essence) of the world (among all)."" Among them Duke of Zhou, respected as a founder of Rujia (Confucianism), is especially prominent and pioneering in humanistic thought. His words were recorded in the Book of History as follows (translation):[citation needed] Humanism is a philosophical and ethical stance that emphasizes the value and agency of human beings, individually and collectively, and generally prefers critical thinking and evidence (rationalism, empiricism) over acceptance of dogma or superstition. The meaning of the term humanism has fluctuated according to the successive intellectual movements which have identified with it. Generally, however, humanism refers to a perspective that affirms some notion of human freedom and progress. In modern times, humanist movements are typically aligned with secularism, and today humanism typically refers to a non-theistic life stance centred on human agency and looking to science rather than revelation from a supernatural source to understand the world. Eliot and her circle, who included her companion George Henry Lewes (the biographer of Goethe) and the abolitionist and social theorist Harriet Martineau, were much influenced by the positivism of Auguste Comte, whom Martineau had translated. Comte had proposed an atheistic culte founded on human principles – a secular Religion of Humanity (which worshiped the dead, since most humans who have ever lived are dead), complete with holidays and liturgy, modeled on the rituals of what was seen as a discredited and dilapidated Catholicism. Although Comte's English followers, like Eliot and Martineau, for the most part rejected the full gloomy panoply of his system, they liked the idea of a religion of humanity. Comte's austere vision of the universe, his injunction to ""vivre pour altrui"" (""live for others"", from which comes the word ""altruism""), and his idealisation of women inform the works of Victorian novelists and poets from George Eliot and Matthew Arnold to Thomas Hardy. Another instance of ancient humanism as an organised system of thought is found in the Gathas of Zarathustra, composed between 1,000 BCE – 600 BCE in Greater Iran. Zarathustra's philosophy in the Gathas lays out a conception of humankind as thinking beings dignified with choice and agency according to the intellect which each receives from Ahura Mazda (God in the form of supreme wisdom). The idea of Ahura Mazda as a non-intervening deistic divine God/Grand Architect of the universe tied with a unique eschatology and ethical system implying that each person is held morally responsible for their choices, made freely in this present life, in the afterlife. The importance placed on thought, action, responsibility, and a non-intervening creator was appealed to by, and inspired a number of, Enlightenment humanist thinkers in Europe such as Voltaire and Montesquieu. Davies identifies Paine's The Age of Reason as ""the link between the two major narratives of what Jean-François Lyotard calls the narrative of legitimation"": the rationalism of the 18th-century Philosophes and the radical, historically based German 19th-century Biblical criticism of the Hegelians David Friedrich Strauss and Ludwig Feuerbach. ""The first is political, largely French in inspiration, and projects 'humanity as the hero of liberty'. The second is philosophical, German, seeks the totality and autonomy of knowledge, and stresses understanding rather than freedom as the key to human fulfilment and emancipation. The two themes converged and competed in complex ways in the 19th century and beyond, and between them set the boundaries of its various humanisms. Homo homini deus est (""The human being is a god to humanity"" or ""god is nothing [other than] the human being to himself""), Feuerbach had written."
Bird_migration,"Many, if not most, birds migrate in flocks. For larger birds, flying in flocks reduces the energy cost. Geese in a V-formation may conserve 12–20% of the energy they would need to fly alone. Red knots Calidris canutus and dunlins Calidris alpina were found in radar studies to fly 5 km/h (3.1 mph) faster in flocks than when they were flying alone. Some bar-tailed godwits Limosa lapponica have the longest known non-stop flight of any migrant, flying 11,000 km from Alaska to their New Zealand non-breeding areas. Prior to migration, 55 percent of their bodyweight is stored as fat to fuel this uninterrupted journey. Navigation is based on a variety of senses. Many birds have been shown to use a sun compass. Using the sun for direction involves the need for making compensation based on the time. Navigation has also been shown to be based on a combination of other abilities including the ability to detect magnetic fields (magnetoception), use visual landmarks as well as olfactory cues. Aristotle however suggested that swallows and other birds hibernated. This belief persisted as late as 1878, when Elliott Coues listed the titles of no less than 182 papers dealing with the hibernation of swallows. Even the ""highly observant"" Gilbert White, in his posthumously published 1789 The Natural History of Selborne, quoted a man's story about swallows being found in a chalk cliff collapse ""while he was a schoolboy at Brighthelmstone"", though the man denied being an eyewitness. However, he also writes that ""as to swallows being found in a torpid state during the winter in the Isle of Wight or any part of this country, I never heard any such account worth attending to"", and that if early swallows ""happen to find frost and snow they immediately withdraw for a time—a circumstance this much more in favour of hiding than migration"", since he doubts they would ""return for a week or two to warmer latitudes"". Some large broad-winged birds rely on thermal columns of rising hot air to enable them to soar. These include many birds of prey such as vultures, eagles, and buzzards, but also storks. These birds migrate in the daytime. Migratory species in these groups have great difficulty crossing large bodies of water, since thermals only form over land, and these birds cannot maintain active flight for long distances. Mediterranean and other seas present a major obstacle to soaring birds, which must cross at the narrowest points. Massive numbers of large raptors and storks pass through areas such as the Strait of Messina, Gibraltar, Falsterbo, and the Bosphorus at migration times. More common species, such as the European honey buzzard Pernis apivorus, can be counted in hundreds of thousands in autumn. Other barriers, such as mountain ranges, can also cause funnelling, particularly of large diurnal migrants. This is a notable factor in the Central American migratory bottleneck. Batumi bottleneck in the Caucasus is one of the heaviest migratory funnels on earth. Avoiding flying over the Black Sea surface and across high mountains, hundreds of thousands of soaring birds funnel through an area around the city of Batumi, Georgia. Birds of prey such as honey buzzards which migrate using thermals lose only 10 to 20% of their weight during migration, which may explain why they forage less during migration than do smaller birds of prey with more active flight such as falcons, hawks and harriers. Bird migration routes have been studied by a variety of techniques including the oldest, marking. Swans have been marked with a nick on the beak since about 1560 in England. Scientific ringing was pioneered by Hans Christian Cornelius Mortensen in 1899. Other techniques include radar and satellite tracking. The control of migration, its timing and response are genetically controlled and appear to be a primitive trait that is present even in non-migratory species of birds. The ability to navigate and orient themselves during migration is a much more complex phenomenon that may include both endogenous programs as well as learning. Records of bird migration were made as much as 3,000 years ago by the Ancient Greek writers Hesiod, Homer, Herodotus and Aristotle. The Bible also notes migrations, as in the Book of Job (39:26), where the inquiry is made: ""Is it by your insight that the hawk hovers, spreads its wings southward?"" The author of Jeremiah (8:7) wrote: ""Even the stork in the heavens knows its seasons, and the turtle dove, the swift and the crane keep the time of their arrival."" Birds need to alter their metabolism in order to meet the demands of migration. The storage of energy through the accumulation of fat and the control of sleep in nocturnal migrants require special physiological adaptations. In addition, the feathers of a bird suffer from wear-and-tear and require to be molted. The timing of this molt - usually once a year but sometimes twice - varies with some species molting prior to moving to their winter grounds and others molting prior to returning to their breeding grounds. Apart from physiological adaptations, migration sometimes requires behavioural changes such as flying in flocks to reduce the energy used in migration or the risk of predation. Hunting along migration routes threatens some bird species. The populations of Siberian cranes (Leucogeranus leucogeranus) that wintered in India declined due to hunting along the route, particularly in Afghanistan and Central Asia. Birds were last seen in their favourite wintering grounds in Keoladeo National Park in 2002. Structures such as power lines, wind farms and offshore oil-rigs have also been known to affect migratory birds. Other migration hazards include pollution, storms, wildfires, and habitat destruction along migration routes, denying migrants food at stopover points. For example, in the East Asian–Australasian Flyway, up to 65% of key intertidal habitat at the Yellow Sea migration bottleneck has been destroyed since the 1950s. Sometimes circumstances such as a good breeding season followed by a food source failure the following year lead to irruptions in which large numbers of a species move far beyond the normal range. Bohemian waxwings Bombycilla garrulus well show this unpredictable variation in annual numbers, with five major arrivals in Britain during the nineteenth century, but 18 between the years 1937 and 2000. Red crossbills Loxia curvirostra too are irruptive, with widespread invasions across England noted in 1251, 1593, 1757, and 1791. Birds fly at varying altitudes during migration. An expedition to Mt. Everest found skeletons of northern pintail Anas acuta and black-tailed godwit Limosa limosa at 5,000 m (16,000 ft) on the Khumbu Glacier. Bar-headed geese Anser indicus have been recorded by GPS flying at up to 6,540 metres (21,460 ft) while crossing the Himalayas, at the same time engaging in the highest rates of climb to altitude for any bird. Anecdotal reports of them flying much higher have yet to be corroborated with any direct evidence. Seabirds fly low over water but gain altitude when crossing land, and the reverse pattern is seen in landbirds. However most bird migration is in the range of 150 to 600 m (490 to 1,970 ft). Bird strike aviation records from the United States show most collisions occur below 600 m (2,000 ft) and almost none above 1,800 m (5,900 ft). It was not until the end of the eighteenth century that migration as an explanation for the winter disappearance of birds from northern climes was accepted. Thomas Bewick's A History of British Birds (Volume 1, 1797) mentions a report from ""a very intelligent master of a vessel"" who, ""between the islands of Minorca and Majorca, saw great numbers of Swallows flying northward"", and states the situation in Britain as follows: Bewick then describes an experiment which succeeded in keeping swallows alive in Britain for several years, where they remained warm and dry through the winters. He concludes: The typical image of migration is of northern landbirds, such as swallows (Hirundinidae) and birds of prey, making long flights to the tropics. However, many Holarctic wildfowl and finch (Fringillidae) species winter in the North Temperate Zone, in regions with milder winters than their summer breeding grounds. For example, the pink-footed goose Anser brachyrhynchus migrates from Iceland to Britain and neighbouring countries, whilst the dark-eyed junco Junco hyemalis migrates from subarctic and arctic climates to the contiguous United States and the American goldfinch from taiga to wintering grounds extending from the American South northwestward to Western Oregon. Migratory routes and wintering grounds are traditional and learned by young during their first migration with their parents. Some ducks, such as the garganey Anas querquedula, move completely or partially into the tropics. The European pied flycatcher Ficedula hypoleuca also follows this migratory trend, breeding in Asia and Europe and wintering in Africa. It has been possible to teach a migration route to a flock of birds, for example in re-introduction schemes. After a trial with Canada geese Branta canadensis, microlight aircraft were used in the US to teach safe migration routes to reintroduced whooping cranes Grus americana. Often, the migration route of a long-distance migrator bird doesn't follow a straight line between breeding and wintering grounds. Rather, it could follow an hooked or arched line, with detours around geographical barriers. For most land-birds, such barriers could consist in seas, large water bodies or high mountain ranges, because of the lack of stopover or feeding sites, or the lack of thermal columns for broad-winged birds. The Arctic tern holds the long-distance migration record for birds, travelling between Arctic breeding grounds and the Antarctic each year. Some species of tubenoses (Procellariiformes) such as albatrosses circle the earth, flying over the southern oceans, while others such as Manx shearwaters migrate 14,000 km (8,700 mi) between their northern breeding grounds and the southern ocean. Shorter migrations are common, including altitudinal migrations on mountains such as the Andes and Himalayas. Migration is the regular seasonal movement, often north and south, undertaken by many species of birds. Bird movements include those made in response to changes in food availability, habitat, or weather. Sometimes, journeys are not termed ""true migration"" because they are irregular (nomadism, invasions, irruptions) or in only one direction (dispersal, movement of young away from natal area). Migration is marked by its annual seasonality. Non-migratory birds are said to be resident or sedentary. Approximately 1800 of the world's 10,000 bird species are long-distance migrants. The migration of birds also aids the movement of other species, including those of ectoparasites such as ticks and lice, which in turn may carry micro-organisms including those of concern to human health. Due to the global spread of avian influenza, bird migration has been studied as a possible mechanism of disease transmission, but it has been found not to present a special risk; import of pet and domestic birds is a greater threat. Some viruses that are maintained in birds without lethal effects, such as the West Nile Virus may however be spread by migrating birds. Birds may also have a role in the dispersal of propagules of plants and plankton. Migratory birds may use two electromagnetic tools to find their destinations: one that is entirely innate and another that relies on experience. A young bird on its first migration flies in the correct direction according to the Earth's magnetic field, but does not know how far the journey will be. It does this through a radical pair mechanism whereby chemical reactions in special photo pigments sensitive to long wavelengths are affected by the field. Although this only works during daylight hours, it does not use the position of the sun in any way. At this stage the bird is in the position of a boy scout with a compass but no map, until it grows accustomed to the journey and can put its other capabilities to use. With experience it learns various landmarks and this ""mapping"" is done by magnetites in the trigeminal system, which tell the bird how strong the field is. Because birds migrate between northern and southern regions, the magnetic field strengths at different latitudes let it interpret the radical pair mechanism more accurately and let it know when it has reached its destination. There is a neural connection between the eye and ""Cluster N"", the part of the forebrain that is active during migrational orientation, suggesting that birds may actually be able to see the magnetic field of the earth. A similar situation occurs with waders (called shorebirds in North America). Many species, such as dunlin Calidris alpina and western sandpiper Calidris mauri, undertake long movements from their Arctic breeding grounds to warmer locations in the same hemisphere, but others such as semipalmated sandpiper C. pusilla travel longer distances to the tropics in the Southern Hemisphere. Seabird migration is similar in pattern to those of the waders and waterfowl. Some, such as the black guillemot Cepphus grylle and some gulls, are quite sedentary; others, such as most terns and auks breeding in the temperate northern hemisphere, move varying distances south in the northern winter. The Arctic tern Sterna paradisaea has the longest-distance migration of any bird, and sees more daylight than any other, moving from its Arctic breeding grounds to the Antarctic non-breeding areas. One Arctic tern, ringed (banded) as a chick on the Farne Islands off the British east coast, reached Melbourne, Australia in just three months from fledging, a sea journey of over 22,000 km (14,000 mi). Many tubenosed birds breed in the southern hemisphere and migrate north in the southern winter. Theoretical analyses show that detours that increase flight distance by up to 20% will often be adaptive on aerodynamic grounds - a bird that loads itself with food to cross a long barrier flies less efficiently. However some species show circuitous migratory routes that reflect historical range expansions and are far from optimal in ecological terms. An example is the migration of continental populations of Swainson's thrush Catharus ustulatus, which fly far east across North America before turning south via Florida to reach northern South America; this route is believed to be the consequence of a range expansion that occurred about 10,000 years ago. Detours may also be caused by differential wind conditions, predation risk, or other factors. The primary physiological cue for migration are the changes in the day length. These changes are also related to hormonal changes in the birds. In the period before migration, many birds display higher activity or Zugunruhe (German: migratory restlessness), first described by Johann Friedrich Naumann in 1795, as well as physiological changes such as increased fat deposition. The occurrence of Zugunruhe even in cage-raised birds with no environmental cues (e.g. shortening of day and falling temperature) has pointed to the role of circannual endogenous programs in controlling bird migrations. Caged birds display a preferential flight direction that corresponds with the migratory direction they would take in nature, changing their preferential direction at roughly the same time their wild conspecifics change course. These advantages offset the high stress, physical exertion costs, and other risks of the migration. Predation can be heightened during migration: Eleonora's falcon Falco eleonorae, which breeds on Mediterranean islands, has a very late breeding season, coordinated with the autumn passage of southbound passerine migrants, which it feeds to its young. A similar strategy is adopted by the greater noctule bat, which preys on nocturnal passerine migrants. The higher concentrations of migrating birds at stopover sites make them prone to parasites and pathogens, which require a heightened immune response. Many bird populations migrate long distances along a flyway. The most common pattern involves flying north in the spring to breed in the temperate or Arctic summer and returning in the autumn to wintering grounds in warmer regions to the south. Of course, in the southern hemisphere the directions are reversed, but there is less land area in the far south to support long-distance migration. Large scale climatic changes, as have been experienced in the past, are expected to have an effect on the timing of migration. Studies have shown a variety of effects including timing changes in migration, breeding as well as population variations. In the tropics there is little variation in the length of day throughout the year, and it is always warm enough for a food supply, but altitudinal migration occurs in some tropical birds. There is evidence that this enables the migrants to obtain more of their preferred foods such as fruits. For some species of waders, migration success depends on the availability of certain key food resources at stopover points along the migration route. This gives the migrants an opportunity to refuel for the next leg of the voyage. Some examples of important stopover locations are the Bay of Fundy and Delaware Bay. Within a population, it is common for different ages and/or sexes to have different patterns of timing and distance. Female chaffinches Fringilla coelebs in Eastern Fennoscandia migrate earlier in the autumn than males do. The primary motivation for migration appears to be food; for example, some hummingbirds choose not to migrate if fed through the winter. Also, the longer days of the northern summer provide extended time for breeding birds to feed their young. This helps diurnal birds to produce larger clutches than related non-migratory species that remain in the tropics. As the days shorten in autumn, the birds return to warmer regions where the available food supply varies little with the season. A related phenomenon called ""abmigration"" involves birds from one region joining similar birds from a different breeding region in the common winter grounds and then migrating back along with the new population. This is especially common in some waterfowl, which shift from one flyway to another. Bird migration is the regular seasonal movement, often north and south along a flyway, between breeding and wintering grounds. Many species of bird migrate. Migration carries high costs in predation and mortality, including from hunting by humans, and is driven primarily by availability of food. It occurs mainly in the northern hemisphere, where birds are funnelled on to specific routes by natural barriers such as the Mediterranean Sea or the Caribbean Sea. Short-distance passerine migrants have two evolutionary origins. Those that have long-distance migrants in the same family, such as the common chiffchaff Phylloscopus collybita, are species of southern hemisphere origins that have progressively shortened their return migration to stay in the northern hemisphere. Reverse migration, where the genetic programming of young birds fails to work properly, can lead to rarities turning up as vagrants thousands of kilometres out of range. The most pelagic species, mainly in the 'tubenose' order Procellariiformes, are great wanderers, and the albatrosses of the southern oceans may circle the globe as they ride the ""roaring forties"" outside the breeding season. The tubenoses spread widely over large areas of open ocean, but congregate when food becomes available. Many are also among the longest-distance migrants; sooty shearwaters Puffinus griseus nesting on the Falkland Islands migrate 14,000 km (8,700 mi) between the breeding colony and the North Atlantic Ocean off Norway. Some Manx shearwaters Puffinus puffinus do this same journey in reverse. As they are long-lived birds, they may cover enormous distances during their lives; one record-breaking Manx shearwater is calculated to have flown 8 million km (5 million miles) during its over-50 year lifespan. Some predators take advantage of the concentration of birds during migration. Greater noctule bats feed on nocturnal migrating passerines. Some birds of prey specialize on migrating waders. The same considerations about barriers and detours that apply to long-distance land-bird migration apply to water birds, but in reverse: a large area of land without bodies of water that offer feeding sites may also be a barrier to a bird that feeds in coastal waters. Detours avoiding such barriers are observed: for example, brent geese Branta bernicla migrating from the Taymyr Peninsula to the Wadden Sea travel via the White Sea coast and the Baltic Sea rather than directly across the Arctic Ocean and northern Scandinavia. Nocturnal migrants minimize predation, avoid overheating, and can feed during the day. One cost of nocturnal migration is the loss of sleep. Migrants may be able to alter their quality of sleep to compensate for the loss. Long distance migrants are believed to disperse as young birds and form attachments to potential breeding sites and to favourite wintering sites. Once the site attachment is made they show high site-fidelity, visiting the same wintering sites year after year. Historically, migration has been recorded as much as 3,000 years ago by Ancient Greek authors including Homer and Aristotle, and in the Book of Job, for species such as storks, turtle doves, and swallows. More recently, Johannes Leche began recording dates of arrivals of spring migrants in Finland in 1749, and scientific studies have used techniques including bird ringing and satellite tracking. Threats to migratory birds have grown with habitat destruction especially of stopover and wintering sites, as well as structures such as power lines and wind farms. Migration in birds is highly labile and is believed to have developed independently in many avian lineages. While it is agreed that the behavioral and physiological adaptations necessary for migration are under genetic control, some authors have argued that no genetic change is necessary for migratory behavior to develop in a sedentary species because the genetic framework for migratory behavior exists in nearly all avian lineages. This explains the rapid appearance of migratory behavior after the most recent glacial maximum. Bird migration is primarily, but not entirely, a Northern Hemisphere phenomenon. This is because land birds in high northern latitudes, where food becomes scarce in winter, leave for areas further south (including the Southern Hemisphere) to overwinter, and because the continental landmass is much larger in the Northern Hemisphere. In contrast, among (pelagic) seabirds, species of the Southern Hemisphere are more likely to migrate. This is because there is a large area of ocean in the Southern Hemisphere, and more islands suitable for seabirds to nest. Aristotle noted that cranes traveled from the steppes of Scythia to marshes at the headwaters of the Nile. Pliny the Elder, in his Historia Naturalis, repeats Aristotle's observations. In polygynous species with considerable sexual dimorphism, males tend to return earlier to the breeding sites than their females. This is termed protandry. Bird migration is not limited to birds that can fly. Most species of penguin (Spheniscidae) migrate by swimming. These routes can cover over 1,000 km (620 mi). Dusky grouse Dendragapus obscurus perform altitudinal migration mostly by walking. Emus Dromaius novaehollandiae in Australia have been observed to undertake long-distance movements on foot during droughts. Migrating birds can lose their way and appear outside their normal ranges. This can be due to flying past their destinations as in the ""spring overshoot"" in which birds returning to their breeding areas overshoot and end up further north than intended. Certain areas, because of their location, have become famous as watchpoints for such birds. Examples are the Point Pelee National Park in Canada, and Spurn in England. Many of the smaller insectivorous birds including the warblers, hummingbirds and flycatchers migrate large distances, usually at night. They land in the morning and may feed for a few days before resuming their migration. The birds are referred to as passage migrants in the regions where they occur for short durations between the origin and destination. Many long-distance migrants appear to be genetically programmed to respond to changing day length. Species that move short distances, however, may not need such a timing mechanism, instead moving in response to local weather conditions. Thus mountain and moorland breeders, such as wallcreeper Tichodroma muraria and white-throated dipper Cinclus cinclus, may move only altitudinally to escape the cold higher ground. Other species such as merlin Falco columbarius and Eurasian skylark Alauda arvensis move further, to the coast or towards the south. Species like the chaffinch are much less migratory in Britain than those of continental Europe, mostly not moving more than 5 km in their lives. Species that have no long-distance migratory relatives, such as the waxwings Bombycilla, are effectively moving in response to winter weather and the loss of their usual winter food, rather than enhanced breeding opportunities. The ability of birds to navigate during migrations cannot be fully explained by endogenous programming, even with the help of responses to environmental cues. The ability to successfully perform long-distance migrations can probably only be fully explained with an accounting for the cognitive ability of the birds to recognize habitats and form mental maps. Satellite tracking of day migrating raptors such as ospreys and honey buzzards has shown that older individuals are better at making corrections for wind drift. Within a species not all populations may be migratory; this is known as ""partial migration"". Partial migration is very common in the southern continents; in Australia, 44% of non-passerine birds and 32% of passerine species are partially migratory. In some species, the population at higher latitudes tends to be migratory and will often winter at lower latitude. The migrating birds bypass the latitudes where other populations may be sedentary, where suitable wintering habitats may already be occupied. This is an example of leap-frog migration. Many fully migratory species show leap-frog migration (birds that nest at higher latitudes spend the winter at lower latitudes), and many show the alternative, chain migration, where populations 'slide' more evenly north and south without reversing order. Orientation behaviour studies have been traditionally carried out using variants of a setup known as the Emlen funnel, which consists of a circular cage with the top covered by glass or wire-screen so that either the sky is visible or the setup is placed in a planetarium or with other controls on environmental cues. The orientation behaviour of the bird inside the cage is studied quantitatively using the distribution of marks that the bird leaves on the walls of the cage. Other approaches used in pigeon homing studies make use of the direction in which the bird vanishes on the horizon. The timing of migration seems to be controlled primarily by changes in day length. Migrating birds navigate using celestial cues from the sun and stars, the earth's magnetic field, and probably also mental maps. Most migrations begin with the birds starting off in a broad front. Often, this front narrows into one or more preferred routes termed flyways. These routes typically follow mountain ranges or coastlines, sometimes rivers, and may take advantage of updrafts and other wind patterns or avoid geographical barriers such as large stretches of open water. The specific routes may be genetically programmed or learned to varying degrees. The routes taken on forward and return migration are often different. A common pattern in North America is clockwise migration, where birds flying North tend to be further West, and flying South tend to shift Eastwards."
Infrared,"In the semiconductor industry, infrared light can be used to characterize materials such as thin films and periodic trench structures. By measuring the reflectance of light from the surface of a semiconductor wafer, the index of refraction (n) and the extinction Coefficient (k) can be determined via the Forouhi-Bloomer dispersion equations. The reflectance from the infrared light can also be used to determine the critical dimension, depth, and sidewall angle of high aspect ratio trench structures. Near-infrared is the region closest in wavelength to the radiation detectable by the human eye, mid- and far-infrared are progressively further from the visible spectrum. Other definitions follow different physical mechanisms (emission peaks, vs. bands, water absorption) and the newest follow technical reasons (the common silicon detectors are sensitive to about 1,050 nm, while InGaAs's sensitivity starts around 950 nm and ends between 1,700 and 2,600 nm, depending on the specific configuration). Unfortunately, international standards for these specifications are not currently available. The infrared portion of the spectrum has several useful benefits for astronomers. Cold, dark molecular clouds of gas and dust in our galaxy will glow with radiated heat as they are irradiated by imbedded stars. Infrared can also be used to detect protostars before they begin to emit visible light. Stars emit a smaller portion of their energy in the infrared spectrum, so nearby cool objects such as planets can be more readily detected. (In the visible light spectrum, the glare from the star will drown out the reflected light from a planet.) Thermographic cameras detect radiation in the infrared range of the electromagnetic spectrum (roughly 900–14,000 nanometers or 0.9–14 μm) and produce images of that radiation. Since infrared radiation is emitted by all objects based on their temperatures, according to the black body radiation law, thermography makes it possible to ""see"" one's environment with or without visible illumination. The amount of radiation emitted by an object increases with temperature, therefore thermography allows one to see variations in temperature (hence the name). IR data transmission is also employed in short-range communication among computer peripherals and personal digital assistants. These devices usually conform to standards published by IrDA, the Infrared Data Association. Remote controls and IrDA devices use infrared light-emitting diodes (LEDs) to emit infrared radiation that is focused by a plastic lens into a narrow beam. The beam is modulated, i.e. switched on and off, to encode the data. The receiver uses a silicon photodiode to convert the infrared radiation to an electric current. It responds only to the rapidly pulsing signal created by the transmitter, and filters out slowly changing infrared radiation from ambient light. Infrared communications are useful for indoor use in areas of high population density. IR does not penetrate walls and so does not interfere with other devices in adjoining rooms. Infrared is the most common way for remote controls to command appliances. Infrared remote control protocols like RC-5, SIRC, are used to communicate with infrared. Infrared vibrational spectroscopy (see also near-infrared spectroscopy) is a technique that can be used to identify molecules by analysis of their constituent bonds. Each chemical bond in a molecule vibrates at a frequency characteristic of that bond. A group of atoms in a molecule (e.g., CH2) may have multiple modes of oscillation caused by the stretching and bending motions of the group as a whole. If an oscillation leads to a change in dipole in the molecule then it will absorb a photon that has the same frequency. The vibrational frequencies of most molecules correspond to the frequencies of infrared light. Typically, the technique is used to study organic compounds using light radiation from 4000–400 cm−1, the mid-infrared. A spectrum of all the frequencies of absorption in a sample is recorded. This can be used to gain information about the sample composition in terms of chemical groups present and also its purity (for example, a wet sample will show a broad O-H absorption around 3200 cm−1). Infrared tracking, also known as infrared homing, refers to a passive missile guidance system, which uses the emission from a target of electromagnetic radiation in the infrared part of the spectrum to track it. Missiles that use infrared seeking are often referred to as ""heat-seekers"", since infrared (IR) is just below the visible spectrum of light in frequency and is radiated strongly by hot bodies. Many objects such as people, vehicle engines, and aircraft generate and retain heat, and as such, are especially visible in the infrared wavelengths of light compared to objects in the background. The concept of emissivity is important in understanding the infrared emissions of objects. This is a property of a surface that describes how its thermal emissions deviate from the ideal of a black body. To further explain, two objects at the same physical temperature will not show the same infrared image if they have differing emissivity. For example, for any pre-set emissivity value, objects with higher emissivity will appear hotter, and those with a lower emissivity will appear cooler. For that reason, incorrect selection of emissivity will give inaccurate results when using infrared cameras and pyrometers. The sensitivity of Earth-based infrared telescopes is significantly limited by water vapor in the atmosphere, which absorbs a portion of the infrared radiation arriving from space outside of selected atmospheric windows. This limitation can be partially alleviated by placing the telescope observatory at a high altitude, or by carrying the telescope aloft with a balloon or an aircraft. Space telescopes do not suffer from this handicap, and so outer space is considered the ideal location for infrared astronomy. Infrared radiation is used in industrial, scientific, and medical applications. Night-vision devices using active near-infrared illumination allow people or animals to be observed without the observer being detected. Infrared astronomy uses sensor-equipped telescopes to penetrate dusty regions of space, such as molecular clouds; detect objects such as planets, and to view highly red-shifted objects from the early days of the universe. Infrared thermal-imaging cameras are used to detect heat loss in insulated systems, to observe changing blood flow in the skin, and to detect overheating of electrical apparatus. Heat is energy in transit that flows due to temperature difference. Unlike heat transmitted by thermal conduction or thermal convection, thermal radiation can propagate through a vacuum. Thermal radiation is characterized by a particular spectrum of many wavelengths that is associated with emission from an object, due to the vibration of its molecules at a given temperature. Thermal radiation can be emitted from objects at any wavelength, and at very high temperatures such radiations are associated with spectra far above the infrared, extending into visible, ultraviolet, and even X-ray regions (i.e., the solar corona). Thus, the popular association of infrared radiation with thermal radiation is only a coincidence based on typical (comparatively low) temperatures often found near the surface of planet Earth. In infrared photography, infrared filters are used to capture the near-infrared spectrum. Digital cameras often use infrared blockers. Cheaper digital cameras and camera phones have less effective filters and can ""see"" intense near-infrared, appearing as a bright purple-white color. This is especially pronounced when taking pictures of subjects near IR-bright areas (such as near a lamp), where the resulting infrared interference can wash out the image. There is also a technique called 'T-ray' imaging, which is imaging using far-infrared or terahertz radiation. Lack of bright sources can make terahertz photography more challenging than most other infrared imaging techniques. Recently T-ray imaging has been of considerable interest due to a number of new developments such as terahertz time-domain spectroscopy. Infrared cleaning is a technique used by some Motion picture film scanner, film scanners and flatbed scanners to reduce or remove the effect of dust and scratches upon the finished scan. It works by collecting an additional infrared channel from the scan at the same position and resolution as the three visible color channels (red, green, and blue). The infrared channel, in combination with the other channels, is used to detect the location of scratches and dust. Once located, those defects can be corrected by scaling or replaced by inpainting. High, cold ice clouds such as Cirrus or Cumulonimbus show up bright white, lower warmer clouds such as Stratus or Stratocumulus show up as grey with intermediate clouds shaded accordingly. Hot land surfaces will show up as dark-grey or black. One disadvantage of infrared imagery is that low cloud such as stratus or fog can be a similar temperature to the surrounding land or sea surface and does not show up. However, using the difference in brightness of the IR4 channel (10.3–11.5 µm) and the near-infrared channel (1.58–1.64 µm), low cloud can be distinguished, producing a fog satellite picture. The main advantage of infrared is that images can be produced at night, allowing a continuous sequence of weather to be studied. Infrared reflectography (fr; it; es), as called by art conservators, can be applied to paintings to reveal underlying layers in a completely non-destructive manner, in particular the underdrawing or outline drawn by the artist as a guide. This often reveals the artist's use of carbon black, which shows up well in reflectograms, as long as it has not also been used in the ground underlying the whole painting. Art conservators are looking to see whether the visible layers of paint differ from the underdrawing or layers in between – such alterations are called pentimenti when made by the original artist. This is very useful information in deciding whether a painting is the prime version by the original artist or a copy, and whether it has been altered by over-enthusiastic restoration work. In general, the more pentimenti the more likely a painting is to be the prime version. It also gives useful insights into working practices. Earth's surface and the clouds absorb visible and invisible radiation from the sun and re-emit much of the energy as infrared back to atmosphere. Certain substances in the atmosphere, chiefly cloud droplets and water vapor, but also carbon dioxide, methane, nitrous oxide, sulfur hexafluoride, and chlorofluorocarbons, absorb this infrared, and re-radiate it in all directions including back to Earth. Thus, the greenhouse effect keeps the atmosphere and surface much warmer than if the infrared absorbers were absent from the atmosphere. Infrared radiation is popularly known as ""heat radiation""[citation needed], but light and electromagnetic waves of any frequency will heat surfaces that absorb them. Infrared light from the Sun accounts for 49% of the heating of Earth, with the rest being caused by visible light that is absorbed then re-radiated at longer wavelengths. Visible light or ultraviolet-emitting lasers can char paper and incandescently hot objects emit visible radiation. Objects at room temperature will emit radiation concentrated mostly in the 8 to 25 µm band, but this is not distinct from the emission of visible light by incandescent objects and ultraviolet by even hotter objects (see black body and Wien's displacement law). The onset of infrared is defined (according to different standards) at various values typically between 700 nm and 800 nm, but the boundary between visible and infrared light is not precisely defined. The human eye is markedly less sensitive to light above 700 nm wavelength, so longer wavelengths make insignificant contributions to scenes illuminated by common light sources. However, particularly intense near-IR light (e.g., from IR lasers, IR LED sources, or from bright daylight with the visible light removed by colored gels) can be detected up to approximately 780 nm, and will be perceived as red light. Sources providing wavelengths as long as 1050 nm can be seen as a dull red glow in intense sources, causing some difficulty in near-IR illumination of scenes in the dark (usually this practical problem is solved by indirect illumination). Leaves are particularly bright in the near IR, and if all visible light leaks from around an IR-filter are blocked, and the eye is given a moment to adjust to the extremely dim image coming through a visually opaque IR-passing photographic filter, it is possible to see the Wood effect that consists of IR-glowing foliage. Infrared is used in night vision equipment when there is insufficient visible light to see. Night vision devices operate through a process involving the conversion of ambient light photons into electrons that are then amplified by a chemical and electrical process and then converted back into visible light. Infrared light sources can be used to augment the available ambient light for conversion by night vision devices, increasing in-the-dark visibility without actually using a visible light source. The discovery of infrared radiation is ascribed to William Herschel, the astronomer, in the early 19th century. Herschel published his results in 1800 before the Royal Society of London. Herschel used a prism to refract light from the sun and detected the infrared, beyond the red part of the spectrum, through an increase in the temperature recorded on a thermometer. He was surprised at the result and called them ""Calorific Rays"". The term 'Infrared' did not appear until late in the 19th century."
Amazon_rainforest,"The region is home to about 2.5 million insect species, tens of thousands of plants, and some 2,000 birds and mammals. To date, at least 40,000 plant species, 2,200 fishes, 1,294 birds, 427 mammals, 428 amphibians, and 378 reptiles have been scientifically classified in the region. One in five of all the bird species in the world live in the rainforests of the Amazon, and one in five of the fish species live in Amazonian rivers and streams. Scientists have described between 96,660 and 128,843 invertebrate species in Brazil alone. As indigenous territories continue to be destroyed by deforestation and ecocide, such as in the Peruvian Amazon indigenous peoples' rainforest communities continue to disappear, while others, like the Urarina continue to struggle to fight for their cultural survival and the fate of their forested territories. Meanwhile, the relationship between non-human primates in the subsistence and symbolism of indigenous lowland South American peoples has gained increased attention, as have ethno-biology and community-based conservation efforts. Deforestation is the conversion of forested areas to non-forested areas. The main sources of deforestation in the Amazon are human settlement and development of the land. Prior to the early 1960s, access to the forest's interior was highly restricted, and the forest remained basically intact. Farms established during the 1960s were based on crop cultivation and the slash and burn method. However, the colonists were unable to manage their fields and the crops because of the loss of soil fertility and weed invasion. The soils in the Amazon are productive for just a short period of time, so farmers are constantly moving to new areas and clearing more land. These farming practices led to deforestation and caused extensive environmental damage. Deforestation is considerable, and areas cleared of forest are visible to the naked eye from outer space. The rainforest contains several species that can pose a hazard. Among the largest predatory creatures are the black caiman, jaguar, cougar, and anaconda. In the river, electric eels can produce an electric shock that can stun or kill, while piranha are known to bite and injure humans. Various species of poison dart frogs secrete lipophilic alkaloid toxins through their flesh. There are also numerous parasites and disease vectors. Vampire bats dwell in the rainforest and can spread the rabies virus. Malaria, yellow fever and Dengue fever can also be contracted in the Amazon region. In 2010 the Amazon rainforest experienced another severe drought, in some ways more extreme than the 2005 drought. The affected region was approximate 1,160,000 square miles (3,000,000 km2) of rainforest, compared to 734,000 square miles (1,900,000 km2) in 2005. The 2010 drought had three epicenters where vegetation died off, whereas in 2005 the drought was focused on the southwestern part. The findings were published in the journal Science. In a typical year the Amazon absorbs 1.5 gigatons of carbon dioxide; during 2005 instead 5 gigatons were released and in 2010 8 gigatons were released. The biodiversity of plant species is the highest on Earth with one 2001 study finding a quarter square kilometer (62 acres) of Ecuadorian rainforest supports more than 1,100 tree species. A study in 1999 found one square kilometer (247 acres) of Amazon rainforest can contain about 90,790 tonnes of living plants. The average plant biomass is estimated at 356 ± 47 tonnes per hectare. To date, an estimated 438,000 species of plants of economic and social interest have been registered in the region with many more remaining to be discovered or catalogued. The total number of tree species in the region is estimated at 16,000. For a long time, it was thought that the Amazon rainforest was only ever sparsely populated, as it was impossible to sustain a large population through agriculture given the poor soil. Archeologist Betty Meggers was a prominent proponent of this idea, as described in her book Amazonia: Man and Culture in a Counterfeit Paradise. She claimed that a population density of 0.2 inhabitants per square kilometre (0.52/sq mi) is the maximum that can be sustained in the rainforest through hunting, with agriculture needed to host a larger population. However, recent anthropological findings have suggested that the region was actually densely populated. Some 5 million people may have lived in the Amazon region in AD 1500, divided between dense coastal settlements, such as that at Marajó, and inland dwellers. By 1900 the population had fallen to 1 million and by the early 1980s it was less than 200,000. During the mid-Eocene, it is believed that the drainage basin of the Amazon was split along the middle of the continent by the Purus Arch. Water on the eastern side flowed toward the Atlantic, while to the west water flowed toward the Pacific across the Amazonas Basin. As the Andes Mountains rose, however, a large basin was created that enclosed a lake; now known as the Solimões Basin. Within the last 5–10 million years, this accumulating water broke through the Purus Arch, joining the easterly flow toward the Atlantic. The first European to travel the length of the Amazon River was Francisco de Orellana in 1542. The BBC's Unnatural Histories presents evidence that Orellana, rather than exaggerating his claims as previously thought, was correct in his observations that a complex civilization was flourishing along the Amazon in the 1540s. It is believed that the civilization was later devastated by the spread of diseases from Europe, such as smallpox. Since the 1970s, numerous geoglyphs have been discovered on deforested land dating between AD 0–1250, furthering claims about Pre-Columbian civilizations. Ondemar Dias is accredited with first discovering the geoglyphs in 1977 and Alceu Ranzi with furthering their discovery after flying over Acre. The BBC's Unnatural Histories presented evidence that the Amazon rainforest, rather than being a pristine wilderness, has been shaped by man for at least 11,000 years through practices such as forest gardening and terra preta. Environmentalists are concerned about loss of biodiversity that will result from destruction of the forest, and also about the release of the carbon contained within the vegetation, which could accelerate global warming. Amazonian evergreen forests account for about 10% of the world's terrestrial primary productivity and 10% of the carbon stores in ecosystems—of the order of 1.1 × 1011 metric tonnes of carbon. Amazonian forests are estimated to have accumulated 0.62 ± 0.37 tons of carbon per hectare per year between 1975 and 1996. Between 1991 and 2000, the total area of forest lost in the Amazon rose from 415,000 to 587,000 square kilometres (160,000 to 227,000 sq mi), with most of the lost forest becoming pasture for cattle. Seventy percent of formerly forested land in the Amazon, and 91% of land deforested since 1970, is used for livestock pasture. Currently, Brazil is the second-largest global producer of soybeans after the United States. New research however, conducted by Leydimere Oliveira et al., has shown that the more rainforest is logged in the Amazon, the less precipitation reaches the area and so the lower the yield per hectare becomes. So despite the popular perception, there has been no economical advantage for Brazil from logging rainforest zones and converting these to pastoral fields. The use of remote sensing for the conservation of the Amazon is also being used by the indigenous tribes of the basin to protect their tribal lands from commercial interests. Using handheld GPS devices and programs like Google Earth, members of the Trio Tribe, who live in the rainforests of southern Suriname, map out their ancestral lands to help strengthen their territorial claims. Currently, most tribes in the Amazon do not have clearly defined boundaries, making it easier for commercial ventures to target their territories. To accurately map the Amazon's biomass and subsequent carbon related emissions, the classification of tree growth stages within different parts of the forest is crucial. In 2006 Tatiana Kuplich organized the trees of the Amazon into four categories: (1) mature forest, (2) regenerating forest [less than three years], (3) regenerating forest [between three and five years of regrowth], and (4) regenerating forest [eleven to eighteen years of continued development]. The researcher used a combination of Synthetic aperture radar (SAR) and Thematic Mapper (TM) to accurately place the different portions of the Amazon into one of the four classifications. There is evidence that there have been significant changes in Amazon rainforest vegetation over the last 21,000 years through the Last Glacial Maximum (LGM) and subsequent deglaciation. Analyses of sediment deposits from Amazon basin paleolakes and from the Amazon Fan indicate that rainfall in the basin during the LGM was lower than for the present, and this was almost certainly associated with reduced moist tropical vegetation cover in the basin. There is debate, however, over how extensive this reduction was. Some scientists argue that the rainforest was reduced to small, isolated refugia separated by open forest and grassland; other scientists argue that the rainforest remained largely intact but extended less far to the north, south, and east than is seen today. This debate has proved difficult to resolve because the practical limitations of working in the rainforest mean that data sampling is biased away from the center of the Amazon basin, and both explanations are reasonably well supported by the available data. Following the Cretaceous–Paleogene extinction event, the extinction of the dinosaurs and the wetter climate may have allowed the tropical rainforest to spread out across the continent. From 66–34 Mya, the rainforest extended as far south as 45°. Climate fluctuations during the last 34 million years have allowed savanna regions to expand into the tropics. During the Oligocene, for example, the rainforest spanned a relatively narrow band. It expanded again during the Middle Miocene, then retracted to a mostly inland formation at the last glacial maximum. However, the rainforest still managed to thrive during these glacial periods, allowing for the survival and evolution of a broad diversity of species. NASA's CALIPSO satellite has measured the amount of dust transported by wind from the Sahara to the Amazon: an average 182 million tons of dust are windblown out of the Sahara each year, at 15 degrees west longitude, across 1,600 miles (2,600 km) over the Atlantic Ocean (some dust falls into the Atlantic), then at 35 degrees West longitude at the eastern coast of South America, 27.7 million tons (15%) of dust fall over the Amazon basin, 132 million tons of dust remain in the air, 43 million tons of dust are windblown and falls on the Caribbean Sea, past 75 degrees west longitude. The needs of soy farmers have been used to justify many of the controversial transportation projects that are currently developing in the Amazon. The first two highways successfully opened up the rainforest and led to increased settlement and deforestation. The mean annual deforestation rate from 2000 to 2005 (22,392 km2 or 8,646 sq mi per year) was 18% higher than in the previous five years (19,018 km2 or 7,343 sq mi per year). Although deforestation has declined significantly in the Brazilian Amazon between 2004 and 2014, there has been an increase to the present day. In 2005, parts of the Amazon basin experienced the worst drought in one hundred years, and there were indications that 2006 could have been a second successive year of drought. A July 23, 2006 article in the UK newspaper The Independent reported Woods Hole Research Center results showing that the forest in its present form could survive only three years of drought. Scientists at the Brazilian National Institute of Amazonian Research argue in the article that this drought response, coupled with the effects of deforestation on regional climate, are pushing the rainforest towards a ""tipping point"" where it would irreversibly start to die. It concludes that the forest is on the brink of being turned into savanna or desert, with catastrophic consequences for the world's climate. Terra preta (black earth), which is distributed over large areas in the Amazon forest, is now widely accepted as a product of indigenous soil management. The development of this fertile soil allowed agriculture and silviculture in the previously hostile environment; meaning that large portions of the Amazon rainforest are probably the result of centuries of human management, rather than naturally occurring as has previously been supposed. In the region of the Xingu tribe, remains of some of these large settlements in the middle of the Amazon forest were found in 2003 by Michael Heckenberger and colleagues of the University of Florida. Among those were evidence of roads, bridges and large plazas. The Amazon rainforest (Portuguese: Floresta Amazônica or Amazônia; Spanish: Selva Amazónica, Amazonía or usually Amazonia; French: Forêt amazonienne; Dutch: Amazoneregenwoud), also known in English as Amazonia or the Amazon Jungle, is a moist broadleaf forest that covers most of the Amazon basin of South America. This basin encompasses 7,000,000 square kilometres (2,700,000 sq mi), of which 5,500,000 square kilometres (2,100,000 sq mi) are covered by the rainforest. This region includes territory belonging to nine nations. The majority of the forest is contained within Brazil, with 60% of the rainforest, followed by Peru with 13%, Colombia with 10%, and with minor amounts in Venezuela, Ecuador, Bolivia, Guyana, Suriname and French Guiana. States or departments in four nations contain ""Amazonas"" in their names. The Amazon represents over half of the planet's remaining rainforests, and comprises the largest and most biodiverse tract of tropical rainforest in the world, with an estimated 390 billion individual trees divided into 16,000 species. One computer model of future climate change caused by greenhouse gas emissions shows that the Amazon rainforest could become unsustainable under conditions of severely reduced rainfall and increased temperatures, leading to an almost complete loss of rainforest cover in the basin by 2100. However, simulations of Amazon basin climate change across many different models are not consistent in their estimation of any rainfall response, ranging from weak increases to strong decreases. The result indicates that the rainforest could be threatened though the 21st century by climate change in addition to deforestation."
Tibet,"The main crops grown are barley, wheat, buckwheat, rye, potatoes, and assorted fruits and vegetables. Tibet is ranked the lowest among China’s 31 provinces on the Human Development Index according to UN Development Programme data. In recent years, due to increased interest in Tibetan Buddhism, tourism has become an increasingly important sector, and is actively promoted by the authorities. Tourism brings in the most income from the sale of handicrafts. These include Tibetan hats, jewelry (silver and gold), wooden items, clothing, quilts, fabrics, Tibetan rugs and carpets. The Central People's Government exempts Tibet from all taxation and provides 90% of Tibet's government expenditures. However most of this investment goes to pay migrant workers who do not settle in Tibet and send much of their income home to other provinces. In 1980, General Secretary and reformist Hu Yaobang visited Tibet and ushered in a period of social, political, and economic liberalization. At the end of the decade, however, analogously to the Tiananmen Square protests of 1989, monks in the Drepung and Sera monasteries started protesting for independence, and so the government halted reforms and started an anti-separatist campaign. Human rights organisations have been critical of the Beijing and Lhasa governments' approach to human rights in the region when cracking down on separatist convulsions that have occurred around monasteries and cities, most recently in the 2008 Tibetan unrest. Qing dynasty rule in Tibet began with their 1720 expedition to the country when they expelled the invading Dzungars. Amdo came under Qing control in 1724, and eastern Kham was incorporated into neighbouring Chinese provinces in 1728. Meanwhile, the Qing government sent resident commissioners called Ambans to Lhasa. In 1750 the Ambans and the majority of the Han Chinese and Manchus living in Lhasa were killed in a riot, and Qing troops arrived quickly and suppressed the rebels in the next year. Like the preceding Yuan dynasty, the Manchus of the Qing dynasty exerted military and administrative control of the region, while granting it a degree of political autonomy. The Qing commander publicly executed a number of supporters of the rebels and, as in 1723 and 1728, made changes in the political structure and drew up a formal organization plan. The Qing now restored the Dalai Lama as ruler, leading the governing council called Kashag, but elevated the role of Ambans to include more direct involvement in Tibetan internal affairs. At the same time the Qing took steps to counterbalance the power of the aristocracy by adding officials recruited from the clergy to key posts. The Indus and Brahmaputra rivers originate from a lake (Tib: Tso Mapham) in Western Tibet, near Mount Kailash. The mountain is a holy pilgrimage site for both Hindus and Tibetans. The Hindus consider the mountain to be the abode of Lord Shiva. The Tibetan name for Mt. Kailash is Khang Rinpoche. Tibet has numerous high-altitude lakes referred to in Tibetan as tso or co. These include Qinghai Lake, Lake Manasarovar, Namtso, Pangong Tso, Yamdrok Lake, Siling Co, Lhamo La-tso, Lumajangdong Co, Lake Puma Yumco, Lake Paiku, Lake Rakshastal, Dagze Co and Dong Co. The Qinghai Lake (Koko Nor) is the largest lake in the People's Republic of China. For several decades, peace reigned in Tibet, but in 1792 the Qing Qianlong Emperor sent a large Chinese army into Tibet to push the invading Nepalese out. This prompted yet another Qing reorganization of the Tibetan government, this time through a written plan called the ""Twenty-Nine Regulations for Better Government in Tibet"". Qing military garrisons staffed with Qing troops were now also established near the Nepalese border. Tibet was dominated by the Manchus in various stages in the 18th century, and the years immediately following the 1792 regulations were the peak of the Qing imperial commissioners' authority; but there was no attempt to make Tibet a Chinese province. The economy of Tibet is dominated by subsistence agriculture, though tourism has become a growing industry in recent decades. The dominant religion in Tibet is Tibetan Buddhism; in addition there is Bön, which is similar to Tibetan Buddhism, and there are also Tibetan Muslims and Christian minorities. Tibetan Buddhism is a primary influence on the art, music, and festivals of the region. Tibetan architecture reflects Chinese and Indian influences. Staple foods in Tibet are roasted barley, yak meat, and butter tea. After the Xinhai Revolution (1911–12) toppled the Qing dynasty and the last Qing troops were escorted out of Tibet, the new Republic of China apologized for the actions of the Qing and offered to restore the Dalai Lama's title. The Dalai Lama refused any Chinese title and declared himself ruler of an independent Tibet. In 1913, Tibet and Mongolia concluded a treaty of mutual recognition. For the next 36 years, the 13th Dalai Lama and the regents who succeeded him governed Tibet. During this time, Tibet fought Chinese warlords for control of the ethnically Tibetan areas in Xikang and Qinghai (parts of Kham and Amdo) along the upper reaches of the Yangtze River. In 1914 the Tibetan government signed the Simla Accord with Britain, ceding the South Tibet region to British India. The Chinese government denounced the agreement as illegal. Following the Xinhai Revolution against the Qing dynasty in 1912, Qing soldiers were disarmed and escorted out of Tibet Area (Ü-Tsang). The region subsequently declared its independence in 1913 without recognition by the subsequent Chinese Republican government. Later, Lhasa took control of the western part of Xikang, China. The region maintained its autonomy until 1951 when, following the Battle of Chamdo, Tibet became incorporated into the People's Republic of China, and the previous Tibetan government was abolished in 1959 after a failed uprising. Today, China governs western and central Tibet as the Tibet Autonomous Region while the eastern areas are now mostly ethnic autonomous prefectures within Sichuan, Qinghai and other neighbouring provinces. There are tensions regarding Tibet's political status and dissident groups that are active in exile. It is also said that Tibetan activists in Tibet have been arrested or tortured. The earliest Tibetan historical texts identify the Zhang Zhung culture as a people who migrated from the Amdo region into what is now the region of Guge in western Tibet. Zhang Zhung is considered to be the original home of the Bön religion. By the 1st century BCE, a neighboring kingdom arose in the Yarlung valley, and the Yarlung king, Drigum Tsenpo, attempted to remove the influence of the Zhang Zhung by expelling the Zhang's Bön priests from Yarlung. He was assassinated and Zhang Zhung continued its dominance of the region until it was annexed by Songtsen Gampo in the 7th century. Prior to Songtsän Gampo, the kings of Tibet were more mythological than factual, and there is insufficient evidence of their existence. After the Dalai Lama's government fled to Dharamsala, India, during the 1959 Tibetan Rebellion, it established a rival government-in-exile. Afterwards, the Central People's Government in Beijing renounced the agreement and began implementation of the halted social and political reforms. During the Great Leap Forward, between 200,000 and 1,000,000 Tibetans died, and approximately 6,000 monasteries were destroyed during the Cultural Revolution. In 1962 China and India fought a brief war over the disputed South Tibet and Aksai Chin regions. Although China won the war, Chinese troops withdrew north of the McMahon Line, effectively ceding South Tibet to India. From January 18–20, 2010 a national conference on Tibet and areas inhabited by Tibetans in Sichuan, Yunnan, Gansu and Qinghai was held in China and a substantial plan to improve development of the areas was announced. The conference was attended by General secretary Hu Jintao, Wu Bangguo, Wen Jiabao, Jia Qinglin, Li Changchun, Xi Jinping, Li Keqiang, He Guoqiang and Zhou Yongkang, all members of CPC Politburo Standing Committee signaling the commitment of senior Chinese leaders to development of Tibet and ethnic Tibetan areas. The plan calls for improvement of rural Tibetan income to national standards by 2020 and free education for all rural Tibetan children. China has invested 310 billion yuan (about 45.6 billion U.S. dollars) in Tibet since 2001. ""Tibet's GDP was expected to reach 43.7 billion yuan in 2009, up 170 percent from that in 2000 and posting an annual growth of 12.3 percent over the past nine years."" In 1904, a British expedition to Tibet, spurred in part by a fear that Russia was extending its power into Tibet as part of The Great Game, invaded the country, hoping that negotiations with the 13th Dalai Lama would be more effective than with Chinese representatives. When the British-led invasion reached Tibet on December 12, 1903, an armed confrontation with the ethnic Tibetans resulted in the Massacre of Chumik Shenko, which resulted in 600 fatalities amongst the Tibetan forces, compared to only 12 on the British side. Afterwards, in 1904 Francis Younghusband imposed a treaty known as the Treaty of Lhasa, which was subsequently repudiated and was succeeded by a 1906 treaty signed between Britain and China. This period also saw some contacts with Jesuits and Capuchins from Europe, and in 1774 a Scottish nobleman, George Bogle, came to Shigatse to investigate prospects of trade for the British East India Company. However, in the 19th century the situation of foreigners in Tibet grew more tenuous. The British Empire was encroaching from northern India into the Himalayas, the Emirate of Afghanistan and the Russian Empire were expanding into Central Asia and each power became suspicious of the others' intentions in Tibet. Muslims have been living in Tibet since as early as the 8th or 9th century. In Tibetan cities, there are small communities of Muslims, known as Kachee (Kache), who trace their origin to immigrants from three main regions: Kashmir (Kachee Yul in ancient Tibetan), Ladakh and the Central Asian Turkic countries. Islamic influence in Tibet also came from Persia. After 1959 a group of Tibetan Muslims made a case for Indian nationality based on their historic roots to Kashmir and the Indian government declared all Tibetan Muslims Indian citizens later on that year. Other Muslim ethnic groups who have long inhabited Tibet include Hui, Salar, Dongxiang and Bonan. There is also a well established Chinese Muslim community (gya kachee), which traces its ancestry back to the Hui ethnic group of China. In 1877, the Protestant James Cameron from the China Inland Mission walked from Chongqing to Batang in Garzê Tibetan Autonomous Prefecture, Sichuan province, and ""brought the Gospel to the Tibetan people."" Beginning in the 20th century, in Diqing Tibetan Autonomous Prefecture in Yunnan, a large number of Lisu people and some Yi and Nu people converted to Christianity. Famous earlier missionaries include James O. Fraser, Alfred James Broomhall and Isobel Kuhn of the China Inland Mission, among others who were active in this area. Other pre-modern Chinese names for Tibet include Wusiguo (Chinese: 烏斯國; pinyin: Wūsīguó; cf. Tibetan dbus, Ü, [wyʔ˨˧˨]), Wusizang (Chinese: 烏斯藏; pinyin: wūsīzàng, cf. Tibetan dbus-gtsang, Ü-Tsang), Tubote (Chinese: 圖伯特; pinyin: Túbótè), and Tanggute (Chinese: 唐古忒; pinyin: Tánggǔtè, cf. Tangut). American Tibetologist Elliot Sperling has argued in favor of a recent tendency by some authors writing in Chinese to revive the term Tubote (simplified Chinese: 图伯特; traditional Chinese: 圖伯特; pinyin: Túbótè) for modern use in place of Xizang, on the grounds that Tubote more clearly includes the entire Tibetan plateau rather than simply the Tibet Autonomous Region.[citation needed] The atmosphere is severely dry nine months of the year, and average annual snowfall is only 18 inches (46 cm), due to the rain shadow effect. Western passes receive small amounts of fresh snow each year but remain traversible all year round. Low temperatures are prevalent throughout these western regions, where bleak desolation is unrelieved by any vegetation bigger than a low bush, and where wind sweeps unchecked across vast expanses of arid plain. The Indian monsoon exerts some influence on eastern Tibet. Northern Tibet is subject to high temperatures in the summer and intense cold in the winter. Tibetan music often involves chanting in Tibetan or Sanskrit, as an integral part of the religion. These chants are complex, often recitations of sacred texts or in celebration of various festivals. Yang chanting, performed without metrical timing, is accompanied by resonant drums and low, sustained syllables. Other styles include those unique to the various schools of Tibetan Buddhism, such as the classical music of the popular Gelugpa school, and the romantic music of the Nyingmapa, Sakyapa and Kagyupa schools. Tibet has various festivals that are commonly performed to worship the Buddha[citation needed] throughout the year. Losar is the Tibetan New Year Festival. Preparations for the festive event are manifested by special offerings to family shrine deities, painted doors with religious symbols, and other painstaking jobs done to prepare for the event. Tibetans eat Guthuk (barley noodle soup with filling) on New Year's Eve with their families. The Monlam Prayer Festival follows it in the first month of the Tibetan calendar, falling between the fourth and the eleventh days of the first Tibetan month. It involves dancing and participating in sports events, as well as sharing picnics. The event was established in 1049 by Tsong Khapa, the founder of the Dalai Lama and the Panchen Lama's order. In 1661 another Jesuit, Johann Grueber, crossed Tibet from Sining to Lhasa (where he spent a month), before heading on to Nepal. He was followed by others who actually built a church in Lhasa. These included the Jesuit Father Ippolito Desideri, 1716–1721, who gained a deep knowledge of Tibetan culture, language and Buddhism, and various Capuchins in 1707–1711, 1716–1733 and 1741–1745, Christianity was used by some Tibetan monarchs and their courts and the Karmapa sect lamas to counterbalance the influence of the Gelugpa sect in the 17th century until in 1745 when all the missionaries were expelled at the lama's insistence. Historically, the population of Tibet consisted of primarily ethnic Tibetans and some other ethnic groups. According to tradition the original ancestors of the Tibetan people, as represented by the six red bands in the Tibetan flag, are: the Se, Mu, Dong, Tong, Dru and Ra. Other traditional ethnic groups with significant population or with the majority of the ethnic group residing in Tibet (excluding a disputed area with India) include Bai people, Blang, Bonan, Dongxiang, Han, Hui people, Lhoba, Lisu people, Miao, Mongols, Monguor (Tu people), Menba (Monpa), Mosuo, Nakhi, Qiang, Nu people, Pumi, Salar, and Yi people. The language has numerous regional dialects which are generally not mutually intelligible. It is employed throughout the Tibetan plateau and Bhutan and is also spoken in parts of Nepal and northern India, such as Sikkim. In general, the dialects of central Tibet (including Lhasa), Kham, Amdo and some smaller nearby areas are considered Tibetan dialects. Other forms, particularly Dzongkha, Sikkimese, Sherpa, and Ladakhi, are considered by their speakers, largely for political reasons, to be separate languages. However, if the latter group of Tibetan-type languages are included in the calculation, then 'greater Tibetan' is spoken by approximately 6 million people across the Tibetan Plateau. Tibetan is also spoken by approximately 150,000 exile speakers who have fled from modern-day Tibet to India and other countries. The Tibetan Empire emerged in the 7th century, but with the fall of the empire the region soon divided into a variety of territories. The bulk of western and central Tibet (Ü-Tsang) was often at least nominally unified under a series of Tibetan governments in Lhasa, Shigatse, or nearby locations; these governments were at various times under Mongol and Chinese overlordship. The eastern regions of Kham and Amdo often maintained a more decentralized indigenous political structure, being divided among a number of small principalities and tribal groups, while also often falling more directly under Chinese rule after the Battle of Chamdo; most of this area was eventually incorporated into the Chinese provinces of Sichuan and Qinghai. The current borders of Tibet were generally established in the 18th century. Standing at 117 metres (384 feet) in height and 360 metres (1,180 feet) in width, the Potala Palace is the most important example of Tibetan architecture. Formerly the residence of the Dalai Lama, it contains over one thousand rooms within thirteen stories, and houses portraits of the past Dalai Lamas and statues of the Buddha. It is divided between the outer White Palace, which serves as the administrative quarters, and the inner Red Quarters, which houses the assembly hall of the Lamas, chapels, 10,000 shrines, and a vast library of Buddhist scriptures. The Potala Palace is a World Heritage Site, as is Norbulingka, the former summer residence of the Dalai Lama. The modern Standard Chinese exonym for the ethnic Tibetan region is Zangqu (Chinese: 藏区; pinyin: Zàngqū), which derives by metonymy from the Tsang region around Shigatse plus the addition of a Chinese suffix, 区 qū, which means ""area, district, region, ward"". Tibetan people, language, and culture, regardless of where they are from, are referred to as Zang (Chinese: 藏; pinyin: Zàng) although the geographical term Xīzàng is often limited to the Tibet Autonomous Region. The term Xīzàng was coined during the Qing dynasty in the reign of the Jiaqing Emperor (1796–1820) through the addition of a prefix meaning ""west"" (西 xī) to Zang. Roman Catholic Jesuits and Capuchins arrived from Europe in the 17th and 18th centuries. Portuguese missionaries Jesuit Father António de Andrade and Brother Manuel Marques first reached the kingdom of Gelu in western Tibet in 1624 and was welcomed by the royal family who allowed them to build a church later on. By 1627, there were about a hundred local converts in the Guge kingdom. Later on, Christianity was introduced to Rudok, Ladakh and Tsang and was welcomed by the ruler of the Tsang kingdom, where Andrade and his fellows established a Jesuit outpost at Shigatse in 1626. Although spoken Tibetan varies according to the region, the written language, based on Classical Tibetan, is consistent throughout. This is probably due to the long-standing influence of the Tibetan empire, whose rule embraced (and extended at times far beyond) the present Tibetan linguistic area, which runs from northern Pakistan in the west to Yunnan and Sichuan in the east, and from north of Qinghai Lake south as far as Bhutan. The Tibetan language has its own script which it shares with Ladakhi and Dzongkha, and which is derived from the ancient Indian Brāhmī script. The most important crop in Tibet is barley, and dough made from barley flour—called tsampa—is the staple food of Tibet. This is either rolled into noodles or made into steamed dumplings called momos. Meat dishes are likely to be yak, goat, or mutton, often dried, or cooked into a spicy stew with potatoes. Mustard seed is cultivated in Tibet, and therefore features heavily in its cuisine. Yak yogurt, butter and cheese are frequently eaten, and well-prepared yogurt is considered something of a prestige item. Butter tea is very popular to drink. The 5th Dalai Lama is known for unifying the Tibetan heartland under the control of the Gelug school of Tibetan Buddhism, after defeating the rival Kagyu and Jonang sects and the secular ruler, the Tsangpa prince, in a prolonged civil war. His efforts were successful in part because of aid from Güshi Khan, the Oirat leader of the Khoshut Khanate. With Güshi Khan as a largely uninvolved overlord, the 5th Dalai Lama and his intimates established a civil administration which is referred to by historians as the Lhasa state. This Tibetan regime or government is also referred to as the Ganden Phodrang. Religion is extremely important to the Tibetans and has a strong influence over all aspects of their lives. Bön is the ancient religion of Tibet, but has been almost eclipsed by Tibetan Buddhism, a distinctive form of Mahayana and Vajrayana, which was introduced into Tibet from the Sanskrit Buddhist tradition of northern India. Tibetan Buddhism is practiced not only in Tibet but also in Mongolia, parts of northern India, the Buryat Republic, the Tuva Republic, and in the Republic of Kalmykia and some other parts of China. During China's Cultural Revolution, nearly all Tibet's monasteries were ransacked and destroyed by the Red Guards. A few monasteries have begun to rebuild since the 1980s (with limited support from the Chinese government) and greater religious freedom has been granted – although it is still limited. Monks returned to monasteries across Tibet and monastic education resumed even though the number of monks imposed is strictly limited. Before the 1950s, between 10 and 20% of males in Tibet were monks. Tibet retained nominal power over religious and regional political affairs, while the Mongols managed a structural and administrative rule over the region, reinforced by the rare military intervention. This existed as a ""diarchic structure"" under the Yuan emperor, with power primarily in favor of the Mongols. Mongolian prince Khuden gained temporal power in Tibet in the 1240s and sponsored Sakya Pandita, whose seat became the capital of Tibet. Drogön Chögyal Phagpa, Sakya Pandita's nephew became Imperial Preceptor of Kublai Khan, founder of the Yuan dynasty. Between 1346 and 1354, Tai Situ Changchub Gyaltsen toppled the Sakya and founded the Phagmodrupa Dynasty. The following 80 years saw the founding of the Gelug school (also known as Yellow Hats) by the disciples of Je Tsongkhapa, and the founding of the important Ganden, Drepung and Sera monasteries near Lhasa. However, internal strife within the dynasty and the strong localism of the various fiefs and political-religious factions led to a long series of internal conflicts. The minister family Rinpungpa, based in Tsang (West Central Tibet), dominated politics after 1435. In 1565 they were overthrown by the Tsangpa Dynasty of Shigatse which expanded its power in different directions of Tibet in the following decades and favoured the Karma Kagyu sect. Tibet has some of the world's tallest mountains, with several of them making the top ten list. Mount Everest, located on the border with Nepal, is, at 8,848 metres (29,029 ft), the highest mountain on earth. Several major rivers have their source in the Tibetan Plateau (mostly in present-day Qinghai Province). These include the Yangtze, Yellow River, Indus River, Mekong, Ganges, Salween and the Yarlung Tsangpo River (Brahmaputra River). The Yarlung Tsangpo Grand Canyon, along the Yarlung Tsangpo River, is among the deepest and longest canyons in the world. The best-known medieval Chinese name for Tibet is Tubo (Chinese: 吐蕃 also written as 土蕃 or 土番; pinyin: Tǔbō or Tǔfān). This name first appears in Chinese characters as 土番 in the 7th century (Li Tai) and as 吐蕃 in the 10th-century (Old Book of Tang describing 608–609 emissaries from Tibetan King Namri Songtsen to Emperor Yang of Sui). In the Middle Chinese spoken during that period, as reconstructed by William H. Baxter, 土番 was pronounced thux-phjon and 吐蕃 was pronounced thux-pjon (with the x representing tone). In 821/822 CE Tibet and China signed a peace treaty. A bilingual account of this treaty, including details of the borders between the two countries, is inscribed on a stone pillar which stands outside the Jokhang temple in Lhasa. Tibet continued as a Central Asian empire until the mid-9th century, when a civil war over succession led to the collapse of imperial Tibet. The period that followed is known traditionally as the Era of Fragmentation, when political control over Tibet became divided between regional warlords and tribes with no dominant centralized authority. The Tibetan name for their land, Bod བོད་, means ""Tibet"" or ""Tibetan Plateau"", although it originally meant the central region around Lhasa, now known in Tibetan as Ü. The Standard Tibetan pronunciation of Bod, [pʰøʔ˨˧˨], is transcribed Bhö in Tournadre Phonetic Transcription, Bö in the THL Simplified Phonetic Transcription and Poi in Tibetan pinyin. Some scholars believe the first written reference to Bod ""Tibet"" was the ancient Bautai people recorded in the Egyptian Greek works Periplus of the Erythraean Sea (1st century CE) and Geographia (Ptolemy, 2nd century CE), itself from the Sanskrit form Bhauṭṭa of the Indian geographical tradition. The history of a unified Tibet begins with the rule of Songtsän Gampo (604–650 CE), who united parts of the Yarlung River Valley and founded the Tibetan Empire. He also brought in many reforms, and Tibetan power spread rapidly, creating a large and powerful empire. It is traditionally considered that his first wife was the Princess of Nepal, Bhrikuti, and that she played a great role in the establishment of Buddhism in Tibet. In 640 he married Princess Wencheng, the niece of the powerful Chinese emperor Taizong of Tang China. The Mongol Yuan dynasty, through the Bureau of Buddhist and Tibetan Affairs, or Xuanzheng Yuan, ruled Tibet through a top-level administrative department. One of the department's purposes was to select a dpon-chen ('great administrator'), usually appointed by the lama and confirmed by the Mongol emperor in Beijing. The Sakya lama retained a degree of autonomy, acting as the political authority of the region, while the dpon-chen held administrative and military power. Mongol rule of Tibet remained separate from the main provinces of China, but the region existed under the administration of the Yuan dynasty. If the Sakya lama ever came into conflict with the dpon-chen, the dpon-chen had the authority to send Chinese troops into the region. Tibet (i/tᵻˈbɛt/; Wylie: Bod, pronounced [pʰø̀ʔ]; Chinese: 西藏; pinyin: Xīzàng) is a region on the Tibetan Plateau in Asia. It is the traditional homeland of the Tibetan people as well as some other ethnic groups such as Monpa, Qiang and Lhoba peoples and is now also inhabited by considerable numbers of Han Chinese and Hui people. Tibet is the highest region on Earth, with an average elevation of 4,900 metres (16,000 ft). The highest elevation in Tibet is Mount Everest, earth's highest mountain rising 8,848 m (29,029 ft) above sea level."
Financial_crisis_of_2007%E2%80%9308,"Rapid increases in a number of commodity prices followed the collapse in the housing bubble. The price of oil nearly tripled from $50 to $147 from early 2007 to 2008, before plunging as the financial crisis began to take hold in late 2008. Experts debate the causes, with some attributing it to speculative flow of money from housing and other investments into commodities, some to monetary policy, and some to the increasing feeling of raw materials scarcity in a fast-growing world, leading to long positions taken on those markets, such as Chinese increasing presence in Africa. An increase in oil prices tends to divert a larger share of consumer spending into gasoline, which creates downward pressure on economic growth in oil importing countries, as wealth flows to oil-producing states. A pattern of spiking instability in the price of oil over the decade leading up to the price high of 2008 has been recently identified. The destabilizing effects of this price variance has been proposed as a contributory factor in the financial crisis. Testimony given to the Financial Crisis Inquiry Commission by Richard M. Bowen III on events during his tenure as the Business Chief Underwriter for Correspondent Lending in the Consumer Lending Group for Citigroup (where he was responsible for over 220 professional underwriters) suggests that by the final years of the U.S. housing bubble (2006–2007), the collapse of mortgage underwriting standards was endemic. His testimony stated that by 2006, 60% of mortgages purchased by Citi from some 1,600 mortgage companies were ""defective"" (were not underwritten to policy, or did not contain all policy-required documents) – this, despite the fact that each of these 1,600 originators was contractually responsible (certified via representations and warrantees) that its mortgage originations met Citi's standards. Moreover, during 2007, ""defective mortgages (from mortgage originators contractually bound to perform underwriting to Citi's standards) increased... to over 80% of production"". The output of goods and services produced by labor and property located in the United States—decreased at an annual rate of approximately 6% in the fourth quarter of 2008 and first quarter of 2009, versus activity in the year-ago periods. The U.S. unemployment rate increased to 10.1% by October 2009, the highest rate since 1983 and roughly twice the pre-crisis rate. The average hours per work week declined to 33, the lowest level since the government began collecting the data in 1964. With the decline of gross domestic product came the decline in innovation. With fewer resources to risk in creative destruction, the number of patent applications flat-lined. Compared to the previous 5 years of exponential increases in patent application, this stagnation correlates to the similar drop in GDP during the same time period. Lower interest rates encouraged borrowing. From 2000 to 2003, the Federal Reserve lowered the federal funds rate target from 6.5% to 1.0%. This was done to soften the effects of the collapse of the dot-com bubble and the September 2001 terrorist attacks, as well as to combat a perceived risk of deflation. As early as 2002 it was apparent that credit was fueling housing instead of business investment as some economists went so far as to advocate that the Fed ""needs to create a housing bubble to replace the Nasdaq bubble"". Moreover, empirical studies using data from advanced countries show that excessive credit growth contributed greatly to the severity of the crisis. IndyMac reported that during April 2008, Moody's and Standard & Poor's downgraded the ratings on a significant number of Mortgage-backed security (MBS) bonds including $160 million of those issued by IndyMac and which the bank retained in its MBS portfolio. IndyMac concluded that these downgrades would have negatively impacted the Company's risk-based capital ratio as of June 30, 2008. Had these lowered ratings been in effect at March 31, 2008, IndyMac concluded that the bank's capital ratio would have been 9.27% total risk-based. IndyMac warned that if its regulators found its capital position to have fallen below ""well capitalized"" (minimum 10% risk-based capital ratio) to ""adequately capitalized"" (8–10% risk-based capital ratio) the bank might no longer be able to use brokered deposits as a source of funds. During April 2009, U.S. Federal Reserve vice-chair Janet Yellen discussed these paradoxes: ""Once this massive credit crunch hit, it didn’t take long before we were in a recession. The recession, in turn, deepened the credit crunch as demand and employment fell, and credit losses of financial institutions surged. Indeed, we have been in the grips of precisely this adverse feedback loop for more than a year. A process of balance sheet deleveraging has spread to nearly every corner of the economy. Consumers are pulling back on purchases, especially on durable goods, to build their savings. Businesses are cancelling planned investments and laying off workers to preserve cash. And, financial institutions are shrinking assets to bolster capital and improve their chances of weathering the current storm. Once again, Minsky understood this dynamic. He spoke of the paradox of deleveraging, in which precautions that may be smart for individuals and firms—and indeed essential to return the economy to a normal state—nevertheless magnify the distress of the economy as a whole."" In separate testimony to Financial Crisis Inquiry Commission, officers of Clayton Holdings—the largest residential loan due diligence and securitization surveillance company in the United States and Europe—testified that Clayton's review of over 900,000 mortgages issued from January 2006 to June 2007 revealed that scarcely 54% of the loans met their originators’ underwriting standards. The analysis (conducted on behalf of 23 investment and commercial banks, including 7 ""too big to fail"" banks) additionally showed that 28% of the sampled loans did not meet the minimal standards of any issuer. Clayton's analysis further showed that 39% of these loans (i.e. those not meeting any issuer's minimal underwriting standards) were subsequently securitized and sold to investors. The U.S. Federal Reserve and central banks around the world have taken steps to expand money supplies to avoid the risk of a deflationary spiral, in which lower wages and higher unemployment lead to a self-reinforcing decline in global consumption. In addition, governments have enacted large fiscal stimulus packages, by borrowing and spending to offset the reduction in private sector demand caused by the crisis. The U.S. Federal Reserve's new and expanded liquidity facilities were intended to enable the central bank to fulfill its traditional lender-of-last-resort role during the crisis while mitigating stigma, broadening the set of institutions with access to liquidity, and increasing the flexibility with which institutions could tap such liquidity. In the early and mid-2000s, the Bush administration called numerous times for investigation into the safety and soundness of the GSEs and their swelling portfolio of subprime mortgages. On September 10, 2003, the House Financial Services Committee held a hearing at the urging of the administration to assess safety and soundness issues and to review a recent report by the Office of Federal Housing Enterprise Oversight (OFHEO) that had uncovered accounting discrepancies within the two entities. The hearings never resulted in new legislation or formal investigation of Fannie Mae and Freddie Mac, as many of the committee members refused to accept the report and instead rebuked OFHEO for their attempt at regulation. Some believe this was an early warning to the systemic risk that the growing market in subprime mortgages posed to the U.S. financial system that went unheeded. Others have pointed out that there were not enough of these loans made to cause a crisis of this magnitude. In an article in Portfolio Magazine, Michael Lewis spoke with one trader who noted that ""There weren’t enough Americans with [bad] credit taking out [bad loans] to satisfy investors' appetite for the end product."" Essentially, investment banks and hedge funds used financial innovation to enable large wagers to be made, far beyond the actual value of the underlying mortgage loans, using derivatives called credit default swaps, collateralized debt obligations and synthetic CDOs. In a Peabody Award winning program, NPR correspondents argued that a ""Giant Pool of Money"" (represented by $70 trillion in worldwide fixed income investments) sought higher yields than those offered by U.S. Treasury bonds early in the decade. This pool of money had roughly doubled in size from 2000 to 2007, yet the supply of relatively safe, income generating investments had not grown as fast. Investment banks on Wall Street answered this demand with products such as the mortgage-backed security and the collateralized debt obligation that were assigned safe ratings by the credit rating agencies. Prior to the crisis, financial institutions became highly leveraged, increasing their appetite for risky investments and reducing their resilience in case of losses. Much of this leverage was achieved using complex financial instruments such as off-balance sheet securitization and derivatives, which made it difficult for creditors and regulators to monitor and try to reduce financial institution risk levels. These instruments also made it virtually impossible to reorganize financial institutions in bankruptcy, and contributed to the need for government bailouts. Initially the companies affected were those directly involved in home construction and mortgage lending such as Northern Rock and Countrywide Financial, as they could no longer obtain financing through the credit markets. Over 100 mortgage lenders went bankrupt during 2007 and 2008. Concerns that investment bank Bear Stearns would collapse in March 2008 resulted in its fire-sale to JP Morgan Chase. The financial institution crisis hit its peak in September and October 2008. Several major institutions either failed, were acquired under duress, or were subject to government takeover. These included Lehman Brothers, Merrill Lynch, Fannie Mae, Freddie Mac, Washington Mutual, Wachovia, Citigroup, and AIG. On Oct. 6, 2008, three weeks after Lehman Brothers filed the largest bankruptcy in U.S. history, Lehman's former CEO found himself before Representative Henry A. Waxman, the California Democrat who chaired the House Committee on Oversight and Government Reform. Fuld said he was a victim of the collapse, blaming a ""crisis of confidence"" in the markets for dooming his firm. The U.S. Financial Crisis Inquiry Commission reported its findings in January 2011. It concluded that ""the crisis was avoidable and was caused by: widespread failures in financial regulation, including the Federal Reserve’s failure to stem the tide of toxic mortgages; dramatic breakdowns in corporate governance including too many financial firms acting recklessly and taking on too much risk; an explosive mix of excessive borrowing and risk by households and Wall Street that put the financial system on a collision course with crisis; key policy makers ill prepared for the crisis, lacking a full understanding of the financial system they oversaw; and systemic breaches in accountability and ethics at all levels"". IndyMac often made loans without verification of the borrower’s income or assets, and to borrowers with poor credit histories. Appraisals obtained by IndyMac on underlying collateral were often questionable as well. As an Alt-A lender, IndyMac’s business model was to offer loan products to fit the borrower’s needs, using an extensive array of risky option-adjustable-rate-mortgages (option ARMs), subprime loans, 80/20 loans, and other nontraditional products. Ultimately, loans were made to many borrowers who simply could not afford to make their payments. The thrift remained profitable only as long as it was able to sell those loans in the secondary mortgage market. IndyMac resisted efforts to regulate its involvement in those loans or tighten their issuing criteria: see the comment by Ruthann Melbourne, Chief Risk Officer, to the regulating agencies. This meant that nearly one-third of the U.S. lending mechanism was frozen and continued to be frozen into June 2009. According to the Brookings Institution, the traditional banking system does not have the capital to close this gap as of June 2009: ""It would take a number of years of strong profits to generate sufficient capital to support that additional lending volume"". The authors also indicate that some forms of securitization are ""likely to vanish forever, having been an artifact of excessively loose credit conditions"". While traditional banks have raised their lending standards, it was the collapse of the shadow banking system that is the primary cause of the reduction in funds available for borrowing. Behavior that may be optimal for an individual (e.g., saving more during adverse economic conditions) can be detrimental if too many individuals pursue the same behavior, as ultimately one person's consumption is another person's income. Too many consumers attempting to save (or pay down debt) simultaneously is called the paradox of thrift and can cause or deepen a recession. Economist Hyman Minsky also described a ""paradox of deleveraging"" as financial institutions that have too much leverage (debt relative to equity) cannot all de-leverage simultaneously without significant declines in the value of their assets. The collateralized debt obligation in particular enabled financial institutions to obtain investor funds to finance subprime and other lending, extending or increasing the housing bubble and generating large fees. This essentially places cash payments from multiple mortgages or other debt obligations into a single pool from which specific securities draw in a specific sequence of priority. Those securities first in line received investment-grade ratings from rating agencies. Securities with lower priority had lower credit ratings but theoretically a higher rate of return on the amount invested. Krugman's contention (that the growth of a commercial real estate bubble indicates that U.S. housing policy was not the cause of the crisis) is challenged by additional analysis. After researching the default of commercial loans during the financial crisis, Xudong An and Anthony B. Sanders reported (in December 2010): ""We find limited evidence that substantial deterioration in CMBS [commercial mortgage-backed securities] loan underwriting occurred prior to the crisis."" Other analysts support the contention that the crisis in commercial real estate and related lending took place after the crisis in residential real estate. Business journalist Kimberly Amadeo reports: ""The first signs of decline in residential real estate occurred in 2006. Three years later, commercial real estate started feeling the effects. Denice A. Gierach, a real estate attorney and CPA, wrote: During a period of tough competition between mortgage lenders for revenue and market share, and when the supply of creditworthy borrowers was limited, mortgage lenders relaxed underwriting standards and originated riskier mortgages to less creditworthy borrowers. In the view of some analysts, the relatively conservative government-sponsored enterprises (GSEs) policed mortgage originators and maintained relatively high underwriting standards prior to 2003. However, as market power shifted from securitizers to originators and as intense competition from private securitizers undermined GSE power, mortgage standards declined and risky loans proliferated. The worst loans were originated in 2004–2007, the years of the most intense competition between securitizers and the lowest market share for the GSEs. One of the first victims was Northern Rock, a medium-sized British bank. The highly leveraged nature of its business led the bank to request security from the Bank of England. This in turn led to investor panic and a bank run in mid-September 2007. Calls by Liberal Democrat Treasury Spokesman Vince Cable to nationalise the institution were initially ignored; in February 2008, however, the British government (having failed to find a private sector buyer) relented, and the bank was taken into public hands. Northern Rock's problems proved to be an early indication of the troubles that would soon befall other banks and financial institutions. The pricing of risk refers to the incremental compensation required by investors for taking on additional risk, which may be measured by interest rates or fees. Several scholars have argued that a lack of transparency about banks' risk exposures prevented markets from correctly pricing risk before the crisis, enabled the mortgage market to grow larger than it otherwise would have, and made the financial crisis far more disruptive than it would have been if risk levels had been disclosed in a straightforward, readily understandable format. United States President Barack Obama and key advisers introduced a series of regulatory proposals in June 2009. The proposals address consumer protection, executive pay, bank financial cushions or capital requirements, expanded regulation of the shadow banking system and derivatives, and enhanced authority for the Federal Reserve to safely wind-down systemically important institutions, among others. In January 2010, Obama proposed additional regulations limiting the ability of banks to engage in proprietary trading. The proposals were dubbed ""The Volcker Rule"", in recognition of Paul Volcker, who has publicly argued for the proposed changes. Market strategist Phil Dow believes distinctions exist ""between the current market malaise"" and the Great Depression. He says the Dow Jones average's fall of more than 50% over a period of 17 months is similar to a 54.7% fall in the Great Depression, followed by a total drop of 89% over the following 16 months. ""It's very troubling if you have a mirror image,"" said Dow. Floyd Norris, the chief financial correspondent of The New York Times, wrote in a blog entry in March 2009 that the decline has not been a mirror image of the Great Depression, explaining that although the decline amounts were nearly the same at the time, the rates of decline had started much faster in 2007, and that the past year had only ranked eighth among the worst recorded years of percentage drops in the Dow. The past two years ranked third, however. European regulators introduced Basel III regulations for banks. It increased capital ratios, limits on leverage, narrow definition of capital (to exclude subordinated debt), limit counter-party risk, and new liquidity requirements. Critics argue that Basel III doesn’t address the problem of faulty risk-weightings. Major banks suffered losses from AAA-rated created by financial engineering (which creates apparently risk-free assets out of high risk collateral) that required less capital according to Basel II. Lending to AA-rated sovereigns has a risk-weight of zero, thus increasing lending to governments and leading to the next crisis. Johan Norberg argues that regulations (Basel III among others) have indeed led to excessive lending to risky governments (see European sovereign-debt crisis) and the ECB pursues even more lending as the solution. Countrywide, sued by California Attorney General Jerry Brown for ""unfair business practices"" and ""false advertising"" was making high cost mortgages ""to homeowners with weak credit, adjustable rate mortgages (ARMs) that allowed homeowners to make interest-only payments"". When housing prices decreased, homeowners in ARMs then had little incentive to pay their monthly payments, since their home equity had disappeared. This caused Countrywide's financial condition to deteriorate, ultimately resulting in a decision by the Office of Thrift Supervision to seize the lender. Moreover, a conflict of interest between professional investment managers and their institutional clients, combined with a global glut in investment capital, led to bad investments by asset managers in over-priced credit assets. Professional investment managers generally are compensated based on the volume of client assets under management. There is, therefore, an incentive for asset managers to expand their assets under management in order to maximize their compensation. As the glut in global investment capital caused the yields on credit assets to decline, asset managers were faced with the choice of either investing in assets where returns did not reflect true credit risk or returning funds to clients. Many asset managers chose to continue to invest client funds in over-priced (under-yielding) investments, to the detriment of their clients, in order to maintain their assets under management. This choice was supported by a ""plausible deniability"" of the risks associated with subprime-based credit assets because the loss experience with early ""vintages"" of subprime loans was so low. There is a direct relationship between declines in wealth and declines in consumption and business investment, which along with government spending, represent the economic engine. Between June 2007 and November 2008, Americans lost an estimated average of more than a quarter of their collective net worth.[citation needed] By early November 2008, a broad U.S. stock index the S&P 500, was down 45% from its 2007 high. Housing prices had dropped 20% from their 2006 peak, with futures markets signaling a 30–35% potential drop. Total home equity in the United States, which was valued at $13 trillion at its peak in 2006, had dropped to $8.8 trillion by mid-2008 and was still falling in late 2008. Total retirement assets, Americans' second-largest household asset, dropped by 22%, from $10.3 trillion in 2006 to $8 trillion in mid-2008. During the same period, savings and investment assets (apart from retirement savings) lost $1.2 trillion and pension assets lost $1.3 trillion. Taken together, these losses total a staggering $8.3 trillion. Since peaking in the second quarter of 2007, household wealth is down $14 trillion. The financial crisis was not widely predicted by mainstream economists except Raghuram Rajan, who instead spoke of the Great Moderation. A number of heterodox economists predicted the crisis, with varying arguments. Dirk Bezemer in his research credits (with supporting argument and estimates of timing) 12 economists with predicting the crisis: Dean Baker (US), Wynne Godley (UK), Fred Harrison (UK), Michael Hudson (US), Eric Janszen (US), Steve Keen (Australia), Jakob Brøchner Madsen & Jens Kjaer Sørensen (Denmark), Kurt Richebächer (US), Nouriel Roubini (US), Peter Schiff (US), and Robert Shiller (US). Examples of other experts who gave indications of a financial crisis have also been given. Not surprisingly, the Austrian economic school regarded the crisis as a vindication and classic example of a predictable credit-fueled bubble that could not forestall the disregarded but inevitable effect of an artificial, manufactured laxity in monetary supply, a perspective that even former Fed Chair Alan Greenspan in Congressional testimony confessed himself forced to return to. The Brookings Institution reported in June 2009 that U.S. consumption accounted for more than a third of the growth in global consumption between 2000 and 2007. ""The US economy has been spending too much and borrowing too much for years and the rest of the world depended on the U.S. consumer as a source of global demand."" With a recession in the U.S. and the increased savings rate of U.S. consumers, declines in growth elsewhere have been dramatic. For the first quarter of 2009, the annualized rate of decline in GDP was 14.4% in Germany, 15.2% in Japan, 7.4% in the UK, 18% in Latvia, 9.8% in the Euro area and 21.5% for Mexico. CDO issuance grew from an estimated $20 billion in Q1 2004 to its peak of over $180 billion by Q1 2007, then declined back under $20 billion by Q1 2008. Further, the credit quality of CDO's declined from 2000 to 2007, as the level of subprime and other non-prime mortgage debt increased from 5% to 36% of CDO assets. As described in the section on subprime lending, the CDS and portfolio of CDS called synthetic CDO enabled a theoretically infinite amount to be wagered on the finite value of housing loans outstanding, provided that buyers and sellers of the derivatives could be found. For example, buying a CDS to insure a CDO ended up giving the seller the same risk as if they owned the CDO, when those CDO's became worthless. The bursting of the U.S. (United States) housing bubble, which peaked in 2004, caused the values of securities tied to U.S. real estate pricing to plummet, damaging financial institutions globally. The financial crisis was triggered by a complex interplay of policies that encouraged home ownership, providing easier access to loans for subprime borrowers, overvaluation of bundled subprime mortgages based on the theory that housing prices would continue to escalate, questionable trading practices on behalf of both buyers and sellers, compensation structures that prioritize short-term deal flow over long-term value creation, and a lack of adequate capital holdings from banks and insurance companies to back the financial commitments they were making. Questions regarding bank solvency, declines in credit availability and damaged investor confidence had an impact on global stock markets, where securities suffered large losses during 2008 and early 2009. Economies worldwide slowed during this period, as credit tightened and international trade declined. Governments and central banks responded with unprecedented fiscal stimulus, monetary policy expansion and institutional bailouts. In the U.S., Congress passed the American Recovery and Reinvestment Act of 2009. The term financial innovation refers to the ongoing development of financial products designed to achieve particular client objectives, such as offsetting a particular risk exposure (such as the default of a borrower) or to assist with obtaining financing. Examples pertinent to this crisis included: the adjustable-rate mortgage; the bundling of subprime mortgages into mortgage-backed securities (MBS) or collateralized debt obligations (CDO) for sale to investors, a type of securitization; and a form of credit insurance called credit default swaps (CDS). The usage of these products expanded dramatically in the years leading up to the crisis. These products vary in complexity and the ease with which they can be valued on the books of financial institutions. On November 3, 2008, the European Commission at Brussels predicted for 2009 an extremely weak growth of GDP, by 0.1%, for the countries of the Eurozone (France, Germany, Italy, Belgium etc.) and even negative number for the UK (−1.0%), Ireland and Spain. On November 6, the IMF at Washington, D.C., launched numbers predicting a worldwide recession by −0.3% for 2009, averaged over the developed economies. On the same day, the Bank of England and the European Central Bank, respectively, reduced their interest rates from 4.5% down to 3%, and from 3.75% down to 3.25%. As a consequence, starting from November 2008, several countries launched large ""help packages"" for their economies. Economist Mark Zandi testified to the Financial Crisis Inquiry Commission in January 2010: ""The securitization markets also remain impaired, as investors anticipate more loan losses. Investors are also uncertain about coming legal and accounting rule changes and regulatory reforms. Private bond issuance of residential and commercial mortgage-backed securities, asset-backed securities, and CDOs peaked in 2006 at close to $2 trillion...In 2009, private issuance was less than $150 billion, and almost all of it was asset-backed issuance supported by the Federal Reserve's TALF program to aid credit card, auto and small-business lenders. Issuance of residential and commercial mortgage-backed securities and CDOs remains dormant."" For a variety of reasons, market participants did not accurately measure the risk inherent with financial innovation such as MBS and CDOs or understand its impact on the overall stability of the financial system. For example, the pricing model for CDOs clearly did not reflect the level of risk they introduced into the system. Banks estimated that $450bn of CDO were sold between ""late 2005 to the middle of 2007""; among the $102bn of those that had been liquidated, JPMorgan estimated that the average recovery rate for ""high quality"" CDOs was approximately 32 cents on the dollar, while the recovery rate for mezzanine CDO was approximately five cents for every dollar. In testimony before the Senate Committee on Commerce, Science, and Transportation on June 3, 2008, former director of the CFTC Division of Trading & Markets (responsible for enforcement) Michael Greenberger specifically named the Atlanta-based IntercontinentalExchange, founded by Goldman Sachs, Morgan Stanley and BP as playing a key role in speculative run-up of oil futures prices traded off the regulated futures exchanges in London and New York. However, the IntercontinentalExchange (ICE) had been regulated by both European and U.S. authorities since its purchase of the International Petroleum Exchange in 2001. Mr Greenberger was later corrected on this matter. In his dissent to the majority report of the Financial Crisis Inquiry Commission, American Enterprise Institute fellow Peter J. Wallison stated his belief that the roots of the financial crisis can be traced directly and primarily to affordable housing policies initiated by HUD in the 1990s and to massive risky loan purchases by government-sponsored entities Fannie Mae and Freddie Mac. Later, based upon information in the SEC's December 2011 securities fraud case against 6 ex-executives of Fannie and Freddie, Peter Wallison and Edward Pinto estimated that, in 2008, Fannie and Freddie held 13 million substandard loans totaling over $2 trillion. Some developing countries that had seen strong economic growth saw significant slowdowns. For example, growth forecasts in Cambodia show a fall from more than 10% in 2007 to close to zero in 2009, and Kenya may achieve only 3–4% growth in 2009, down from 7% in 2007. According to the research by the Overseas Development Institute, reductions in growth can be attributed to falls in trade, commodity prices, investment and remittances sent from migrant workers (which reached a record $251 billion in 2007, but have fallen in many countries since). This has stark implications and has led to a dramatic rise in the number of households living below the poverty line, be it 300,000 in Bangladesh or 230,000 in Ghana. Especially states with a fragile political system have to fear that investors from Western states withdraw their money because of the crisis. Bruno Wenn of the German DEG recommends to provide a sound economic policymaking and good governance to attract new investors This boom in innovative financial products went hand in hand with more complexity. It multiplied the number of actors connected to a single mortgage (including mortgage brokers, specialized originators, the securitizers and their due diligence firms, managing agents and trading desks, and finally investors, insurances and providers of repo funding). With increasing distance from the underlying asset these actors relied more and more on indirect information (including FICO scores on creditworthiness, appraisals and due diligence checks by third party organizations, and most importantly the computer models of rating agencies and risk management desks). Instead of spreading risk this provided the ground for fraudulent acts, misjudgments and finally market collapse. In 2005 a group of computer scientists built a computational model for the mechanism of biased ratings produced by rating agencies, which turned out to be adequate to what actually happened in 2006–2008.[citation needed] As part of the housing and credit booms, the number of financial agreements called mortgage-backed securities (MBS) and collateralized debt obligations (CDO), which derived their value from mortgage payments and housing prices, greatly increased. Such financial innovation enabled institutions and investors around the world to invest in the U.S. housing market. As housing prices declined, major global financial institutions that had borrowed and invested heavily in subprime MBS reported significant losses. Feminist economists Ailsa McKay and Margunn Bjørnholt argue that the financial crisis and the response to it revealed a crisis of ideas in mainstream economics and within the economics profession, and call for a reshaping of both the economy, economic theory and the economics profession. They argue that such a reshaping should include new advances within feminist economics and ecological economics that take as their starting point the socially responsible, sensible and accountable subject in creating an economy and economic theories that fully acknowledge care for each other as well as the planet. Many causes for the financial crisis have been suggested, with varying weight assigned by experts. The U.S. Senate's Levin–Coburn Report concluded that the crisis was the result of ""high risk, complex financial products; undisclosed conflicts of interest; the failure of regulators, the credit rating agencies, and the market itself to rein in the excesses of Wall Street."" The Financial Crisis Inquiry Commission concluded that the financial crisis was avoidable and was caused by ""widespread failures in financial regulation and supervision"", ""dramatic failures of corporate governance and risk management at many systemically important financial institutions"", ""a combination of excessive borrowing, risky investments, and lack of transparency"" by financial institutions, ill preparation and inconsistent action by government that ""added to the uncertainty and panic"", a ""systemic breakdown in accountability and ethics"", ""collapsing mortgage-lending standards and the mortgage securitization pipeline"", deregulation of over-the-counter derivatives, especially credit default swaps, and ""the failures of credit rating agencies"" to correctly price risk. The 1999 repeal of the Glass-Steagall Act effectively removed the separation between investment banks and depository banks in the United States. Critics argued that credit rating agencies and investors failed to accurately price the risk involved with mortgage-related financial products, and that governments did not adjust their regulatory practices to address 21st-century financial markets. Research into the causes of the financial crisis has also focused on the role of interest rate spreads. Typical American families did not fare as well, nor did those ""wealthy-but-not wealthiest"" families just beneath the pyramid's top. On the other hand, half of the poorest families did not have wealth declines at all during the crisis. The Federal Reserve surveyed 4,000 households between 2007 and 2009, and found that the total wealth of 63 percent of all Americans declined in that period. 77 percent of the richest families had a decrease in total wealth, while only 50 percent of those on the bottom of the pyramid suffered a decrease. Another example relates to AIG, which insured obligations of various financial institutions through the usage of credit default swaps. The basic CDS transaction involved AIG receiving a premium in exchange for a promise to pay money to party A in the event party B defaulted. However, AIG did not have the financial strength to support its many CDS commitments as the crisis progressed and was taken over by the government in September 2008. U.S. taxpayers provided over $180 billion in government support to AIG during 2008 and early 2009, through which the money flowed to various counterparties to CDS transactions, including many large global financial institutions. As financial assets became more and more complex, and harder and harder to value, investors were reassured by the fact that both the international bond rating agencies and bank regulators, who came to rely on them, accepted as valid some complex mathematical models which theoretically showed the risks were much smaller than they actually proved to be. George Soros commented that ""The super-boom got out of hand when the new products became so complicated that the authorities could no longer calculate the risks and started relying on the risk management methods of the banks themselves. Similarly, the rating agencies relied on the information provided by the originators of synthetic products. It was a shocking abdication of responsibility."" Despite the dominance of the above formula, there are documented attempts of the financial industry, occurring before the crisis, to address the formula limitations, specifically the lack of dependence dynamics and the poor representation of extreme events. The volume ""Credit Correlation: Life After Copulas"", published in 2007 by World Scientific, summarizes a 2006 conference held by Merrill Lynch in London where several practitioners attempted to propose models rectifying some of the copula limitations. See also the article by Donnelly and Embrechts and the book by Brigo, Pallavicini and Torresetti, that reports relevant warnings and research on CDOs appeared in 2006. In November 2008, economist Dean Baker observed: ""There is a really good reason for tighter credit. Tens of millions of homeowners who had substantial equity in their homes two years ago have little or nothing today. Businesses are facing the worst downturn since the Great Depression. This matters for credit decisions. A homeowner with equity in her home is very unlikely to default on a car loan or credit card debt. They will draw on this equity rather than lose their car and/or have a default placed on their credit record. On the other hand, a homeowner who has no equity is a serious default risk. In the case of businesses, their creditworthiness depends on their future profits. Profit prospects look much worse in November 2008 than they did in November 2007... While many banks are obviously at the brink, consumers and businesses would be facing a much harder time getting credit right now even if the financial system were rock solid. The problem with the economy is the loss of close to $6 trillion in housing wealth and an even larger amount of stock wealth. Countering Krugman, Peter J. Wallison wrote: ""It is not true that every bubble—even a large bubble—has the potential to cause a financial crisis when it deflates."" Wallison notes that other developed countries had ""large bubbles during the 1997–2007 period"" but ""the losses associated with mortgage delinquencies and defaults when these bubbles deflated were far lower than the losses suffered in the United States when the 1997–2007 [bubble] deflated."" According to Wallison, the reason the U.S. residential housing bubble (as opposed to other types of bubbles) led to financial crisis was that it was supported by a huge number of substandard loans – generally with low or no downpayments. In a June 2008 speech, President and CEO of the New York Federal Reserve Bank Timothy Geithner—who in 2009 became Secretary of the United States Treasury—placed significant blame for the freezing of credit markets on a ""run"" on the entities in the ""parallel"" banking system, also called the shadow banking system. These entities became critical to the credit markets underpinning the financial system, but were not subject to the same regulatory controls. Further, these entities were vulnerable because of maturity mismatch, meaning that they borrowed short-term in liquid markets to purchase long-term, illiquid and risky assets. This meant that disruptions in credit markets would make them subject to rapid deleveraging, selling their long-term assets at depressed prices. He described the significance of these entities: On July 11, 2008, citing liquidity concerns, the FDIC put IndyMac Bank into conservatorship. A bridge bank, IndyMac Federal Bank, FSB, was established to assume control of IndyMac Bank's assets, its secured liabilities, and its insured deposit accounts. The FDIC announced plans to open IndyMac Federal Bank, FSB on July 14, 2008. Until then, depositors would have access their insured deposits through ATMs, their existing checks, and their existing debit cards. Telephone and Internet account access was restored when the bank reopened. The FDIC guarantees the funds of all insured accounts up to US$100,000, and has declared a special advance dividend to the roughly 10,000 depositors with funds in excess of the insured amount, guaranteeing 50% of any amounts in excess of $100,000. Yet, even with the pending sale of Indymac to IMB Management Holdings, an estimated 10,000 uninsured depositors of Indymac are still at a loss of over $270 million. A cover story in BusinessWeek magazine claims that economists mostly failed to predict the worst international economic crisis since the Great Depression of the 1930s. The Wharton School of the University of Pennsylvania's online business journal examines why economists failed to predict a major global financial crisis. Popular articles published in the mass media have led the general public to believe that the majority of economists have failed in their obligation to predict the financial crisis. For example, an article in the New York Times informs that economist Nouriel Roubini warned of such crisis as early as September 2006, and the article goes on to state that the profession of economics is bad at predicting recessions. According to The Guardian, Roubini was ridiculed for predicting a collapse of the housing market and worldwide recession, while The New York Times labelled him ""Dr. Doom"". By September 2008, average U.S. housing prices had declined by over 20% from their mid-2006 peak. As prices declined, borrowers with adjustable-rate mortgages could not refinance to avoid the higher payments associated with rising interest rates and began to default. During 2007, lenders began foreclosure proceedings on nearly 1.3 million properties, a 79% increase over 2006. This increased to 2.3 million in 2008, an 81% increase vs. 2007. By August 2008, 9.2% of all U.S. mortgages outstanding were either delinquent or in foreclosure. By September 2009, this had risen to 14.4%. Stock trader and financial risk engineer Nassim Nicholas Taleb, author of the 2007 book The Black Swan, spent years warning against the breakdown of the banking system in particular and the economy in general owing to their use of bad risk models and reliance on forecasting, and their reliance on bad models, and framed the problem as part of ""robustness and fragility"". He also took action against the establishment view by making a big financial bet on banking stocks and making a fortune from the crisis (""They didn't listen, so I took their money""). According to David Brooks from the New York Times, ""Taleb not only has an explanation for what’s happening, he saw it coming."" To other analysts the delay between CRA rule changes (in 1995) and the explosion of subprime lending is not surprising, and does not exonerate the CRA. They contend that there were two, connected causes to the crisis: the relaxation of underwriting standards in 1995 and the ultra-low interest rates initiated by the Federal Reserve after the terrorist attack on September 11, 2001. Both causes had to be in place before the crisis could take place. Critics also point out that publicly announced CRA loan commitments were massive, totaling $4.5 trillion in the years between 1994 and 2007. They also argue that the Federal Reserve’s classification of CRA loans as “prime” is based on the faulty and self-serving assumption that high-interest-rate loans (3 percentage points over average) equal “subprime” loans. The World Bank reported in February 2009 that the Arab World was far less severely affected by the credit crunch. With generally good balance of payments positions coming into the crisis or with alternative sources of financing for their large current account deficits, such as remittances, Foreign Direct Investment (FDI) or foreign aid, Arab countries were able to avoid going to the market in the latter part of 2008. This group is in the best position to absorb the economic shocks. They entered the crisis in exceptionally strong positions. This gives them a significant cushion against the global downturn. The greatest impact of the global economic crisis will come in the form of lower oil prices, which remains the single most important determinant of economic performance. Steadily declining oil prices would force them to draw down reserves and cut down on investments. Significantly lower oil prices could cause a reversal of economic performance as has been the case in past oil shocks. Initial impact will be seen on public finances and employment for foreign workers. The first visible institution to run into trouble in the United States was the Southern California–based IndyMac, a spin-off of Countrywide Financial. Before its failure, IndyMac Bank was the largest savings and loan association in the Los Angeles market and the seventh largest mortgage originator in the United States. The failure of IndyMac Bank on July 11, 2008, was the fourth largest bank failure in United States history up until the crisis precipitated even larger failures, and the second largest failure of a regulated thrift. IndyMac Bank's parent corporation was IndyMac Bancorp until the FDIC seized IndyMac Bank. IndyMac Bancorp filed for Chapter 7 bankruptcy in July 2008. It threatened the collapse of large financial institutions, which was prevented by the bailout of banks by national governments, but stock markets still dropped worldwide. In many areas, the housing market also suffered, resulting in evictions, foreclosures and prolonged unemployment. The crisis played a significant role in the failure of key businesses, declines in consumer wealth estimated in trillions of U.S. dollars, and a downturn in economic activity leading to the 2008–2012 global recession and contributing to the European sovereign-debt crisis. The active phase of the crisis, which manifested as a liquidity crisis, can be dated from August 9, 2007, when BNP Paribas terminated withdrawals from three hedge funds citing ""a complete evaporation of liquidity"". A 2000 United States Department of the Treasury study of lending trends for 305 cities from 1993 to 1998 showed that $467 billion of mortgage lending was made by Community Reinvestment Act (CRA)-covered lenders into low and mid level income (LMI) borrowers and neighborhoods, representing 10% of all U.S. mortgage lending during the period. The majority of these were prime loans. Sub-prime loans made by CRA-covered institutions constituted a 3% market share of LMI loans in 1998, but in the run-up to the crisis, fully 25% of all sub-prime lending occurred at CRA-covered institutions and another 25% of sub-prime loans had some connection with CRA. In addition, an analysis by the Federal Reserve Bank of Dallas in 2009, however, concluded that the CRA was not responsible for the mortgage loan crisis, pointing out that CRA rules have been in place since 1995 whereas the poor lending emerged only a decade later. Furthermore, most sub-prime loans were not made to the LMI borrowers targeted by the CRA, especially in the years 2005–2006 leading up to the crisis. Nor did it find any evidence that lending under the CRA rules increased delinquency rates or that the CRA indirectly influenced independent mortgage lenders to ramp up sub-prime lending. The U.S. recession that began in December 2007 ended in June 2009, according to the U.S. National Bureau of Economic Research (NBER) and the financial crisis appears to have ended about the same time. In April 2009 TIME magazine declared ""More Quickly Than It Began, The Banking Crisis Is Over."" The United States Financial Crisis Inquiry Commission dates the crisis to 2008. President Barack Obama declared on January 27, 2010, ""the markets are now stabilized, and we've recovered most of the money we spent on the banks."" Current Governor of the Reserve Bank of India Raghuram Rajan had predicted the crisis in 2005 when he became chief economist at the International Monetary Fund.In 2005, at a celebration honouring Alan Greenspan, who was about to retire as chairman of the US Federal Reserve, Rajan delivered a controversial paper that was critical of the financial sector. In that paper, ""Has Financial Development Made the World Riskier?"", Rajan ""argued that disaster might loom."" Rajan argued that financial sector managers were encouraged to ""take risks that generate severe adverse consequences with small probability but, in return, offer generous compensation the rest of the time. These risks are known as tail risks. But perhaps the most important concern is whether banks will be able to provide liquidity to financial markets so that if the tail risk does materialise, financial positions can be unwound and losses allocated so that the consequences to the real economy are minimised."" The Fed then raised the Fed funds rate significantly between July 2004 and July 2006. This contributed to an increase in 1-year and 5-year adjustable-rate mortgage (ARM) rates, making ARM interest rate resets more expensive for homeowners. This may have also contributed to the deflating of the housing bubble, as asset prices generally move inversely to interest rates, and it became riskier to speculate in housing. U.S. housing and financial assets dramatically declined in value after the housing bubble burst. Advanced economies led global economic growth prior to the financial crisis with ""emerging"" and ""developing"" economies lagging behind. The crisis completely overturned this relationship. The International Monetary Fund found that ""advanced"" economies accounted for only 31% of global GDP while emerging and developing economies accounted for 69% of global GDP from 2007 to 2014. In the tables, the names of emergent economies are shown in boldface type, while the names of developed economies are in Roman (regular) type. Bernanke explained that between 1996 and 2004, the U.S. current account deficit increased by $650 billion, from 1.5% to 5.8% of GDP. Financing these deficits required the country to borrow large sums from abroad, much of it from countries running trade surpluses. These were mainly the emerging economies in Asia and oil-exporting nations. The balance of payments identity requires that a country (such as the U.S.) running a current account deficit also have a capital account (investment) surplus of the same amount. Hence large and growing amounts of foreign funds (capital) flowed into the U.S. to finance its imports. Predatory lending refers to the practice of unscrupulous lenders, enticing borrowers to enter into ""unsafe"" or ""unsound"" secured loans for inappropriate purposes. A classic bait-and-switch method was used by Countrywide Financial, advertising low interest rates for home refinancing. Such loans were written into extensively detailed contracts, and swapped for more expensive loan products on the day of closing. Whereas the advertisement might state that 1% or 1.5% interest would be charged, the consumer would be put into an adjustable rate mortgage (ARM) in which the interest charged would be greater than the amount of interest paid. This created negative amortization, which the credit consumer might not notice until long after the loan transaction had been consummated. From 2004 to 2007, the top five U.S. investment banks each significantly increased their financial leverage (see diagram), which increased their vulnerability to a financial shock. Changes in capital requirements, intended to keep U.S. banks competitive with their European counterparts, allowed lower risk weightings for AAA securities. The shift from first-loss tranches to AAA tranches was seen by regulators as a risk reduction that compensated the higher leverage. These five institutions reported over $4.1 trillion in debt for fiscal year 2007, about 30% of USA nominal GDP for 2007. Lehman Brothers went bankrupt and was liquidated, Bear Stearns and Merrill Lynch were sold at fire-sale prices, and Goldman Sachs and Morgan Stanley became commercial banks, subjecting themselves to more stringent regulation. With the exception of Lehman, these companies required or received government support. Lehman reported that it had been in talks with Bank of America and Barclays for the company's possible sale. However, both Barclays and Bank of America ultimately declined to purchase the entire company. Critics such as economist Paul Krugman and U.S. Treasury Secretary Timothy Geithner have argued that the regulatory framework did not keep pace with financial innovation, such as the increasing importance of the shadow banking system, derivatives and off-balance sheet financing. A recent OECD study suggest that bank regulation based on the Basel accords encourage unconventional business practices and contributed to or even reinforced the financial crisis. In other cases, laws were changed or enforcement weakened in parts of the financial system. Key examples include: Economist Paul Krugman and U.S. Treasury Secretary Timothy Geithner explain the credit crisis via the implosion of the shadow banking system, which had grown to nearly equal the importance of the traditional commercial banking sector as described above. Without the ability to obtain investor funds in exchange for most types of mortgage-backed securities or asset-backed commercial paper, investment banks and other entities in the shadow banking system could not provide funds to mortgage firms and other corporations. This credit freeze brought the global financial system to the brink of collapse. The response of the Federal Reserve, the European Central Bank, the Bank of England and other central banks was immediate and dramatic. During the last quarter of 2008, these central banks purchased US$2.5 trillion of government debt and troubled private assets from banks. This was the largest liquidity injection into the credit market, and the largest monetary policy action, in world history. Following a model initiated by the United Kingdom bank rescue package, the governments of European nations and the USA guaranteed the debt issued by their banks and raised the capital of their national banking systems, ultimately purchasing $1.5 trillion newly issued preferred stock in their major banks. In October 2010, Nobel laureate Joseph Stiglitz explained how the U.S. Federal Reserve was implementing another monetary policy —creating currency— as a method to combat the liquidity trap. By creating $600 billion and inserting[clarification needed] this directly into banks, the Federal Reserve intended to spur banks to finance more domestic loans and refinance mortgages. However, banks instead were spending the money in more profitable areas by investing internationally in emerging markets. Banks were also investing in foreign currencies, which Stiglitz and others point out may lead to currency wars while China redirects its currency holdings away from the United States. Senator Charles Schumer (D-NY) would later point out that brokered deposits made up more than 37 percent of IndyMac's total deposits and ask the Federal Deposit Insurance Corporation (FDIC) whether it had considered ordering IndyMac to reduce its reliance on these deposits. With $18.9 billion in total deposits reported on March 31, Senator Schumer would have been referring to a little over $7 billion in brokered deposits. While the breakout of maturities of these deposits is not known exactly, a simple averaging would have put the threat of brokered deposits loss to IndyMac at $500 million a month, had the regulator disallowed IndyMac from acquiring new brokered deposits on June 30. Falling prices also resulted in homes worth less than the mortgage loan, providing a financial incentive to enter foreclosure. The ongoing foreclosure epidemic that began in late 2006 in the U.S. continues to drain wealth from consumers and erodes the financial strength of banking institutions. Defaults and losses on other loan types also increased significantly as the crisis expanded from the housing market to other parts of the economy. Total losses are estimated in the trillions of U.S. dollars globally. The U.S. Senate passed a reform bill in May 2010, following the House which passed a bill in December 2009. These bills must now be reconciled. The New York Times provided a comparative summary of the features of the two bills, which address to varying extent the principles enumerated by the Obama administration. For instance, the Volcker Rule against proprietary trading is not part of the legislation, though in the Senate bill regulators have the discretion but not the obligation to prohibit these trades. These institutions, as well as certain regulated banks, had also assumed significant debt burdens while providing the loans described above and did not have a financial cushion sufficient to absorb large loan defaults or MBS losses. These losses impacted the ability of financial institutions to lend, slowing economic activity. Concerns regarding the stability of key financial institutions drove central banks to provide funds to encourage lending and restore faith in the commercial paper markets, which are integral to funding business operations. Governments also bailed out key financial institutions and implemented economic stimulus programs, assuming significant additional financial commitments. While the housing and credit bubbles were building, a series of factors caused the financial system to both expand and become increasingly fragile, a process called financialization. U.S. Government policy from the 1970s onward has emphasized deregulation to encourage business, which resulted in less oversight of activities and less disclosure of information about new activities undertaken by banks and other evolving financial institutions. Thus, policymakers did not immediately recognize the increasingly important role played by financial institutions such as investment banks and hedge funds, also known as the shadow banking system. Some experts believe these institutions had become as important as commercial (depository) banks in providing credit to the U.S. economy, but they were not subject to the same regulations. In September 2008, the crisis hit its most critical stage. There was the equivalent of a bank run on the money market funds, which frequently invest in commercial paper issued by corporations to fund their operations and payrolls. Withdrawal from money markets were $144.5 billion during one week, versus $7.1 billion the week prior. This interrupted the ability of corporations to rollover (replace) their short-term debt. The U.S. government responded by extending insurance for money market accounts analogous to bank deposit insurance via a temporary guarantee and with Federal Reserve programs to purchase commercial paper. The TED spread, an indicator of perceived credit risk in the general economy, spiked up in July 2007, remained volatile for a year, then spiked even higher in September 2008, reaching a record 4.65% on October 10, 2008. The majority report of the Financial Crisis Inquiry Commission, written by the six Democratic appointees, the minority report, written by 3 of the 4 Republican appointees, studies by Federal Reserve economists, and the work of several independent scholars generally contend that government affordable housing policy was not the primary cause of the financial crisis. Although they concede that governmental policies had some role in causing the crisis, they contend that GSE loans performed better than loans securitized by private investment banks, and performed better than some loans originated by institutions that held loans in their own portfolios. Paul Krugman has even claimed that the GSE never purchased subprime loans – a claim that is widely disputed. Several commentators have suggested that if the liquidity crisis continues, an extended recession or worse could occur. The continuing development of the crisis has prompted fears of a global economic collapse although there are now many cautiously optimistic forecasters in addition to some prominent sources who remain negative. The financial crisis is likely to yield the biggest banking shakeout since the savings-and-loan meltdown. Investment bank UBS stated on October 6 that 2008 would see a clear global recession, with recovery unlikely for at least two years. Three days later UBS economists announced that the ""beginning of the end"" of the crisis had begun, with the world starting to make the necessary actions to fix the crisis: capital injection by governments; injection made systemically; interest rate cuts to help borrowers. The United Kingdom had started systemic injection, and the world's central banks were now cutting interest rates. UBS emphasized the United States needed to implement systemic injection. UBS further emphasized that this fixes only the financial crisis, but that in economic terms ""the worst is still to come"". UBS quantified their expected recession durations on October 16: the Eurozone's would last two quarters, the United States' would last three quarters, and the United Kingdom's would last four quarters. The economic crisis in Iceland involved all three of the country's major banks. Relative to the size of its economy, Iceland’s banking collapse is the largest suffered by any country in economic history. When home prices declined in the latter half of 2007 and the secondary mortgage market collapsed, IndyMac was forced to hold $10.7 billion of loans it could not sell in the secondary market. Its reduced liquidity was further exacerbated in late June 2008 when account holders withdrew $1.55 billion or about 7.5% of IndyMac's deposits. This “run” on the thrift followed the public release of a letter from Senator Charles Schumer to the FDIC and OTS. The letter outlined the Senator’s concerns with IndyMac. While the run was a contributing factor in the timing of IndyMac’s demise, the underlying cause of the failure was the unsafe and unsound manner in which the thrift was operated. The securitization markets supported by the shadow banking system started to close down in the spring of 2007 and nearly shut-down in the fall of 2008. More than a third of the private credit markets thus became unavailable as a source of funds. According to the Brookings Institution, the traditional banking system does not have the capital to close this gap as of June 2009: ""It would take a number of years of strong profits to generate sufficient capital to support that additional lending volume."" The authors also indicate that some forms of securitization are ""likely to vanish forever, having been an artifact of excessively loose credit conditions."""
Catalan_language,"Catalan dialects are relatively uniform, and are mutually intelligible. They are divided into two blocks, Eastern and Western, differing mostly in pronunciation. The terms ""Catalan"" and ""Valencian"" (respectively used in Catalonia and the Valencian Community) are two different varieties of the same language. There are two institutions regulating the two standard varieties, the Institute of Catalan Studies in Catalonia and the Valencian Academy of the Language in Valencia. In the Balearic Islands, IEC's standard is used but adapted for the Balearic dialect by the University of the Balearic Islands's philological section. In this way, for instance, IEC says it is correct writing cantam as much as cantem ('we sing') but the University says that the priority form in the Balearic Islands must be ""cantam"" in all fields. Another feature of the Balearic standard is the non-ending in the 1st person singular present indicative: jo compr ('I buy'), jo tem ('I fear'), jo dorm ('I sleep'). In 2011, the Aragonese government passed a decree for the establishment of a new language regulator of Catalan in La Franja (the so-called Catalan-speaking areas of Aragon). The new entity, designated as Acadèmia Aragonesa del Català, shall allow a facultative education in Catalan and a standardization of the Catalan language in La Franja. The inflection of determinatives is complex, specially because of the high number of elisions, but is similar to the neighboring languages. Catalan has more contractions of preposition + article than Spanish, like dels (""of + the [plural]""), but not as many as Italian (which has sul, col, nel, etc.). In contrast with other Romance languages, Catalan has many monosyllabic words; and those ending in a wide variety of consonants and some consonant clusters. Also, Catalan has final obstruent devoicing, thus featuring many couplets like amic ""(male friend"") vs. amiga (""female friend""). With the union of the crowns of Castille and Aragon (1479), the use of Spanish gradually became more prestigious. Starting in the 16th century, Catalan literature experienced a decline, the language came under the influence of Spanish, and the urban and literary classes became bilingual. The most notable difference between both standards is some tonic ⟨e⟩ accentuation, for instance: francès, anglès (IEC) – francés, anglés (AVL). Nevertheless, AVL's standard keeps the grave accent ⟨è⟩, without pronouncing this ⟨e⟩ as /ɛ/, in some words like: què ('what'), or València. Other divergences include the use of ⟨tl⟩ (AVL) in some words instead of ⟨tll⟩ like in ametla/ametlla ('almond'), espatla/espatlla ('back'), the use of elided demonstratives (este 'this', eixe 'that') in the same level as reinforced ones (aquest, aqueix) or the use of many verbal forms common in Valencian, and some of these common in the rest of Western Catalan too, like subjunctive mood or inchoative conjugation in -ix- at the same level as -eix- or the priority use of -e morpheme in 1st person singular in present indicative (-ar verbs): jo compre instead of jo compro ('I buy'). In parallel, however, the 19th century saw a Catalan literary revival (Renaixença), which has continued up to the present day. This period starts with Aribau's Ode to the Homeland (1833); followed in the second half of the 19th century, and the early 20th by the work of Verdaguer (poetry), Oller (realist novel), and Guimerà (drama). Since the Spanish transition to democracy (1975–1982), Catalan has been recognized as an official language, language of education, and language of mass media, all of which have contributed to its increased prestige. There is no parallel in Europe of such a large, bilingual, non-state speech community. During the 11th and 12th centuries the Catalan rulers expanded up to north of the Ebro river, and in the 13th century they conquered the Land of Valencia and the Balearic Islands. The city of Alghero in Sardinia was repopulated with Catalan speakers in the 14th century. The language also reached Murcia, which became Spanish-speaking in the 15th century. As in the other Western Romance languages, the main plural expression is the suffix -s, which may create morphological alternations similar to the ones found in gender inflection, albeit more rarely. The most important one is the addition of -o- before certain consonant groups, a phonetic phenomenon that does not affect feminine forms: el pols/els polsos (""the pulse""/""the pulses"") vs. la pols/les pols (""the dust""/""the dusts""). In Central Catalan, unstressed vowels reduce to three: /a e ɛ/ > [ə]; /o ɔ u/ > [u]; /i/ remains distinct. The other dialects have different vowel reduction processes (see the section pronunciation of dialects in this article). The same happens with Arabic loanwords. Thus, Catalan alfàbia ""large earthenware jar"" and rajola ""tile"", of Arabic origin, contrast with Spanish tinaja and teja, of Latin origin; whereas Catalan oli ""oil"" and oliva ""olive"", of Latin origin, contrast with Spanish aceite and aceituna. However, the Arabic element in Spanish is generally much more prevalent. In Eastern Catalan (except Majorcan), unstressed vowels reduce to three: /a e ɛ/ > [ə]; /o ɔ u/ > [u]; /i/ remains distinct. There are a few instances of unreduced [e], [o] in some words. Alguerese has lowered [ə] to [a]. Literary Catalan allows the use of words from different dialects, except those of very restricted use. However, from the 19th century onwards, there is a tendency of favoring words of Northern dialects in detriment of others, even though nowadays there is a greater freedom of choice. The ascription of Catalan to the Occitano-Romance branch of Gallo-Romance languages is not shared by all linguists and philologists, particularly among Spanish ones, such as Ramón Menéndez Pidal. In the Alicante province Catalan is being replaced by Spanish, and in Alghero by Italian. There are also well ingrained diglossic attitudes against Catalan in the Valencian Community, Ibiza, and to a lesser extent, in the rest of the Balearic islands. Catalan shares many traits with the other neighboring Romance languages (Italian, Sardinian, Occitan, and Spanish). However, despite being mostly situated in the Iberian Peninsula, Catalan has marked differences with the Ibero-Romance group (Spanish and Portuguese) in terms of pronunciation, grammar, and especially vocabulary; showing instead its closest affinity with Occitan and to a lesser extent Gallo-Romance (French, Franco-Provençal, Gallo-Italian). There is a tendency to abandon traditionally gender-invariable adjectives in favour of marked ones, something prevalent in Occitan and French. Thus, one can find bullent/bullenta (""boiling"") in contrast with traditional bullent/bullent. This flexibility allows Catalan to use extraposition extensively, much more than French or Spanish. Thus, Catalan can have m'hi recomanaren (""they recommended me to him""), whereas in French one must say ils m'ont recommandé à lui, and Spanish me recomendaron a él. This allows the placement of almost any nominal term as a sentence topic, without having to use so often the passive voice (as in French or English), or identifying the direct object with a preposition (as in Spanish). The Germanic superstrate has had different outcomes in Spanish and Catalan. For example, Catalan fang ""mud"" and rostir ""to roast"", of Germanic origin, contrast with Spanish lodo and asar, of Latin origin; whereas Catalan filosa ""spinning wheel"" and pols ""temple"", of Latin origin, contrast with Spanish rueca and sien, of Germanic origin. The process of morphological derivation in Catalan follows the same principles as the other Romance languages, where agglutination is common. Many times, several affixes are appended to a preexisting lexeme, and some sound alternations can occur, for example elèctric [əˈlɛktrik] (""electrical"") vs. electricitat [ələktrisiˈtat]. Prefixes are usually appended to verbs, for as in preveure (""foresee""). Catalan pronouns exhibit T–V distinction, like all other Romance languages (and most European languages, but not Modern English). This feature implies the use of a different set of second person pronouns for formality. Catalan bears varying degrees of similarity to the linguistic varieties subsumed under the cover term Occitan language (see also differences between Occitan and Catalan and Gallo-Romance languages). Thus, as it should be expected from closely related languages, Catalan today shares many traits with other Romance languages. Following the French capture of Algeria (1833), that region saw several waves of Catalan-speaking settlers. People from the Spanish Alacant province settled around Oran, whereas Algiers received immigration from Northern Catalonia and Minorca. Their speech was known as patuet. By 1911, the number of Catalan speakers was around 100,000. After the declaration of independence of Algeria in 1962, almost all the Catalan speakers fled to Northern Catalonia (as Pieds-Noirs) or Alacant. Despite the position of the official organizations, an opinion poll carried out between 2001 and 2004 showed that the majority of the Valencian people consider Valencian different from Catalan. This position is promoted by people who do not use Valencian regularly. Furthermore, the data indicates that younger generations educated in Valencian are much less likely to hold these views. A minority of Valencian scholars active in fields other than linguistics defends the position of the Royal Academy of Valencian Culture (Acadèmia de Cultura Valenciana, RACV), which uses for Valencian a standard independent from Catalan. Since the Spanish transition to democracy (1975–1982), Catalan has been institutionalizated as an official language, language of education, and language of mass media; all of which have contributed to its increased prestige. In Catalonia, there is no parallel of a large, bilingual, European, non-state speech community. The teaching of Catalan is mandatory in all schools, but it is possible to use Spanish for studying in the public education system of Catalonia in two situations, if the teacher assigned to a class chooses to use Spanish, or during the learning process of one or some recently arrived students.  There is also some intergenerational shift towards Catalan. Catalan verbs are traditionally divided into three conjugations, with vowel themes -a-, -e-, -i-, the last two being split into two subtypes. However, this division is mostly theoretical. Only the first conjugation is nowadays productive (with about 3500 common verbs), whereas the third (the subtype of servir, with about 700 common verbs) is semiproductive. The verbs of the second conjugation are fewer than 100, and it is not possible to create new ones, except by compounding. In Alghero, the IEC has adapted its standard to the Alguerese dialect. In this standard one can find, among other features: the definite article lo instead of el, special possessive pronouns and determinants la mia ('mine'), lo sou/la sua ('his/her'), lo tou/la tua ('yours'), and so on, the use of -v- /v/ in the imperfect tense in all conjugations: cantava, creixiva, llegiva; the use of many archaic words, usual words in Alguerese: manco instead of menys ('less'), calqui u instead of algú ('someone'), qual/quala instead of quin/quina ('which'), and so on; and the adaptation of weak pronouns. Like all the Romance languages, Catalan verbal inflection is more complex than the nominal. Suffixation is omnipresent, whereas morphological alternations play a secondary role. Vowel alternances are active, as well as infixation and suppletion. However, these are not as productive as in Spanish, and are mostly restricted to irregular verbs. Linguists, including Valencian scholars, deal with Catalan and Valencian as the same language. The official regulating body of the language of the Valencian Community, the Valencian Academy of Language (Acadèmia Valenciana de la Llengua, AVL) declares the linguistic unity between Valencian and Catalan varieties. The morphology of Catalan personal pronouns is complex, specially in unstressed forms, which are numerous (13 distinct forms, compared to 11 in Spanish or 9 in Italian).  Features include the gender-neutral ho and the great degree of freedom when combining different unstressed pronouns (65 combinations). In nouns and adjectives, maintenance of /n/ of medieval plurals in proparoxytone words.
E.g. hòmens 'men', jóvens 'youth'. In nouns and adjectives, loss of /n/ of medieval plurals in proparoxytone words.
E.g. homes 'men', joves 'youth'. The Catalan verbal system is basically common to all Western Romance, except that most dialects have replaced the synthetic indicative perfect with a periphrastic form of anar (""to go"") + infinitive. Catalan shares many traits with its neighboring Romance languages. However, despite being mostly situated in the Iberian Peninsula, Catalan differs more from Iberian Romance (such as Spanish and Portuguese) in terms of vocabulary, pronunciation, and grammar than from Gallo-Romance (Occitan, French, Gallo-Italic languages, etc.). These similarities are most notable with Occitan. Shortly after the French Revolution (1789), the French First Republic prohibited official use of, and enacted discriminating policies against, the nonstandard languages of France (patois), such as Catalan, Alsatian, Breton, Occitan, Flemish, and Basque. Central, Western, and Balearic differ in the lexical incidence of stressed /e/ and /ɛ/. Usually, words with /ɛ/ in Central Catalan correspond to /ə/ in Balearic and /e/ in Western Catalan. Words with /e/ in Balearic almost always have /e/ in Central and Western Catalan as well.[vague] As a result, Central Catalan has a much higher incidence of /e/. With the Treaty of the Pyrenees (1659), Spain ceded the northern part of Catalonia to France, and soon thereafter the local Catalan varieties came under the influence of French, which in 1700 became the sole official language of the region. The endonym is pronounced /kə.təˈɫa/ in the Eastern Catalan dialects, and /ka.taˈɫa/ in the Western dialects. In the Valencian Community, the term valencià (/va.len.siˈa/) is frequently used instead. The names ""Catalan"" and ""Valencian"" are two names for the same language. See also status of Valencian below. In Majorcan, unstressed vowels reduce to four: /a e ɛ/ follow the Eastern Catalan reduction pattern; however /o ɔ/ reduce to [o], with /u/ remaining distinct, as in Western Catalan. In the Low Middle Ages, Catalan went through a golden age, reaching a peak of maturity and cultural richness. Examples include the work of Majorcan polymath Ramon Llull (1232–1315), the Four Great Chronicles (13th–14th centuries), and the Valencian school of poetry culminating in Ausiàs March (1397–1459). By the 15th century, the city of Valencia had become the sociocultural center of the Crown of Aragon, and Catalan was present all over the Mediterranean world. During this period, the Royal Chancery propagated a highly standardized language. Catalan was widely used as an official language in Sicily until the 15th century, and in Sardinia until the 17th. During this period, the language was what Costa Carreras terms ""one of the 'great languages' of medieval Europe"". Despite its relative lexical unity, the two dialectal blocks of Catalan (Eastern and Western) show some differences in word choices. Any lexical divergence within any of the two groups can be explained as an archaism. Also, usually Central Catalan acts as an innovative element. Catalan is split in two major dialectal blocks: Eastern Catalan, and Western Catalan. The main difference lies in the treatment of unstressed a and e; which have merged to /ə/ in Eastern dialects, but which remain distinct as /a/ and /e/ in Western dialects. There are a few other differences in pronunciation, verbal morphology, and vocabulary. Catalan evolved from Vulgar Latin around the eastern Pyrenees in the 9th century. During the Low Middle Ages it saw a golden age as the literary and dominant language of the Crown of Aragon, and was widely used all over the Mediterranean. The union of Aragon with the other territories of Spain in 1479 marked the start of the decline of the language. In 1659 Spain ceded Northern Catalonia to France, and Catalan was banned in both states in the early 18th century. 19th-century Spain saw a Catalan literary revival, which culminated in the 1913 orthographic standardization, and the officialization of the language during the Second Spanish Republic (1931–39). However, the Francoist dictatorship (1939–75) banned the language again. Nowadays, France only recognizes French as an official language. Nevertheless, on 10 December 2007, the General Council of the Pyrénées-Orientales officially recognized Catalan as one of the languages of the department and seeks to further promote it in public life and education. Central Catalan is considered the standard pronunciation of the language and has the highest number of speakers. It is spoken in the densely populated regions of the Barcelona province, the eastern half of the province of Tarragona, and most of the province of Girona. Martorell's outstanding novel of chivalry Tirant lo Blanc (1490) shows a transition from Medieval to Renaissance values, something that can also be seen in Metge's work. The first book produced with movable type in the Iberian Peninsula was printed in Catalan. Catalan sociolinguistics studies the situation of Catalan in the world and the different varieties that this language presents. It is a subdiscipline of Catalan philology and other affine studies and has as an objective to analyse the relation between the Catalan language, the speakers and the close reality (including the one of other languages in contact). Standard Catalan, virtually accepted by all speakers, is mostly based on Eastern Catalan, which is the most widely used dialect. Nevertheless, the standards of Valencia and the Balearics admit alternative forms, mostly traditional ones, which are not current in eastern Catalonia. The decline of Catalan continued in the 16th and 17th centuries. The Catalan defeat in the War of Spanish Succession (1714) initiated a series of measures imposing the use of Spanish in legal documentation. These territories are sometimes referred to as the Països Catalans (Catalan Countries), a denomination based on cultural affinity and common heritage, that has also had a subsequent political interpretation but no official status. Various interpretations of the term may include some or all of these regions. Central Catalan is considered the standard pronunciation of the language. The descriptions below are mostly for this variety. For the differences in pronunciation of the different dialects, see the section pronunciation of dialects in this article. On the other hand, there are several language shift processes currently taking place. In Northern Catalonia, Catalan has followed the same trend as the other minority languages of France, with most of its native speakers being 60 or older (as of 2004). Catalan is studied as a foreign language by 30% of the primary education students, and by 15% of the secondary. The cultural association La Bressola promotes a network of community-run schools engaged in Catalan language immersion programs. According to Ethnologue, the lexical similarity between Catalan and other Romance languages is: 87% with Italian; 85% with Portuguese; 80% with Spanish; 76% with Ladin; 75% with Sardinian; and 73% with Romanian. In Western Catalan, unstressed vowels reduce to five: /e ɛ/ > [e]; /o ɔ/ > [o]; /a u i/ remain distinct. This reduction pattern, inherited from Proto-Romance, is also found in Italian and Portuguese. Some Western dialects present further reduction or vowel harmony in some cases. Situated between two large linguistic blocks (Ibero-Romance and Gallo-Romance), Catalan has many unique lexical choices, such as enyorar ""to miss somebody"", apaivagar ""to calm down somebody"", or rebutjar ""reject"". Like other languages, Catalan has a large list of learned words from Greek and Latin. This process started very early, and one can find such examples in Ramon Llull's work. On the fourteenth and fifteenth centuries Catalan had a number of Greco-Latin learned words much superior to other Romance languages, as it can be attested for example in Roís de Corella's writings. The word Catalan derives from the territory of Catalonia, itself of disputed etymology. The main theory suggests that Catalunya (Latin Gathia Launia) derives from the name Gothia or Gauthia (""Land of the Goths""), since the origins of the Catalan counts, lords and people were found in the March of Gothia, whence Gothland > Gothlandia > Gothalania > Catalonia theoretically derived. There is evidence that, at least from the a.d. 2nd century, the vocabulary and phonology of Roman Tarraconensis was different from the rest of Roman Hispania. Differentiation has arisen generally because Spanish, Asturian, and Galician-Portuguese share certain peripheral archaisms (Spanish hervir, Asturian/Portuguese ferver vs. Catalan bullir, Occitan bolir ""to boil"") and innovatory regionalisms (Sp novillo, Ast nuviellu vs. Cat torell, Oc taurèl ""bullock""), while Catalan has a shared history with the Western Romance innovative core, especially Occitan. Central Catalan has abandoned almost completely unstressed possessives (mon, etc.) in favour of constructions of article + stressed forms (el meu, etc.), a feature shared with Italian. In the 11th century, documents written in macaronic Latin begin to show Catalan elements, with texts written almost completely in Romance appearing by 1080. Old Catalan shared many features with Gallo-Romance, diverging from Old Occitan between the 11th and 14th centuries. In Spain, every person officially has two surnames, one of which is the father's first surname and the other is the mother's first surname. The law contemplates the possibility of joining both surnames with the Catalan conjunction i (""and""). The AVL, created by the Valencian parliament, is in charge of dictating the official rules governing the use of Valencian, and its standard is based on the Norms of Castelló (Normes de Castelló). Currently, everyone who writes in Valencian uses this standard, except the Royal Academy of Valencian Culture (Acadèmia de Cultura Valenciana, RACV), which uses for Valencian an independent standard. This clash of opinions has sparked much controversy. For example, during the drafting of the European Constitution in 2004, the Spanish government supplied the EU with translations of the text into Basque, Galician, Catalan, and Valencian, but the latter two were identical. Catalan has an inflectional grammar, with two genders (masculine, feminine), and two numbers (singular, plural). Pronouns are also inflected for case, animacy[citation needed] and politeness, and can be combined in very complex ways. Verbs are split in several paradigms and are inflected for person, number, tense, aspect, mood, and gender. In terms of pronunciation, Catalan has many words ending in a wide variety of consonants and some consonant clusters, in contrast with many other Romance languages. By the 9th century, Catalan had evolved from Vulgar Latin on both sides of the eastern end of the Pyrenees, as well as the territories of the Roman province of Hispania Tarraconensis to the south. From the 8th century onwards the Catalan counts extended their territory southwards and westwards at the expense of the Muslims, bringing their language with them. This process was given definitive impetus with the separation of the County of Barcelona from the Carolingian Empire in 988. In verbs, 1st person present indicative desinence is -e (∅ in verbs of the 2nd and 3rd conjugation), or -o.
E.g. parle, tem, sent (Valencian); parlo, temo, sento (Northwestern). In verbs, 1st person present indicative desinence is -o, -i or ∅ in all conjugations.
E.g. parlo (Central), parl (Balearic), parli (Northern), ('I speak'). Valencian is classified as a Western dialect, along with the northwestern varieties spoken in Western Catalonia (provinces of Lleida and the western half of Tarragona). The various forms of Catalan and Valencian are mutually intelligible (ranging from 90% to 95%) In gender inflection, the most notable feature is (compared to Portuguese, Spanish or Italian), the loss of the typical masculine suffix -o. Thus, the alternance of -o/-a, has been replaced by ø/-a. There are only a few exceptions, like minso/minsa (""scarce""). Many not completely predictable morphological alternations may occur, such as: During much of its history, and especially during the Francoist dictatorship (1939–1975), the Catalan language has often been degraded as a mere dialect of Spanish. This view, based on political and ideological considerations, has no linguistic validity. Spanish and Catalan have important differences in their sound systems, lexicon, and grammatical features, placing the language in a number of respects closer to Occitan (and French).  In English, the term referring to a person first appears in the mid 14th century as Catelaner, followed in the 15th century as Catellain (from French). It is attested a language name since at least 1652. Catalan can be pronounced as /ˈkætəlæn/, /kætəˈlæn/ or /ˈkætələn/. In Andorra, Catalan has always been the sole official language. Since the promulgation of the 1993 constitution, several Andorranization policies have been enforced, like Catalan medium education. Catalan has inherited the typical vowel system of Vulgar Latin, with seven stressed phonemes: /a ɛ e i ɔ o u/, a common feature in Western Romance, except Spanish. Balearic has also instances of stressed /ə/. Dialects differ in the different degrees of vowel reduction, and the incidence of the pair /ɛ e/. According to the Statistical Institute of Catalonia in 2008 the Catalan language is the second most commonly used in Catalonia, after Spanish, as a native or self-defining language. The Generalitat of Catalunya spends part of its annual budget on the promotion of the use of Catalan in Catalonia and in other territories. Catalan has few suppletive couplets, like Italian and Spanish, and unlike French. Thus, Catalan has noi/noia (""boy""/""girl"") and gall/gallina (""cock""/""hen""), whereas French has garçon/fille and coq/poule. The dialects of the Catalan language feature a relative uniformity, especially when compared to other Romance languages; both in terms of vocabulary, semantics, syntax, morphology, and phonology. Mutual intelligibility between dialects is very high, estimates ranging from 90% to 95%. The only exception is the isolated idiosyncratic Alguerese dialect. Western Catalan comprises the two dialects of Northwestern Catalan and Valencian; the Eastern block comprises four dialects: Central Catalan, Balearic, Rossellonese, and Alguerese. Each dialect can be further subdivided in several subdialects. Catalan (/ˈkætəlæn/; autonym: català [kətəˈla] or [kataˈla]) is a Romance language named for its origins in Catalonia, in what is northeastern Spain and adjoining parts of France. It is the national and only official language of Andorra, and a co-official language of the Spanish autonomous communities of Catalonia, the Balearic Islands, and Valencia (where the language is known as Valencian, and there exist regional standards). It also has semi-official status in the city of Alghero on the Italian island of Sardinia. It is also spoken with no official recognition in parts of the Spanish autonomous communities of Aragon (La Franja) and Murcia (Carche), and in the historic French region of Roussillon/Northern Catalonia, roughly equivalent to the department of Pyrénées-Orientales."
Association_football,"The length of the pitch for international adult matches is in the range of 100–110 m (110–120 yd) and the width is in the range of 64–75 m (70–80 yd). Fields for non-international matches may be 90–120 m (100–130 yd) length and 45–90 m (50–100 yd) in width, provided that the pitch does not become square. In 2008, the IFAB initially approved a fixed size of 105 m (344 ft) long and 68 m (223 ft) wide as a standard pitch dimension for international matches; however, this decision was later put on hold and was never actually implemented. Phaininda and episkyros were Greek ball games. An image of an episkyros player depicted in low relief on a vase at the National Archaeological Museum of Athens appears on the UEFA European Championship Cup. Athenaeus, writing in 228 AD, referenced the Roman ball game harpastum. Phaininda, episkyros and harpastum were played involving hands and violence. They all appear to have resembled rugby football, wrestling and volleyball more than what is recognizable as modern football. As with pre-codified ""mob football"", the antecedent of all modern football codes, these three games involved more handling the ball than kicking. Non-competitive games included kemari in Japan, chuk-guk in Korea and woggabaliri in Australia. A number of players may be replaced by substitutes during the course of the game. The maximum number of substitutions permitted in most competitive international and domestic league games is three, though the permitted number may vary in other competitions or in friendly matches. Common reasons for a substitution include injury, tiredness, ineffectiveness, a tactical switch, or timewasting at the end of a finely poised game. In standard adult matches, a player who has been substituted may not take further part in a match. IFAB recommends ""that a match should not continue if there are fewer than seven players in either team."" Any decision regarding points awarded for abandoned games is left to the individual football associations. In the late 1990s and early 2000s, the IFAB experimented with ways of creating a winner without requiring a penalty shootout, which was often seen as an undesirable way to end a match. These involved rules ending a game in extra time early, either when the first goal in extra time was scored (golden goal), or if one team held a lead at the end of the first period of extra time (silver goal). Golden goal was used at the World Cup in 1998 and 2002. The first World Cup game decided by a golden goal was France's victory over Paraguay in 1998. Germany was the first nation to score a golden goal in a major competition, beating Czech Republic in the final of Euro 1996. Silver goal was used in Euro 2004. Both these experiments have been discontinued by IFAB. The growth in women's football has seen major competitions being launched at both national and international level mirroring the male competitions. Women's football has faced many struggles. It had a ""golden age"" in the United Kingdom in the early 1920s when crowds reached 50,000 at some matches; this was stopped on 5 December 1921 when England's Football Association voted to ban the game from grounds used by its member clubs. The FA's ban was rescinded in December 1969 with UEFA voting to officially recognise women's football in 1971. The FIFA Women's World Cup was inaugurated in 1991 and has been held every four years since, while women's football has been an Olympic event since 1996. A standard adult football match consists of two periods of 45 minutes each, known as halves. Each half runs continuously, meaning that the clock is not stopped when the ball is out of play. There is usually a 15-minute half-time break between halves. The end of the match is known as full-time. The referee is the official timekeeper for the match, and may make an allowance for time lost through substitutions, injured players requiring attention, or other stoppages. This added time is called additional time in FIFA documents, but is most commonly referred to as stoppage time or injury time, while loss time can also be used as a synonym. The duration of stoppage time is at the sole discretion of the referee. The referee alone signals the end of the match. In matches where a fourth official is appointed, toward the end of the half the referee signals how many minutes of stoppage time he intends to add. The fourth official then informs the players and spectators by holding up a board showing this number. The signalled stoppage time may be further extended by the referee. Added time was introduced because of an incident which happened in 1891 during a match between Stoke and Aston Villa. Trailing 1–0 and with just two minutes remaining, Stoke were awarded a penalty. Villa's goalkeeper kicked the ball out of the ground, and by the time the ball had been recovered, the 90 minutes had elapsed and the game was over. The same law also states that the duration of either half is extended until the penalty kick to be taken or retaken is completed, thus no game shall end with a penalty to be taken. At a professional level, most matches produce only a few goals. For example, the 2005–06 season of the English Premier League produced an average of 2.48 goals per match. The Laws of the Game do not specify any player positions other than goalkeeper, but a number of specialised roles have evolved. Broadly, these include three main categories: strikers, or forwards, whose main task is to score goals; defenders, who specialise in preventing their opponents from scoring; and midfielders, who dispossess the opposition and keep possession of the ball to pass it to the forwards on their team. Players in these positions are referred to as outfield players, to distinguish them from the goalkeeper. These positions are further subdivided according to the area of the field in which the player spends most time. For example, there are central defenders, and left and right midfielders. The ten outfield players may be arranged in any combination. The number of players in each position determines the style of the team's play; more forwards and fewer defenders creates a more aggressive and offensive-minded game, while the reverse creates a slower, more defensive style of play. While players typically spend most of the game in a specific position, there are few restrictions on player movement, and players can switch positions at any time. The layout of a team's players is known as a formation. Defining the team's formation and tactics is usually the prerogative of the team's manager. The governing bodies in each country operate league systems in a domestic season, normally comprising several divisions, in which the teams gain points throughout the season depending on results. Teams are placed into tables, placing them in order according to points accrued. Most commonly, each team plays every other team in its league at home and away in each season, in a round-robin tournament. At the end of a season, the top team is declared the champion. The top few teams may be promoted to a higher division, and one or more of the teams finishing at the bottom are relegated to a lower division. The goalkeepers are the only players allowed to touch the ball with their hands or arms while it is in play and only in their penalty area. Outfield players mostly use their feet to strike or pass the ball, but may also use their head or torso to do so instead. The team that scores the most goals by the end of the match wins. If the score is level at the end of the game, either a draw is declared or the game goes into extra time and/or a penalty shootout depending on the format of the competition. The Laws of the Game were originally codified in England by The Football Association in 1863. Association football is governed internationally by the International Federation of Association Football (FIFA; French: Fédération Internationale de Football Association), which organises World Cups for both men and women every four years. After the World Cup, the most important international football competitions are the continental championships, which are organised by each continental confederation and contested between national teams. These are the European Championship (UEFA), the Copa América (CONMEBOL), African Cup of Nations (CAF), the Asian Cup (AFC), the CONCACAF Gold Cup (CONCACAF) and the OFC Nations Cup (OFC). The FIFA Confederations Cup is contested by the winners of all six continental championships, the current FIFA World Cup champions and the country which is hosting the Confederations Cup. This is generally regarded as a warm-up tournament for the upcoming FIFA World Cup and does not carry the same prestige as the World Cup itself. The most prestigious competitions in club football are the respective continental championships, which are generally contested between national champions, for example the UEFA Champions League in Europe and the Copa Libertadores in South America. The winners of each continental competition contest the FIFA Club World Cup. Along with the general administration of the sport, football associations and competition organisers also enforce good conduct in wider aspects of the game, dealing with issues such as comments to the press, clubs' financial management, doping, age fraud and match fixing. Most competitions enforce mandatory suspensions for players who are sent off in a game. Some on-field incidents, if considered very serious (such as allegations of racial abuse), may result in competitions deciding to impose heavier sanctions than those normally associated with a red card. Some associations allow for appeals against player suspensions incurred on-field if clubs feel a referee was incorrect or unduly harsh. The referee may punish a player's or substitute's misconduct by a caution (yellow card) or dismissal (red card). A second yellow card at the same game leads to a red card, and therefore to a dismissal. A player given a yellow card is said to have been ""booked"", the referee writing the player's name in his official notebook. If a player has been dismissed, no substitute can be brought on in their place. Misconduct may occur at any time, and while the offences that constitute misconduct are listed, the definitions are broad. In particular, the offence of ""unsporting behaviour"" may be used to deal with most events that violate the spirit of the game, even if they are not listed as specific offences. A referee can show a yellow or red card to a player, substitute or substituted player. Non-players such as managers and support staff cannot be shown the yellow or red card, but may be expelled from the technical area if they fail to conduct themselves in a responsible manner. The primary law is that players other than goalkeepers may not deliberately handle the ball with their hands or arms during play, though they do use their hands during a throw-in restart. Although players usually use their feet to move the ball around, they may use any part of their body (notably, ""heading"" with the forehead) other than their hands or arms. Within normal play, all players are free to play the ball in any direction and move throughout the pitch, though the ball cannot be received in an offside position. The Cambridge Rules, first drawn up at Cambridge University in 1848, were particularly influential in the development of subsequent codes, including association football. The Cambridge Rules were written at Trinity College, Cambridge, at a meeting attended by representatives from Eton, Harrow, Rugby, Winchester and Shrewsbury schools. They were not universally adopted. During the 1850s, many clubs unconnected to schools or universities were formed throughout the English-speaking world, to play various forms of football. Some came up with their own distinct codes of rules, most notably the Sheffield Football Club, formed by former public school pupils in 1857, which led to formation of a Sheffield FA in 1867. In 1862, John Charles Thring of Uppingham School also devised an influential set of rules. There are 17 laws in the official Laws of the Game, each containing a collection of stipulation and guidelines. The same laws are designed to apply to all levels of football, although certain modifications for groups such as juniors, seniors, women and people with physical disabilities are permitted. The laws are often framed in broad terms, which allow flexibility in their application depending on the nature of the game. The Laws of the Game are published by FIFA, but are maintained by the International Football Association Board (IFAB). In addition to the seventeen laws, numerous IFAB decisions and other directives contribute to the regulation of football. Association football in itself does not have a classical history. Notwithstanding any similarities to other ball games played around the world FIFA have recognised that no historical connection exists with any game played in antiquity outside Europe. The modern rules of association football are based on the mid-19th century efforts to standardise the widely varying forms of football played in the public schools of England. The history of football in England dates back to at least the eighth century AD. In many parts of the world football evokes great passions and plays an important role in the life of individual fans, local communities, and even nations. R. Kapuscinski says that Europeans who are polite, modest, or humble fall easily into rage when playing or watching football games. The Côte d'Ivoire national football team helped secure a truce to the nation's civil war in 2006 and it helped further reduce tensions between government and rebel forces in 2007 by playing a match in the rebel capital of Bouaké, an occasion that brought both armies together peacefully for the first time. By contrast, football is widely considered to have been the final proximate cause for the Football War in June 1969 between El Salvador and Honduras. The sport also exacerbated tensions at the beginning of the Yugoslav Wars of the 1990s, when a match between Dinamo Zagreb and Red Star Belgrade degenerated into rioting in May 1990. The world's oldest football competition is the FA Cup, which was founded by C. W. Alcock and has been contested by English teams since 1872. The first official international football match also took place in 1872, between Scotland and England in Glasgow, again at the instigation of C. W. Alcock. England is also home to the world's first football league, which was founded in Birmingham in 1888 by Aston Villa director William McGregor. The original format contained 12 clubs from the Midlands and Northern England. In league competitions, games may end in a draw. In knockout competitions where a winner is required various methods may be employed to break such a deadlock, some competitions may invoke replays. A game tied at the end of regulation time may go into extra time, which consists of two further 15-minute periods. If the score is still tied after extra time, some competitions allow the use of penalty shootouts (known officially in the Laws of the Game as ""kicks from the penalty mark"") to determine which team will progress to the next stage of the tournament. Goals scored during extra time periods count toward the final score of the game, but kicks from the penalty mark are only used to decide the team that progresses to the next part of the tournament (with goals scored in a penalty shootout not making up part of the final score). Association football is played in accordance with a set of rules known as the Laws of the Game. The game is played using a spherical ball of 68.5–69.5 cm (27.0–27.4 in) circumference, known as the football (or soccer ball). Two teams of eleven players each compete to get the ball into the other team's goal (between the posts and under the bar), thereby scoring a goal. The team that has scored more goals at the end of the game is the winner; if both teams have scored an equal number of goals then the game is a draw. Each team is led by a captain who has only one official responsibility as mandated by the Laws of the Game: to be involved in the coin toss prior to kick-off or penalty kicks. The laws of the game are determined by the International Football Association Board (IFAB). The Board was formed in 1886 after a meeting in Manchester of The Football Association, the Scottish Football Association, the Football Association of Wales, and the Irish Football Association. FIFA, the international football body, was formed in Paris in 1904 and declared that they would adhere to Laws of the Game of the Football Association. The growing popularity of the international game led to the admittance of FIFA representatives to the International Football Association Board in 1913. The board consists of four representatives from FIFA and one representative from each of the four British associations. These ongoing efforts contributed to the formation of The Football Association (The FA) in 1863, which first met on the morning of 26 October 1863 at the Freemasons' Tavern in Great Queen Street, London. The only school to be represented on this occasion was Charterhouse. The Freemason's Tavern was the setting for five more meetings between October and December, which eventually produced the first comprehensive set of rules. At the final meeting, the first FA treasurer, the representative from Blackheath, withdrew his club from the FA over the removal of two draft rules at the previous meeting: the first allowed for running with the ball in hand; the second for obstructing such a run by hacking (kicking an opponent in the shins), tripping and holding. Other English rugby clubs followed this lead and did not join the FA and instead in 1871 formed the Rugby Football Union. The eleven remaining clubs, under the charge of Ebenezer Cobb Morley, went on to ratify the original thirteen laws of the game. These rules included handling of the ball by ""marks"" and the lack of a crossbar, rules which made it remarkably similar to Victorian rules football being developed at that time in Australia. The Sheffield FA played by its own rules until the 1870s with the FA absorbing some of its rules until there was little difference between the games. As the Laws were formulated in England, and were initially administered solely by the four British football associations within IFAB, the standard dimensions of a football pitch were originally expressed in imperial units. The Laws now express dimensions with approximate metric equivalents (followed by traditional units in brackets), though use of imperial units remains popular in English-speaking countries with a relatively recent history of metrication (or only partial metrication), such as Britain. The basic equipment or kit players are required to wear includes a shirt, shorts, socks, footwear and adequate shin guards. An athletic supporter and protective cup is highly recommended for male players by medical experts and professionals. Headgear is not a required piece of basic equipment, but players today may choose to wear it to protect themselves from head injury. Players are forbidden to wear or use anything that is dangerous to themselves or another player, such as jewellery or watches. The goalkeeper must wear clothing that is easily distinguishable from that worn by the other players and the match officials. In front of the goal is the penalty area. This area is marked by the goal line, two lines starting on the goal line 16.5 m (18 yd) from the goalposts and extending 16.5 m (18 yd) into the pitch perpendicular to the goal line, and a line joining them. This area has a number of functions, the most prominent being to mark where the goalkeeper may handle the ball and where a penalty foul by a member of the defending team becomes punishable by a penalty kick. Other markings define the position of the ball or players at kick-offs, goal kicks, penalty kicks and corner kicks. In game play, players attempt to create goal-scoring opportunities through individual control of the ball, such as by dribbling, passing the ball to a team-mate, and by taking shots at the goal, which is guarded by the opposing goalkeeper. Opposing players may try to regain control of the ball by intercepting a pass or through tackling the opponent in possession of the ball; however, physical contact between opponents is restricted. Football is generally a free-flowing game, with play stopping only when the ball has left the field of play or when play is stopped by the referee for an infringement of the rules. After a stoppage, play recommences with a specified restart. There has been a football tournament at every Summer Olympic Games since 1900, except at the 1932 games in Los Angeles. Before the inception of the World Cup, the Olympics (especially during the 1920s) had the same status as the World Cup. Originally, the event was for amateurs only; however, since the 1984 Summer Olympics, professional players have been permitted, albeit with certain restrictions which prevent countries from fielding their strongest sides. The Olympic men's tournament is played at Under-23 level. In the past the Olympics have allowed a restricted number of over-age players per team. A women's tournament was added in 1996; in contrast to the men's event, full international sides without age restrictions play the women's Olympic tournament. Each team consists of a maximum of eleven players (excluding substitutes), one of whom must be the goalkeeper. Competition rules may state a minimum number of players required to constitute a team, which is usually seven. Goalkeepers are the only players allowed to play the ball with their hands or arms, provided they do so within the penalty area in front of their own goal. Though there are a variety of positions in which the outfield (non-goalkeeper) players are strategically placed by a coach, these positions are not defined or required by the Laws."
Sumer,"Sumerian religion seems to have been founded upon two separate cosmogenic myths. The first saw creation as the result of a series of hieros gami or sacred marriages, involving the reconciliation of opposites, postulated as a coming together of male and female divine beings; the gods. This continued to influence the whole Mesopotamian mythos. Thus in the Enuma Elish the creation was seen as the union of fresh and salt water; as male Abzu, and female Tiamat. The product of that union, Lahm and Lahmu, ""the muddy ones"", were titles given to the gate keepers of the E-Abzu temple of Enki, in Eridu, the first Sumerian city. Describing the way that muddy islands emerge from the confluence of fresh and salty water at the mouth of the Euphrates, where the river deposited its load of silt, a second hieros gamos supposedly created Anshar and Kishar, the ""sky-pivot"" or axle, and the ""earth pivot"", parents in turn of Anu (the sky) and Ki (the earth). Another important Sumerian hieros gamos was that between Ki, here known as Ninhursag or ""Lady Sacred Mountain"", and Enki of Eridu, the god of fresh water which brought forth greenery and pasture. The most important archaeological discoveries in Sumer are a large number of tablets written in cuneiform. Sumerian writing, while proven to be not the oldest example of writing on earth, is considered to be a great milestone in the development of man's ability to not only create historical records but also in creating pieces of literature both in the form of poetic epics and stories as well as prayers and laws. Although pictures — that is, hieroglyphs — were first used, cuneiform and then Ideograms (where symbols were made to represent ideas) soon followed. Triangular or wedge-shaped reeds were used to write on moist clay. A large body of hundreds of thousands of texts in the Sumerian language have survived, such as personal or business letters, receipts, lexical lists, laws, hymns, prayers, stories, daily records, and even libraries full of clay tablets. Monumental inscriptions and texts on different objects like statues or bricks are also very common. Many texts survive in multiple copies because they were repeatedly transcribed by scribes-in-training. Sumerian continued to be the language of religion and law in Mesopotamia long after Semitic speakers had become dominant. The term ""Sumerian"" is the common name given to the ancient non-Semitic inhabitants of Mesopotamia, Sumer, by the Semitic Akkadians. The Sumerians referred to themselves as ùĝ saĝ gíg-ga (cuneiform: 𒌦 𒊕 𒈪 𒂵), phonetically /uŋ saŋ giga/, literally meaning ""the black-headed people"", and to their land as ki-en-gi(-r) ('place' + 'lords' + 'noble'), meaning ""place of the noble lords"". The Akkadian word Shumer may represent the geographical name in dialect, but the phonological development leading to the Akkadian term šumerû is uncertain. Hebrew Shinar, Egyptian Sngr, and Hittite Šanhar(a), all referring to southern Mesopotamia, could be western variants of Shumer. Periodically ""clean slate"" decrees were signed by rulers which cancelled all the rural (but not commercial) debt and allowed bondservants to return to their homes. Customarily rulers did it at the beginning of the first full year of their reign, but they could also be proclaimed at times of military conflict or crop failure. The first known ones were made by Enmetena and Urukagina of Lagash in 2400-2350 BC. According to Hudson, the purpose of these decrees was to prevent debts mounting to a degree that they threatened fighting force which could happen if peasants lost the subsistence land or became bondservants due to the inability to repay the debt. Although short-lived, one of the first empires known to history was that of Eannatum of Lagash, who annexed practically all of Sumer, including Kish, Uruk, Ur, and Larsa, and reduced to tribute the city-state of Umma, arch-rival of Lagash. In addition, his realm extended to parts of Elam and along the Persian Gulf. He seems to have used terror as a matter of policy. Eannatum's Stele of the Vultures depicts vultures pecking at the severed heads and other body parts of his enemies. His empire collapsed shortly after his death. They invented and developed arithmetic by using several different number systems including a mixed radix system with an alternating base 10 and base 6. This sexagesimal system became the standard number system in Sumer and Babylonia. They may have invented military formations and introduced the basic divisions between infantry, cavalry, and archers. They developed the first known codified legal and administrative systems, complete with courts, jails, and government records. The first true city-states arose in Sumer, roughly contemporaneously with similar entities in what are now Syria and Lebanon. Several centuries after the invention of cuneiform, the use of writing expanded beyond debt/payment certificates and inventory lists to be applied for the first time, about 2600 BC, to messages and mail delivery, history, legend, mathematics, astronomical records, and other pursuits. Conjointly with the spread of writing, the first formal schools were established, usually under the auspices of a city-state's primary temple. According to Archibald Sayce, the primitive pictograms of the early Sumerian (i.e. Uruk) era suggest that ""Stone was scarce, but was already cut into blocks and seals. Brick was the ordinary building material, and with it cities, forts, temples and houses were constructed. The city was provided with towers and stood on an artificial platform; the house also had a tower-like appearance. It was provided with a door which turned on a hinge, and could be opened with a sort of key; the city gate was on a larger scale, and seems to have been double. The foundation stones — or rather bricks — of a house were consecrated by certain objects that were deposited under them."" The most impressive and famous of Sumerian buildings are the ziggurats, large layered platforms which supported temples. Sumerian cylinder seals also depict houses built from reeds not unlike those built by the Marsh Arabs of Southern Iraq until as recently as 400 CE. The Sumerians also developed the arch, which enabled them to develop a strong type of dome. They built this by constructing and linking several arches. Sumerian temples and palaces made use of more advanced materials and techniques,[citation needed] such as buttresses, recesses, half columns, and clay nails. The Sumerians were one of the first known beer drinking societies. Cereals were plentiful and were the key ingredient in their early brew. They brewed multiple kinds of beer consisting of wheat, barley, and mixed grain beers. Beer brewing was very important to the Sumerians. It was referenced in the Epic of Gilgamesh when Enkidu was introduced to the food and beer of Gilgamesh's people: ""Drink the beer, as is the custom of the land... He drank the beer-seven jugs! and became expansive and sang with joy!"" It was believed that when people died, they would be confined to a gloomy world of Ereshkigal, whose realm was guarded by gateways with various monsters designed to prevent people entering or leaving. The dead were buried outside the city walls in graveyards where a small mound covered the corpse, along with offerings to monsters and a small amount of food. Those who could afford it sought burial at Dilmun. Human sacrifice was found in the death pits at the Ur royal cemetery where Queen Puabi was accompanied in death by her servants. It is also said that the Sumerians invented the first oboe-like instrument, and used them at royal funerals. In the early Sumerian Uruk period, the primitive pictograms suggest that sheep, goats, cattle, and pigs were domesticated. They used oxen as their primary beasts of burden and donkeys or equids as their primary transport animal and ""woollen clothing as well as rugs were made from the wool or hair of the animals. ... By the side of the house was an enclosed garden planted with trees and other plants; wheat and probably other cereals were sown in the fields, and the shaduf was already employed for the purpose of irrigation. Plants were also grown in pots or vases."" The Semitic Akkadian language is first attested in proper names of the kings of Kish c. 2800 BC, preserved in later king lists. There are texts written entirely in Old Akkadian dating from c. 2500 BC. Use of Old Akkadian was at its peak during the rule of Sargon the Great (c. 2270–2215 BC), but even then most administrative tablets continued to be written in Sumerian, the language used by the scribes. Gelb and Westenholz differentiate three stages of Old Akkadian: that of the pre-Sargonic era, that of the Akkadian empire, and that of the ""Neo-Sumerian Renaissance"" that followed it. Akkadian and Sumerian coexisted as vernacular languages for about one thousand years, but by around 1800 BC, Sumerian was becoming more of a literary language familiar mainly only to scholars and scribes. Thorkild Jacobsen has argued that there is little break in historical continuity between the pre- and post-Sargon periods, and that too much emphasis has been placed on the perception of a ""Semitic vs. Sumerian"" conflict. However, it is certain that Akkadian was also briefly imposed on neighboring parts of Elam that were previously conquered by Sargon. These deities formed a core pantheon; there were additionally hundreds of minor ones. Sumerian gods could thus have associations with different cities, and their religious importance often waxed and waned with those cities' political power. The gods were said to have created human beings from clay for the purpose of serving them. The temples organized the mass labour projects needed for irrigation agriculture. Citizens had a labor duty to the temple, though they could avoid it by a payment of silver. The Sumerians developed a complex system of metrology c. 4000 BC. This advanced metrology resulted in the creation of arithmetic, geometry, and algebra. From c. 2600 BC onwards, the Sumerians wrote multiplication tables on clay tablets and dealt with geometrical exercises and division problems. The earliest traces of the Babylonian numerals also date back to this period. The period c. 2700 – 2300 BC saw the first appearance of the abacus, and a table of successive columns which delimited the successive orders of magnitude of their sexagesimal number system. The Sumerians were the first to use a place value numeral system. There is also anecdotal evidence the Sumerians may have used a type of slide rule in astronomical calculations. They were the first to find the area of a triangle and the volume of a cube. It is speculated by some archaeologists that Sumerian speakers were farmers who moved down from the north, after perfecting irrigation agriculture there. The Ubaid pottery of southern Mesopotamia has been connected via Choga Mami transitional ware to the pottery of the Samarra period culture (c. 5700 – 4900 BC C-14) in the north, who were the first to practice a primitive form of irrigation agriculture along the middle Tigris River and its tributaries. The connection is most clearly seen at Tell Awayli (Oueilli, Oueili) near Larsa, excavated by the French in the 1980s, where eight levels yielded pre-Ubaid pottery resembling Samarran ware. According to this theory, farming peoples spread down into southern Mesopotamia because they had developed a temple-centered social organization for mobilizing labor and technology for water control, enabling them to survive and prosper in a difficult environment.[citation needed] Evidence of wheeled vehicles appeared in the mid 4th millennium BC, near-simultaneously in Mesopotamia, the Northern Caucasus (Maykop culture) and Central Europe. The wheel initially took the form of the potter's wheel. The new concept quickly led to wheeled vehicles and mill wheels. The Sumerians' cuneiform writing system is the oldest (or second oldest after the Egyptian hieroglyphs) which has been deciphered (the status of even older inscriptions such as the Jiahu symbols and Tartaria tablets is controversial). The Sumerians were among the first astronomers, mapping the stars into sets of constellations, many of which survived in the zodiac and were also recognized by the ancient Greeks. They were also aware of the five planets that are easily visible to the naked eye. The earliest dynastic king on the Sumerian king list whose name is known from any other legendary source is Etana, 13th king of the first dynasty of Kish. The earliest king authenticated through archaeological evidence is Enmebaragesi of Kish (c. 26th century BC), whose name is also mentioned in the Gilgamesh epic—leading to the suggestion that Gilgamesh himself might have been a historical king of Uruk. As the Epic of Gilgamesh shows, this period was associated with increased war. Cities became walled, and increased in size as undefended villages in southern Mesopotamia disappeared. (Gilgamesh is credited with having built the walls of Uruk). Ziggurats (Sumerian temples) each had an individual name and consisted of a forecourt, with a central pond for purification. The temple itself had a central nave with aisles along either side. Flanking the aisles would be rooms for the priests. At one end would stand the podium and a mudbrick table for animal and vegetable sacrifices. Granaries and storehouses were usually located near the temples. After a time the Sumerians began to place the temples on top of multi-layered square constructions built as a series of rising terraces, giving rise to the Ziggurat style. The almost constant wars among the Sumerian city-states for 2000 years helped to develop the military technology and techniques of Sumer to a high level. The first war recorded in any detail was between Lagash and Umma in c. 2525 BC on a stele called the Stele of the Vultures. It shows the king of Lagash leading a Sumerian army consisting mostly of infantry. The infantrymen carried spears, wore copper helmets, and carried rectangular shields. The spearmen are shown arranged in what resembles the phalanx formation, which requires training and discipline; this implies that the Sumerians may have made use of professional soldiers. Sumerian cities during the Uruk period were probably theocratic and were most likely headed by a priest-king (ensi), assisted by a council of elders, including both men and women. It is quite possible that the later Sumerian pantheon was modeled upon this political structure. There was little evidence of organized warfare or professional soldiers during the Uruk period, and towns were generally unwalled. During this period Uruk became the most urbanized city in the world, surpassing for the first time 50,000 inhabitants. This period is generally taken to coincide with a major shift in population from southern Mesopotamia toward the north. Ecologically, the agricultural productivity of the Sumerian lands was being compromised as a result of rising salinity. Soil salinity in this region had been long recognized as a major problem. Poorly drained irrigated soils, in an arid climate with high levels of evaporation, led to the buildup of dissolved salts in the soil, eventually reducing agricultural yields severely. During the Akkadian and Ur III phases, there was a shift from the cultivation of wheat to the more salt-tolerant barley, but this was insufficient, and during the period from 2100 BC to 1700 BC, it is estimated that the population in this area declined by nearly three fifths. This greatly upset the balance of power within the region, weakening the areas where Sumerian was spoken, and comparatively strengthening those where Akkadian was the major language. Henceforth Sumerian would remain only a literary and liturgical language, similar to the position occupied by Latin in medieval Europe. As is known from the ""Sumerian Farmer's Almanac"", after the flood season and after the Spring Equinox and the Akitu or New Year Festival, using the canals, farmers would flood their fields and then drain the water. Next they made oxen stomp the ground and kill weeds. They then dragged the fields with pickaxes. After drying, they plowed, harrowed, and raked the ground three times, and pulverized it with a mattock, before planting seed. Unfortunately the high evaporation rate resulted in a gradual increase in the salinity of the fields. By the Ur III period, farmers had switched from wheat to the more salt-tolerant barley as their principal crop. Later, the 3rd dynasty of Ur under Ur-Nammu and Shulgi, whose power extended as far as southern Assyria, was the last great ""Sumerian renaissance"", but already the region was becoming more Semitic than Sumerian, with the rise in power of the Akkadian speaking Semites in Assyria and elsewhere, and the influx of waves of Semitic Martu (Amorites) who were to found several competing local powers including Isin, Larsa, Eshnunna and eventually Babylon. The last of these eventually came to dominate the south of Mesopotamia as the Babylonian Empire, just as the Old Assyrian Empire had already done so in the north from the late 21st century BC. The Sumerian language continued as a sacerdotal language taught in schools in Babylonia and Assyria, much as Latin was used in the Medieval period, for as long as cuneiform was utilized. Commercial credit and agricultural consumer loans were the main types of loans. The trade credit was usually extended by temples in order to finance trade expeditions and was nominated in silver. The interest rate was set at 1/60 a month (one shekel per mina) some time before 2000 BC and it remained at that level for about two thousand years. Rural loans commonly arose as a result of unpaid obligations due to an institution (such as a temple), in this case the arrears were considered to be lent to the debtor. They were denominated in barley or other crops and the interest rate was typically much higher than for commercial loans and could amount to 1/3 to 1/2 of the loan principal. The Sumerian city-states rose to power during the prehistoric Ubaid and Uruk periods. Sumerian written history reaches back to the 27th century BC and before, but the historical record remains obscure until the Early Dynastic III period, c. the 23rd century BC, when a now deciphered syllabary writing system was developed, which has allowed archaeologists to read contemporary records and inscriptions. Classical Sumer ends with the rise of the Akkadian Empire in the 23rd century BC. Following the Gutian period, there is a brief Sumerian Renaissance in the 21st century BC, cut short in the 20th century BC by Semitic Amorite invasions. The Amorite ""dynasty of Isin"" persisted until c. 1700 BC, when Mesopotamia was united under Babylonian rule. The Sumerians were eventually absorbed into the Akkadian (Assyro-Babylonian) population. By the time of the Uruk period (c. 4100–2900 BC calibrated), the volume of trade goods transported along the canals and rivers of southern Mesopotamia facilitated the rise of many large, stratified, temple-centered cities (with populations of over 10,000 people) where centralized administrations employed specialized workers. It is fairly certain that it was during the Uruk period that Sumerian cities began to make use of slave labor captured from the hill country, and there is ample evidence for captured slaves as workers in the earliest texts. Artifacts, and even colonies of this Uruk civilization have been found over a wide area—from the Taurus Mountains in Turkey, to the Mediterranean Sea in the west, and as far east as central Iran. The Sumerians were a non-Semitic caucasoid people, and spoke a language isolate; a number of linguists believed they could detect a substrate language beneath Sumerian, because names of some of Sumer's major cities are not Sumerian, revealing influences of earlier inhabitants. However, the archaeological record shows clear uninterrupted cultural continuity from the time of the early Ubaid period (5300 – 4700 BC C-14) settlements in southern Mesopotamia. The Sumerian people who settled here farmed the lands in this region that were made fertile by silt deposited by the Tigris and the Euphrates rivers. Though women were protected by late Sumerian law and were able to achieve a higher status in Sumer than in other contemporary civilizations, the culture was male-dominated. The Code of Ur-Nammu, the oldest such codification yet discovered, dating to the Ur-III ""Sumerian Renaissance"", reveals a glimpse at societal structure in late Sumerian law. Beneath the lu-gal (""great man"" or king), all members of society belonged to one of two basic strata: The ""lu"" or free person, and the slave (male, arad; female geme). The son of a lu was called a dumu-nita until he married. A woman (munus) went from being a daughter (dumu-mi), to a wife (dam), then if she outlived her husband, a widow (numasu) and she could then remarry. The Ubaid period is marked by a distinctive style of fine quality painted pottery which spread throughout Mesopotamia and the Persian Gulf. During this time, the first settlement in southern Mesopotamia was established at Eridu (Cuneiform: NUN.KI), c. 5300 BC, by farmers who brought with them the Hadji Muhammed culture, which first pioneered irrigation agriculture. It appears that this culture was derived from the Samarran culture from northern Mesopotamia. It is not known whether or not these were the actual Sumerians who are identified with the later Uruk culture. Eridu remained an important religious center when it was gradually surpassed in size by the nearby city of Uruk. The story of the passing of the me (gifts of civilization) to Inanna, goddess of Uruk and of love and war, by Enki, god of wisdom and chief god of Eridu, may reflect this shift in hegemony. The Sumerian language is generally regarded as a language isolate in linguistics because it belongs to no known language family; Akkadian, by contrast, belongs to the Semitic branch of the Afroasiatic languages. There have been many failed attempts to connect Sumerian to other language groups. It is an agglutinative language; in other words, morphemes (""units of meaning"") are added together to create words, unlike analytic languages where morphemes are purely added together to create sentences. Some authors have proposed that there may be evidence of a sub-stratum or add-stratum language for geographic features and various crafts and agricultural activities, called variously Proto-Euphratean or Proto Tigrean, but this is disputed by others. Native Sumerian rule re-emerged for about a century in the Neo-Sumerian Empire or Third Dynasty of Ur (Sumerian Renaissance) approximately 2100-2000 BC, but the Akkadian language also remained in use. The Sumerian city of Eridu, on the coast of the Persian Gulf, is considered to have been the world's first city, where three separate cultures may have fused — that of peasant Ubaidian farmers, living in mud-brick huts and practicing irrigation; that of mobile nomadic Semitic pastoralists living in black tents and following herds of sheep and goats; and that of fisher folk, living in reed huts in the marshlands, who may have been the ancestors of the Sumerians. However, some scholars contest the idea of a Proto-Euphratean language or one substrate language. It has been suggested by them and others, that the Sumerian language was originally that of the hunter and fisher peoples, who lived in the marshland and the Eastern Arabia littoral region, and were part of the Arabian bifacial culture. Reliable historical records begin much later; there are none in Sumer of any kind that have been dated before Enmebaragesi (c. 26th century BC). Professor Juris Zarins believes the Sumerians were settled along the coast of Eastern Arabia, today's Persian Gulf region, before it flooded at the end of the Ice Age."
Germans,"In 1870, after France attacked Prussia, Prussia and its new allies in Southern Germany (among them Bavaria) were victorious in the Franco-Prussian War. It created the German Empire in 1871 as a German nation-state, effectively excluding the multi-ethnic Austrian Habsburg monarchy and Liechtenstein. Integrating the Austrians nevertheless remained a strong desire for many people of Germany and Austria, especially among the liberals, the social democrats and also the Catholics who were a minority within the Protestant Germany. More recently, films such as Das Boot (1981), The Never Ending Story (1984) Run Lola Run (1998), Das Experiment (2001), Good Bye Lenin! (2003), Gegen die Wand (Head-on) (2004) and Der Untergang (Downfall) (2004) have enjoyed international success. In 2002 the Academy Award for Best Foreign Language Film went to Caroline Link's Nowhere in Africa, in 2007 to Florian Henckel von Donnersmarck's The Lives of Others. The Berlin International Film Festival, held yearly since 1951, is one of the world's foremost film and cinema festivals. In the midst of the European sovereign-debt crisis, Radek Sikorski, Poland's Foreign Minister, stated in November 2011, ""I will probably be the first Polish foreign minister in history to say so, but here it is: I fear German power less than I am beginning to fear German inactivity. You have become Europe's indispensable nation."" According to Jacob Heilbrunn, a senior editor at The National Interest, such a statement is unprecedented when taking into consideration Germany's history. ""This was an extraordinary statement from a top official of a nation that was ravaged by Germany during World War II. And it reflects a profound shift taking place throughout Germany and Europe about Berlin's position at the center of the Continent."" Heilbrunn believes that the adage, ""what was good for Germany was bad for the European Union"" has been supplanted by a new mentality—what is in the interest of Germany is also in the interest of its neighbors. The evolution in Germany's national identity stems from focusing less on its Nazi past and more on its Prussian history, which many Germans believe was betrayed—and not represented—by Nazism. The evolution is further precipitated by Germany's conspicuous position as Europe's strongest economy. Indeed, this German sphere of influence has been welcomed by the countries that border it, as demonstrated by Polish foreign minister Radek Sikorski's effusive praise for his country's western neighbor. This shift in thinking is boosted by a newer generation of Germans who see World War II as a distant memory. Following the defeat in World War I, influence of German-speaking elites over Central and Eastern Europe was greatly limited. At the treaty of Versailles Germany was substantially reduced in size. Austria-Hungary was split up. Rump-Austria, which to a certain extent corresponded to the German-speaking areas of Austria-Hungary (a complete split into language groups was impossible due to multi-lingual areas and language-exclaves) adopted the name ""German-Austria"" (German: Deutschösterreich). The name German-Austria was forbidden by the victorious powers of World War I. Volga Germans living in the Soviet Union were interned in gulags or forcibly relocated during the Second World War. People of German origin are found in various places around the globe. United States is home to approximately 50 million German Americans or one third of the German diaspora, making it the largest centre of German-descended people outside Germany. Brazil is the second largest with 5 million people claiming German ancestry. Other significant centres are Canada, Argentina, South Africa and France each accounting for at least 1 million. While the exact number of German-descended people is difficult to calculate, the available data makes it safe to claim the number is exceeding 100 million people. Since the 2006 FIFA World Cup, the internal and external evaluation of Germany's national image has changed. In the annual Nation Brands Index global survey, Germany became significantly and repeatedly more highly ranked after the tournament. People in 20 different states assessed the country's reputation in terms of culture, politics, exports, its people and its attractiveness to tourists, immigrants and investments. Germany has been named the world's second most valued nation among 50 countries in 2010. Another global opinion poll, for the BBC, revealed that Germany is recognised for the most positive influence in the world in 2010. A majority of 59% have a positive view of the country, while 14% have a negative view. After World War II, eastern European countries such as the Soviet Union, Poland, Czechoslovakia, Hungary, Romania and Yugoslavia expelled the Germans from their territories. Many of those had inhabited these lands for centuries, developing a unique culture. Germans were also forced to leave the former eastern territories of Germany, which were annexed by Poland (Silesia, Pomerania, parts of Brandenburg and southern part of East Prussia) and the Soviet Union (northern part of East Prussia). Between 12 and 16,5 million ethnic Germans and German citizens were expelled westwards to allied-occupied Germany. The Napoleonic Wars were the cause of the final dissolution of the Holy Roman Empire, and ultimately the cause for the quest for a German nation state in 19th-century German nationalism. After the Congress of Vienna, Austria and Prussia emerged as two competitors. Austria, trying to remain the dominant power in Central Europe, led the way in the terms of the Congress of Vienna. The Congress of Vienna was essentially conservative, assuring that little would change in Europe and preventing Germany from uniting. These terms came to a sudden halt following the Revolutions of 1848 and the Crimean War in 1856, paving the way for German unification in the 1860s. By the 1820s, large numbers of Jewish German women had intermarried with Christian German men and had converted to Christianity. Jewish German Eduard Lasker was a prominent German nationalist figure who promoted the unification of Germany in the mid-19th century. Persons who speak German as their first language, look German and whose families have lived in Germany for generations are considered ""most German"", followed by categories of diminishing Germanness such as Aussiedler (people of German ancestry whose families have lived in Eastern Europe but who have returned to Germany), Restdeutsche (people living in lands that have historically belonged to Germany but which is currently outside of Germany), Auswanderer (people whose families have emigrated from Germany and who still speak German), German speakers in German-speaking nations such as Austrians, and finally people of German emigrant background who no longer speak German. Of approximately 100 million native speakers of German in the world, roughly 80 million consider themselves Germans.[citation needed] There are an additional 80 million people of German ancestry mainly in the United States, Brazil (mainly in the South Region of the country), Argentina, Canada, South Africa, the post-Soviet states (mainly in Russia and Kazakhstan), and France, each accounting for at least 1 million.[note 2] Thus, the total number of Germans lies somewhere between 100 and more than 150 million, depending on the criteria applied (native speakers, single-ancestry ethnic Germans, partial German ancestry, etc.). After Christianization, the Roman Catholic Church and local rulers led German expansion and settlement in areas inhabited by Slavs and Balts, known as Ostsiedlung. During the wars waged in the Baltic by the Catholic German Teutonic Knights; the lands inhabited by the ethnic group of the Old Prussians (the current reference to the people known then simply as the ""Prussians""), were conquered by the Germans. The Old Prussians were an ethnic group related to the Latvian and Lithuanian Baltic peoples. The former German state of Prussia took its name from the Baltic Prussians, although it was led by Germans who had assimilated the Old Prussians; the old Prussian language was extinct by the 17th or early 18th century. The Slavic people of the Teutonic-controlled Baltic were assimilated into German culture and eventually there were many intermarriages of Slavic and German families, including amongst the Prussia's aristocracy known as the Junkers. Prussian military strategist Karl von Clausewitz is a famous German whose surname is of Slavic origin. Massive German settlement led to the assimilation of Baltic (Old Prussians) and Slavic (Wends) populations, who were exhausted by previous warfare. Pan-Germanism's origins began in the early 19th century following the Napoleonic Wars. The wars launched a new movement that was born in France itself during the French Revolution. Nationalism during the 19th century threatened the old aristocratic regimes. Many ethnic groups of Central and Eastern Europe had been divided for centuries, ruled over by the old Monarchies of the Romanovs and the Habsburgs. Germans, for the most part, had been a loose and disunited people since the Reformation when the Holy Roman Empire was shattered into a patchwork of states. The new German nationalists, mostly young reformers such as Johann Tillmann of East Prussia, sought to unite all the German-speaking and ethnic-German (Volksdeutsche) people. For decades after the Second World War, any national symbol or expression was a taboo. However, the Germans are becoming increasingly patriotic. During a study in 2009, in which some 2,000 German citizens age 14 and upwards filled out a questionnaire, nearly 60% of those surveyed agreed with the sentiment ""I'm proud to be German."" And 78%, if free to choose their nation, would opt for German nationality with ""near or absolute certainty"". Another study in 2009, carried out by the Identity Foundation in Düsseldorf, showed that 73% of the Germans were proud of their country, twice more than 8 years earlier. According to Eugen Buss, a sociology professor at the University of Hohenheim, there's an ongoing normalisation and more and more Germans are becoming openly proud of their country. By the Middle Ages, large numbers of Jews lived in the Holy Roman Empire and had assimilated into German culture, including many Jews who had previously assimilated into French culture and had spoken a mixed Judeo-French language. Upon assimilating into German culture, the Jewish German peoples incorporated major parts of the German language and elements of other European languages into a mixed language known as Yiddish. However tolerance and assimilation of Jews in German society suddenly ended during the Crusades with many Jews being forcefully expelled from Germany and Western Yiddish disappeared as a language in Germany over the centuries, with German Jewish people fully adopting the German language. As of 2008[update], Germany is the fourth largest music market in the world and has exerted a strong influence on Dance and Rock music, and pioneered trance music. Artists such as Herbert Grönemeyer, Scorpions, Rammstein, Nena, Dieter Bohlen, Tokio Hotel and Modern Talking have enjoyed international fame. German musicians and, particularly, the pioneering bands Tangerine Dream and Kraftwerk have also contributed to the development of electronic music. Germany hosts many large rock music festivals annually. The Rock am Ring festival is the largest music festival in Germany, and among the largest in the world. German artists also make up a large percentage of Industrial music acts, which is called Neue Deutsche Härte. Germany hosts some of the largest Goth scenes and festivals in the entire world, with events like Wave-Gothic-Treffen and M'era Luna Festival easily attracting up to 30,000 people. Amongst Germany's famous artists there are various Dutch entertainers, such as Johannes Heesters. German philosophers have helped shape western philosophy from as early as the Middle Ages (Albertus Magnus). Later, Leibniz (17th century) and most importantly Kant played central roles in the history of philosophy. Kantianism inspired the work of Schopenhauer and Nietzsche as well as German idealism defended by Fichte and Hegel. Engels helped develop communist theory in the second half of the 19th century while Heidegger and Gadamer pursued the tradition of German philosophy in the 20th century. A number of German intellectuals were also influential in sociology, most notably Adorno, Habermas, Horkheimer, Luhmann, Simmel, Tönnies, and Weber. The University of Berlin founded in 1810 by linguist and philosopher Wilhelm von Humboldt served as an influential model for a number of modern western universities. The event of the Protestant Reformation and the politics that ensued has been cited as the origins of German identity that arose in response to the spread of a common German language and literature. Early German national culture was developed through literary and religious figures including Martin Luther, Johann Wolfgang von Goethe and Friedrich Schiller. The concept of a German nation was developed by German philosopher Johann Gottfried Herder. The popularity of German identity arose in the aftermath of the French Revolution. Sport forms an integral part of German life, as demonstrated by the fact that 27 million Germans are members of a sports club and an additional twelve million pursue such an activity individually. Football is by far the most popular sport, and the German Football Federation (Deutscher Fußballbund) with more than 6.3 million members is the largest athletic organisation in the country. It also attracts the greatest audience, with hundreds of thousands of spectators attending Bundesliga matches and millions more watching on television. The Germanic peoples during the Migrations Period came into contact with other peoples; in the case of the populations settling in the territory of modern Germany, they encountered Celts to the south, and Balts and Slavs towards the east. The Limes Germanicus was breached in AD 260. Migrating Germanic tribes commingled with the local Gallo-Roman populations in what is now Swabia and Bavaria. The arrival of the Huns in Europe resulted in Hun conquest of large parts of Eastern Europe, the Huns initially were allies of the Roman Empire who fought against Germanic tribes, but later the Huns cooperated with the Germanic tribe of the Ostrogoths, and large numbers of Germans lived within the lands of the Hunnic Empire of Attila. Attila had both Hunnic and Germanic families and prominent Germanic chiefs amongst his close entourage in Europe. The Huns living in Germanic territories in Eastern Europe adopted an East Germanic language as their lingua franca. A major part of Attila's army were Germans, during the Huns' campaign against the Roman Empire. After Attila's unexpected death the Hunnic Empire collapsed with the Huns disappearing as a people in Europe – who either escaped into Asia, or otherwise blended in amongst Europeans. Roman Catholicism was the sole established religion in the Holy Roman Empire until the Reformation changed this drastically. In 1517, Martin Luther challenged the Catholic Church as he saw it as a corruption of Christian faith. Through this, he altered the course of European and world history and established Protestantism. The Thirty Years' War (1618–1648) was one of the most destructive conflicts in European history. The war was fought primarily in what is now Germany, and at various points involved most of the countries of Europe. The war was fought largely as a religious conflict between Protestants and Catholics in the Holy Roman Empire. The work of David Hilbert and Max Planck was crucial to the foundation of modern physics, which Werner Heisenberg and Erwin Schrödinger developed further. They were preceded by such key physicists as Hermann von Helmholtz, Joseph von Fraunhofer, and Gabriel Daniel Fahrenheit, among others. Wilhelm Conrad Röntgen discovered X-rays, an accomplishment that made him the first winner of the Nobel Prize in Physics in 1901. The Walhalla temple for ""laudable and distinguished Germans"", features a number of scientists, and is located east of Regensburg, in Bavaria. The native language of Germans is German, a West Germanic language, related to and classified alongside English and Dutch, and sharing many similarities with the North Germanic and Scandinavian languages. Spoken by approximately 100 million native speakers, German is one of the world's major languages and the most widely spoken first language in the European Union. German has been replaced by English as the dominant language of science-related Nobel Prize laureates during the second half of the 20th century. It was a lingua franca in the Holy Roman Empire. At the same time, naval innovations led to a German domination of trade in the Baltic Sea and parts of Eastern Europe through the Hanseatic League. Along the trade routes, Hanseatic trade stations became centers of the German culture. German town law (Stadtrecht) was promoted by the presence of large, relatively wealthy German populations, their influence and political power. Thus people who would be considered ""Germans"", with a common culture, language, and worldview different from that of the surrounding rural peoples, colonized trading towns as far north of present-day Germany as Bergen (in Norway), Stockholm (in Sweden), and Vyborg (now in Russia). The Hanseatic League was not exclusively German in any ethnic sense: many towns who joined the league were outside the Holy Roman Empire and a number of them may only loosely be characterized as German. The Empire itself was not entirely German either. It had a multi-ethnic and multi-lingual structure, some of the smaller ethnicities and languages used at different times were Dutch, Italian, French, Czech and Polish. In 1866, the feud between Austria and Prussia finally came to a head. There were several reasons behind this war. As German nationalism grew strongly inside the German Confederation and neither could decide on how Germany was going to be unified into a nation-state. The Austrians favoured the Greater Germany unification but were not willing to give up any of the non-German-speaking land inside of the Austrian Empire and take second place to Prussia. The Prussians however wanted to unify Germany as Little Germany primarily by the Kingdom of Prussia, whilst excluding Austria. In the final battle of the German war (Battle of Königgrätz) the Prussians successfully defeated the Austrians and succeeded in creating the North German Confederation. Conflict between the Germanic tribes and the forces of Rome under Julius Caesar forced major Germanic tribes to retreat to the east bank of the Rhine. Roman emperor Augustus in 12 BC ordered the conquest of the Germans, but the catastrophic Roman defeat at the Battle of the Teutoburg Forest resulted in the Roman Empire abandoning its plans to completely conquer Germany. Germanic peoples in Roman territory were culturally Romanized, and although much of Germany remained free of direct Roman rule, Rome deeply influenced the development of German society, especially the adoption of Christianity by the Germans who obtained it from the Romans. In Roman-held territories with Germanic populations, the Germanic and Roman peoples intermarried, and Roman, Germanic, and Christian traditions intermingled. The adoption of Christianity would later become a major influence in the development of a common German identity. German cinema dates back to the very early years of the medium with the work of Max Skladanowsky. It was particularly influential during the years of the Weimar Republic with German expressionists such as Robert Wiene and Friedrich Wilhelm Murnau. The Nazi era produced mostly propaganda films although the work of Leni Riefenstahl still introduced new aesthetics in film. From the 1960s, New German Cinema directors such as Volker Schlöndorff, Werner Herzog, Wim Wenders, Rainer Werner Fassbinder placed West-German cinema back onto the international stage with their often provocative films, while the Deutsche Film-Aktiengesellschaft controlled film production in the GDR. According to the latest nationwide census, Roman Catholics constituted 30.8% of the total population of Germany, followed by the Evangelical Protestants at 30.3%. Other religions, atheists or not specified constituted 38.8% of the population at the time. Among ""others"" are Protestants not included in Evangelical Church of Germany, and other Christians such as the Restorationist New Apostolic Church. Protestantism was more common among the citizens of Germany. The North and East Germany is predominantly Protestant, the South and West rather Catholic. Nowadays there is a non-religious majority in Hamburg and the East German states. The migration-period peoples who later coalesced into a ""German"" ethnicity were the Germanic tribes of the Saxons, Franci, Thuringii, Alamanni and Bavarii. These five tribes, sometimes with inclusion of the Frisians, are considered as the major groups to take part in the formation of the Germans. The varieties of the German language are still divided up into these groups. Linguists distinguish low Saxon, Franconian, Bavarian, Thuringian and Alemannic varieties in modern German. By the 9th century, the large tribes which lived on the territory of modern Germany had been united under the rule of the Frankish king Charlemagne, known in German as Karl der Große. Much of what is now Eastern Germany became Slavonic-speaking (Sorbs and Veleti), after these areas were vacated by Germanic tribes (Vandals, Lombards, Burgundians and Suebi amongst others) which had migrated into the former areas of the Roman Empire. By the 1860s the Kingdom of Prussia and the Austrian Empire were the two most powerful nations dominated by German-speaking elites. Both sought to expand their influence and territory. The Austrian Empire – like the Holy Roman Empire – was a multi-ethnic state, but German-speaking people there did not have an absolute numerical majority; the creation of the Austro-Hungarian Empire was one result of the growing nationalism of other ethnicities especially the Hungarians. Prussia under Otto von Bismarck would ride on the coat-tails of nationalism to unite all of modern-day Germany. The German Empire (""Second Reich"") was created in 1871 following the proclamation of Wilhelm I as head of a union of German-speaking states, while disregarding millions of its non-German subjects who desired self-determination from German rule. In the field of music, Germany claims some of the most renowned classical composers of the world including Bach, Mozart and Beethoven, who marked the transition between the Classical and Romantic eras in Western classical music. Other composers of the Austro-German tradition who achieved international fame include Brahms, Wagner, Haydn, Schubert, Händel, Schumann, Liszt, Mendelssohn Bartholdy, Johann Strauss II, Bruckner, Mahler, Telemann, Richard Strauss, Schoenberg, Orff, and most recently, Henze, Lachenmann, and Stockhausen. A German ethnicity emerged in the course of the Middle Ages, ultimately as a result of the formation of the kingdom of Germany within East Francia and later the Holy Roman Empire, beginning in the 9th century. The process was gradual and lacked any clear definition, and the use of exonyms designating ""the Germans"" develops only during the High Middle Ages. The title of rex teutonicum ""King of the Germans"" is first used in the late 11th century, by the chancery of Pope Gregory VII, to describe the future Holy Roman Emperor of the German Nation Henry IV. Natively, the term ein diutscher (""a German"") is used for the people of Germany from the 12th century. The Nazis, led by Adolf Hitler, attempted to unite all the people they claimed were ""Germans"" (Volksdeutsche) into one realm, including ethnic Germans in eastern Europe, many of whom had emigrated more than one hundred fifty years before and developed separate cultures in their new lands. This idea was initially welcomed by many ethnic Germans in Sudetenland, Austria, Poland, Danzig and western Lithuania, particularly the Germans from Klaipeda (Memel). The Swiss resisted the idea. They had viewed themselves as a distinctly separate nation since the Peace of Westphalia of 1648."
Xbox_360,"The Xbox Live Marketplace is a virtual market designed for the console that allows Xbox Live users to download purchased or promotional content. The service offers movie and game trailers, game demos, Xbox Live Arcade games and Xbox 360 Dashboard themes as well as add-on game content (items, costumes, levels etc.). These features are available to both Free and Gold members on Xbox Live. A hard drive or memory unit is required to store products purchased from Xbox Live Marketplace. In order to download priced content, users are required to purchase Microsoft Points for use as scrip; though some products (such as trailers and demos) are free to download. Microsoft Points can be obtained through prepaid cards in 1,600 and 4,000-point denominations. Microsoft Points can also be purchased through Xbox Live with a credit card in 500, 1,000, 2,000 and 5,000-point denominations. Users are able to view items available to download on the service through a PC via the Xbox Live Marketplace website. An estimated seventy percent of Xbox Live users have downloaded items from the Marketplace. Launched worldwide across 2005–2006, the Xbox 360 was initially in short supply in many regions, including North America and Europe. The earliest versions of the console suffered from a high failure rate, indicated by the so-called ""Red Ring of Death"", necessitating an extension of the device's warranty period. Microsoft released two redesigned models of the console: the Xbox 360 S in 2010, and the Xbox 360 E in 2013. As of June 2014, 84 million Xbox 360 consoles have been sold worldwide, making it the sixth-highest-selling video game console in history, and the highest-selling console made by an American company. Although not the best-selling console of its generation, the Xbox 360 was deemed by TechRadar to be the most influential through its emphasis on digital media distribution and multiplayer gaming on Xbox Live. The Xbox 360's successor, the Xbox One, was released on November 22, 2013. Microsoft has stated they plan to support the Xbox 360 until 2016. The Xbox One is also backwards compatible with the Xbox 360. On May 26, 2009, Microsoft announced the future release of the Zune HD (in the fall of 2009), the next addition to the Zune product range. This is of an impact on the Xbox Live Video Store as it was also announced that the Zune Video Marketplace and the Xbox Live Video Store will be merged to form the Zune Marketplace, which will be arriving on Xbox Live in 7 countries initially, the United Kingdom, the United States, France, Italy, Germany, Ireland and Spain. Further details were released at the Microsoft press conference at E3 2009. Kinect is a ""controller-free gaming and entertainment experience"" for the Xbox 360. It was first announced on June 1, 2009 at the Electronic Entertainment Expo, under the codename, Project Natal. The add-on peripheral enables users to control and interact with the Xbox 360 without a game controller by using gestures, spoken commands and presented objects and images. The Kinect accessory is compatible with all Xbox 360 models, connecting to new models via a custom connector, and to older ones via a USB and mains power adapter. During their CES 2010 keynote speech, Robbie Bach and Microsoft CEO Steve Ballmer went on to say that Kinect will be released during the holiday period (November–January) and it will work with every 360 console. Its name and release date of 2010-11-04 were officially announced on 2010-06-13, prior to Microsoft's press conference at E3 2010. The Xbox 360 features an online service, Xbox Live, which was expanded from its previous iteration on the original Xbox and received regular updates during the console's lifetime. Available in free and subscription-based varieties, Xbox Live allows users to: play games online; download games (through Xbox Live Arcade) and game demos; purchase and stream music, television programs, and films through the Xbox Music and Xbox Video portals; and access third-party content services through media streaming applications. In addition to online multimedia features, the Xbox 360 allows users to stream media from local PCs. Several peripherals have been released, including wireless controllers, expanded hard drive storage, and the Kinect motion sensing camera. The release of these additional services and peripherals helped the Xbox brand grow from gaming-only to encompassing all multimedia, turning it into a hub for living-room computing entertainment. Since these problems surfaced, Microsoft has attempted to modify the console to improve its reliability. Modifications include a reduction in the number, size, and placement of components, the addition of dabs of epoxy on the corners and edges of the CPU and GPU as glue to prevent movement relative to the board during heat expansion, and a second GPU heatsink to dissipate more heat. With the release of the redesigned Xbox 360 S, the warranty for the newer models does not include the three-year extended coverage for ""General Hardware Failures"". The newer Xbox 360 S model indicates system overheating when the console's power button begins to flash red, unlike previous models where the first and third quadrant of the ring would light up red around the power button if overheating occurred. The system will then warn the user of imminent system shutdown until the system has cooled, whereas a flashing power button that alternates between green and red is an indication of a ""General Hardware Failure"" unlike older models where three of the quadrants would light up red. Xbox Live Arcade is an online service operated by Microsoft that is used to distribute downloadable video games to Xbox and Xbox 360 owners. In addition to classic arcade games such as Ms. Pac-Man, the service offers some new original games like Assault Heroes. The Xbox Live Arcade also features games from other consoles, such as the PlayStation game Castlevania: Symphony of the Night and PC games such as Zuma. The service was first launched on November 3, 2004, using a DVD to load, and offered games for about US$5 to $15. Items are purchased using Microsoft Points, a proprietary currency used to reduce credit card transaction charges. On November 22, 2005, Xbox Live Arcade was re-launched with the release of the Xbox 360, in which it was now integrated with the Xbox 360's dashboard. The games are generally aimed toward more casual gamers; examples of the more popular titles are Geometry Wars, Street Fighter II' Hyper Fighting, and Uno. On March 24, 2010, Microsoft introduced the Game Room to Xbox Live. Game Room is a gaming service for Xbox 360 and Microsoft Windows that lets players compete in classic arcade and console games in a virtual arcade. The Xbox 360's original graphical user interface was the Xbox 360 Dashboard; a tabbed interface that featured five ""Blades"" (formerly four blades), and was designed by AKQA and Audiobrain. It could be launched automatically when the console booted without a disc in it, or when the disc tray was ejected, but the user had the option to select what the console does if a game is in the tray on start up, or if inserted when already on. A simplified version of it was also accessible at any time via the Xbox Guide button on the gamepad. This simplified version showed the user's gamercard, Xbox Live messages and friends list. It also allowed for personal and music settings, in addition to voice or video chats, or returning to the Xbox Dashboard from the game. The Xbox 360 launched with 14 games in North America and 13 in Europe. The console's best-selling game for 2005, Call of Duty 2, sold over a million copies. Five other games sold over a million copies in the console's first year on the market: Ghost Recon Advanced Warfighter, The Elder Scrolls IV: Oblivion, Dead or Alive 4, Saints Row, and Gears of War. Gears of War would become the best-selling game on the console with 3 million copies in 2006, before being surpassed in 2007 by Halo 3 with over 8 million copies. When the Xbox 360 was released, Microsoft's online gaming service Xbox Live was shut down for 24 hours and underwent a major upgrade, adding a basic non-subscription service called Xbox Live Silver (later renamed Xbox Live Free) to its already established premium subscription-based service (which was renamed Gold). Xbox Live Free is included with all SKUs of the console. It allows users to create a user profile, join on message boards, and access Microsoft's Xbox Live Arcade and Marketplace and talk to other members. A Live Free account does not generally support multiplayer gaming; however, some games that have rather limited online functions already, (such as Viva Piñata) or games that feature their own subscription service (e.g. EA Sports games) can be played with a Free account. Xbox Live also supports voice the latter a feature possible with the Xbox Live Vision. While the original Xbox sold poorly in Japan, selling just 2 million units while it was on the market (between 2002 and 2005),[citation needed] the Xbox 360 sold even more poorly, selling only 1.5 million units from 2005 to 2011. Edge magazine reported in August 2011 that initially lackluster and subsequently falling sales in Japan, where Microsoft had been unable to make serious inroads into the dominance of domestic rivals Sony and Nintendo, had led to retailers scaling down and in some cases discontinuing sales of the Xbox 360 completely. To aid customers with defective consoles, Microsoft extended the Xbox 360's manufacturer's warranty to three years for hardware failure problems that generate a ""General Hardware Failure"" error report. A ""General Hardware Failure"" is recognized on all models released before the Xbox 360 S by three quadrants of the ring around the power button flashing red. This error is often known as the ""Red Ring of Death"". In April 2009 the warranty was extended to also cover failures related to the E74 error code. The warranty extension is not granted for any other types of failures that do not generate these specific error codes. Music, photos and videos can be played from standard USB mass storage devices, Xbox 360 proprietary storage devices (such as memory cards or Xbox 360 hard drives), and servers or computers with Windows Media Center or Windows XP with Service pack 2 or higher within the local-area network in streaming mode. As the Xbox 360 uses a modified version of the UPnP AV protocol, some alternative UPnP servers such as uShare (part of the GeeXboX project) and MythTV can also stream media to the Xbox 360, allowing for similar functionality from non-Windows servers. This is possible with video files up to HD-resolution and with several codecs (MPEG-2, MPEG-4, WMV) and container formats (WMV, MOV, TS). At launch, the Xbox 360 was available in two configurations: the ""Xbox 360"" package (unofficially known as the 20 GB Pro or Premium), priced at US$399 or GB£279.99, and the ""Xbox 360 Core"", priced at US$299 and GB£209.99. The original shipment of the Xbox 360 version included a cut-down version of the Media Remote as a promotion. The Elite package was launched later at US$479. The ""Xbox 360 Core"" was replaced by the ""Xbox 360 Arcade"" in October 2007 and a 60 GB version of the Xbox 360 Pro was released on August 1, 2008. The Pro package was discontinued and marked down to US$249 on August 28, 2009 to be sold until stock ran out, while the Elite was also marked down in price to US$299. In May 2008 Microsoft announced that 10 million Xbox 360s had been sold and that it was the ""first current generation gaming console"" to surpass the 10 million figure in the US. In the US, the Xbox 360 was the leader in current-generation home console sales until June 2008, when it was surpassed by the Wii. The Xbox 360 has sold a total of 870,000 units in Canada as of August 1, 2008. Between January 2011 and October 2013, the Xbox 360 was the best-selling console in the United States for these 32 consecutive months. The Xbox 360's advantage over its competitors was due to the release of high profile titles from both first party and third party developers. The 2007 Game Critics Awards honored the platform with 38 nominations and 12 wins – more than any other platform. By March 2008, the Xbox 360 had reached a software attach rate of 7.5 games per console in the US; the rate was 7.0 in Europe, while its competitors were 3.8 (PS3) and 3.5 (Wii), according to Microsoft. At the 2008 Game Developers Conference, Microsoft announced that it expected over 1,000 games available for Xbox 360 by the end of the year. As well as enjoying exclusives such as additions to the Halo franchise and Gears of War, the Xbox 360 has managed to gain a simultaneous release of titles that were initially planned to be PS3 exclusives, including Devil May Cry, Ace Combat, Virtua Fighter, Grand Theft Auto IV, Final Fantasy XIII, Tekken 6, Metal Gear Solid : Rising, and L.A. Noire. In addition, Xbox 360 versions of cross-platform games were generally considered superior to their PS3 counterparts in 2006 and 2007, due in part to the difficulties of programming for the PS3. In 2009, IGN named the Xbox 360 the sixth-greatest video game console of all time, out of a field of 25. Although not the best-selling console of the seventh-generation, the Xbox 360 was deemed by TechRadar to be the most influential, by emphasizing digital media distribution and online gaming through Xbox Live, and by popularizing game achievement awards. PC Magazine considered the Xbox 360 the prototype for online gaming as it ""proved that online gaming communities could thrive in the console space"". Five years after the Xbox 360's original debut, the well-received Kinect motion capture camera was released, which set the record of being the fastest selling consumer electronic device in history, and extended the life of the console. Edge ranked Xbox 360 the second-best console of the 1993–2013 period, stating ""It had its own social network, cross-game chat, new indie games every week, and the best version of just about every multiformat game...Killzone is no Halo and nowadays Gran Turismo is no Forza, but it's not about the exclusives—there's nothing to trump Naughty Dog's PS3 output, after all. Rather, it's about the choices Microsoft made back in the original Xbox's lifetime. The PC-like architecture meant those early EA Sports titles ran at 60fps compared to only 30 on PS3, Xbox Live meant every dedicated player had an existing friends list, and Halo meant Microsoft had the killer next-generation exclusive. And when developers demo games on PC now they do it with a 360 pad—another industry benchmark, and a critical one."" Known during development as Xbox Next, Xenon, Xbox 2, Xbox FS or NextBox, the Xbox 360 was conceived in early 2003. In February 2003, planning for the Xenon software platform began, and was headed by Microsoft's Vice President J Allard. That month, Microsoft held an event for 400 developers in Bellevue, Washington to recruit support for the system. Also that month, Peter Moore, former president of Sega of America, joined Microsoft. On August 12, 2003, ATI signed on to produce the graphic processing unit for the new console, a deal which was publicly announced two days later. Before the launch of the Xbox 360, several Alpha development kits were spotted using Apple's Power Mac G5 hardware. This was because the system's PowerPC 970 processor running the same PowerPC architecture that the Xbox 360 would eventually run under IBM's Xenon processor. The cores of the Xenon processor were developed using a slightly modified version of the PlayStation 3's Cell Processor PPE architecture. According to David Shippy and Mickie Phipps, the IBM employees were ""hiding"" their work from Sony and Toshiba, IBM's partners in developing the Cell Processor. Jeff Minter created the music visualization program Neon which is included with the Xbox 360. On November 6, 2006, Microsoft announced the Xbox Video Marketplace, an exclusive video store accessible through the console. Launched in the United States on November 22, 2006, the first anniversary of the Xbox 360's launch, the service allows users in the United States to download high-definition and standard-definition television shows and movies onto an Xbox 360 console for viewing. With the exception of short clips, content is not currently available for streaming, and must be downloaded. Movies are also available for rental. They expire in 14 days after download or at the end of the first 24 hours after the movie has begun playing, whichever comes first. Television episodes can be purchased to own, and are transferable to an unlimited number of consoles. Downloaded files use 5.1 surround audio and are encoded using VC-1 for video at 720p, with a bitrate of 6.8 Mbit/s. Television content is offered from MTV, VH1, Comedy Central, Turner Broadcasting, and CBS; and movie content is Warner Bros., Paramount, and Disney, along with other publishers. Two major hardware revisions of the Xbox 360 have succeeded the original models; the Xbox 360 S (also referred to as the ""Slim"") replaced the original ""Elite"" and ""Arcade"" models in 2010. The S model carries a smaller, streamlined appearance with an angular case, and utilizes a redesigned motherboard designed to alleviate the hardware and overheating issues experienced by prior models. It also includes a proprietary port for use with the Kinect sensor. The Xbox 360 E, a further streamlined variation of the 360 S with a two-tone rectangular case inspired by Xbox One, was released in 2013. In addition to its revised aesthetics, Xbox 360 E also has one fewer USB port and no longer supports S/PDIF. Six games were initially available in Japan, while eagerly anticipated titles such as Dead or Alive 4 and Enchanted Arms were released in the weeks following the console's launch. Games targeted specifically for the region, such as Chromehounds, Ninety-Nine Nights, and Phantasy Star Universe, were also released in the console's first year. Microsoft also had the support of Japanese developer Mistwalker, founded by Final Fantasy creator Hironobu Sakaguchi. Mistwalker's first game, Blue Dragon, was released in 2006 and had a limited-edition bundle which sold out quickly with over 10,000 pre-orders. Blue Dragon is one of three Xbox 360 games to surpass 200,000 units in Japan, along with Tales of Vesperia and Star Ocean: The Last Hope. Mistwalker's second game, Lost Odyssey also sold over 100,000 copies. The Xbox 360 sold much better than its predecessor, and although not the best-selling console of the seventh-generation, it is regarded as a success since it strengthened Microsoft as a major force in the console market at the expense of well-established rivals. The inexpensive Nintendo Wii did sell the most console units but eventually saw a collapse of third-party software support in its later years, and it has been viewed by some as a fad since the succeeding Wii U had a poor debut in 2012. The PlayStation 3 struggled for a time due to being too expensive and initially lacking quality titles, making it far less dominant than its predecessor, the PlayStation 2, and it took until late in the PlayStation 3's lifespan for its sales and game titles to reach parity with the Xbox 360. TechRadar proclaimed that ""Xbox 360 passes the baton as the king of the hill – a position that puts all the more pressure on its successor, Xbox One"". Xbox Live Gold includes the same features as Free and includes integrated online game playing capabilities outside of third-party subscriptions. Microsoft has allowed previous Xbox Live subscribers to maintain their profile information, friends list, and games history when they make the transition to Xbox Live Gold. To transfer an Xbox Live account to the new system, users need to link a Windows Live ID to their gamertag on Xbox.com. When users add an Xbox Live enabled profile to their console, they are required to provide the console with their passport account information and the last four digits of their credit card number, which is used for verification purposes and billing. An Xbox Live Gold account has an annual cost of US$59.99, C$59.99, NZ$90.00, GB£39.99, or €59.99. As of January 5, 2011, Xbox Live has over 30 million subscribers. TechRadar deemed the Xbox 360 as the most influential game system through its emphasis of digital media distribution, Xbox Live online gaming service, and game achievement feature. During the console's lifetime, the Xbox brand has grown from gaming-only to encompassing all multimedia, turning it into a hub for ""living-room computing environment"". Five years after the Xbox 360's original debut, the well-received Kinect motion capture camera was released, which became the fastest selling consumer electronic device in history, and extended the life of the console. At the 2007, 2008, and 2009 Consumer Electronics Shows, Microsoft had announced that IPTV services would soon be made available to use through the Xbox 360. In 2007, Microsoft chairman Bill Gates stated that IPTV on Xbox 360 was expected to be available to consumers by the holiday season, using the Microsoft TV IPTV Edition platform. In 2008, Gates and president of Entertainment & Devices Robbie Bach announced a partnership with BT in the United Kingdom, in which the BT Vision advanced TV service, using the newer Microsoft Mediaroom IPTV platform, would be accessible via Xbox 360, planned for the middle of the year. BT Vision's DVR-based features would not be available on Xbox 360 due to limited hard drive capacity. In 2010, while announcing version 2.0 of Microsoft Mediaroom, Microsoft CEO Steve Ballmer mentioned that AT&T's U-verse IPTV service would enable Xbox 360s to be used as set-top boxes later in the year. As of January 2010, IPTV on Xbox 360 has yet to be deployed beyond limited trials. The Xbox 360 supports videos in Windows Media Video (WMV) format (including high-definition and PlaysForSure videos), as well as H.264 and MPEG-4 media. The December 2007 dashboard update added support for the playback of MPEG-4 ASP format videos. The console can also display pictures and perform slideshows of photo collections with various transition effects, and supports audio playback, with music player controls accessible through the Xbox 360 Guide button. Users may play back their own music while playing games or using the dashboard, and can play music with an interactive visual synthesizer."
Chicago_Cubs,"The shift in the Cubs' fortunes was characterized June 23 on the ""NBC Saturday Game of the Week"" contest against the St. Louis Cardinals. it has since been dubbed simply ""The Sandberg Game."" With the nation watching and Wrigley Field packed, Sandberg emerged as a superstar with not one, but two game-tying home runs against Cardinals closer Bruce Sutter. With his shots in the 9th and 10th innings Wrigley Field erupted and Sandberg set the stage for a comeback win that cemented the Cubs as the team to beat in the East. No one would catch them, except the Padres in the playoffs. Jack Brickhouse manned the Cubs radio and especially the TV booth for parts of five decades, the 34-season span from 1948 to 1981. He covered the games with a level of enthusiasm that often seemed unjustified by the team's poor performance on the field for many of those years. His trademark call ""Hey Hey!"" always followed a home run. That expression is spelled out in large letters vertically on both foul pole screens at Wrigley Field. ""Whoo-boy!"" and ""Wheeee!"" and ""Oh, brother!"" were among his other pet expressions. When he approached retirement age, he personally recommended his successor. After finishing last in the NL Central with 66 wins in 2006, the Cubs re-tooled and went from ""worst to first"" in 2007. In the offseason they signed Alfonso Soriano to a contract at 8 years for $136 million, and replaced manager Dusty Baker with fiery veteran manager Lou Piniella. After a rough start, which included a brawl between Michael Barrett and Carlos Zambrano, the Cubs overcame the Milwaukee Brewers, who had led the division for most of the season, with winning streaks in June and July, coupled with a pair of dramatic, late-inning wins against the Reds, and ultimately clinched the NL Central with a record of 85–77. The Cubs traded Barrett to the Padres, and later acquired Jason Kendall from Oakland. Kendall was highly successful with his management of the pitching rotation and helped at the plate as well. By September, Geovany Soto became the full-time starter behind the plate, replacing the veteran Kendall. They met Arizona in the NLDS, but controversy followed as Piniella, in a move that has since come under scrutiny, pulled Carlos Zambrano after the sixth inning of a pitcher's duel with D-Backs ace Brandon Webb, to ""....save Zambrano for (a potential) Game 4."" The Cubs, however, were unable to come through, losing the first game and eventually stranding over 30 baserunners in a 3-game Arizona sweep. Rookie Starlin Castro debuted in early May (2010) as the starting shortstop. However, the club played poorly in the early season, finding themselves 10 games under .500 at the end of June. In addition, long-time ace Carlos Zambrano was pulled from a game against the White Sox on June 25 after a tirade and shoving match with Derrek Lee, and was suspended indefinitely by Jim Hendry, who called the conduct ""unacceptable."" On August 22, Lou Piniella, who had already announced his retirement at the end of the season, announced that he would leave the Cubs prematurely to take care of his sick mother. Mike Quade took over as the interim manager for the final 37 games of the year. Despite being well out of playoff contention the Cubs went 24–13 under Quade, the best record in baseball during that 37 game stretch, earning Quade to have the interim tag removed on October 19. The curious location on Catalina Island stemmed from Cubs owner William Wrigley Jr.'s then-majority interest in the island in 1919. Wrigley constructed a ballpark on the island to house the Cubs in spring training: it was built to the same dimensions as Wrigley Field. (The ballpark is long gone, but a clubhouse built by Wrigley to house the Cubs exists as the Catalina County Club.) However, by 1951 the team chose to leave Catalina Island and spring training was shifted to Mesa, Arizona. The Cubs' 30-year association with Catalina is chronicled in the book, The Cubs on Catalina, by Jim Vitti . . . which was named International 'Book of the Year' by The Sporting News. In the NLCS, the Cubs easily won the first two games at Wrigley Field against the San Diego Padres. The Padres were the winners of the Western Division with Steve Garvey, Tony Gwynn, Eric Show, Goose Gossage and Alan Wiggins. With wins of 13–0 and 4–2, the Cubs needed to win only one game of the next three in San Diego to make it to the World Series. After being beaten in Game 3 7–1, the Cubs lost Game 4 when Smith, with the game tied 5–5, allowed a game-winning home run to Garvey in the bottom of the ninth inning. In Game 5 the Cubs took a 3–0 lead into the 6th inning, and a 3–2 lead into the seventh with Sutcliffe (who won the Cy Young Award that year) still on the mound. Then, Leon Durham had a sharp grounder go under his glove. This critical error helped the Padres win the game 6–3, with a 4-run 7th inning and keep Chicago out of the 1984 World Series against the Detroit Tigers. The loss ended a spectacular season for the Cubs, one that brought alive a slumbering franchise and made the Cubs relevant for a whole new generation of Cubs fans. The Cubs enjoyed one more pennant at the close of World War II, finishing 98–56. Due to the wartime travel restrictions, the first three games of the 1945 World Series were played in Detroit, where the Cubs won two games, including a one-hitter by Claude Passeau, and the final four were played at Wrigley. In Game 4 of the Series, the Curse of the Billy Goat was allegedly laid upon the Cubs when P.K. Wrigley ejected Billy Sianis, who had come to Game 4 with two box seat tickets, one for him and one for his goat. They paraded around for a few innings, but Wrigley demanded the goat leave the park due to its unpleasant odor. Upon his ejection, Mr. Sianis uttered, ""The Cubs, they ain't gonna win no more."" The Cubs lost Game 4, lost the Series, and have not been back since. It has also been said by many that Sianis put a ""curse"" on the Cubs, apparently preventing the team from playing in the World Series. After losing the 1945 World Series to the Detroit Tigers, the Cubs finished with winning seasons the next two years, but those teams did not enter post-season play. The team's commitment to contend was complete when Green made a midseason deal on June 15 to shore up the starting rotation due to injuries to Rick Reuschel (5–5) and Sanderson. The deal brought 1979 NL Rookie of the Year pitcher Rick Sutcliffe from the Cleveland Indians. Joe Carter (who was with the Triple-A Iowa Cubs at the time) and center fielder Mel Hall were sent to Cleveland for Sutcliffe and back-up catcher Ron Hassey (.333 with Cubs in 1984). Sutcliffe (5–5 with the Indians) immediately joined Sanderson (8–5 3.14), Eckersley (10–8 3.03), Steve Trout (13–7 3.41) and Dick Ruthven (6–10 5.04) in the starting rotation. Sutcliffe proceeded to go 16–1 for Cubs and capture the Cy Young Award. In 2013, Tom Ricketts and team president Crane Kenney unveiled plans for a five-year, $575 million privately funded renovation of Wrigley Field. Called the 1060 Project, the proposed plans included vast improvements to the stadium's facade, infrastructure, restrooms, concourses, suites, press box, bullpens, and clubhouses, as well as a 6,000-square foot jumbotron to be added in the left field bleachers, batting tunnels, a 3,000-square-foot video board in right field, and, eventually, an adjacent hotel, plaza, and office-retail complex. In previously years mostly all efforts to conduct any large-scale renovations to the field had been opposed by the city, former mayor Richard M. Daley (a staunch White Sox fan), and especially the rooftop owners. Harry Caray's stamp on the team is perhaps even deeper than that of Brickhouse, although his 17-year tenure, from 1982 to 1997, was half as long. First, Caray had already become a well-known Chicago figure by broadcasting White Sox games for a decade, after having been a St Louis Cardinals icon for 25 years. Caray also had the benefit of being in the booth during the NL East title run in 1984, which was widely seen due to WGN's status as a cable-TV superstation. His trademark call of ""Holy Cow!"" and his enthusiastic singing of ""Take me out to the ballgame"" during the 7th inning stretch (as he had done with the White Sox) made Caray a fan favorite both locally and nationally. In 1989, the first full season with night baseball at Wrigley Field, Don Zimmer's Cubs were led by a core group of veterans in Ryne Sandberg, Rick Sutcliffe and Andre Dawson, who were boosted by a crop of youngsters such as Mark Grace, Shawon Dunston, Greg Maddux, Rookie of the Year Jerome Walton, and Rookie of the Year Runner-Up Dwight Smith. The Cubs won the NL East once again that season winning 93 games. This time the Cubs met the San Francisco Giants in the NLCS. After splitting the first two games at home, the Cubs headed to the Bay Area, where despite holding a lead at some point in each of the next three games, bullpen meltdowns and managerial blunders ultimately led to three straight losses. The Cubs couldn't overcome the efforts of Will Clark, whose home run off Maddux, just after a managerial visit to the mound, led Maddux to think Clark knew what pitch was coming. Afterward, Maddux would speak into his glove during any mound conversation, beginning what is a norm today. Mark Grace was 11–17 in the series with 8 RBI. Eventually, the Giants lost to the ""Bash Brothers"" and the Oakland A's in the famous ""Earthquake Series."" The official Cubs team mascot is a young bear cub, named Clark, described by the team's press release as a young and friendly Cub. Clark made his debut at Advocate Health Care on January 13, 2014, the same day as the press release announcing his installation as the club's first ever official physical mascot. The bear cub itself was used in the clubs since the early 1900s and was the inspiration of the Chicago Staleys changing their team's name to the Chicago Bears, due to the Cubs allowing the football team to play at Wrigley Field in the 1930s. An album entitled Take Me Out to a Cubs Game was released in 2008. It is a collection of 17 songs and other recordings related to the team, including Harry Caray's final performance of ""Take Me Out to the Ball Game"" on September 21, 1997, the Steve Goodman song mentioned above, and a newly recorded rendition of ""Talkin' Baseball"" (subtitled ""Baseball and the Cubs"") by Terry Cashman. The album was produced in celebration of the 100th anniversary of the Cubs' 1908 World Series victory and contains sounds and songs of the Cubs and Wrigley Field. In 1984, each league had two divisions, East and West. The divisional winners met in a best-of-5 series to advance to the World Series, in a ""2–3"" format, first two games were played at the home of the team who did not have home field advantage. Then the last three games were played at the home of the team, with home field advantage. Thus the first two games were played at Wrigley Field and the next three at the home of their opponents, San Diego. A common and unfounded myth is that since Wrigley Field did not have lights at that time the National League decided to give the home field advantage to the winner of the NL West. In fact, home field advantage had rotated between the winners of the East and West since 1969 when the league expanded. In even numbered years, the NL West had home field advantage. In odd numbered years, the NL East had home field advantage. Since the NL East winners had had home field advantage in 1983, the NL West winners were entitled to it. In 1902, Spalding, who by this time had revamped the roster to boast what would soon be one of the best teams of the early century, sold the club to Jim Hart. The franchise was nicknamed the Cubs by the Chicago Daily News in 1902, although not officially becoming the Chicago Cubs until the 1907 season. During this period, which has become known as baseball's dead-ball era, Cub infielders Joe Tinker, Johnny Evers, and Frank Chance were made famous as a double-play combination by Franklin P. Adams' poem Baseball's Sad Lexicon. The poem first appeared in the July 18, 1910 edition of the New York Evening Mail. Mordecai ""Three-Finger"" Brown, Jack Taylor, Ed Reulbach, Jack Pfiester, and Orval Overall were several key pitchers for the Cubs during this time period. With Chance acting as player-manager from 1905 to 1912, the Cubs won four pennants and two World Series titles over a five-year span. Although they fell to the ""Hitless Wonders"" White Sox in the 1906 World Series, the Cubs recorded a record 116 victories and the best winning percentage (.763) in Major League history. With mostly the same roster, Chicago won back-to-back World Series championships in 1907 and 1908, becoming the first Major League club to play three times in the Fall Classic and the first to win it twice. However, the Cubs have not won a World Series since; this remains the longest championship drought in North American professional sports. After losing an extra-inning game in Game 1, the Cubs rallied and took a 3 games to 1 lead over the Wild Card Florida Marlins in the NLCS. Florida shut the Cubs out in Game 5, but young pitcher Mark Prior led the Cubs in Game 6 as they took a 3–0 lead into the 8th inning and it was at this point when a now-infamous incident took place. Several spectators attempted to catch a foul ball off the bat of Luis Castillo. A Chicago Cubs fan by the name of Steve Bartman, of Northbrook, Illinois, reached for the ball and deflected it away from the glove of Moisés Alou for the second out of the 8th inning. Alou reacted angrily toward the stands, and after the game stated that he would have caught the ball. Alou at one point recanted, saying he would not have been able to make the play, but later said this was just an attempt to make Bartman feel better and believing the whole incident should be forgotten. Interference was not called on the play, as the ball was ruled to be on the spectator side of the wall. Castillo was eventually walked by Prior. Two batters later, and to the chagrin of the packed stadium, Cubs shortstop Alex Gonzalez misplayed an inning ending double play, loading the bases and leading to eight Florida runs and a Marlin victory. Despite sending Kerry Wood to the mound and holding a lead twice, the Cubs ultimately dropped Game 7, and failed to reach the World Series. In the following two decades after Sianis' ill will, the Cubs played mostly forgettable baseball, finishing among the worst teams in the National League on an almost annual basis. Longtime infielder/manager Phil Cavarretta, who had been a key player during the '45 season, was fired during spring training in 1954 after admitting the team was unlikely to finish above fifth place. Although shortstop Ernie Banks would become one of the star players in the league during the next decade, finding help for him proved a difficult task, as quality players such as Hank Sauer were few and far between. This, combined with poor ownership decisions such as the College of Coaches, and the ill-fated trade of future Hall of Famer Lou Brock to the Cardinals for pitcher Ernie Broglio (who won only 7 games over the next three seasons), hampered on-field performance. Despite losing fan favorite Grace to free agency, and the lack of production from newcomer Todd Hundley, skipper Don Baylor's Cubs put together a good season in 2001. The season started with Mack Newton being brought in to preach ""positive thinking."" One of the biggest stories of the season transpired as the club made a midseason deal for Fred McGriff, which was drawn out for nearly a month as McGriff debated waiving his no-trade clause, as the Cubs led the wild card race by 2.5 games in early September. That run died when Preston Wilson hit a three run walk off homer off of closer Tom ""Flash"" Gordon, which halted the team's momentum. The team was unable to make another serious charge, and finished at 88–74, five games behind both Houston and St. Louis, who tied for first. Sosa had perhaps his finest season and Jon Lieber led the staff with a 20 win season. The Ricketts family acquired a majority interest in the Cubs in 2009, ending the Tribune years. Apparently handcuffed by the Tribune's bankruptcy and the sale of the club to the Ricketts family, the Cubs' quest for a NL Central 3-peat started with notice that there would be less invested into contracts than in previous years. Chicago engaged St. Louis in a see-saw battle for first place into August 2009, but the Cardinals played to a torrid 20–6 pace that month, designating their rivals to battle in the Wild Card race, from which they were eliminated in the season's final week. The Cubs were plagued by injuries in 2009, and were only able to field their Opening Day starting lineup three times the entire season. Third baseman Aramis Ramírez injured his throwing shoulder in an early May game against the Milwaukee Brewers, sidelining him until early July and forcing journeyman players like Mike Fontenot and Aaron Miles into more prominent roles. Additionally, key players like Derrek Lee (who still managed to hit .306 with 35 HR and 111 RBI that season), Alfonso Soriano and Geovany Soto also nursed nagging injuries. The Cubs posted a winning record (83–78) for the third consecutive season, the first time the club had done so since 1972, and a new era of ownership under the Ricketts' family was approved by MLB owners in early October. On November 2, 2014, the Cubs announced that Joe Maddon had signed a five-year contract to be the 54th manager in team history. On December 10, 2014, Maddon announced that the team had signed free agent Jon Lester to a 6-year, $155 million contract. Many other trades and acquisitions occurred during the off season. The opening day lineup for the Cubs contained five new players including rookie right fielder Jorge Soler. Rookies Kris Bryant and Addison Russell were in the starting lineup by mid-April, and rookie Kyle Schwarber was added in mid-June. The Cubs finished the 2015 season with a record of 97–65, third best in the majors. On October 7, in the 2015 National League Wild Card Game, Jake Arrieta pitched a complete game shutout and the Cubs defeated the Pittsburgh Pirates 4–0. On April 25, 1976, at Dodger Stadium, father-and-son protestors ran into the outfield and tried to set fire to a U.S. flag. When Cubs outfielder Rick Monday noticed the flag on the ground and the man and boy fumbling with matches and lighter fluid, he dashed over and snatched the flag to thunderous applause. When he came up to bat in the next half-inning, he got a standing ovation from the crowd and the stadium titantron flashed the message, ""RICK MONDAY... YOU MADE A GREAT PLAY..."" Monday later said, ""If you're going to burn the flag, don't do it around me. I've been to too many veterans' hospitals and seen too many broken bodies of guys who tried to protect it."" The Cubs had no official physical mascot prior to Clark, though a man in a 'polar bear' looking outfit, called ""The Bear-man"" (or Beeman), which was mildly popular with the fans, paraded the stands briefly in the early 1990s. There is no record of whether or not he was just a fan in a costume or employed by the club. Through the 2013 season, there were ""Cubbie-bear"" mascots outside of Wrigley on game day, but none are employed by the team. They pose for pictures with fans for tips. The most notable of these was ""Billy Cub"" who worked outside of the stadium until for over 6 years until July 2013, when the club asked him to stop. Billy Cub, who is played by fan John Paul Weier, had unsuccessfully petitioned the team to become the official mascot. Another unofficial but much more well-known mascot is Ronnie ""Woo Woo"" Wickers who is a longtime fan and local celebrity in the Chicago area. He is known to Wrigley Field visitors for his idiosyncratic cheers at baseball games, generally punctuated with an exclamatory ""Woo!"" (e.g., ""Cubs, woo! Cubs, woo! Big-Z, woo! Zambrano, woo! Cubs, woo!"") Longtime Cubs announcer Harry Caray dubbed Wickers ""Leather Lungs"" for his ability to shout for hours at a time. He is not employed by the team, although the club has on two separate occasions allowed him into the broadcast booth and allow him some degree of freedom once he purchases or is given a ticket by fans to get into the games. He is largely allowed to roam the park and interact with fans by Wrigley Field security. The 2013 season resulted in much as the same the year before. Shortly before the trade deadline, the Cubs traded Matt Garza to the Texas Rangers for Mike Olt, C. J. Edwards, Neil Ramirez, and Justin Grimm. Three days later, the Cubs sent Alfonso Soriano to the New York Yankees for minor leaguer Corey Black. The mid season fire sale led to another last place finish in the NL Central, finishing with a record of 66-96. Although there was a five-game improvement in the record from the year before, Anthony Rizzo and Starlin Castro seemed to take steps backward in their development. On September 30, 2013, Theo Epstein made the decision to fire manager Dale Sveum after just two seasons at the helm of the Cubs. The regression of several young players was thought to be the main focus point, as the front office said Dale would not be judged based on wins and losses. In two seasons as skipper, Sveum finished with a record of 127-197. On September 23, 1908, the Cubs and New York Giants were involved in a tight pennant race. The two clubs were tied in the bottom of the ninth inning at the Polo Grounds, and N.Y. had runners on first and third and two outs when Al Bridwell singled, scoring Moose McCormick from third with the Giants' apparent winning run, but the runner on first base, rookie Fred Merkle, left the field without touching second base. As fans swarmed the field, Cub infielder Johnny Evers retrieved the ball and touched second. Since there were two outs, a forceout was called at second base, ending the inning and the game. Because of the tie the Giants and Cubs ended up tied for first place. The Giants lost the ensuing one-game playoff and the Cubs went on to the World Series. Despite the fact that the Cubs had won 89 games, this fallout was decidedly unlovable, as the Cubs traded superstar Sammy Sosa after he had left the season's final game early and then lied about it publicly. Already a controversial figure in the clubhouse after his corked-bat incident, Sammy's actions alienated much of his once strong fan base as well as the few teammates still on good terms with him, (many teammates grew tired of Sosa playing loud salsa music in the locker room) and possibly tarnished his place in Cubs' lore for years to come. The disappointing season also saw fans start to become frustrated with the constant injuries to ace pitchers Mark Prior and Kerry Wood. Additionally, the '04 season led to the departure of popular commentator Steve Stone, who had become increasingly critical of management during broadcasts and was verbally attacked by reliever Kent Mercker. Things were no better in 2005, despite a career year from first baseman Derrek Lee and the emergence of closer Ryan Dempster. The club struggled and suffered more key injuries, only managing to win 79 games after being picked by many to be a serious contender for the N.L. pennant. In 2006, bottom fell out as the Cubs finished 66–96, last in the NL Central. In 1969 the Cubs, managed by Leo Durocher, built a substantial lead in the newly created National League Eastern Division by mid-August. Ken Holtzman pitched a no-hitter on August 19, and the division lead grew to 8 1⁄2 games over the St. Louis Cardinals and by 9 1⁄2 games over the New York Mets. After the game of September 2, the Cubs record was 84-52 with the Mets in second place at 77-55. But then a losing streak began just as a Mets winning streak was beginning. The Cubs lost the final game of a series at Cincinnati, then came home to play the resurgent Pittsburgh Pirates (who would finish in third place). After losing the first two games by scores of 9-2 and 13-4, the Cubs led going into the ninth inning. A win would be a positive springboard since the Cubs were to play a crucial series with the Mets the very next day. But Willie Stargell drilled a 2-out, 2-strike pitch from the Cubs' ace reliever, Phil Regan, onto Sheffield Avenue to tie the score in the top of the ninth. The Cubs would lose 7-5 in extra innings. Burdened by a four-game losing streak, the Cubs traveled to Shea Stadium for a short two-game set. The Mets won both games, and the Cubs left New York with a record of 84-58 just 1⁄2 game in front. Disaster followed in Philadelphia, as a 99 loss Phillies team nonetheless defeated the Cubs twice, to extend Chicago's losing streak to eight games. In a key play in the second game, on September 11, Cubs starter Dick Selma threw a surprise pickoff attempt to third baseman Ron Santo, who was nowhere near the bag or the ball. Selma's throwing error opened the gates to a Phillies rally. After that second Philly loss, the Cubs were 84-60 and the Mets had pulled ahead at 85-57. The Mets would not look back. The Cubs' eight-game losing streak finally ended the next day in St. Louis, but the Mets were in the midst of a ten-game winning streak, and the Cubs, wilting from team fatigue, generally deteriorated in all phases of the game. The Mets (who had lost a record 120 games 7 years earlier), would go on to win the World Series. The Cubs, despite a respectable 92-70 record, would be remembered for having lost a remarkable 17½ games in the standings to the Mets in the last quarter of the season. After back-to-back pennants in 1880 and 1881, Hulbert died, and Spalding, who had retired to start Spalding sporting goods, assumed ownership of the club. The White Stockings, with Anson acting as player/manager, captured their third consecutive pennant in 1882, and Anson established himself as the game's first true superstar. In 1885 and '86, after winning N.L. pennants, the White Stockings met the short-lived American Association champion in that era's version of a World Series. Both seasons resulted in match ups with the St. Louis Brown Stockings, with the clubs tying in 1885 and with St. Louis winning in 1886. This was the genesis of what would eventually become one of the greatest rivalries in sports. In all, the Anson-led Chicago Base Ball Club won six National League pennants between 1876 and 1886. As a result, Chicago's club nickname transitioned, and by 1890 they had become known as the Chicago Colts, or sometimes ""Anson's Colts"", referring to Cap's influence within the club. Anson was the first player in history credited with collecting 3,000 career hits. After a disappointing record of 59-73 and a 9th-place finish in 1897, Anson was released by the Cubs as both a player and manager. Due to Anson's absence from the club after 22 years, local newspaper reporters started to refer to the Cubs as the ""Orphans"". In 1906, the franchise recorded a Major League record 116 wins (tied by the 2001 Seattle Mariners) and posted a modern-era record winning percentage of .763, which still stands today. They appeared in their first World Series the same year, falling to their crosstown rivals, the Chicago White Sox, four games to two. The Cubs won back-to-back World Series championships in 1907 and 1908, becoming the first Major League team to play in three consecutive Fall Classics, and the first to win it twice. The team has appeared in seven World Series following their 1908 title, most recently in 1945. The Cubs have not won the World Series in 107 years, the longest championship drought of any major North American professional sports team, and are often referred to as the ""Lovable Losers"" because of this distinction. They are also known as ""The North Siders"" because Wrigley Field, their home park since 1916, is located in Chicago's North Side Lake View community at 1060 West Addison Street. The Cubs have a major rivalry with the St. Louis Cardinals. The Chicago Cubs have not won a World Series championship since 1908, and have not appeared in the Fall Classic since 1945, although between their postseason appearance in 1984 and their most recent in 2015, they have made the postseason seven times. 107 seasons is the longest championship drought in all four of the major North American professional sports leagues, which also includes the National Football League (NFL), the National Basketball Association (NBA), and the National Hockey League (NHL). In fact, the Cubs' last World Series title occurred before those other three leagues even existed, and even the Cubs' last World Series appearance predates the founding of the NBA. The much publicized drought was concurrent to championship droughts by the Boston Red Sox and the Chicago White Sox, who both had over 80 years between championships. It is this unfortunate distinction that has led to the club often being known as ""The Lovable Losers."" The team was one win away from breaking what is often called the ""Curse of the Billy Goat"" in 1984 and 2003 (Steve Bartman incident), but was unable get the victory that would send it to the World Series. On May 11, 2000, Glenallen Hill, facing Brewers starter Steve Woodard, became the first, and thus far only player, to hit a pitched ball onto the roof of a five-story residential building across Waveland Ave, beyond Wrigley Field's left field wall. The shot was estimated at well over 500 feet (150 m), but the Cubs fell to Milwaukee 12–8. No batted ball has ever hit the center field scoreboard, although the original ""Slammin' Sammy"", golfer Sam Snead, hit it with a golf ball in an exhibition in the 1950s. In 1948, Bill Nicholson barely missed the scoreboard when he launched a home run ball onto Sheffield Avenue and in 1959, Roberto Clemente came even closer with a home run ball hit onto Waveland Avenue. In 2001, a Sammy Sosa shot landed across Waveland and bounced a block down Kenmore Avenue. Dave Kingman hit a shot in 1979 that hit the third porch roof on the east side of Kenmore, estimated at 555 feet (169 m), and is regarded as the longest home run in Wrigley Field history. On May 26, 2015, the Cubs rookie third baseman, Kris Bryant, hit a homerun that traveled an estimated 477 feet (145 m) off the park's new videoboard in left field. Later the same year, he hit a homer that traveled 495 feet (151 m) that also ricocheted off of the videoboard On October 13, 2015, Kyle Schwarber's 438-foot home run landed on the equally new right field videoboard. The '98 season would begin on a somber note with the death of legendary broadcaster Harry Caray. After the retirement of Sandberg and the trade of Dunston, the Cubs had holes to fill and the signing of Henry Rodríguez, known affectionately as ""H-Rod"" to bat cleanup provided protection for Sammy Sosa in the lineup, as Rodriguez slugged 31 round-trippers in his first season in Chicago. Kevin Tapani led the club with a career high 19 wins, Rod Beck anchored a strong bullpen and Mark Grace turned in one of his best seasons. The Cubs were swamped by media attention in 1998, and the team's two biggest headliners were Sosa and rookie flamethrower Kerry Wood. Wood's signature performance was one-hitting the Houston Astros, a game in which he tied the major league record of 20 strikeouts in nine innings. His torrid strikeout numbers earned Wood the nickname ""Kid K,"" and ultimately earned him the 1998 NL Rookie of the Year award. Sosa caught fire in June, hitting a major league record 20 home runs in the month, and his home run race with Cardinals slugger Mark McGwire transformed the pair into international superstars in a matter of weeks. McGwire finished the season with a new major league record of 70 home runs, but Sosa's .308 average and 66 homers earned him the National League MVP Award. After a down-to-the-wire Wild Card chase with the San Francisco Giants, Chicago and San Francisco ended the regular season tied, and thus squared off in a one-game playoff at Wrigley Field in which third baseman Gary Gaetti hit the eventual game winning homer. The win propelled the Cubs into the postseason once again with a 90–73 regular season tally. Unfortunately, the bats went cold in October, as manager Jim Riggleman's club batted .183 and scored only four runs en route to being swept by Atlanta. On a positive note, the home run chase between Sosa, McGwire and Ken Griffey, Jr. helped professional baseball to bring in a new crop of fans as well as bringing back some fans who had been disillusioned by the 1994 strike. The Cubs retained many players who experienced career years in '98, and after a fast start in 1999, they collapsed again (starting with being swept at the hands of the cross-town White Sox in mid-June) and finished in the bottom of the division for the next two seasons. In 1981, after 6 decades under the Wrigley family, the Cubs were purchased by Tribune Company for $20,500,000. Tribune, owners of the Chicago Tribune, Los Angeles Times, WGN Television, WGN Radio and many other media outlets, controlled the club until December 2007, when Sam Zell completed his purchase of the entire Tribune organization and announced his intention to sell the baseball team. After a nearly two-year process which involved potential buyers such as Mark Cuban and a group led by Hank Aaron, a family trust of TD Ameritrade founder Joe Ricketts won the bidding process as the 2009 season came to a close. Ultimately, the sale was unanimously approved by MLB owners and the Ricketts family took control on October 27, 2009. The Cubs began play as the Chicago White Stockings, joining the National League (NL) as a charter member. Owner William Hulbert signed multiple star players, such as pitcher Albert Spalding and infielders Ross Barnes, Deacon White, and Adrian ""Cap"" Anson, to join the team prior to the N.L.'s first season. The White Stockings played their home games at West Side Grounds,against the bloods and quickly established themselves as one of the new league's top teams. Spalding won forty-seven games and Barnes led the league in hitting at .429 as Chicago won the first ever National League pennant, which at the time was the game's top prize. The Chicago Cubs are an American professional baseball team located on the North Side of Chicago, Illinois. The Cubs compete in Major League Baseball (MLB) as a members of the National League (NL) Central division; the team plays its home baseball games at Wrigley Field. The Cubs are also one of two active major league teams based in Chicago; the other is the Chicago White Sox, who are a member of the American League (AL) Central division. The team is currently owned by Thomas S. Ricketts, son of TD Ameritrade founder Joe Ricketts. The Chicago White Stockings, (today's Chicago Cubs), began spring training in Hot Springs, Arkansas in 1886. President Albert Spalding (founder of Spalding Sporting Goods) and player/manager Cap Anson brought their players to Hot Springs and played at the Hot Springs Baseball Grounds. The concept was for the players to have training and fitness before the start of the regular season. After the White Stockings had a successful season in 1886, winning the National League Pennant, other teams began bringing their players to ""spring training"".  The Chicago Cubs, St. Louis Browns, New York Yankees, St. Louis Cardinals, Cleveland Spiders, Detroit Tigers, Pittsburgh Pirates, Cincinnati Reds, New York Highlanders, Brooklyn Dodgers and Boston Red Sox were among the early squads to arrive. Whittington Park (1894) and later Majestic Park (1909) and Fogel Field (1912) were all built in Hot Springs specifically to host Major League teams.  The Cubs' current spring training facility is located in Sloan Park in |Mesa, Arizona, where they play in the Cactus League. The park seats 15,000, making it Major League baseball's largest spring training facility by capacity. The Cubs annually sell out most of their games both at home and on the road. Before Sloan Park opened in 2014, the team played games at HoHoKam Park - Dwight Patterson Field from 1979. ""HoHoKam"" is literally translated from Native American as ""those who vanished."" The North Siders have called Mesa their spring home for most seasons since 1952. Located in Chicago's Lake View neighborhood, Wrigley Field sits on an irregular block bounded by Clark and Addison Streets and Waveland and Sheffield Avenues. The area surrounding the ballpark is typically referred to as Wrigleyville. There is a dense collection of sports bars and restaurants in the area, most with baseball inspired themes, including Sluggers, Murphy's Bleachers and The Cubby Bear. Many of the apartment buildings surrounding Wrigley Field on Waveland and Sheffield Avenues have built bleachers on their rooftops for fans to view games and other sell space for advertisement. One building on Sheffield Avenue has a sign atop its roof which says ""Eamus Catuli!"" which is Latin for ""Let's Go Cubs!"" and another chronicles the time since the last Division title, pennant, and World Series championship. The 02 denotes two years since the 2008 NL Central title, 65 years since the 1945 pennant and 102 years since the 1908 World Series championship. On game days, many residents rent out their yards and driveways to people looking for parking spots. The uniqueness of the neighborhood itself has ingrained itself into the culture of the Chicago Cubs as well as the Wrigleyville neighborhood, and has led to being used for concerts and other sporting events, such as the 2010 NHL Winter Classic between the Chicago Blackhawks and Detroit Red Wings, as well as a 2010 NCAA men's football game between the Northwestern Wildcats and Illinois Fighting Illini. During the summer of 1969, a Chicago studio group produced a single record called ""Hey Hey! Holy Mackerel! (The Cubs Song)"" whose title and lyrics incorporated the catch-phrases of the respective TV and radio announcers for the Cubs, Jack Brickhouse and Vince Lloyd. Several members of the Cubs recorded an album called Cub Power which contained a cover of the song. The song received a good deal of local airplay that summer, associating it very strongly with that bittersweet season. It was played much less frequently thereafter, although it remained an unofficial Cubs theme song for some years after. The team played its first games in 1876 as a founding member of the National League (NL), eventually becoming known officially as the Chicago Cubs for the 1903 season. Officially, the Cubs are tied for the distinction of being the oldest currently active U.S. professional sports club, along with the Atlanta Braves, which also began play in the NL in 1876 as the Boston Red Stockings (Major League Baseball does not officially recognize the National Association of Professional Base Ball Players as a major league.) The Cubs had high expectations in 2002, but the squad played poorly. On July 5, 2002 the Cubs promoted assistant general manager and player personnel director Jim Hendry to the General Manager position. The club responded by hiring Dusty Baker and by making some major moves in '03. Most notably, they traded with the Pittsburgh Pirates for outfielder Kenny Lofton and third baseman Aramis Ramírez, and rode dominant pitching, led by Kerry Wood and Mark Prior, as the Cubs led the division down the stretch. Caray had lively discussions with commentator Steve Stone, who was hand-picked by Harry himself, and producer Arne Harris. Caray often playfully quarreled with Stone over Stone's cigar and why Stone was single, while Stone would counter with poking fun at Harry being ""under the influence."" Stone disclosed in his book ""Where's Harry"" that most of this ""arguing"" was staged, and usually a ploy developed by Harry himself to add flavor to the broadcast. The Cubs still have a ""guest conductor"", usually a celebrity, lead the crowd in singing ""Take me out to the ballgame"" during the 7th inning stretch to honor Caray's memory. In 2004, the Cubs were a consensus pick by most media outlets to win the World Series. The offseason acquisition of Derek Lee (who was acquired in a trade with Florida for Hee-seop Choi) and the return of Greg Maddux only bolstered these expectation. Despite a mid-season deal for Nomar Garciaparra, misfortune struck the Cubs again. They led the Wild Card by 1.5 games over San Francisco and Houston on September 25, and both of those teams lost that day, giving the Cubs a chance at increasing the lead to a commanding 2.5 games with only eight games remaining in the season, but reliever LaTroy Hawkins blew a save to the Mets, and the Cubs lost the game in extra innings, a defeat that seemingly deflated the team, as they proceeded to drop 6 of their last 8 games as the Astros won the Wild Card. On April 23, 2008, against the Colorado Rockies, the Cubs recorded the 10,000th regular-season win in their franchise's history dating back to the beginning of the National League in 1876. The Cubs reached the milestone with an overall National League record of 10,000-9,465. Chicago was only the second club in Major League Baseball history to attain this milestone, the first having been the San Francisco Giants in mid-season 2005. The Cubs, however, hold the mark for victories for a team in a single city. The Chicago club's 77–77 record in the National Association (1871, 1874–1875) is not included in MLB record keeping. Post-season series are also not included in the totals. To honor the milestone, the Cubs flew an extra white flag displaying ""10,000"" in blue, along with the customary ""W"" flag. The former location in Mesa is actually the second HoHoKam Park; the first was built in 1976 as the spring-training home of the Oakland Athletics who left the park in 1979. Apart from HoHoKam Park and Sloan Park the Cubs also have another Mesa training facility called Fitch Park, this complex provides 25,000 square feet (2,300 m2) of team facilities, including major league clubhouse, four practice fields, one practice infield, enclosed batting tunnels, batting cages, a maintenance facility, and administrative offices for the Cubs. Green shored up the 1984 roster with a series of transactions. In December, 1983 Scott Sanderson was acquired from Montreal in a three-team deal with San Diego for Carmelo Martínez. Pinch hitter Richie Hebner (.333 BA in 1984) was signed as a free-agent. In spring training, moves continued: LF Gary Matthews and CF Bobby Dernier came from Philadelphia on March 26, for Bill Campbell and a minor leaguer. Reliever Tim Stoddard (10–6 3.82, 7 saves) was acquired the same day for a minor leaguer; veteran pitcher Ferguson Jenkins was released. The ""Bleacher Bums"" is a name given to fans, many of whom spend much of the day heckling, who sit in the bleacher section at Wrigley Field. Initially, the group was called ""bums"" because it referred to a group of fans who were at most games, and since those games were all day games, it was assumed they did not work. Many of those fans were, and are still, students at Chicago area colleges, such as DePaul University, Loyola, Northwestern University, and Illinois-Chicago. A Broadway play, starring Joe Mantegna, Dennis Farina, Dennis Franz, and James Belushi ran for years and was based on a group of Cub fans who frequented the club's games. The group was started in 1967 by dedicated fans Ron Grousl, Tom Nall and ""mad bugler"" Mike Murphy, who was a sports radio host during mid days on Chicago-based WSCR AM 670 ""The Score"". Murphy alleges that Grousl started the Wrigley tradition of throwing back opposing teams' home run balls. The current group is headed by Derek Schaul (Derek the Five Dollar Kid). Prior to the 2006 season, they were updated, with new shops and private bar (The Batter's Eye) being added, and Bud Light bought naming rights to the bleacher section, dubbing them the Bud Light Bleachers. Bleachers at Wrigley are general admission, except during the playoffs. The bleachers have been referred to as the ""World's Largest Beer Garden."" A popular T-shirt (sold inside the park and licensed by the club) which says ""Wrigley Bleachers"" on the front and the phrase ""Shut Up and Drink Your Beer"" on the reverse fuels this stereotype. In only his third career start, Kerry Wood struck out 20 batters against Houston on May 6, 1998. This is the franchise record and tied for the Major League record for the most strikeouts in one game by one pitcher (the only other pitcher to strike out 20 batters in a nine-inning game was Roger Clemens, who achieved it twice). The game is often considered the most dominant pitching performance of all time. Interestingly, Wood's first pitch struck home plate umpire Jerry Meals in the facemask. Wood then struck out the first five batters he faced. Wood hit one batter, Craig Biggio, and allowed one hit, a scratch single by Ricky Gutiérrez off third baseman Kevin Orie's glove. The play was nearly scored an error, which would have given Wood a no-hitter. The 1989 film Back to the Future Part II depicts the Chicago Cubs defeating a baseball team from Miami in the 2015 World Series, ending the longest championship drought in all four of the major North American professional sports leagues. In 2015, the Miami Marlins failed to make the playoffs and were able to make it to the 2015 National League Wild Card round and move on to the 2015 National League Championship Series by October 21, 2015, the date where protagonist Marty McFly traveled to the future in the film. However, it was on October 21 that the Cubs were swept by the New York Mets in the NLCS. ""Baseball's Sad Lexicon,"" also known as ""Tinker to Evers to Chance"" after its refrain, is a 1910 baseball poem by Franklin Pierce Adams. The poem is presented as a single, rueful stanza from the point of view of a New York Giants fan seeing the talented Chicago Cubs infield of shortstop Joe Tinker, second baseman Johnny Evers, and first baseman Frank Chance complete a double play. The trio began playing together with the Cubs in 1902, and formed a double play combination that lasted through April 1912. The Cubs won the pennant four times between 1906 and 1910, often defeating the Giants en route to the World Series. On October 1, 1932, in game three of the World Series between the Cubs and the New York Yankees, Babe Ruth allegedly stepped to the plate, pointed his finger to Wrigley Field's center field bleachers and hit a long home run to center. There is speculation as to whether the ""facts"" surrounding the story are true or not, but nevertheless Ruth did help the Yankees secure a World Series win that year and the home run accounted for his 15th and last home run in the post season before he retired in 1935. In June, 1998 Sammy Sosa exploded into the pursuit of Roger Maris' home run record. Sosa had 13 home runs entering the month, representing less than half of Mark McGwire's total. Sosa had his first of four multi-home run games that month on June 1, and went on to break Rudy York's record with 20 home runs in the month, a record that still stands. By the end of his historic month, the outfielder's 33 home runs tied him with Ken Griffey, Jr. and left him only four behind McGwire's 37. Sosa finished with 66 and won the NL MVP Award. The Cubs successfully defended their National League Central title in 2008, going to the postseason in consecutive years for the first time since 1906–08. The offseason was dominated by three months of unsuccessful trade talks with the Orioles involving 2B Brian Roberts, as well as the signing of Chunichi Dragons star Kosuke Fukudome. The team recorded their 10,000th win in April, while establishing an early division lead. Reed Johnson and Jim Edmonds were added early on and Rich Harden was acquired from the Oakland Athletics in early July. The Cubs headed into the All-Star break with the N.L.'s best record, and tied the league record with eight representatives to the All-Star game, including catcher Geovany Soto, who was named Rookie of the Year. The Cubs took control of the division by sweeping a four-game series in Milwaukee. On September 14, in a game moved to Miller Park due to Hurricane Ike, Zambrano pitched a no-hitter against the Astros, and six days later the team clinched by beating St. Louis at Wrigley. The club ended the season with a 97–64 record and met Los Angeles in the NLDS. The heavily favored Cubs took an early lead in Game 1, but James Loney's grand slam off Ryan Dempster changed the series' momentum. Chicago committed numerous critical errors and were outscored 20–6 in a Dodger sweep, which provided yet another sudden ending. Despite trading for pitcher Matt Garza and signing free-agent slugger Carlos Peña, the Cubs finished the 2011 season 20 games under .500 with a record of 71-91. Weeks after the season came to an end, the club was rejuvenated in the form of a new philosophy, as new owner Tom Ricketts signed Theo Epstein away from the Boston Red Sox, naming him club President and giving him a five-year contract worth over $18 million, and subsequently discharged manager Mike Quade. Epstein, a proponent of sabremetrics and one of the architects of two world series titles in Boston brought along Jed Hoyer to fill the role of GM and hired Dale Sveum as manager. Although the team had a dismal 2012 season, losing 101 games (the worst record since 1966) it was largely expected. The youth movement ushered in by Epstein and Hoyer began as longtime fan favorite Kerry Wood retired in May, followed by Ryan Dempster and Geovany Soto being traded to Texas at the All-Star break for a group of minor league prospects headlined by Christian Villanueva. The development of Castro, Anthony Rizzo, Darwin Barney, Brett Jackson and pitcher Jeff Samardzija as well as the replenishing of the minor-league system with prospects such as Javier Baez, Albert Almora, and Jorge Soler became the primary focus of the season, a philosophy which the new management said would carry over at least through the 2013 season. Near the end of the first decade of the double-Bills' guidance, the Cubs won the NL pennant in 1929 and then achieved the unusual feat of winning a pennant every three years, following up the 1929 flag with league titles in 1932, 1935, and 1938. Unfortunately, their success did not extend to the Fall Classic, as they fell to their AL rivals each time. The '32 series against the Yankees featured Babe Ruth's ""called shot"" at Wrigley Field in Game 3. There were some historic moments for the Cubs as well; In 1930, Hack Wilson, one of the top home run hitters in the game, had one of the most impressive seasons in MLB history, hitting 56 home runs and establishing the current runs-batted-in record of 191. That 1930 club, which boasted six eventual Hall of Famers (Wilson, Gabby Hartnett, Rogers Hornsby, George ""High Pockets"" Kelly, Kiki Cuyler and manager Joe McCarthy) established the current team batting average record of .309. In 1935 the Cubs claimed the pennant in thrilling fashion, winning a record 21 games in a row in September. The '38 club saw Dizzy Dean lead the team's pitching staff and provided a historic moment when they won a crucial late-season game at Wrigley Field over the Pittsburgh Pirates with a walk-off home run by Gabby Hartnett, which became known in baseball lore as ""The Homer in the Gloamin'"". In 1914, advertising executive Albert Lasker obtained a large block of the club's shares and before the 1916 season assumed majority ownership of the franchise. Lasker brought in a wealthy partner, Charles Weeghman, the proprietor of a popular chain of lunch counters who had previously owned the Chicago Whales of the short-lived Federal League. As principal owners, the pair moved the club from the West Side Grounds to the much newer Weeghman Park, which had been constructed for the Whales only two years earlier, where they remain to this day. The Cubs responded by winning a pennant in the war-shortened season of 1918, where they played a part in another team's curse: the Boston Red Sox defeated Grover Cleveland Alexander's Cubs four games to two in the 1918 World Series, Boston's last Series championship until 2004. Following the '69 season, the club posted winning records for the next few seasons, but no playoff action. After the core players of those teams started to move on, the 70s got worse for the team, and they became known as ""The Loveable Losers."" In 1977, the team found some life, but ultimately experienced one of its biggest collapses. The Cubs hit a high-water mark on June 28 at 47–22, boasting an 8 1⁄2 game NL East lead, as they were led by Bobby Murcer (27 Hr/89 RBI), and Rick Reuschel (20–10). However, the Philadelphia Phillies cut the lead to two by the All-star break, as the Cubs sat 19 games over .500, but they swooned late in the season, going 20–40 after July 31. The Cubs finished in 4th place at 81–81, while Philadelphia surged, finishing with 101 wins. The following two seasons also saw the Cubs get off to a fast start, as the team rallied to over 10 games above .500 well into both seasons, only to again wear down and play poorly later on, and ultimately settling back to mediocrity. This trait became known as the ""June Swoon."" Again, the Cubs' unusually high number of day games is often pointed to as one reason for the team's inconsistent late season play. Before signing a developmental agreement with the Kane County Cougars in 2012, the Cubs had a Class A minor league affiliation on two occasions with the Peoria Chiefs (1985–1995 and 2004–2012). Ryne Sandberg managed the Chiefs from 2006 to 2010. In the period between those associations with the Chiefs the club had affiliations with the Dayton Dragons and Lansing Lugnuts. The Lugnuts were often affectionately referred to by Chip Caray as ""Steve Stone's favorite team."" The 2007 developmental contract with the Tennessee Smokies was preceded by Double A affiliations with the Orlando Cubs and West Tenn Diamond Jaxx. On September 16, 2014 the Cubs announced a move of their top Class A affiliate from Daytona in the Florida State League to Myrtle Beach in the Carolina League for the 2015 season. Two days later, on the 18th, the Cubs signed a 4-year player development contract with the South Bend Silver Hawks of the Midwest League, ending their brief relationship with the Kane County Cougars and shortly thereafter renaming the Silver Hawks the South Bend Cubs. In 1975, a group of Chicago Cubs fans based in Washington, D.C. formed the Emil Verban Society. The society is a select club of high profile Cub fans, currently headed by Illinois Senator Dick Durbin which is named for Emil Verban, who in three seasons with the Cubs in the 1940s batted .280 with 39 runs batted in and one home run. Verban was picked as the epitome of a Cub player, explains columnist George Will, because ""He exemplified mediocrity under pressure, he was competent but obscure and typifying of the work ethics."" Verban initially believed he was being ridiculed, but his ill feeling disappeared several years later when he was flown to Washington to meet President Ronald Reagan, also a society member, at the White House. Hillary Clinton, Jim Belushi, Joe Mantegna, Rahm Emanuel, Dick Cheney and many others have been included among its membership. In addition to Mesa, the club has held spring training in Hot Springs, Arkansas (1886, 1896–1900), (1909–1910) New Orleans (1870, 1907, 1911–1912); Champaign, Illinois (1901–02, 1906); Los Angeles (1903–04, 1948–1949), Santa Monica, California (1905); French Lick, Indiana (1908, 1943–1945); Tampa, Florida (1913–1916); Pasadena, California (1917–1921); Santa Catalina Island, California (1922–1942, 1946–1947, 1950–1951); Rendezvous Park in Mesa (1952–1965); Blair Field in Long Beach, California (1966); and Scottsdale, Arizona (1967–1978). After over a dozen more subpar seasons, in 1981 the Cubs hired GM Dallas Green from Philadelphia to turn around the franchise. Green had managed the 1980 Phillies to the World Series title. One of his early GM moves brought in a young Phillies minor-league 3rd baseman named Ryne Sandberg, along with Larry Bowa for Iván DeJesús. The 1983 Cubs had finished 71–91 under Lee Elia, who was fired before the season ended by Green. Green continued the culture of change and overhauled the Cubs roster, front-office and coaching staff prior to 1984. Jim Frey was hired to manage the 1984 Cubs, with Don Zimmer coaching 3rd base and Billy Connors serving as pitching coach. The confusion may stem from the fact that Major League Baseball did decide that, should the Cubs make it to the World Series, the American League winner would have home field advantage unless the Cubs hosted home games at an alternate site since the Cubs home field of Wrigley Field did not yet have lights. Rumor was the Cubs could hold home games across town at Comiskey Park, home of the American League's Chicago White Sox. Rather than hold any games in the cross town rival Sox Park, the Cubs made arrangements with the August A. Busch, owner of the St. Louis Cardinals, to use Busch Stadium in St. Louis as the Cubs ""home field"" for the World Series. This was approved by Major League Baseball and would have enabled the Cubs to host games 1 and 2, along with games 6 and 7 if necessary. At the time home field advantage was rotated between each league. Odd numbered years the AL had home field advantage. Even numbered years the NL had home field advantage. In the 1982 World Series the St. Louis Cardinals of the NL had home field advantage. In the 1983 World Series the Baltimore Orioles of the AL had home field advantage. Hack Wilson set a record of 56 home-runs and 190 runs-batted-in in 1930, breaking Lou Gehrig's MLB record of 176 RBI. (In 1999, a long-lost extra RBI mistakenly credited to Charlie Grimm had been found by Cooperstown researcher Cliff Kachline and verified by historian Jerome Holtzman, increasing the record number to 191.) As of 2014 the record still stands, with no serious threats coming since Gehrig (184) and Hank Greenberg (183) in the same era. The closest anyone has come to the mark in the last 75 years was Manny Ramirez's 165 RBI in 1999. In addition to the RBI record, Wilson 56 home-runs stood as the National League record until 1998, when Sammy Sosa and Mark McGwire hit 66 and 70, respectively. Wilson was named ""Most Useful"" player that year by the Baseball Writers' Association of America, as the official N.L. Most Valuable Player Award was not awarded until the next season."
Somerset,"The county has several museums; those at Bath include the American Museum in Britain, the Museum of Bath Architecture, the Herschel Museum of Astronomy, the Jane Austen Centre, and the Roman Baths. Other visitor attractions which reflect the cultural heritage of the county include: Claverton Pumping Station, Dunster Working Watermill, the Fleet Air Arm Museum at Yeovilton, Nunney Castle, The Helicopter Museum in Weston-super-Mare, King John's Hunting Lodge in Axbridge, Blake Museum Bridgwater, Radstock Museum, Museum of Somerset in Taunton, the Somerset Rural Life Museum in Glastonbury, and Westonzoyland Pumping Station Museum. Towns such as Castle Cary and Frome grew around the medieval weaving industry. Street developed as a centre for the production of woollen slippers and, later, boots and shoes, with C. & J. Clark establishing its headquarters in the town. C&J Clark's shoes are no longer manufactured there as the work was transferred to lower-wage areas, such as China and Asia. Instead, in 1993, redundant factory buildings were converted to form Clarks Village, the first purpose-built factory outlet in the UK. C&J Clark also had shoe factories, at one time at Bridgwater, Minehead, Westfield and Weston super Mare to provide employment outside the main summer tourist season, but those satellite sites were closed in the late 1980s, before the main site at Street. Dr. Martens shoes were also made in Somerset, by the Northampton-based R. Griggs Group, using redundant skilled shoemakers from C&J Clark; that work has also been transferred to Asia. In Arthurian legend, Avalon became associated with Glastonbury Tor when monks at Glastonbury Abbey claimed to have discovered the bones of King Arthur and his queen. What is more certain is that Glastonbury was an important religious centre by 700 and claims to be ""the oldest above-ground Christian church in the World"" situated ""in the mystical land of Avalon."" The claim is based on dating the founding of the community of monks at AD 63, the year of the legendary visit of Joseph of Arimathea, who was supposed to have brought the Holy Grail. During the Middle Ages there were also important religious sites at Woodspring Priory and Muchelney Abbey. The present Diocese of Bath and Wells covers Somerset – with the exception of the Parish of Abbots Leigh with Leigh Woods in North Somerset – and a small area of Dorset. The Episcopal seat of the Bishop of Bath and Wells is now in the Cathedral Church of Saint Andrew in the city of Wells, having previously been at Bath Abbey. Before the English Reformation, it was a Roman Catholic diocese; the county now falls within the Roman Catholic Diocese of Clifton. The Benedictine monastery Saint Gregory's Abbey, commonly known as Downside Abbey, is at Stratton-on-the-Fosse, and the ruins of the former Cistercian Cleeve Abbey are near the village of Washford. The usefulness of the canals was short-lived, though some have now been restored for recreation. The 19th century also saw the construction of railways to and through Somerset. The county was served by five pre-1923 Grouping railway companies: the Great Western Railway (GWR); a branch of the Midland Railway (MR) to Bath Green Park (and another one to Bristol); the Somerset and Dorset Joint Railway, and the London and South Western Railway (L&SWR). The former main lines of the GWR are still in use today, although many of its branch lines were scrapped under the notorious Beeching Axe. The former lines of the Somerset and Dorset Joint Railway closed completely, as has the branch of the Midland Railway to Bath Green Park (and to Bristol St Philips); however, the L&SWR survived as a part of the present West of England Main Line. None of these lines, in Somerset, are electrified. Two branch lines, the West and East Somerset Railways, were rescued and transferred back to private ownership as ""heritage"" lines. The fifth railway was a short-lived light railway, the Weston, Clevedon and Portishead Light Railway. The West Somerset Mineral Railway carried the iron ore from the Brendon Hills to Watchet. Somerset has a high indigenous British population, with 98.8% registering as white British and 92.4% of these as born in the United Kingdom. Chinese is the largest ethnic group, while the black minority ethnic proportion of the total population is 2.9%. Over 25% of Somerset's population is concentrated in Taunton, Bridgwater and Yeovil. The rest of the county is rural and sparsely populated. Over 9 million tourist nights are spent in Somerset each year, which significantly increases the population at peak times. Until the 1960s the piers at Weston-super-Mare, Clevedon, Portishead and Minehead were served by the paddle steamers of P and A Campbell who ran regular services to Barry and Cardiff as well as Ilfracombe and Lundy Island. The pier at Burnham-on-Sea was used for commercial goods, one of the reasons for the Somerset and Dorset Joint Railway was to provide a link between the Bristol Channel and the English Channel. The pier at Burnham-on-Sea is the shortest pier in the UK. In the 1970s the Royal Portbury Dock was constructed to provide extra capacity for the Port of Bristol. Along with the rest of South West England, Somerset has a temperate climate which is generally wetter and milder than the rest of the country. The annual mean temperature is approximately 10 °C (50.0 °F). Seasonal temperature variation is less extreme than most of the United Kingdom because of the adjacent sea temperatures. The summer months of July and August are the warmest with mean daily maxima of approximately 21 °C (69.8 °F). In winter mean minimum temperatures of 1 °C (33.8 °F) or 2 °C (35.6 °F) are common. In the summer the Azores high pressure affects the south-west of England, but convective cloud sometimes forms inland, reducing the number of hours of sunshine. Annual sunshine rates are slightly less than the regional average of 1,600 hours. In December 1998 there were 20 days without sun recorded at Yeovilton. Most the rainfall in the south-west is caused by Atlantic depressions or by convection. Most of the rainfall in autumn and winter is caused by the Atlantic depressions, which is when they are most active. In summer, a large proportion of the rainfall is caused by sun heating the ground leading to convection and to showers and thunderstorms. Average rainfall is around 700 mm (28 in). About 8–15 days of snowfall is typical. November to March have the highest mean wind speeds, and June to August the lightest winds. The predominant wind direction is from the south-west. Many Somerset soldiers died during the First World War, with the Somerset Light Infantry suffering nearly 5,000 casualties. War memorials were put up in most of the county's towns and villages; only nine, described as the Thankful Villages, had none of their residents killed. During the Second World War the county was a base for troops preparing for the D-Day landings. Some of the hospitals which were built for the casualties of the war remain in use. The Taunton Stop Line was set up to repel a potential German invasion. The remains of its pill boxes can still be seen along the coast, and south through Ilminster and Chard. Tourism is a major industry, estimated in 2001 to support around 23,000 people. Attractions include the coastal towns, part of the Exmoor National Park, the West Somerset Railway (a heritage railway), and the museum of the Fleet Air Arm at RNAS Yeovilton. The town of Glastonbury has mythical associations, including legends of a visit by the young Jesus of Nazareth and Joseph of Arimathea, with links to the Holy Grail, King Arthur, and Camelot, identified by some as Cadbury Castle, an Iron Age hill fort. Glastonbury also gives its name to an annual open-air rock festival held in nearby Pilton. There are show caves open to visitors in the Cheddar Gorge, as well as its locally produced cheese, although there is now only one remaining cheese maker in the village of Cheddar. The people of Somerset are mentioned in the Anglo-Saxon Chronicle's entry for AD 845, in the inflected form ""Sumursætum"", and the county is recorded in the entry for 1015 using the same name. The archaic name Somersetshire was mentioned in the Chronicle's entry for 878. Although ""Somersetshire"" was in common use as an alternative name for the county, it went out of fashion in the late 19th century, and is no longer used possibly due to the adoption of ""Somerset"" as the county's official name after the establishment of the county council in 1889. As with other counties not ending in ""shire,"" the suffix was superfluous, as there was no need to differentiate between the county and a town within it. The county has a long tradition of supplying freestone and building stone. Quarries at Doulting supplied freestone used in the construction of Wells Cathedral. Bath stone is also widely used. Ralph Allen promoted its use in the early 18th century, as did Hans Price in the 19th century, but it was used long before then. It was mined underground at Combe Down and Bathampton Down Mines, and as a result of cutting the Box Tunnel, at locations in Wiltshire such as Box. Bath stone is still used on a reduced scale today, but more often as a cladding rather than a structural material. Further south, Hamstone is the colloquial name given to stone from Ham Hill, which is also widely used in the construction industry. Blue Lias has been used locally as a building stone and as a raw material for lime mortar and Portland cement. Until the 1960s, Puriton had Blue Lias stone quarries, as did several other Polden villages. Its quarries also supplied a cement factory at Dunball, adjacent to the King's Sedgemoor Drain. Its derelict, early 20th century remains, was removed when the M5 motorway was constructed in the mid-1970s. Since the 1920s, the county has supplied aggregates. Foster Yeoman is Europe's large supplier of limestone aggregates, with quarries at Merehead Quarry. It has a dedicated railway operation, Mendip Rail, which is used to transport aggregates by rail from a group of Mendip quarries. State schools in Somerset are provided by three local education authorities: Bath and North East Somerset, North Somerset, and the larger Somerset County Council. All state schools are comprehensive. In some areas primary, infant and junior schools cater for ages four to eleven, after which the pupils move on to secondary schools. There is a three-tier system of first, middle and upper schools in the Cheddar Valley, and in West Somerset, while most other schools in the county use the two-tier system. Somerset has 30 state and 17 independent secondary schools; Bath and North East Somerset has 13 state and 5 independent secondary schools; and North Somerset has 10 state and 2 independent secondary schools, excluding sixth form colleges. Somerset is a rural county of rolling hills such as the Blackdown Hills, Mendip Hills, Quantock Hills and Exmoor National Park, and large flat expanses of land including the Somerset Levels. There is evidence of human occupation from Paleolithic times, and of subsequent settlement in the Roman and Anglo-Saxon periods. The county played a significant part in the consolidation of power and rise of King Alfred the Great, and later in the English Civil War and the Monmouth Rebellion. The city of Bath is famous for its substantial Georgian architecture and is a UNESCO World Heritage Site. The boundaries of Somerset are largely unaltered from medieval times. The River Avon formed much of the border with Gloucestershire, except that the hundred of Bath Forum, which straddles the Avon, formed part of Somerset. Bristol began as a town on the Gloucestershire side of the Avon, however as it grew it extended across the river into Somerset. In 1373 Edward III proclaimed ""that the town of Bristol with its suburbs and precincts shall henceforth be separate from the counties of Gloucester and Somerset... and that it should be a county by itself"". The main coastal towns are, from the west to the north-east, Minehead, Watchet, Burnham-on-Sea, Weston-super-Mare, Clevedon and Portishead. The coastal area between Minehead and the eastern extreme of the administrative county's coastline at Brean Down is known as Bridgwater Bay, and is a National Nature Reserve. North of that, the coast forms Weston Bay and Sand Bay whose northern tip, Sand Point, marks the lower limit of the Severn Estuary. In the mid and north of the county the coastline is low as the level wetlands of the levels meet the sea. In the west, the coastline is high and dramatic where the plateau of Exmoor meets the sea, with high cliffs and waterfalls. Hinkley Point C nuclear power station is a project to construct a 3,200 MW two reactor nuclear power station. On 18 October 2010, the British government announced that Hinkley Point – already the site of the disused Hinkley Point A and the still operational Hinkley Point B power stations – was one of the eight sites it considered suitable for future nuclear power stations. NNB Generation Company, a subsidiary of EDF, submitted an application for development consent to the Infrastructure Planning Commission on 31 October 2011. A protest group, Stop Hinkley, was formed to campaign for the closure of Hinkley Point B and oppose any expansion at the Hinkley Point site. In December 2013, the European Commission opened an investigation to assess whether the project breaks state-aid rules. On 8 October 2014 it was announced that the European Commission has approved the project, with an overwhelming majority and only four commissioners voting against the decision. The ceremonial county of Somerset consists of a two-tier non-metropolitan county, which is administered by Somerset County Council and five district councils, and two unitary authority areas (whose councils combine the functions of a county and a district). The five districts of Somerset are West Somerset, South Somerset, Taunton Deane, Mendip, and Sedgemoor. The two unitary authorities — which were established on 1 April 1996 following the break-up of the short-lived county of Avon — are North Somerset, and Bath & North East Somerset. All of the ceremonial county of Somerset is covered by the Avon and Somerset Constabulary, a police force which also covers Bristol and South Gloucestershire. The Devon and Somerset Fire and Rescue Service was formed in 2007 upon the merger of the Somerset Fire and Rescue Service with its neighbouring Devon service; it covers the area of Somerset County Council as well as the entire ceremonial county of Devon. The unitary districts of North Somerset and Bath & North East Somerset are instead covered by the Avon Fire and Rescue Service, a service which also covers Bristol and South Gloucestershire. The South Western Ambulance Service covers the entire South West of England, including all of Somerset; prior to February 2013 the unitary districts of Somerset came under the Great Western Ambulance Service, which merged into South Western. The Dorset and Somerset Air Ambulance is a charitable organisation based in the county. A number of decoy towns were constructed in Somerset in World War II to protect Bristol and other towns, at night. They were designed to mimic the geometry of ""blacked out"" streets, railway lines, and Bristol Temple Meads railway station, to encourage bombers away from these targets. One, on the radio beam flight path to Bristol, was constructed on Beacon Batch. It was laid out by Shepperton Studios, based on aerial photographs of the city's railway marshalling yards. The decoys were fitted with dim red lights, simulating activities like the stoking of steam locomotives. Burning bales of straw soaked in creosote were used to simulate the effects of incendiary bombs dropped by the first wave of Pathfinder night bombers; meanwhile, incendiary bombs dropped on the correct location were quickly smothered, wherever possible. Drums of oil were also ignited to simulate the effect of a blazing city or town, with the aim of fooling subsequent waves of bombers into dropping their bombs on the wrong location. The Chew Magna decoy town was hit by half-a-dozen bombs on 2 December 1940, and over a thousand incendiaries on 3 January 1941. The following night the Uphill decoy town, protecting Weston-super-Mare's airfield, was bombed; a herd of dairy cows was hit, killing some and severely injuring others. The University of Bath and Bath Spa University are higher education establishments in the north-east of the county. The University of Bath gained its Royal Charter in 1966, although its origins go back to the Bristol Trade School (founded 1856) and Bath School of Pharmacy (founded 1907). It has a purpose-built campus at Claverton on the outskirts of Bath, and has 15,000 students. Bath Spa University, which is based at Newton St Loe, achieved university status in 2005, and has origins including the Bath Academy of Art (founded 1898), Bath Teacher Training College, and the Bath College of Higher Education. It has several campuses and 5,500 students. The Somerset Coal Canal was built in the early 19th century to reduce the cost of transportation of coal and other heavy produce. The first 16 kilometres (10 mi), running from a junction with the Kennet and Avon Canal, along the Cam valley, to a terminal basin at Paulton, were in use by 1805, together with several tramways. A planned 11.7 km (7.3 mi) branch to Midford was never built, but in 1815 a tramway was laid along its towing path. In 1871 the tramway was purchased by the Somerset and Dorset Joint Railway (S&DJR), and operated until the 1950s. Somerset has 11,500 listed buildings, 523 scheduled monuments, 192 conservation areas, 41 parks and gardens including those at Barrington Court, Holnicote Estate, Prior Park Landscape Garden and Tintinhull Garden, 36 English Heritage sites and 19 National Trust sites, including Clevedon Court, Fyne Court, Montacute House and Tyntesfield as well as Stembridge Tower Mill, the last remaining thatched windmill in England. Other historic houses in the county which have remained in private ownership or used for other purposes include Halswell House and Marston Bigot. A key contribution of Somerset architecture is its medieval church towers. Jenkins writes, ""These structures, with their buttresses, bell-opening tracery and crowns, rank with Nottinghamshire alabaster as England's finest contribution to medieval art."" There is an extensive network of caves, including Wookey Hole, underground rivers, and gorges, including the Cheddar Gorge and Ebbor Gorge. The county has many rivers, including the Axe, Brue, Cary, Parrett, Sheppey, Tone and Yeo. These both feed and drain the flat levels and moors of mid and west Somerset. In the north of the county the River Chew flows into the Bristol Avon. The Parrett is tidal almost to Langport, where there is evidence of two Roman wharfs. At the same site during the reign of King Charles I, river tolls were levied on boats to pay for the maintenance of the bridge. Bath Rugby play at the Recreation Ground in Bath, and the Somerset County Cricket Club are based at the County Ground in Taunton. The county gained its first Football League club in 2003, when Yeovil Town won promotion to Division Three as Football Conference champions. They had achieved numerous FA Cup victories over football League sides in the past 50 years, and since joining the elite they have won promotion again—as League Two champions in 2005. They came close to yet another promotion in 2007, when they reached the League One playoff final, but lost to Blackpool at the newly reopened Wembley Stadium. Yeovil achieved promotion to the Championship in 2013 after beating Brentford in the playoff final. Horse racing courses are at Taunton and Wincanton. Agriculture and food and drink production continue to be major industries in the county, employing over 15,000 people. Apple orchards were once plentiful, and Somerset is still a major producer of cider. The towns of Taunton and Shepton Mallet are involved with the production of cider, especially Blackthorn Cider, which is sold nationwide, and there are specialist producers such as Burrow Hill Cider Farm and Thatchers Cider. Gerber Products Company in Bridgwater is the largest producer of fruit juices in Europe, producing brands such as ""Sunny Delight"" and ""Ocean Spray."" Development of the milk-based industries, such as Ilchester Cheese Company and Yeo Valley Organic, have resulted in the production of ranges of desserts, yoghurts and cheeses, including Cheddar cheese—some of which has the West Country Farmhouse Cheddar Protected Designation of Origin (PDO). After the Romans left, Britain was invaded by Anglo-Saxon peoples. By AD 600 they had established control over much of what is now England, but Somerset was still in native British hands. The British held back Saxon advance into the south-west for some time longer, but by the early eighth century King Ine of Wessex had pushed the boundaries of the West Saxon kingdom far enough west to include Somerset. The Saxon royal palace in Cheddar was used several times in the 10th century to host the Witenagemot. After the Norman Conquest, the county was divided into 700 fiefs, and large areas were owned by the crown, with fortifications such as Dunster Castle used for control and defence. Somerset contains HM Prison Shepton Mallet, which was England's oldest prison still in use prior to its closure in 2013, having opened in 1610. In the English Civil War Somerset was largely Parliamentarian, with key engagements being the Sieges of Taunton and the Battle of Langport. In 1685 the Monmouth Rebellion was played out in Somerset and neighbouring Dorset. The rebels landed at Lyme Regis and travelled north, hoping to capture Bristol and Bath, but they were defeated in the Battle of Sedgemoor at Westonzoyland, the last pitched battle fought in England. Arthur Wellesley took his title, Duke of Wellington from the town of Wellington; he is commemorated on a nearby hill by a large, spotlit obelisk, known as the Wellington Monument. Somerton took over from Ilchester as the county town in the late thirteenth century, but it declined in importance and the status of county town transferred to Taunton about 1366. The county has two cities, Bath and Wells, and 30 towns (including the county town of Taunton, which has no town council but instead is the chief settlement of the county's only borough). The largest urban areas in terms of population are Bath, Weston-super-Mare, Taunton, Yeovil and Bridgwater. Many settlements developed because of their strategic importance in relation to geographical features, such as river crossings or valleys in ranges of hills. Examples include Axbridge on the River Axe, Castle Cary on the River Cary, North Petherton on the River Parrett, and Ilminster, where there was a crossing point on the River Isle. Midsomer Norton lies on the River Somer; while the Wellow Brook and the Fosse Way Roman road run through Radstock. Chard is the most southerly town in Somerset, and at an altitude of 121 m (397 ft) it is also the highest. The Industrial Revolution in the Midlands and Northern England spelled the end for most of Somerset's cottage industries. Farming continued to flourish, however, and the Bath and West of England Society for the Encouragement of Agriculture, Arts, Manufactures and Commerce was founded in 1777 to improve farming methods. Despite this, 20 years later John Billingsley conducted a survey of the county's agriculture in 1795 and found that agricultural methods could still be improved. Coal mining was an important industry in north Somerset during the 18th and 19th centuries, and by 1800 it was prominent in Radstock. The Somerset Coalfield reached its peak production by the 1920s, but all the pits have now been closed, the last in 1973. Most of the surface buildings have been removed, and apart from a winding wheel outside Radstock Museum, little evidence of their former existence remains. Further west, the Brendon Hills were mined for iron ore in the late 19th century; this was taken by the West Somerset Mineral Railway to Watchet Harbour for shipment to the furnaces at Ebbw Vale. Some of the county's secondary schools have specialist school status. Some schools have sixth forms and others transfer their sixth formers to colleges. Several schools can trace their origins back many years, such as The Blue School in Wells and Richard Huish College in Taunton. Others have changed their names over the years such as Beechen Cliff School which was started in 1905 as the City of Bath Boys' School and changed to its present name in 1972 when the grammar school was amalgamated with a local secondary modern school, to form a comprehensive school. Many others were established and built since the Second World War. In 2006, 5,900 pupils in Somerset sat GCSE examinations, with 44.5% achieving 5 grades A-C including English and Maths (compared to 45.8% for England). Bridgwater was developed during the Industrial Revolution as the area's leading port. The River Parrett was navigable by large ships as far as Bridgwater. Cargoes were then loaded onto smaller boats at Langport Quay, next to the Bridgwater Bridge, to be carried further up river to Langport; or they could turn off at Burrowbridge and then travel via the River Tone to Taunton. The Parrett is now only navigable as far as Dunball Wharf. Bridgwater, in the 19th and 20th centuries, was a centre for the manufacture of bricks and clay roof tiles, and later cellophane, but those industries have now stopped. With its good links to the motorway system, Bridgwater has developed as a distribution hub for companies such as Argos, Toolstation, Morrisons and Gerber Juice. AgustaWestland manufactures helicopters in Yeovil, and Normalair Garratt, builder of aircraft oxygen systems, is also based in the town. Many towns have encouraged small-scale light industries, such as Crewkerne's Ariel Motor Company, one of the UK's smallest car manufacturers. Somerset is an important supplier of defence equipment and technology. A Royal Ordnance Factory, ROF Bridgwater was built at the start of the Second World War, between the villages of Puriton and Woolavington, to manufacture explosives. The site was decommissioned and closed in July 2008. Templecombe has Thales Underwater Systems, and Taunton presently has the United Kingdom Hydrographic Office and Avimo, which became part of Thales Optics. It has been announced twice, in 2006 and 2007, that manufacturing is to end at Thales Optics' Taunton site, but the trade unions and Taunton Deane District Council are working to reverse or mitigate these decisions. Other high-technology companies include the optics company Gooch and Housego, at Ilminster. There are Ministry of Defence offices in Bath, and Norton Fitzwarren is the home of 40 Commando Royal Marines. The Royal Naval Air Station in Yeovilton, is one of Britain's two active Fleet Air Arm bases and is home to the Royal Navy's Lynx helicopters and the Royal Marines Commando Westland Sea Kings. Around 1,675 service and 2,000 civilian personnel are stationed at Yeovilton and key activities include training of aircrew and engineers and the Royal Navy's Fighter Controllers and surface-based aircraft controllers. Population growth is higher than the national average, with a 6.4% increase, in the Somerset County Council area, since 1991, and a 17% increase since 1981. The population density is 1.4 persons per hectare, which can be compared to 2.07 persons per hectare for the South West region. Within the county, population density ranges 0.5 in West Somerset to 2.2 persons per hectare in Taunton Deane. The percentage of the population who are economically active is higher than the regional and national average, and the unemployment rate is lower than the regional and national average. There is also a range of independent or public schools. Many of these are for pupils between 11 and 18 years, such as King's College, Taunton and Taunton School. King's School, Bruton, was founded in 1519 and received royal foundation status around 30 years later in the reign of Edward VI. Millfield is the largest co-educational boarding school. There are also preparatory schools for younger children, such as All Hallows, and Hazlegrove Preparatory School. Chilton Cantelo School offers places both to day pupils and boarders aged 7 to 16. Other schools provide education for children from the age of 3 or 4 years through to 18, such as King Edward's School, Bath, Queen's College, Taunton and Wells Cathedral School which is one of the five established musical schools for school-age children in Britain. Some of these schools have religious affiliations, such as Monkton Combe School, Prior Park College, Sidcot School which is associated with the Religious Society of Friends, Downside School which is a Roman Catholic public school in Stratton-on-the-Fosse, situated next to the Benedictine Downside Abbey, and Kingswood School, which was founded by John Wesley in 1748 in Kingswood near Bristol, originally for the education of the sons of the itinerant ministers (clergy) of the Methodist Church. To the north-east of the Somerset Levels, the Mendip Hills are moderately high limestone hills. The central and western Mendip Hills was designated an Area of Outstanding Natural Beauty in 1972 and covers 198 km2 (76 sq mi). The main habitat on these hills is calcareous grassland, with some arable agriculture. To the south-west of the Somerset Levels are the Quantock Hills which was England's first Area of Outstanding Natural Beauty designated in 1956 which is covered in heathland, oak woodlands, ancient parklands with plantations of conifer and covers 99 square kilometres. The Somerset Coalfield is part of a larger coalfield which stretches into Gloucestershire. To the north of the Mendip hills is the Chew Valley and to the south, on the clay substrate, are broad valleys which support dairy farming and drain into the Somerset Levels. Traditional willow growing and weaving (such as basket weaving) is not as extensive as it used to be but is still carried out on the Somerset Levels and is commemorated at the Willows and Wetlands Visitor Centre. Fragments of willow basket were found near the Glastonbury Lake Village, and it was also used in the construction of several Iron Age causeways. The willow was harvested using a traditional method of pollarding, where a tree would be cut back to the main stem. During the 1930s more than 3,600 hectares (8,900 acres) of willow were being grown commercially on the Levels. Largely due to the displacement of baskets with plastic bags and cardboard boxes, the industry has severely declined since the 1950s. By the end of the 20th century only about 140 hectares (350 acres) were grown commercially, near the villages of Burrowbridge, Westonzoyland and North Curry. The Somerset Levels is now the only area in the UK where basket willow is grown commercially. The Somerset Levels (or Somerset Levels and Moors as they are less commonly but more correctly known) are a sparsely populated wetland area of central Somerset, between the Quantock and Mendip hills. They consist of marine clay levels along the coast, and the inland (often peat based) moors. The Levels are divided into two by the Polden Hills; land to the south is drained by the River Parrett while land to the north is drained by the River Axe and the River Brue. The total area of the Levels amounts to about 647.5 square kilometres (160,000 acres) and broadly corresponds to the administrative district of Sedgemoor but also includes the south west of Mendip district. Approximately 70% of the area is grassland and 30% is arable. Stretching about 32 kilometres (20 mi) inland, this expanse of flat land barely rises above sea level. Before it was drained, much of the land was under a shallow brackish sea in winter and was marsh land in summer. Drainage began with the Romans, and was restarted at various times: by the Anglo-Saxons; in the Middle Ages by the Glastonbury Abbey, from 1400–1770; and during the Second World War, with the construction of the Huntspill River. Pumping and management of water levels still continues. The Glastonbury Festival of Contemporary Performing Arts takes place most years in Pilton, near Shepton Mallet, attracting over 170,000 music and culture lovers from around the world to see world-famous entertainers. The Big Green Gathering which grew out of the Green fields at the Glastonbury Festival is held in the Mendip Hills between Charterhouse and Compton Martin each summer. The annual Bath Literature Festival is one of several local festivals in the county; others include the Frome Festival and the Trowbridge Village Pump Festival, which, despite its name, is held at Farleigh Hungerford in Somerset. The annual circuit of West Country Carnivals is held in a variety of Somerset towns during the autumn, forming a major regional festival, and the largest Festival of Lights in Europe."
Seven_Years%27_War,"In April 1758, the British concluded the Anglo-Prussian Convention with Frederick in which they committed to pay him an annual subsidy of £670,000. Britain also dispatched 9,000 troops to reinforce Ferdinand's Hanoverian army, the first British troop commitment on the continent and a reversal in the policy of Pitt. Ferdinand had succeeded in driving the French from Hanover and Westphalia and re-captured the port of Emden in March 1758 before crossing the Rhine with his own forces, which caused alarm in France. Despite Ferdinand's victory over the French at the Battle of Krefeld and the brief occupation of Düsseldorf, he was compelled by the successful manoeuvering of larger French forces to withdraw across the Rhine. The war had also brought to an end the ""Old System"" of alliances in Europe, In the years after the war, under the direction of Lord Sandwich, the British did try to re-establish this system. But after her surprising grand success against a coalition of great powers, European states such as Austria, The Dutch Republic, Sweden, Denmark-Norway, Ottoman Empire, and Russia now saw Britain as a greater threat than France and did not join them, while the Prussians were angered by what they considered a British betrayal in 1762. Consequently, when the American War of Independence turned into a global war between 1778–83, Britain found itself opposed by a strong coalition of European powers, and lacking any substantial ally. Many middle and small powers in Europe, unlike in the previous wars, tried to steer clear away from the escalating conflict, even though they had interests in the conflict or with the belligerents, like Denmark-Norway. The Dutch Republic, long-time British ally, kept its neutrality intact, fearing the odds against Britain and Prussia fighting the great powers of Europe, even tried to prevent Britain's domination in India. Naples, Sicily, and Savoy, although sided with Franco-Spanish party, declined to join the coalition under the fear of British power. The taxation needed for war caused the Russian people considerable hardship, being added to the taxation of salt and alcohol begun by Empress Elizabeth in 1759 to complete her addition to the Winter Palace. Like Sweden, Russia concluded a separate peace with Prussia. Years later, Kaunitz kept trying to establish France's alliance with Austria. He tried as hard as he could for Austria to not get entangled in Hanover's political affairs, and was even willing to trade Austrian Netherlands for France's aid in recapturing Silesia. Frustrated by this decision and by the Dutch Republic's insistence on neutrality, Britain soon turned to Russia. On September 30, 1755, Britain pledged financial aid to Russia in order to station 50,000 troops on the Livonian-Lithunian border, so they could defend Britain's interests in Hanover immediately. Besthuzev, assuming the preparation was directed against Prussia, was more than happy to obey the request of the British. Unbeknownst to the other powers, King George II also made overtures to the Prussian king; Frederick, who began fearing the Austro-Russian intentions, and was excited to welcome a rapprochement with Britain. On January 16, 1756, the Convention of Westminster was signed wherein Britain and Prussia promised to aid one another in order to achieve lasting peace and stability in Europe. The most important French fort planned was intended to occupy a position at ""the Forks"" where the Allegheny and Monongahela Rivers meet to form the Ohio River (present day Pittsburgh, Pennsylvania). Peaceful British attempts to halt this fort construction were unsuccessful, and the French proceeded to build the fort they named Fort Duquesne. British colonial militia from Virginia were then sent to drive them out. Led by George Washington, they ambushed a small French force at Jumonville Glen on 28 May 1754 killing ten, including commander Jumonville. The French retaliated by attacking Washington's army at Fort Necessity on 3 July 1754 and forced Washington to surrender. The Hanoverian king George II of Great Britain was passionately devoted to his family’s continental holdings, but his commitments in Germany were counterbalanced by the demands of the British colonies overseas. If war against France for colonial expansion was to be resumed, then Hanover had to be secured against Franco-Prussian attack. France was very much interested in colonial expansion and was willing to exploit the vulnerability of Hanover in war against Great Britain, but it had no desire to divert forces to central Europe for Prussia's interest. During the war, the Seven Nations of Canada were allied with the French. These were Native Americans of the Laurentian valley—the Algonquin, the Abenaki, the Huron, and others. Although the Algonquin tribes and the Seven Nations were not directly concerned with the fate of the Ohio River Valley, they had been victims of the Iroquois Confederation. The Iroquois had encroached on Algonquin territory and pushed the Algonquins west beyond Lake Michigan. Therefore, the Algonquin and the Seven Nations were interested in fighting against the Iroquois. Throughout New England, New York, and the North-west Native American tribes formed differing alliances with the major belligerents. The Iroquois, dominant in what is now Upstate New York, sided with the British but did not play a large role in the war. Accordingly, leaving Field Marshal Count Kurt von Schwerin in Silesia with 25,000 soldiers to guard against incursions from Moravia or Hungary, and leaving Field Marshal Hans von Lehwaldt in East Prussia to guard against Russian invasion from the east, Frederick set off with his army for Saxony. The Prussian army marched in three columns. On the right was a column of about 15,000 men under the command of Prince Ferdinand of Brunswick. On the left was a column of 18,000 men under the command of the Duke of Brunswick-Bevern. In the centre was Frederick II, himself with Field Marshal James Keith commanding a corps of 30,000 troops. Ferdinand of Brunswick was to close in on the town of Chemnitz. The Duke of Brunswick-Bevern was to traverse Lusatia to close in on Bautzen. Meanwhile, Frederick and Field Marshal Keith would make for Dresden. The war was successful for Great Britain, which gained the bulk of New France in North America, Spanish Florida, some individual Caribbean islands in the West Indies, the colony of Senegal on the West African coast, and superiority over the French trading outposts on the Indian subcontinent. The Native American tribes were excluded from the settlement; a subsequent conflict, known as Pontiac's War, was also unsuccessful in returning them to their pre-war status. In Europe, the war began disastrously for Prussia, but a combination of good luck and successful strategy saw King Frederick the Great manage to retrieve the Prussian position and retain the status quo ante bellum. Prussia emerged as a new European great power. The involvement of Portugal, Spain and Sweden did not return them to their former status as great powers. France was deprived of many of its colonies and had saddled itself with heavy war debts that its inefficient financial system could barely handle. Spain lost Florida but gained French Louisiana and regained control of its colonies, e.g., Cuba and the Philippines, which had been captured by the British during the war. France and other European powers will soon avenge their defeat in 1778 when American Revolutionary War broke out, with hopes of destroying Britain's dominance once and for all. In 1762, towards the end of the war, French forces attacked St. John's, Newfoundland. If successful, the expedition would have strengthened France's hand at the negotiating table. Although they took St. John's and raided nearby settlements, the French forces were eventually defeated by British troops at the Battle of Signal Hill. This was the final battle of the war in North America, and it forced the French to surrender to Lieutenant Colonel William Amherst. The victorious British now controlled all of eastern North America. The war has been described as the first ""world war"", although this label was also given to various earlier conflicts like the Eighty Years' War, the Thirty Years' War, the War of the Spanish Succession and the War of the Austrian Succession, and to later conflicts like the Napoleonic Wars. The term ""Second Hundred Years' War"" has been used in order to describe the almost continuous level of world-wide conflict during the entire 18th century, reminiscent of the more famous and compact struggle of the 14th century. The Treaty of Hubertusburg, between Austria, Prussia, and Saxony, was signed on February 15, 1763, at a hunting lodge between Dresden and Leipzig. Negotiations had started there on December 31, 1762. Frederick, who had considered ceding East Prussia to Russia if Peter III helped him secure Saxony, finally insisted on excluding Russia (in fact, no longer a belligerent) from the negotiations. At the same time, he refused to evacuate Saxony until its elector had renounced any claim to reparation. The Austrians wanted at least to retain Glatz, which they had in fact reconquered, but Frederick would not allow it. The treaty simply restored the status quo of 1748, with Silesia and Glatz reverting to Frederick and Saxony to its own elector. The only concession that Prussia made to Austria was to consent to the election of Archduke Joseph as Holy Roman emperor. News of this arrived in Europe, where Britain and France unsuccessfully attempted to negotiate a solution. The two nations eventually dispatched regular troops to North America to enforce their claims. The first British action was the assault on Acadia on 16 June 1755 in the Battle of Fort Beauséjour, which was immediately followed by their expulsion of the Acadians. In July British Major General Edward Braddock led about 2,000 army troops and provincial militia on an expedition to retake Fort Duquesne, but the expedition ended in disastrous defeat. In further action, Admiral Edward Boscawen fired on the French ship Alcide on 8 June 1755, capturing it and two troop ships. In September 1755, French and British troops met in the inconclusive Battle of Lake George. Britain now threatened to withdraw its subsidies if Prussia didn't consider offering concessions to secure peace. As the Prussian armies had dwindled to just 60,000 men and with Berlin itself under siege, Frederick's survival was severely threatened. Then on 5 January 1762 the Russian Empress Elizabeth died. Her Prussophile successor, Peter III, at once recalled Russian armies from Berlin (see: the Treaty of Saint Petersburg (1762)) and mediated Frederick's truce with Sweden. He also placed a corps of his own troops under Frederick's command This turn of events has become known as the Miracle of the House of Brandenburg. Frederick was then able to muster a larger army of 120,000 men and concentrate it against Austria. He drove them from much of Saxony, while his brother Henry won a victory in Silesia in the Battle of Freiberg (29 October 1762). At the same time, his Brunswick allies captured the key town of Göttingen and compounded this by taking Cassel. Despite this, the Austrians, under the command of General Laudon, captured Glatz (now Kłodzko, Poland) in Silesia. In the Battle of Liegnitz Frederick scored a strong victory despite being outnumbered three to one. The Russians under General Saltykov and Austrians under General Lacy briefly occupied his capital, Berlin, in October, but could not hold it for long. The end of that year saw Frederick once more victorious, defeating the able Daun in the Battle of Torgau; but he suffered very heavy casualties, and the Austrians retreated in good order. The troops were reembarked and moved to the Bay of St. Lunaire in Brittany where, on 3 September, they were landed to operate against St. Malo; however, this action proved impractical. Worsening weather forced the two armies to separate: the ships sailed for the safer anchorage of St. Cast, while the army proceeded overland. The tardiness of Bligh in moving his forces allowed a French force of 10,000 from Brest to catch up with him and open fire on the reembarkation troops. A rear-guard of 1,400 under General Dury held off the French while the rest of the army embarked. They could not be saved; 750, including Dury, were killed and the rest captured. The British—by inclination as well as for practical reasons—had tended to avoid large-scale commitments of troops on the Continent. They sought to offset the disadvantage of this in Europe by allying themselves with one or more Continental powers whose interests were antithetical to those of their enemies, particularly France.:15–16 By subsidising the armies of continental allies, Britain could turn London's enormous financial power to military advantage. In the Seven Years' War, the British chose as their principal partner the greatest general of the day, Frederick the Great of Prussia, then the rising power in central Europe, and paid Frederick substantial subsidies for his campaigns.:106 This was accomplished in the Diplomatic Revolution of 1756, in which Britain ended its long-standing alliance with Austria in favor of Prussia, leaving Austria to side with France. In marked contrast to France, Britain strove to prosecute the war actively in the colonies, taking full advantage of its naval power. :64–66 The British pursued a dual strategy – naval blockade and bombardment of enemy ports, and rapid movement of troops by sea. They harassed enemy shipping and attacked enemy colonies, frequently using colonists from nearby British colonies in the effort. British Prime Minister William Pitt's focus on the colonies for the 1758 campaign paid off with the taking of Louisbourg after French reinforcements were blocked by British naval victory in the Battle of Cartagena and in the successful capture of Fort Duquesne and Fort Frontenac. The British also continued the process of deporting the Acadian population with a wave of major operations against Île Saint-Jean (present-day Prince Edward Island), the St. John River valley, and the Petitcodiac River valley. The celebration of these successes was dampened by their embarrassing defeat in the Battle of Carillon (Ticonderoga), in which 4,000 French troops repulsed 16,000 British. Austria was not able to retake Silesia or make any significant territorial gain. However, it did prevent Prussia from invading parts of Saxony. More significantly, its military performance proved far better than during the War of the Austrian Succession and seemed to vindicate Maria Theresa's administrative and military reforms. Hence, Austria's prestige was restored in great part and the empire secured its position as a major player in the European system. Also, by promising to vote for Joseph II in the Imperial elections, Frederick II accepted the Habsburg preeminence in the Holy Roman Empire. The survival of Prussia as a first-rate power and the enhanced prestige of its king and its army, however, was potentially damaging in the long run to Austria's influence in Germany. 1762 brought two new countries into the war. Britain declared war against Spain on 4 January 1762; Spain reacted by issuing their own declaration of war against Britain on 18 January. Portugal followed by joining the war on Britain's side. Spain, aided by the French, launched an invasion of Portugal and succeeded in capturing Almeida. The arrival of British reinforcements stalled a further Spanish advance, and the Battle of Valencia de Alcántara saw British-Portuguese forces overrun a major Spanish supply base. The invaders were stopped on the heights in front of Abrantes (called the pass to Lisbon) where the Anglo-Portuguese were entrenched. Eventually the Anglo-Portuguese army, aided by guerrillas and practicing a scorched earth strategy, chased the greatly reduced Franco-Spanish army back to Spain, recovering almost all the lost towns, among them the Spanish headquarters in Castelo Branco full of wounded and sick that had been left behind. The War of the Austrian Succession had seen the belligerents aligned on a time-honoured basis. France’s traditional enemies, Great Britain and Austria, had coalesced just as they had done against Louis XIV. Prussia, the leading anti-Austrian state in Germany, had been supported by France. Neither group, however, found much reason to be satisfied with its partnership: British subsidies to Austria had produced nothing of much help to the British, while the British military effort had not saved Silesia for Austria. Prussia, having secured Silesia, had come to terms with Austria in disregard of French interests. Even so, France had concluded a defensive alliance with Prussia in 1747, and the maintenance of the Anglo-Austrian alignment after 1748 was deemed essential by the Duke of Newcastle, British secretary of state in the ministry of his brother Henry Pelham. The collapse of that system and the aligning of France with Austria and of Great Britain with Prussia constituted what is known as the “diplomatic revolution” or the “reversal of alliances.” Despite the debatable strategic success and the operational failure of the descent on Rochefort, William Pitt—who saw purpose in this type of asymmetric enterprise—prepared to continue such operations. An army was assembled under the command of Charles Spencer, 3rd Duke of Marlborough; he was aided by Lord George Sackville. The naval squadron and transports for the expedition were commanded by Richard Howe. The army landed on 5 June 1758 at Cancalle Bay, proceeded to St. Malo, and, finding that it would take prolonged siege to capture it, instead attacked the nearby port of St. Servan. It burned shipping in the harbor, roughly 80 French privateers and merchantmen, as well as four warships which were under construction. The force then re-embarked under threat of the arrival of French relief forces. An attack on Havre de Grace was called off, and the fleet sailed on to Cherbourg; the weather being bad and provisions low, that too was abandoned, and the expedition returned having damaged French privateering and provided further strategic demonstration against the French coast. By this point Frederick was increasingly concerned by the Russian advance from the east and marched to counter it. Just east of the Oder in Brandenburg-Neumark, at the Battle of Zorndorf (now Sarbinowo, Poland), a Prussian army of 35,000 men under Frederick on Aug. 25, 1758, fought a Russian army of 43,000 commanded by Count William Fermor. Both sides suffered heavy casualties – the Prussians 12,800, the Russians 18,000 – but the Russians withdrew, and Frederick claimed victory. In the undecided Battle of Tornow on 25 September, a Swedish army repulsed six assaults by a Prussian army but did not push on Berlin following the Battle of Fehrbellin. The year 1759 saw several Prussian defeats. At the Battle of Kay, or Paltzig, the Russian Count Saltykov with 47,000 Russians defeated 26,000 Prussians commanded by General Carl Heinrich von Wedel. Though the Hanoverians defeated an army of 60,000 French at Minden, Austrian general Daun forced the surrender of an entire Prussian corps of 13,000 in the Battle of Maxen. Frederick himself lost half his army in the Battle of Kunersdorf (now Kunowice Poland), the worst defeat in his military career and one that drove him to the brink of abdication and thoughts of suicide. The disaster resulted partly from his misjudgment of the Russians, who had already demonstrated their strength at Zorndorf and at Gross-Jägersdorf (now Motornoye, Russia), and partly from good cooperation between the Russian and Austrian forces. The Saxon and Austrian armies were unprepared, and their forces were scattered. Frederick occupied Dresden with little or no opposition from the Saxons. At the Battle of Lobositz on 1 October 1756, Frederick prevented the isolated Saxon army from being reinforced by an Austrian army under General Browne. The Prussians then occupied Saxony; after the Siege of Pirna, the Saxon army surrendered in October 1756, and was forcibly incorporated into the Prussian army. The attack on neutral Saxony caused outrage across Europe and led to the strengthening of the anti-Prussian coalition. The only significant Austrian success was the partial occupation of Silesia. Far from being easy, Frederick's early successes proved indecisive and very costly for Prussia's smaller army. This led him to remark that he did not fight the same Austrians as he had during the previous war. The history of the Seven Years' War in North America, particularly the expulsion of the Acadians, the siege of Quebec, the death of Wolfe, and the Battle of Fort William Henry generated a vast number of ballads, broadsides, images, and novels (see Longfellow's Evangeline, Benjamin West's The Death of General Wolfe, James Fenimore Cooper's The Last of the Mohicans), maps and other printed materials, which testify to how this event held the imagination of the British and North American public long after Wolfe's death in 1759. In 1756 Austria was making military preparations for war with Prussia and pursuing an alliance with Russia for this purpose. On June 2, 1746, Austria and Russia concluded a defensive alliance that covered their own territory and Poland against attack by Prussia or the Ottoman Empire. They also agreed to a secret clause that promised the restoration of Silesia and the countship of Glatz (now Kłodzko, Poland) to Austria in the event of hostilities with Prussia. Their real desire, however, was to destroy Frederick’s power altogether, reducing his sway to his electorate of Brandenburg and giving East Prussia to Poland, an exchange that would be accompanied by the cession of the Polish Duchy of Courland to Russia. Aleksey Petrovich, Graf (count) Bestuzhev-Ryumin, grand chancellor of Russia under Empress Elizabeth, was hostile to both France and Prussia, but he could not persuade Austrian statesman Wenzel Anton von Kaunitz to commit to offensive designs against Prussia so long as Prussia was able to rely on French support. All of Britain's campaigns against New France succeeded in 1759, part of what became known as an Annus Mirabilis. Fort Niagara and Fort Carillon on 8 July 1758 fell to sizable British forces, cutting off French frontier forts further west. On 13 September 1759, following a three-month siege of Quebec, General James Wolfe defeated the French on the Plains of Abraham outside the city. The French staged a counteroffensive in the spring of 1760, with initial success at the Battle of Sainte-Foy, but they were unable to retake Quebec, due to British naval superiority following the battle of Neuville. The French forces retreated to Montreal, where on 8 September they surrendered to overwhelming British numerical superiority. The carefully coded word in the agreement proved no less catalytic for the other European powers. The results were absolute chaos. Empress Elizabeth of Russia was outraged at the duplicity of Britain's position. Not only that France was so enraged, and terrified, by the sudden betrayal of its only ally. Austria, particularly Kaunitz, used this situation to their utmost advantage. The now-isolated France was forced to accede to the Austro-Russian alliance or face ruin. Thereafter, on May 1, 1756, the First Treaty of Versailles was signed, in which both nations pledged 24.000 troops to defend each other in the case of an attack. This diplomatic revolution proved to be an important cause of the war; although both treaties were self-defensive in nature, the actions of both coalitions made the war virtually inevitable. The British government was close to bankruptcy, and Britain now faced the delicate task of pacifying its new French-Canadian subjects as well as the many American Indian tribes who had supported France. George III's Proclamation of 1763, which forbade white settlement beyond the crest of the Appalachians, was intended to appease the latter but led to considerable outrage in the Thirteen Colonies, whose inhabitants were eager to acquire native lands. The Quebec Act of 1774, similarly intended to win over the loyalty of French Canadians, also spurred resentment among American colonists. The act protected Catholic religion and French language, which enraged the Americans, but the Québécois remained loyal and did not rebel. The war was continuing indecisively when on 14 October Marshal Daun's Austrians surprised the main Prussian army at the Battle of Hochkirch in Saxony. Frederick lost much of his artillery but retreated in good order, helped by dense woods. The Austrians had ultimately made little progress in the campaign in Saxony despite Hochkirch and had failed to achieve a decisive breakthrough. After a thwarted attempt to take Dresden, Daun's troops were forced to withdraw to Austrian territory for the winter, so that Saxony remained under Prussian occupation. At the same time, the Russians failed in an attempt to take Kolberg in Pomerania (now Kołobrzeg, Poland) from the Prussians. Not only that, Austria now found herself estranged with the new developments within the empire itself. Beside the rise of Prussia, Augustus III, although ineffective, could mustered up an army not only from Saxony, but also Poland, considering the elector was also the King of Poland. Bavaria's growing power and independence was also apparent as she had more voices on the path that its army should have taken, and managed to slip out of the war at its own will. Most importantly, with the now somehow-belligerent Hanover united personally under George III of Great Britain, It can amassed a considerable power, even brought Britain in, on the future conflicts. This power dynamic is important to the future and the latter conflicts of the empire. The war also proved that Maria Theresa's reforms were still not enough to compete with Prussia: unlike its enemy, the Austrians went almost bankrupt at the end of war. Hence, she dedicated the next two decades to the consolidation of her administration. Frederick saw Saxony and Polish west Prussia as potential fields for expansion but could not expect French support if he started an aggressive war for them. If he joined the French against the British in the hope of annexing Hanover, he might fall victim to an Austro-Russian attack. The hereditary elector of Saxony, Augustus III, was also elective King of Poland as Augustus III, but the two territories were physically separated by Brandenburg and Silesia. Neither state could pose as a great power. Saxony was merely a buffer between Prussia and Austrian Bohemia, whereas Poland, despite its union with the ancient lands of Lithuania, was prey to pro-French and pro-Russian factions. A Prussian scheme for compensating Frederick Augustus with Bohemia in exchange for Saxony obviously presupposed further spoliation of Austria. The French planned to invade the British Isles during 1759 by accumulating troops near the mouth of the Loire and concentrating their Brest and Toulon fleets. However, two sea defeats prevented this. In August, the Mediterranean fleet under Jean-François de La Clue-Sabran was scattered by a larger British fleet under Edward Boscawen at the Battle of Lagos. In the Battle of Quiberon Bay on 20 November, the British admiral Edward Hawke with 23 ships of the line caught the French Brest fleet with 21 ships of the line under Marshal de Conflans and sank, captured, or forced many of them aground, putting an end to the French plans. In the attempt to satisfy Austria at the time, Britain gave their electoral vote in Hanover for the candidacy of Maria Theresa's son, Joseph, as the Holy Roman Emperor, much to the dismay of Frederick and Prussia. Not only that, Britain would soon join the Austro-Russian alliance, but complications arose. Britain's basic framework for the alliance itself was to protect Hanover's interests against France. While at the same time, Kaunitz kept approaching the French in the hope of establishing such alliance with Austria. Not only that, France had no intention to ally with Russia, who meddled with their affairs in Austria's succession war, years earlier, and saw the complete dismemberment of Prussia as unacceptable to the stability of Central Europe. Great Britain lost Minorca in the Mediterranean to the French in 1756 but captured the French colonies in Senegal in 1758. The British Royal Navy took the French sugar colonies of Guadeloupe in 1759 and Martinique in 1762 as well as the Spanish cities of Havana in Cuba, and Manila in the Philippines, both prominent Spanish colonial cities. However, expansion into the hinterlands of both cities met with stiff resistance. In the Philippines, the British were confined to Manila until their agreed upon withdrawal at the war's end. Britain had been surprised by the sudden Prussian offensive but now began shipping supplies and ₤670,000 (equivalent to ₤89.9 million in 2015) to its new ally. A combined force of allied German states was organised by the British to protect Hanover from French invasion, under the command of the Duke of Cumberland. The British attempted to persuade the Dutch Republic to join the alliance, but the request was rejected, as the Dutch wished to remain fully neutral. Despite the huge disparity in numbers, the year had been successful for the Prussian-led forces on the continent, in contrast to disappointing British campaigns in North America. In early 1758, Frederick launched an invasion of Moravia, and laid siege to Olmütz (now Olomouc, Czech Republic). Following an Austrian victory at the Battle of Domstadtl that wiped out a supply convoy destined for Olmütz, Frederick broke off the siege and withdrew from Moravia. It marked the end of his final attempt to launch a major invasion of Austrian territory. East Prussia had been occupied by Russian forces over the winter and would remain under their control until 1762, although Frederick did not see the Russians as an immediate threat and instead entertained hopes of first fighting a decisive battle against Austria that would knock them out of the war. Pitt now prepared to send troops into Germany; and both Marlborough and Sackville, disgusted by what they perceived as the futility of the ""descents"", obtained commissions in that army. The elderly General Bligh was appointed to command a new ""descent"", escorted by Howe. The campaign began propitiously with the Raid on Cherbourg. Covered by naval bombardment, the army drove off the French force detailed to oppose their landing, captured Cherbourg, and destroyed its fortifications, docks, and shipping. Between 10 and 17 October 1757, a Hungarian general, Count András Hadik, serving in the Austrian army, executed what may be the most famous hussar action in history. When the Prussian King Frederick was marching south with his powerful armies, the Hungarian general unexpectedly swung his force of 5,000, mostly hussars, around the Prussians and occupied part of their capital, Berlin, for one night. The city was spared for a negotiated ransom of 200,000 thalers. When Frederick heard about this humiliating occupation, he immediately sent a larger force to free the city. Hadik, however, left the city with his Hussars and safely reached the Austrian lines. Subsequently, Hadik was promoted to the rank of Marshal in the Austrian army. In India, the British retained the Northern Circars, but returned all the French trading ports. The treaty, however, required that the fortifications of these settlements be destroyed and never rebuilt, while only minimal garrisons could be maintained there, thus rendering them worthless as military bases. Combined with the loss of France's ally in Bengal and the defection of Hyderabad to the British as a result of the war, this effectively brought French power in India to an end, making way for British hegemony and eventual control of the subcontinent. Realizing that war was imminent, Prussia preemptively struck Saxony and quickly overran it. The result caused uproar across Europe. Because of Prussia's alliance with Britain, Austria formed an alliance with France, seeing an opportunity to recapture Silesia, which had been lost in a previous war. Reluctantly, by following the imperial diet, most of the states of the empire joined Austria's cause. The Anglo-Prussian alliance was joined by smaller German states (especially Hanover). Sweden, fearing Prussia's expansionist tendencies, went to war in 1757 to protect its Baltic dominions, seeing its chance when virtually all of Europe opposed Prussia. Spain, bound by the Pacte de Famille, intervened on behalf of France and together they launched an utterly unsuccessful invasion of Portugal in 1762. The Russian Empire was originally aligned with Austria, fearing Prussia's ambition on the Polish-Lithuanian Commonwealth, but switched sides upon the succession of Tsar Peter III in 1762. In 1756 and 1757 the French captured forts Oswego and William Henry from the British. The latter victory was marred when France's native allies broke the terms of capitulation and attacked the retreating British column, which was under French guard, slaughtering and scalping soldiers and taking captive many men, women and children while the French refused to protect their captives. French naval deployments in 1757 also successfully defended the key Fortress of Louisbourg on Cape Breton Island, securing the seaward approaches to Quebec. The Anglo-French hostilities were ended in 1763 by the Treaty of Paris, which involved a complex series of land exchanges, the most important being France's cession to Spain of Louisiana, and to Great Britain the rest of New France except for the islands of St. Pierre and Miquelon. Faced with the choice of retrieving either New France or its Caribbean island colonies of Guadeloupe and Martinique, France chose the latter to retain these lucrative sources of sugar, writing off New France as an unproductive, costly territory. France also returned Minorca to the British. Spain lost control of Florida to Great Britain, but it received from the French the Île d'Orléans and all of the former French holdings west of the Mississippi River. The exchanges suited the British as well, as their own Caribbean islands already supplied ample sugar, and, with the acquisition of New France and Florida, they now controlled all of North America east of the Mississippi. In early 1757, Frederick II again took the initiative by marching into the Kingdom of Bohemia, hoping to inflict a decisive defeat on Austrian forces. After winning the bloody Battle of Prague on 6 May 1757, in which both forces suffered major casualties, the Prussians forced the Austrians back into the fortifications of Prague. The Prussian army then laid siege to the city. Following the battle at Prague, Frederick took 5,000 troops from the siege at Prague and sent them to reinforce the 19,000-man army under the Duke of Brunswick-Bevern at Kolin in Bohemia. Austrian Marshal Daun arrived too late to participate in the battle of Prague, but picked up 16,000 men who had escaped from the battle. With this army he slowly moved to relieve Prague. The Prussian army was too weak to simultaneously besiege Prague and keep Daun away, and Frederick was forced to attack prepared positions. The resulting Battle of Kolin was a sharp defeat for Frederick, his first military defeat. His losses further forced him to lift the siege and withdraw from Bohemia altogether. Realizing that war was imminent, Prussia preemptively struck Saxony and quickly overran it. The result caused uproar across Europe. Because of Prussia's alliance with Britain, Austria formed an alliance with France, seeing an opportunity to recapture Silesia, which had been lost in a previous war. Reluctantly, by following the imperial diet, most of the states of the empire joined Austria's cause. The Anglo-Prussian alliance was joined by smaller German states (especially Hanover). Sweden, fearing Prussia's expansionist tendencies, went to war in 1757 to protect its Baltic dominions, seeing its chance when virtually all of Europe opposed Prussia. Spain, bound by the Pacte de Famille, intervened on behalf of France and together they launched a disastrous invasion of Portugal in 1762. The Russian Empire was originally aligned with Austria, fearing Prussia's ambition on the Polish-Lithuanian Commonwealth, but switched sides upon the succession of Tsar Peter III in 1762. Things were looking grim for Prussia now, with the Austrians mobilising to attack Prussian-controlled soil and a French army under Soubise approaching from the west. However, in November and December of 1757, the whole situation in Germany was reversed. First, Frederick devastated Prince Soubise's French force at the Battle of Rossbach on 5 November 1757 and then routed a vastly superior Austrian force at the Battle of Leuthen on 5 December 1757 With these victories, Frederick once again established himself as Europe's premier general and his men as Europe's most accomplished soldiers. In spite of this, the Prussians were now facing the prospect of four major powers attacking on four fronts (France from the West, Austria from the South, Russia from the East and Sweden from the North). Meanwhile, a combined force from a number of smaller German states such as Bavaria had been established under Austrian leadership, thus threatening Prussian control of Saxony. For much of the eighteenth century, France approached its wars in the same way. It would let colonies defend themselves or would offer only minimal help (sending them limited numbers of troops or inexperienced soldiers), anticipating that fights for the colonies would most likely be lost anyway. This strategy was to a degree forced upon France: geography, coupled with the superiority of the British navy, made it difficult for the French navy to provide significant supplies and support to French colonies. Similarly, several long land borders made an effective domestic army imperative for any French ruler. Given these military necessities, the French government, unsurprisingly, based its strategy overwhelmingly on the army in Europe: it would keep most of its army on the continent, hoping for victories closer to home. The plan was to fight to the end of hostilities and then, in treaty negotiations, to trade territorial acquisitions in Europe to regain lost overseas possessions. This approach did not serve France well in the war, as the colonies were indeed lost, but although much of the European war went well, by its end France had few counterbalancing European successes. By 1763, the war in Central Europe was essentially a stalemate. Frederick had retaken most of Silesia and Saxony but not the latter's capital, Dresden. His financial situation was not dire, but his kingdom was devastated and his army severely weakened. His manpower had dramatically decreased, and he had lost so many effective officers and generals that a new offensive was perhaps impossible. British subsidies had been stopped by the new Prime Minister Lord Bute, and the Russian Emperor had been overthrown by his wife, Catherine, who ended Russia's alliance with Prussia and withdrew from the war. Austria, however, like most participants, was facing a severe financial crisis and had to decrease the size of its army, something which greatly affected its offensive power. Indeed, after having effectively sustained a long war, its administration was in disarray. By that time, it still held Dresden, the southeastern parts of Saxony, the county of Glatz, and southern Silesia, but the prospect of victory was dim without Russian support. In 1763 a peace settlement was reached at the Treaty of Hubertusburg, ending the war in central Europe. The British Prime Minister, the Duke of Newcastle, was optimistic that the new series of alliances could prevent war from breaking out in Europe. However, a large French force was assembled at Toulon, and the French opened the campaign against the British by an attack on Minorca in the Mediterranean. A British attempt at relief was foiled at the Battle of Minorca, and the island was captured on 28 June (for which Admiral Byng was court-martialed and executed). War between Britain and France had been formally declared on 18 May nearly two years after fighting had broken out in the Ohio Country. Prussia emerged from the war as a great power whose importance could no longer be challenged. Frederick the Great’s personal reputation was enormously enhanced, as his debt to fortune (Russia’s volte-face after Elizabeth’s death) and to the British subsidy were soon forgotten while the memory of his energy and his military genius was strenuously kept alive. Russia, on the other hand, made one great invisible gain from the war: the elimination of French influence in Poland. The First Partition of Poland (1772) was to be a Russo-Prussian transaction, with Austria only reluctantly involved and with France simply ignored. Later that summer, the Russians invaded Memel with 75,000 troops. Memel had one of the strongest fortresses in Prussia. However, after five days of artillery bombardment the Russian army was able to storm it. The Russians then used Memel as a base to invade East Prussia and defeated a smaller Prussian force in the fiercely contested Battle of Gross-Jägersdorf on 30 August 1757. However, it was not yet able to take Königsberg and retreated soon afterward. Still, it was a new threat to Prussia. Not only was Frederick forced to break off his invasion of Bohemia, he was now forced to withdraw further into Prussian-controlled territory. His defeats on the battlefield brought still more opportunist nations into the war. Sweden declared war on Prussia and invaded Pomerania with 17,000 men. Sweden felt this small army was all that was needed to occupy Pomerania and felt the Swedish army would not need to engage with the Prussians because the Prussians were occupied on so many other fronts. William Pitt, who entered the cabinet in 1756, had a grand vision for the war that made it entirely different from previous wars with France. As prime minister Pitt committed Britain to a grand strategy of seizing the entire French Empire, especially its possessions in North America and India. Britain's main weapon was the Royal Navy, which could control the seas and bring as many invasion troops as were needed. He also planned to use colonial forces from the Thirteen American colonies, working under the command of British regulars, to invade new France. In order to tie the French army down he subsidized his European allies. Pitt Head of the government from 1756 to 1761, and even after that the British continued his strategy. It proved completely successful. Pitt had a clear appreciation of the enormous value of imperial possessions, and realized how vulnerable was the French Empire. The war was successful for Great Britain, which gained the bulk of New France in North America, Spanish Florida, some individual Caribbean islands in the West Indies, the colony of Senegal on the West African coast, and superiority over the French trading outposts on the Indian subcontinent. The Native American tribes were excluded from the settlement; a subsequent conflict, known as Pontiac's War, was also unsuccessful in returning them to their pre-war status. In Europe, the war began disastrously for Prussia, but a combination of good luck and successful strategy saw King Frederick the Great manage to retrieve the Prussian position and retain the status quo ante bellum. Prussia emerged as a new European great power. Although Austria failed to retrieve the territory of Silesia from Prussia (its original goal) its military prowess was also noted by the other powers. The involvement of Portugal, Spain and Sweden did not return them to their former status as great powers. France was deprived of many of its colonies and had saddled itself with heavy war debts that its inefficient financial system could barely handle. Spain lost Florida but gained French Louisiana and regained control of its colonies, e.g., Cuba and the Philippines, which had been captured by the British during the war. France and other European powers avenged their defeat in 1778 when the American Revolutionary War broke out, with hopes of destroying Britain's dominance once and for all. This problem was compounded when the main Hanoverian army under Cumberland was defeated at the Battle of Hastenbeck and forced to surrender entirely at the Convention of Klosterzeven following a French Invasion of Hanover. The Convention removed Hanover and Brunswick from the war, leaving the Western approach to Prussian territory extremely vulnerable. Frederick sent urgent requests to Britain for more substantial assistance, as he was now without any outside military support for his forces in Germany. On the eastern front, progress was very slow. The Russian army was heavily dependent upon its main magazines in Poland, and the Prussian army launched several successful raids against them. One of them, led by general Platen in September resulted in the loss of 2,000 Russians, mostly captured, and the destruction of 5,000 wagons. Deprived of men, the Prussians had to resort to this new sort of warfare, raiding, to delay the advance of their enemies. Nonetheless, at the end of the year, they suffered two critical setbacks. The Russians under Zakhar Chernyshev and Pyotr Rumyantsev stormed Kolberg in Pomerania, while the Austrians captured Schweidnitz. The loss of Kolberg cost Prussia its last port on the Baltic Sea. In Britain, it was speculated that a total Prussian collapse was now imminent. Calculating that no further Russian advance was likely until 1758, Frederick moved the bulk of his eastern forces to Pomerania under the command of Marshal Lehwaldt where they were to repel the Swedish invasion. In short order, the Prussian army drove the Swedes back, occupied most of Swedish Pomerania, and blockaded its capital Stralsund. George II of Great Britain, on the advice of his British ministers, revoked the Convention of Klosterzeven, and Hanover reentered the war. Over the winter the new commander of the Hanoverian forces, Duke Ferdinand of Brunswick, regrouped his army and launched a series of offensives that drove the French back across the River Rhine. The British had suffered further defeats in North America, particularly at Fort William Henry. At home, however, stability had been established. Since 1756, successive governments led by Newcastle and Pitt had fallen. In August 1757, the two men agreed to a political partnership and formed a coalition government that gave new, firmer direction to the war effort. The new strategy emphasised both Newcastle's commitment to British involvement on the Continent, particularly in defence of Germany, and William Pitt's determination to use naval power to seize French colonies around the globe. This ""dual strategy"" would dominate British policy for the next five years. Frederick II of Prussia had received reports of the clashes in North America and had formed an alliance with Great Britain. On 29 August 1756, he led Prussian troops across the border of Saxony, one of the small German states in league with Austria. He intended this as a bold pre-emption of an anticipated Austro-French invasion of Silesia. He had three goals in his new war on Austria. First, he would seize Saxony and eliminate it as a threat to Prussia, then using the Saxon army and treasury to aid the Prussian war effort. His second goal was to advance into Bohemia where he might set up winter quarters at Austria's expense. Thirdly, he wanted to invade Moravia from Silesia, seize the fortress at Olmütz, and advance on Vienna to force an end to the war. French policy was, moreover, complicated by the existence of the le Secret du roi—a system of private diplomacy conducted by King Louis XV. Unbeknownst to his foreign minister, Louis had established a network of agents throughout Europe with the goal of pursuing personal political objectives that were often at odds with France’s publicly stated policies. Louis’s goals for le Secret du roi included an attempt to win the Polish crown for his kinsman Louis François de Bourbon, prince de Conti, and the maintenance of Poland, Sweden, and Turkey as French client states in opposition to Russian and Austrian interests. The Seven Years' War was fought between 1755 and 1764, the main conflict occurring in the seven-year period from 1756 to 1763. It involved every great power of the time except the Ottoman Empire, and affected Europe, the Americas, West Africa, India, and the Philippines. Considered a prelude to the two world wars and the greatest European war since the Thirty Years War of the 17th century, it once again split Europe into two coalitions, led by Great Britain on one side and France on the other. For the first time, aiming to curtail Britain and Prussia's ever-growing might, France formed a grand coalition of its own, which ended with failure as Britain rose as the world's predominant power, altering the European balance of power."
Communication,"In a slightly more complex form a sender and a receiver are linked reciprocally. This second attitude of communication, referred to as the constitutive model or constructionist view, focuses on how an individual communicates as the determining factor of the way the message will be interpreted. Communication is viewed as a conduit; a passage in which information travels from one individual to another and this information becomes separate from the communication itself. A particular instance of communication is called a speech act. The sender's personal filters and the receiver's personal filters may vary depending upon different regional traditions, cultures, or gender; which may alter the intended meaning of message contents. In the presence of ""communication noise"" on the transmission channel (air, in this case), reception and decoding of content may be faulty, and thus the speech act may not achieve the desired effect. One problem with this encode-transmit-receive-decode model is that the processes of encoding and decoding imply that the sender and receiver each possess something that functions as a codebook, and that these two code books are, at the very least, similar if not identical. Although something like code books is implied by the model, they are nowhere represented in the model, which creates many conceptual difficulties. Nonverbal communication describes the process of conveying meaning in the form of non-word messages. Examples of nonverbal communication include haptic communication, chronemic communication, gestures, body language, facial expression, eye contact, and how one dresses. Nonverbal communication also relates to intent of a message. Examples of intent are voluntary, intentional movements like shaking a hand or winking, as well as involuntary, such as sweating. Speech also contains nonverbal elements known as paralanguage, e.g. rhythm, intonation, tempo, and stress. There may even be a pheromone component. Research has shown that up to 55% of human communication may occur through non-verbal facial expressions, and a further 38% through paralanguage. It affects communication most at the subconscious level and establishes trust. Likewise, written texts include nonverbal elements such as handwriting style, spatial arrangement of words and the use of emoticons to convey emotion. In a simple model, often referred to as the transmission model or standard view of communication, information or content (e.g. a message in natural language) is sent in some form (as spoken language) from an emisor/ sender/ encoder to a destination/ receiver/ decoder. This common conception of communication simply views communication as a means of sending and receiving information. The strengths of this model are simplicity, generality, and quantifiability. Claude Shannon and Warren Weaver structured this model based on the following elements: Communication is usually described along a few major dimensions: Message (what type of things are communicated), source / emisor / sender / encoder (by whom), form (in which form), channel (through which medium), destination / receiver / target / decoder (to whom), and Receiver. Wilbur Schram (1954) also indicated that we should also examine the impact that a message has (both desired and undesired) on the target of the message. Between parties, communication includes acts that confer knowledge and experiences, give advice and commands, and ask questions. These acts may take many forms, in one of the various manners of communication. The form depends on the abilities of the group communicating. Together, communication content and form make messages that are sent towards a destination. The target can be oneself, another person or being, another entity (such as a corporation or group of beings). Fungi communicate to coordinate and organize their growth and development such as the formation of Marcelia and fruiting bodies. Fungi communicate with their own and related species as well as with non fungal organisms in a great variety of symbiotic interactions, especially with bacteria, unicellular eukaryote, plants and insects through biochemicals of biotic origin. The biochemicals trigger the fungal organism to react in a specific manner, while if the same chemical molecules are not part of biotic messages, they do not trigger the fungal organism to react. This implies that fungal organisms can differentiate between molecules taking part in biotic messages and similar molecules being irrelevant in the situation. So far five different primary signalling molecules are known to coordinate different behavioral patterns such as filamentation, mating, growth, and pathogenicity. Behavioral coordination and production of signaling substances is achieved through interpretation processes that enables the organism to differ between self or non-self, a biotic indicator, biotic message from similar, related, or non-related species, and even filter out ""noise"", i.e. similar molecules without biotic content. Communication is observed within the plant organism, i.e. within plant cells and between plant cells, between plants of the same or related species, and between plants and non-plant organisms, especially in the root zone. Plant roots communicate with rhizome bacteria, fungi, and insects within the soil. These interactions are governed by syntactic, pragmatic, and semantic rules,[citation needed] and are possible because of the decentralized ""nervous system"" of plants. The original meaning of the word ""neuron"" in Greek is ""vegetable fiber"" and recent research has shown that most of the microorganism plant communication processes are neuron-like. Plants also communicate via volatiles when exposed to herbivory attack behavior, thus warning neighboring plants. In parallel they produce other volatiles to attract parasites which attack these herbivores. In stress situations plants can overwrite the genomes they inherited from their parents and revert to that of their grand- or great-grandparents.[citation needed] Family communication study looks at topics such as family rules, family roles or family dialectics and how those factors could affect the communication between family members. Researchers develop theories to understand communication behaviors. Family communication study also digs deep into certain time periods of family life such as marriage, parenthood or divorce and how communication stands in those situations. It is important for family members to understand communication as a trusted way which leads to a well constructed family. The first major model for communication was introduced by Claude Shannon and Warren Weaver for Bell Laboratories in 1949 The original model was designed to mirror the functioning of radio and telephone technologies. Their initial model consisted of three primary parts: sender, channel, and receiver. The sender was the part of a telephone a person spoke into, the channel was the telephone itself, and the receiver was the part of the phone where one could hear the other person. Shannon and Weaver also recognized that often there is static that interferes with one listening to a telephone conversation, which they deemed noise. Companies with limited resources may choose to engage in only a few of these activities, while larger organizations may employ a full spectrum of communications. Since it is difficult to develop such a broad range of skills, communications professionals often specialize in one or two of these areas but usually have at least a working knowledge of most of them. By far, the most important qualifications communications professionals can possess are excellent writing ability, good 'people' skills, and the capacity to think critically and strategically. Theories of coregulation describe communication as a creative and dynamic continuous process, rather than a discrete exchange of information. Canadian media scholar Harold Innis had the theory that people use different types of media to communicate and which one they choose to use will offer different possibilities for the shape and durability of society (Wark, McKenzie 1997). His famous example of this is using ancient Egypt and looking at the ways they built themselves out of media with very different properties stone and papyrus. Papyrus is what he called 'Space Binding'. it made possible the transmission of written orders across space, empires and enables the waging of distant military campaigns and colonial administration. The other is stone and 'Time Binding', through the construction of temples and the pyramids can sustain their authority generation to generation, through this media they can change and shape communication in their society (Wark, McKenzie 1997). The broad field of animal communication encompasses most of the issues in ethology. Animal communication can be defined as any behavior of one animal that affects the current or future behavior of another animal. The study of animal communication, called zoo semiotics (distinguishable from anthroposemiotics, the study of human communication) has played an important part in the development of ethology, sociobiology, and the study of animal cognition. Animal communication, and indeed the understanding of the animal world in general, is a rapidly growing field, and even in the 21st century so far, a great share of prior understanding related to diverse fields such as personal symbolic name use, animal emotions, animal culture and learning, and even sexual conduct, long thought to be well understood, has been revolutionized. A special field of animal communication has been investigated in more detail such as vibrational communication. Effective verbal or spoken communication is dependent on a number of factors and cannot be fully isolated from other important interpersonal skills such as non-verbal communication, listening skills and clarification. Human language can be defined as a system of symbols (sometimes known as lexemes) and the grammars (rules) by which the symbols are manipulated. The word ""language"" also refers to common properties of languages. Language learning normally occurs most intensively during human childhood. Most of the thousands of human languages use patterns of sound or gesture for symbols which enable communication with others around them. Languages tend to share certain properties, although there are exceptions. There is no defined line between a language and a dialect. Constructed languages such as Esperanto, programming languages, and various mathematical formalism is not necessarily restricted to the properties shared by human languages. Communication is two-way process not merely one-way."
Police,"Colquhoun's utilitarian approach to the problem – using a cost-benefit argument to obtain support from businesses standing to benefit – allowed him to achieve what Henry and John Fielding failed for their Bow Street detectives. Unlike the stipendiary system at Bow Street, the river police were full-time, salaried officers prohibited from taking private fees. His other contribution was the concept of preventive policing; his police were to act as a highly visible deterrent to crime by their permanent presence on the Thames. Colquhoun's innovations were a critical development leading up to Robert Peel's ""new"" police three decades later. In Canada, the Royal Newfoundland Constabulary was founded in 1729, making it the first police force in present-day Canada. It was followed in 1834 by the Toronto Police, and in 1838 by police forces in Montreal and Quebec City. A national force, the Dominion Police, was founded in 1868. Initially the Dominion Police provided security for parliament, but its responsibilities quickly grew. The famous Royal Northwest Mounted Police was founded in 1873. The merger of these two police forces in 1920 formed the world-famous Royal Canadian Mounted Police. In Miranda the court created safeguards against self-incriminating statements made after an arrest. The court held that ""The prosecution may not use statements, whether exculpatory or inculpatory, stemming from questioning initiated by law enforcement officers after a person has been taken into custody or otherwise deprived of his freedom of action in any significant way, unless it demonstrates the use of procedural safeguards effective to secure the Fifth Amendment's privilege against self-incrimination"" Meanwhile, the authorities in Glasgow, Scotland successfully petitioned the government to pass the Glasgow Police Act establishing the City of Glasgow Police in 1800. Other Scottish towns soon followed suit and set up their own police forces through acts of parliament. In Ireland, the Irish Constabulary Act of 1822 marked the beginning of the Royal Irish Constabulary. The Act established a force in each barony with chief constables and inspectors general under the control of the civil administration at Dublin Castle. By 1841 this force numbered over 8,600 men. In France during the Middle Ages, there were two Great Officers of the Crown of France with police responsibilities: The Marshal of France and the Constable of France. The military policing responsibilities of the Marshal of France were delegated to the Marshal's provost, whose force was known as the Marshalcy because its authority ultimately derived from the Marshal. The marshalcy dates back to the Hundred Years' 'War, and some historians trace it back to the early 12th century. Another organisation, the Constabulary (French: Connétablie), was under the command of the Constable of France. The constabulary was regularised as a military body in 1337. Under King Francis I (who reigned 1515–1547), the Maréchaussée was merged with the Constabulary. The resulting force was also known as the Maréchaussée, or, formally, the Constabulary and Marshalcy of France. In 1797, Patrick Colquhoun was able to persuade the West Indies merchants who operated at the Pool of London on the River Thames, to establish a police force at the docks to prevent rampant theft that was causing annual estimated losses of £500,000 worth of cargo. The idea of a police, as it then existed in France, was considered as a potentially undesirable foreign import. In building the case for the police in the face of England's firm anti-police sentiment, Colquhoun framed the political rationale on economic indicators to show that a police dedicated to crime prevention was ""perfectly congenial to the principle of the British constitution."" Moreover, he went so far as to praise the French system, which had reached ""the greatest degree of perfection"" in his estimation. In 1566, the first police investigator of Rio de Janeiro was recruited. By the 17th century, most captaincies already had local units with law enforcement functions. On July 9, 1775 a Cavalry Regiment was created in the state of Minas Gerais for maintaining law and order. In 1808, the Portuguese royal family relocated to Brazil, because of the French invasion of Portugal. King João VI established the ""Intendência Geral de Polícia"" (General Police Intendancy) for investigations. He also created a Royal Police Guard for Rio de Janeiro in 1809. In 1831, after independence, each province started organizing its local ""military police"", with order maintenance tasks. The Federal Railroad Police was created in 1852. Michel Foucault claims that the contemporary concept of police as a paid and funded functionary of the state was developed by German and French legal scholars and practitioners in Public administration and Statistics in the 17th and early 18th centuries, most notably with Nicolas Delamare's Traité de la Police (""Treatise on the Police""), first published in 1705. The German Polizeiwissenschaft (Science of Police) first theorized by Philipp von Hörnigk a 17th-century Austrian Political economist and civil servant and much more famously by Johann Heinrich Gottlob Justi who produced an important theoretical work known as Cameral science on the formulation of police. Foucault cites Magdalene Humpert author of Bibliographie der Kameralwissenschaften (1937) in which the author makes note of a substantial bibliography was produced of over 4000 pieces of the practice of Polizeiwissenschaft however, this maybe a mistranslation of Foucault's own work the actual source of Magdalene Humpert states over 14,000 items were produced from the 16th century dates ranging from 1520-1850. Law enforcement, however, constitutes only part of policing activity. Policing has included an array of activities in different situations, but the predominant ones are concerned with the preservation of order. In some societies, in the late 18th and early 19th centuries, these developed within the context of maintaining the class system and the protection of private property. Many police forces suffer from police corruption to a greater or lesser degree. The police force is usually a public sector service, meaning they are paid through taxes. Not a lot of empirical work on the practices of inter/transnational information and intelligence sharing has been undertaken. A notable exception is James Sheptycki's study of police cooperation in the English Channel region (2002), which provides a systematic content analysis of information exchange files and a description of how these transnational information and intelligence exchanges are transformed into police case-work. The study showed that transnational police information sharing was routinized in the cross-Channel region from 1968 on the basis of agreements directly between the police agencies and without any formal agreement between the countries concerned. By 1992, with the signing of the Schengen Treaty, which formalized aspects of police information exchange across the territory of the European Union, there were worries that much, if not all, of this intelligence sharing was opaque, raising questions about the efficacy of the accountability mechanisms governing police information sharing in Europe (Joubert and Bevers, 1996). Motorcycles are also commonly used, particularly in locations that a car may not be able to reach, to control potential public order situations involving meetings of motorcyclists and often in escort duties where motorcycle police officers can quickly clear a path for escorted vehicles. Bicycle patrols are used in some areas because they allow for more open interaction with the public. In addition, their quieter operation can facilitate approaching suspects unawares and can help in pursuing them attempting to escape on foot. Edwin Chadwick's 1829 article, ""Preventive police"" in the London Review, argued that prevention ought to be the primary concern of a police body, which was not the case in practice. The reason, argued Chadwick, was that ""A preventive police would act more immediately by placing difficulties in obtaining the objects of temptation."" In contrast to a deterrent of punishment, a preventive police force would deter criminality by making crime cost-ineffective - ""crime doesn't pay"". In the second draft of his 1829 Police Act, the ""object"" of the new Metropolitan Police, was changed by Robert Peel to the ""principal object,"" which was the ""prevention of crime."" Later historians would attribute the perception of England's ""appearance of orderliness and love of public order"" to the preventive principle entrenched in Peel's police system. The word ""police"" was borrowed from French into the English language in the 18th century, but for a long time it applied only to French and continental European police forces. The word, and the concept of police itself, were ""disliked as a symbol of foreign oppression"" (according to Britannica 1911). Before the 19th century, the first use of the word ""police"" recorded in government documents in the United Kingdom was the appointment of Commissioners of Police for Scotland in 1714 and the creation of the Marine Police in 1798. In the American Old West, policing was often of very poor quality.[citation needed] The Army often provided some policing alongside poorly resourced sheriffs and temporarily organized posses.[citation needed] Public organizations were supplemented by private contractors, notably the Pinkerton National Detective Agency, which was hired by individuals, businessmen, local governments and the federal government. At its height, the Pinkerton Agency's numbers exceeded those of the United States Army.[citation needed] As conceptualized by the Polizeiwissenschaft,according to Foucault the police had an administrative,economic and social duty (""procuring abundance""). It was in charge of demographic concerns and needed to be incorporated within the western political philosophy system of raison d'état and therefore giving the superficial appearance of empowering the population (and unwittingly supervising the population), which, according to mercantilist theory, was to be the main strength of the state. Thus, its functions largely overreached simple law enforcement activities and included public health concerns, urban planning (which was important because of the miasma theory of disease; thus, cemeteries were moved out of town, etc.), and surveillance of prices. In Terry v. Ohio (1968) the court divided seizure into two parts, the investigatory stop and arrest. The court further held that during an investigatory stop a police officer's search "" [is] confined to what [is] minimally necessary to determine whether [a suspect] is armed, and the intrusion, which [is] made for the sole purpose of protecting himself and others nearby, [is] confined to ascertaining the presence of weapons"" (U.S. Supreme Court). Before Terry, every police encounter constituted an arrest, giving the police officer the full range of search authority. Search authority during a Terry stop (investigatory stop) is limited to weapons only. All police officers in the United Kingdom, whatever their actual rank, are 'constables' in terms of their legal position. This means that a newly appointed constable has the same arrest powers as a Chief Constable or Commissioner. However, certain higher ranks have additional powers to authorize certain aspects of police operations, such as a power to authorize a search of a suspect's house (section 18 PACE in England and Wales) by an officer of the rank of Inspector, or the power to authorize a suspect's detention beyond 24 hours by a Superintendent. The 1829 Metropolitan Police Act created a modern police force by limiting the purview of the force and its powers, and envisioning it as merely an organ of the judicial system. Their job was apolitical; to maintain the peace and apprehend criminals for the courts to process according to the law. This was very different to the 'Continental model' of the police force that had been developed in France, where the police force worked within the parameters of the absolutist state as an extension of the authority of the monarch and functioned as part of the governing state. Studies of this kind outside of Europe are even rarer, so it is difficult to make generalizations, but one small-scale study that compared transnational police information and intelligence sharing practices at specific cross-border locations in North America and Europe confirmed that low visibility of police information and intelligence sharing was a common feature (Alain, 2001). Intelligence-led policing is now common practice in most advanced countries (Ratcliffe, 2007) and it is likely that police intelligence sharing and information exchange has a common morphology around the world (Ratcliffe, 2007). James Sheptycki has analyzed the effects of the new information technologies on the organization of policing-intelligence and suggests that a number of 'organizational pathologies' have arisen that make the functioning of security-intelligence processes in transnational policing deeply problematic. He argues that transnational police information circuits help to ""compose the panic scenes of the security-control society"". The paradoxical effect is that, the harder policing agencies work to produce security, the greater are feelings of insecurity. The terms international policing, transnational policing, and/or global policing began to be used from the early 1990s onwards to describe forms of policing that transcended the boundaries of the sovereign nation-state (Nadelmann, 1993), (Sheptycki, 1995). These terms refer in variable ways to practices and forms for policing that, in some sense, transcend national borders. This includes a variety of practices, but international police cooperation, criminal intelligence exchange between police agencies working in different nation-states, and police development-aid to weak, failed or failing states are the three types that have received the most scholarly attention. Modern police forces make extensive use of radio communications equipment, carried both on the person and installed in vehicles, to co-ordinate their work, share information, and get help quickly. In recent years, vehicle-installed computers have enhanced the ability of police communications, enabling easier dispatching of calls, criminal background checks on persons of interest to be completed in a matter of seconds, and updating officers' daily activity log and other, required reports on a real-time basis. Other common pieces of police equipment include flashlights/torches, whistles, police notebooks and ""ticket books"" or citations. This office was first held by Gabriel Nicolas de la Reynie, who had 44 commissaires de police (police commissioners) under his authority. In 1709, these commissioners were assisted by inspecteurs de police (police inspectors). The city of Paris was divided into 16 districts policed by the commissaires, each assigned to a particular district and assisted by a growing bureaucracy. The scheme of the Paris police force was extended to the rest of France by a royal edict of October 1699, resulting in the creation of lieutenants general of police in all large French cities and towns. Police development-aid to weak, failed or failing states is another form of transnational policing that has garnered attention. This form of transnational policing plays an increasingly important role in United Nations peacekeeping and this looks set to grow in the years ahead, especially as the international community seeks to develop the rule of law and reform security institutions in States recovering from conflict (Goldsmith and Sheptycki, 2007) With transnational police development-aid the imbalances of power between donors and recipients are stark and there are questions about the applicability and transportability of policing models between jurisdictions (Hills, 2009). Law enforcement in Ancient China was carried out by ""prefects"" for thousands of years since it developed in both the Chu and Jin kingdoms of the Spring and Autumn period. In Jin, dozens of prefects were spread across the state, each having limited authority and employment period. They were appointed by local magistrates, who reported to higher authorities such as governors, who in turn were appointed by the emperor, and they oversaw the civil administration of their ""prefecture"", or jurisdiction. Under each prefect were ""subprefects"" who helped collectively with law enforcement in the area. Some prefects were responsible for handling investigations, much like modern police detectives. Prefects could also be women. The concept of the ""prefecture system"" spread to other cultures such as Korea and Japan. With the initial investment of £4,200, the new trial force of the Thames River Police began with about 50 men charged with policing 33,000 workers in the river trades, of whom Colquhoun claimed 11,000 were known criminals and ""on the game."" The force was a success after its first year, and his men had ""established their worth by saving £122,000 worth of cargo and by the rescuing of several lives."" Word of this success spread quickly, and the government passed the Marine Police Bill on 28 July 1800, transforming it from a private to public police agency; now the oldest police force in the world. Colquhoun published a book on the experiment, The Commerce and Policing of the River Thames. It found receptive audiences far outside London, and inspired similar forces in other cities, notably, New York City, Dublin, and Sydney. In contrast, the police are entitled to protect private rights in some jurisdictions. To ensure that the police would not interfere in the regular competencies of the courts of law, some police acts require that the police may only interfere in such cases where protection from courts cannot be obtained in time, and where, without interference of the police, the realization of the private right would be impeded. This would, for example, allow police to establish a restaurant guest's identity and forward it to the innkeeper in a case where the guest cannot pay the bill at nighttime because his wallet had just been stolen from the restaurant table. They can also be armed with non-lethal (more accurately known as ""less than lethal"" or ""less-lethal"") weaponry, particularly for riot control. Non-lethal weapons include batons, tear gas, riot control agents, rubber bullets, riot shields, water cannons and electroshock weapons. Police officers often carry handcuffs to restrain suspects. The use of firearms or deadly force is typically a last resort only to be used when necessary to save human life, although some jurisdictions (such as Brazil) allow its use against fleeing felons and escaped convicts. A ""shoot-to-kill"" policy was recently introduced in South Africa, which allows police to use deadly force against any person who poses a significant threat to them or civilians. With the country having one of the highest rates of violent crime, president Jacob Zuma states that South Africa needs to handle crime differently from other countries. As one of their first acts after end of the War of the Castilian Succession in 1479, Ferdinand and Isabella established the centrally organized and efficient Holy Brotherhood (Santa Hermandad) as a national police force. They adapted an existing brotherhood to the purpose of a general police acting under officials appointed by themselves, and endowed with great powers of summary jurisdiction even in capital cases. The original brotherhoods continued to serve as modest local police-units until their final suppression in 1835. Despite popular conceptions promoted by movies and television, many US police departments prefer not to maintain officers in non-patrol bureaus and divisions beyond a certain period of time, such as in the detective bureau, and instead maintain policies that limit service in such divisions to a specified period of time, after which officers must transfer out or return to patrol duties.[citation needed] This is done in part based upon the perception that the most important and essential police work is accomplished on patrol in which officers become acquainted with their beats, prevent crime by their presence, respond to crimes in progress, manage crises, and practice their skills.[citation needed] Historical studies reveal that policing agents have undertaken a variety of cross-border police missions for many years (Deflem, 2002). For example, in the 19th century a number of European policing agencies undertook cross-border surveillance because of concerns about anarchist agitators and other political radicals. A notable example of this was the occasional surveillance by Prussian police of Karl Marx during the years he remained resident in London. The interests of public police agencies in cross-border co-operation in the control of political radicalism and ordinary law crime were primarily initiated in Europe, which eventually led to the establishment of Interpol before the Second World War. There are also many interesting examples of cross-border policing under private auspices and by municipal police forces that date back to the 19th century (Nadelmann, 1993). It has been established that modern policing has transgressed national boundaries from time to time almost from its inception. It is also generally agreed that in the post–Cold War era this type of practice became more significant and frequent (Sheptycki, 2000). A police force is a constituted body of persons empowered by the state to enforce the law, protect property, and limit civil disorder. Their powers include the legitimized use of force. The term is most commonly associated with police services of a sovereign state that are authorized to exercise the police power of that state within a defined legal or territorial area of responsibility. Police forces are often defined as being separate from military or other organizations involved in the defense of the state against foreign aggressors; however, gendarmerie are military units charged with civil policing. In the United States, August Vollmer introduced other reforms, including education requirements for police officers. O.W. Wilson, a student of Vollmer, helped reduce corruption and introduce professionalism in Wichita, Kansas, and later in the Chicago Police Department. Strategies employed by O.W. Wilson included rotating officers from community to community to reduce their vulnerability to corruption, establishing of a non-partisan police board to help govern the police force, a strict merit system for promotions within the department, and an aggressive recruiting drive with higher police salaries to attract professionally qualified officers. During the professionalism era of policing, law enforcement agencies concentrated on dealing with felonies and other serious crime, rather than broader focus on crime prevention. The first centrally organised police force was created by the government of King Louis XIV in 1667 to police the city of Paris, then the largest city in Europe. The royal edict, registered by the Parlement of Paris on March 15, 1667 created the office of lieutenant général de police (""lieutenant general of police""), who was to be the head of the new Paris police force, and defined the task of the police as ""ensuring the peace and quiet of the public and of private individuals, purging the city of what may cause disturbances, procuring abundance, and having each and everyone live according to their station and their duties"". Unmarked vehicles are used primarily for sting operations or apprehending criminals without alerting them to their presence. Some police forces use unmarked or minimally marked cars for traffic law enforcement, since drivers slow down at the sight of marked police vehicles and unmarked vehicles make it easier for officers to catch speeders and traffic violators. This practice is controversial, with for example, New York State banning this practice in 1996 on the grounds that it endangered motorists who might be pulled over by people impersonating police officers. Perhaps the greatest question regarding the future development of transnational policing is: in whose interest is it? At a more practical level, the question translates into one about how to make transnational policing institutions democratically accountable (Sheptycki, 2004). For example, according to the Global Accountability Report for 2007 (Lloyd, et al. 2007) Interpol had the lowest scores in its category (IGOs), coming in tenth with a score of 22% on overall accountability capabilities (p. 19). As this report points out, and the existing academic literature on transnational policing seems to confirm, this is a secretive area and one not open to civil society involvement. Peel, widely regarded as the father of modern policing, was heavily influenced by the social and legal philosophy of Jeremy Bentham, who called for a strong and centralized, but politically neutral, police force for the maintenance of social order, for the protection of people from crime and to act as a visible deterrent to urban crime and disorder. Peel decided to standardise the police force as an official paid profession, to organise it in a civilian fashion, and to make it answerable to the public."
Karl_Popper,"In an interview that Popper gave in 1969 with the condition that it shall be kept secret until after his death, he summarised his position on God as follows: ""I don't know whether God exists or not. ... Some forms of atheism are arrogant and ignorant and should be rejected, but agnosticism—to admit that we don't know and to search—is all right. ... When I look at what I call the gift of life, I feel a gratitude which is in tune with some religious ideas of God. However, the moment I even speak of it, I am embarrassed that I may do something wrong to God in talking about God."" He objected to organised religion, saying ""it tends to use the name of God in vain"", noting the danger of fanaticism because of religious conflicts: ""The whole thing goes back to myths which, though they may have a kernel of truth, are untrue. Why then should the Jewish myth be true and the Indian and Egyptian myths not be true?"" In a letter unrelated to the interview, he stressed his tolerant attitude: ""Although I am not for religion, I do think that we should show respect for anybody who believes honestly."" Popper left school at the age of 16 and attended lectures in mathematics, physics, philosophy, psychology and the history of music as a guest student at the University of Vienna. In 1919, Popper became attracted by Marxism and subsequently joined the Association of Socialist School Students. He also became a member of the Social Democratic Workers' Party of Austria, which was at that time a party that fully adopted the Marxist ideology. After the street battle in the Hörlgasse on 15 June 1919, when police shot eight of his unarmed party comrades, he became disillusioned by what he saw to be the ""pseudo-scientific"" historical materialism of Marx, abandoned the ideology, and remained a supporter of social liberalism throughout his life. In 2004, philosopher and psychologist Michel ter Hark (Groningen, The Netherlands) published a book, called Popper, Otto Selz and the rise of evolutionary epistemology, in which he claimed that Popper took some of his ideas from his tutor, the German psychologist Otto Selz. Selz never published his ideas, partly because of the rise of Nazism, which forced him to quit his work in 1933, and the prohibition of referring to Selz' work. Popper, the historian of ideas and his scholarship, is criticised in some academic quarters for his rejection of Plato, Hegel and Marx. Popper contrasts his views with the notion of the ""hopeful monster"" that has large phenotype mutations and calls it the ""hopeful behavioural monster"". After behaviour has changed radically, small but quick changes of the phenotype follow to make the organism fitter to its changed goals. This way it looks as if the phenotype were changing guided by some invisible hand, while it is merely natural selection working in combination with the new behaviour. For example, according to this hypothesis, the eating habits of the giraffe must have changed before its elongated neck evolved. Popper contrasted this view as ""evolution from within"" or ""active Darwinism"" (the organism actively trying to discover new ways of life and being on a quest for conquering new ecological niches), with the naturalistic ""evolution from without"" (which has the picture of a hostile environment only trying to kill the mostly passive organism, or perhaps segregate some of its groups). Popper won many awards and honours in his field, including the Lippincott Award of the American Political Science Association, the Sonning Prize, the Otto Hahn Peace Medal of the United Nations Association of Germany in Berlin and fellowships in the Royal Society, British Academy, London School of Economics, King's College London, Darwin College, Cambridge, and Charles University, Prague. Austria awarded him the Grand Decoration of Honour in Gold for Services to the Republic of Austria in 1986, and the Federal Republic of Germany its Grand Cross with Star and Sash of the Order of Merit, and the peace class of the Order Pour le Mérite. He received the Humanist Laureate Award from the International Academy of Humanism. He was knighted by Queen Elizabeth II in 1965, and was elected a Fellow of the Royal Society in 1976. He was invested with the Insignia of a Companion of Honour in 1982. Another objection is that it is not always possible to demonstrate falsehood definitively, especially if one is using statistical criteria to evaluate a null hypothesis. More generally it is not always clear, if evidence contradicts a hypothesis, that this is a sign of flaws in the hypothesis rather than of flaws in the evidence. However, this is a misunderstanding of what Popper's philosophy of science sets out to do. Rather than offering a set of instructions that merely need to be followed diligently to achieve science, Popper makes it clear in The Logic of Scientific Discovery that his belief is that the resolution of conflicts between hypotheses and observations can only be a matter of the collective judgment of scientists, in each individual case. Popper died of ""complications of cancer, pneumonia and kidney failure"" in Kenley at the age of 92 on 17 September 1994. He had been working continuously on his philosophy until two weeks before, when he suddenly fell terminally ill. After cremation, his ashes were taken to Vienna and buried at Lainzer cemetery adjacent to the ORF Centre, where his wife Josefine Anna Popper (called ‘Hennie’) had already been buried. Popper's estate is managed by his secretary and personal assistant Melitta Mew and her husband Raymond. Popper's manuscripts went to the Hoover Institution at Stanford University, partly during his lifetime and partly as supplementary material after his death. Klagenfurt University possesses Popper's library, including his precious bibliophilia, as well as hard copies of the original Hoover material and microfilms of the supplementary material. The remaining parts of the estate were mostly transferred to The Karl Popper Charitable Trust. In October 2008 Klagenfurt University acquired the copyrights from the estate. Popper puzzled over the stark contrast between the non-scientific character of Freud and Adler's theories in the field of psychology and the revolution set off by Einstein's theory of relativity in physics in the early 20th century. Popper thought that Einstein's theory, as a theory properly grounded in scientific thought and method, was highly ""risky"", in the sense that it was possible to deduce consequences from it which were, in the light of the then-dominant Newtonian physics, highly improbable (e.g., that light is deflected towards solid bodies—confirmed by Eddington's experiments in 1919), and which would, if they turned out to be false, falsify the whole theory. In contrast, nothing could, even in principle, falsify psychoanalytic theories. He thus came to the conclusion that psychoanalytic theories had more in common with primitive myths than with genuine science. About the creation-evolution controversy, Popper wrote that he considered it ""a somewhat sensational clash between a brilliant scientific hypothesis concerning the history of the various species of animals and plants on earth, and an older metaphysical theory which, incidentally, happened to be part of an established religious belief"" with a footnote to the effect that ""[he] agree[s] with Professor C.E. Raven when, in his Science, Religion, and the Future, 1943, he calls this conflict ""a storm in a Victorian tea-cup""; though the force of this remark is perhaps a little impaired by the attention he pays to the vapours still emerging from the cup—to the Great Systems of Evolutionist Philosophy, produced by Bergson, Whitehead, Smuts, and others."" Popper played a vital role in establishing the philosophy of science as a vigorous, autonomous discipline within philosophy, through his own prolific and influential works, and also through his influence on his own contemporaries and students. Popper founded in 1946 the Department of Philosophy, Logic and Scientific Method at the London School of Economics and there lectured and influenced both Imre Lakatos and Paul Feyerabend, two of the foremost philosophers of science in the next generation of philosophy of science. (Lakatos significantly modified Popper's position,:1 and Feyerabend repudiated it entirely, but the work of both is deeply influenced by Popper and engaged with many of the problems that Popper set.) In a book called Science Versus Crime, Houck writes that Popper's falsificationism can be questioned logically: it is not clear how Popper would deal with a statement like ""for every metal, there is a temperature at which it will melt."" The hypothesis cannot be falsified by any possible observation, for there will always be a higher temperature than tested at which the metal may in fact melt, yet it seems to be a valid scientific hypothesis. These examples were pointed out by Carl Gustav Hempel. Hempel came to acknowledge that Logical Positivism's verificationism was untenable, but argued that falsificationism was equally untenable on logical grounds alone. The simplest response to this is that, because Popper describes how theories attain, maintain and lose scientific status, individual consequences of currently accepted scientific theories are scientific in the sense of being part of tentative scientific knowledge, and both of Hempel's examples fall under this category. For instance, atomic theory implies that all metals melt at some temperature. In his early years Popper was impressed by Marxism, whether of Communists or socialists. An event that happened in 1919 had a profound effect on him: During a riot, caused by the Communists, the police shot several unarmed people, including some of Popper's friends, when they tried to free party comrades from prison. The riot had, in fact, been part of a plan by which leaders of the Communist party with connections to Béla Kun tried to take power by a coup; Popper did not know about this at that time. However, he knew that the riot instigators were swayed by the Marxist doctrine that class struggle would produce vastly more dead men than the inevitable revolution brought about as quickly as possible, and so had no scruples to put the life of the rioters at risk to achieve their selfish goal of becoming the future leaders of the working class. This was the start of his later criticism of historicism. Popper began to reject Marxist historicism, which he associated with questionable means, and later socialism, which he associated with placing equality before freedom (to the possible disadvantage of equality). Logically, no number of positive outcomes at the level of experimental testing can confirm a scientific theory, but a single counterexample is logically decisive: it shows the theory, from which the implication is derived, to be false. To say that a given statement (e.g., the statement of a law of some scientific theory) -- [call it ""T""] -- is ""falsifiable"" does not mean that ""T"" is false. Rather, it means that, if ""T"" is false, then (in principle), ""T"" could be shown to be false, by observation or by experiment. Popper's account of the logical asymmetry between verification and falsifiability lies at the heart of his philosophy of science. It also inspired him to take falsifiability as his criterion of demarcation between what is, and is not, genuinely scientific: a theory should be considered scientific if, and only if, it is falsifiable. This led him to attack the claims of both psychoanalysis and contemporary Marxism to scientific status, on the basis that their theories are not falsifiable. In response to a given problem situation (), a number of competing conjectures, or tentative theories (), are systematically subjected to the most rigorous attempts at falsification possible. This process, error elimination (), performs a similar function for science that natural selection performs for biological evolution. Theories that better survive the process of refutation are not more true, but rather, more ""fit""—in other words, more applicable to the problem situation at hand (). Consequently, just as a species' biological fitness does not ensure continued survival, neither does rigorous testing protect a scientific theory from refutation in the future. Yet, as it appears that the engine of biological evolution has, over many generations, produced adaptive traits equipped to deal with more and more complex problems of survival, likewise, the evolution of theories through the scientific method may, in Popper's view, reflect a certain type of progress: toward more and more interesting problems (). For Popper, it is in the interplay between the tentative theories (conjectures) and error elimination (refutation) that scientific knowledge advances toward greater and greater problems; in a process very much akin to the interplay between genetic variation and natural selection. Popper held that rationality is not restricted to the realm of empirical or scientific theories, but that it is merely a special case of the general method of criticism, the method of finding and eliminating contradictions in knowledge without ad-hoc-measures. According to this view, rational discussion about metaphysical ideas, about moral values and even about purposes is possible. Popper's student W.W. Bartley III tried to radicalise this idea and made the controversial claim that not only can criticism go beyond empirical knowledge, but that everything can be rationally criticised. Knowledge, for Popper, was objective, both in the sense that it is objectively true (or truthlike), and also in the sense that knowledge has an ontological status (i.e., knowledge as object) independent of the knowing subject (Objective Knowledge: An Evolutionary Approach, 1972). He proposed three worlds: World One, being the physical world, or physical states; World Two, being the world of mind, or mental states, ideas, and perceptions; and World Three, being the body of human knowledge expressed in its manifold forms, or the products of the second world made manifest in the materials of the first world (i.e., books, papers, paintings, symphonies, and all the products of the human mind). World Three, he argued, was the product of individual human beings in exactly the same sense that an animal path is the product of individual animals, and that, as such, has an existence and evolution independent of any individual knowing subjects. The influence of World Three, in his view, on the individual human mind (World Two) is at least as strong as the influence of World One. In other words, the knowledge held by a given individual mind owes at least as much to the total accumulated wealth of human knowledge, made manifest, as to the world of direct experience. As such, the growth of human knowledge could be said to be a function of the independent evolution of World Three. Many contemporary philosophers, such as Daniel Dennett, have not embraced Popper's Three World conjecture, due mostly, it seems, to its resemblance to mind-body dualism. Karl Popper was born in Vienna (then in Austria-Hungary) in 1902, to upper middle-class parents. All of Karl Popper's grandparents were Jewish, but the Popper family converted to Lutheranism before Karl was born, and so he received Lutheran baptism. They understood this as part of their cultural assimilation, not as an expression of devout belief. Karl's father Simon Siegmund Carl Popper was a lawyer from Bohemia and a doctor of law at the Vienna University, and mother Jenny Schiff was of Silesian and Hungarian descent. After establishing themselves in Vienna, the Poppers made a rapid social climb in Viennese society: Simon Siegmund Carl became a partner in the law firm of Vienna's liberal Burgomaster Herr Grübl and, after Grübl's death in 1898, Simon took over the business. (Malachi Hacohen records that Herr Grübl's first name was Raimund, after which Karl received his middle name. Popper himself, in his autobiography, erroneously recalls that Herr Grübl's first name was Carl.) His father was a bibliophile who had 12,000–14,000 volumes in his personal library. Popper inherited both the library and the disposition from him. In 1937, Popper finally managed to get a position that allowed him to emigrate to New Zealand, where he became lecturer in philosophy at Canterbury University College of the University of New Zealand in Christchurch. It was here that he wrote his influential work The Open Society and its Enemies. In Dunedin he met the Professor of Physiology John Carew Eccles and formed a lifelong friendship with him. In 1946, after the Second World War, he moved to the United Kingdom to become reader in logic and scientific method at the London School of Economics. Three years later, in 1949, he was appointed professor of logic and scientific method at the University of London. Popper was president of the Aristotelian Society from 1958 to 1959. He retired from academic life in 1969, though he remained intellectually active for the rest of his life. In 1985, he returned to Austria so that his wife could have her relatives around her during the last months of her life; she died in November that year. After the Ludwig Boltzmann Gesellschaft failed to establish him as the director of a newly founded branch researching the philosophy of science, he went back again to the United Kingdom in 1986, settling in Kenley, Surrey. To Popper, who was an anti-justificationist, traditional philosophy is misled by the false principle of sufficient reason. He thinks that no assumption can ever be or needs ever to be justified, so a lack of justification is not a justification for doubt. Instead, theories should be tested and scrutinised. It is not the goal to bless theories with claims of certainty or justification, but to eliminate errors in them. He writes, ""there are no such things as good positive reasons; nor do we need such things [...] But [philosophers] obviously cannot quite bring [themselves] to believe that this is my opinion, let alone that it is right"" (The Philosophy of Karl Popper, p. 1043) He worked in street construction for a short amount of time, but was unable to cope with the heavy labour. Continuing to attend university as a guest student, he started an apprenticeship as cabinetmaker, which he completed as a journeyman. He was dreaming at that time of starting a daycare facility for children, for which he assumed the ability to make furniture might be useful. After that he did voluntary service in one of psychoanalyst Alfred Adler's clinics for children. In 1922, he did his matura by way of a second chance education and finally joined the University as an ordinary student. He completed his examination as an elementary teacher in 1924 and started working at an after-school care club for socially endangered children. In 1925, he went to the newly founded Pädagogisches Institut and continued studying philosophy and psychology. Around that time he started courting Josefine Anna Henninger, who later became his wife. Upon this basis, along with that of the logical content of assertions (where logical content is inversely proportional to probability), Popper went on to develop his important notion of verisimilitude or ""truthlikeness"". The intuitive idea behind verisimilitude is that the assertions or hypotheses of scientific theories can be objectively measured with respect to the amount of truth and falsity that they imply. And, in this way, one theory can be evaluated as more or less true than another on a quantitative basis which, Popper emphasises forcefully, has nothing to do with ""subjective probabilities"" or other merely ""epistemic"" considerations. In All Life is Problem Solving, Popper sought to explain the apparent progress of scientific knowledge – that is, how it is that our understanding of the universe seems to improve over time. This problem arises from his position that the truth content of our theories, even the best of them, cannot be verified by scientific testing, but can only be falsified. Again, in this context the word ""falsified"" does not refer to something being ""fake""; rather, that something can be (i.e., is capable of being) shown to be false by observation or experiment. Some things simply do not lend themselves to being shown to be false, and therefore, are not falsifiable. If so, then how is it that the growth of science appears to result in a growth in knowledge? In Popper's view, the advance of scientific knowledge is an evolutionary process characterised by his formula: Popper had his own sophisticated views on evolution that go much beyond what the frequently-quoted passages say. In effect, Popper agreed with some of the points of both creationists and naturalists, but also disagreed with both views on crucial aspects. Popper understood the universe as a creative entity that invents new things, including life, but without the necessity of something like a god, especially not one who is pulling strings from behind the curtain. He said that evolution must, as the creationists say, work in a goal-directed way but disagreed with their view that it must necessarily be the hand of god that imposes these goals onto the stage of life. Gray does not, however, give any indication of what available evidence these theories were at odds with, and his appeal to ""crucial support"" illustrates the very inductivist approach to science that Popper sought to show was logically illegitimate. For, according to Popper, Einstein's theory was at least equally as well corroborated as Newton's upon its initial conception; they both equally well accounted for all the hitherto available evidence. Moreover, since Einstein also explained the empirical refutations of Newton's theory, general relativity was immediately deemed suitable for tentative acceptance on the Popperian account. Indeed, Popper wrote, several decades before Gray's criticism, in reply to a critical essay by Imre Lakatos: In The Open Society and Its Enemies and The Poverty of Historicism, Popper developed a critique of historicism and a defence of the ""Open Society"". Popper considered historicism to be the theory that history develops inexorably and necessarily according to knowable general laws towards a determinate end. He argued that this view is the principal theoretical presupposition underpinning most forms of authoritarianism and totalitarianism. He argued that historicism is founded upon mistaken assumptions regarding the nature of scientific law and prediction. Since the growth of human knowledge is a causal factor in the evolution of human history, and since ""no society can predict, scientifically, its own future states of knowledge"", it follows, he argued, that there can be no predictive science of human history. For Popper, metaphysical and historical indeterminism go hand in hand. The failure of democratic parties to prevent fascism from taking over Austrian politics in the 1920s and 1930s traumatised Popper. He suffered from the direct consequences of this failure, since events after the Anschluss, the annexation of Austria by the German Reich in 1938, forced him into permanent exile. His most important works in the field of social science—The Poverty of Historicism (1944) and The Open Society and Its Enemies (1945)—were inspired by his reflection on the events of his time and represented, in a sense, a reaction to the prevalent totalitarian ideologies that then dominated Central European politics. His books defended democratic liberalism as a social and political philosophy. They also represented extensive critiques of the philosophical presuppositions underpinning all forms of totalitarianism. In 1928, he earned a doctorate in psychology, under the supervision of Karl Bühler. His dissertation was entitled ""Die Methodenfrage der Denkpsychologie"" (The question of method in cognitive psychology). In 1929, he obtained the authorisation to teach mathematics and physics in secondary school, which he started doing. He married his colleague Josefine Anna Henninger (1906–1985) in 1930. Fearing the rise of Nazism and the threat of the Anschluss, he started to use the evenings and the nights to write his first book Die beiden Grundprobleme der Erkenntnistheorie (The Two Fundamental Problems of the Theory of Knowledge). He needed to publish one to get some academic position in a country that was safe for people of Jewish descent. However, he ended up not publishing the two-volume work, but a condensed version of it with some new material, Logik der Forschung (The Logic of Scientific Discovery), in 1934. Here, he criticised psychologism, naturalism, inductionism, and logical positivism, and put forth his theory of potential falsifiability as the criterion demarcating science from non-science. In 1935 and 1936, he took unpaid leave to go to the United Kingdom for a study visit. As early as 1934, Popper wrote of the search for truth as ""one of the strongest motives for scientific discovery."" Still, he describes in Objective Knowledge (1972) early concerns about the much-criticised notion of truth as correspondence. Then came the semantic theory of truth formulated by the logician Alfred Tarski and published in 1933. Popper writes of learning in 1935 of the consequences of Tarski's theory, to his intense joy. The theory met critical objections to truth as correspondence and thereby rehabilitated it. The theory also seemed, in Popper's eyes, to support metaphysical realism and the regulative idea of a search for truth. Other awards and recognition for Popper included the City of Vienna Prize for the Humanities (1965), Karl Renner Prize (1978), Austrian Decoration for Science and Art (1980), Dr. Leopold Lucas Prize (1981), Ring of Honour of the City of Vienna (1983) and the Premio Internazionale of the Italian Federico Nietzsche Society (1988). In 1992, he was awarded the Kyoto Prize in Arts and Philosophy for ""symbolising the open spirit of the 20th century"" and for his ""enormous influence on the formation of the modern intellectual climate"". He does not argue that any such conclusions are therefore true, or that this describes the actual methods of any particular scientist.[citation needed] Rather, it is recommended as an essential principle of methodology that, if enacted by a system or community, will lead to slow but steady progress of a sort (relative to how well the system or community enacts the method). It has been suggested that Popper's ideas are often mistaken for a hard logical account of truth because of the historical co-incidence of their appearing at the same time as logical positivism, the followers of which mistook his aims for their own. The Quine-Duhem thesis argues that it's impossible to test a single hypothesis on its own, since each one comes as part of an environment of theories. Thus we can only say that the whole package of relevant theories has been collectively falsified, but cannot conclusively say which element of the package must be replaced. An example of this is given by the discovery of the planet Neptune: when the motion of Uranus was found not to match the predictions of Newton's laws, the theory ""There are seven planets in the solar system"" was rejected, and not Newton's laws themselves. Popper discussed this critique of naïve falsificationism in Chapters 3 and 4 of The Logic of Scientific Discovery. For Popper, theories are accepted or rejected via a sort of selection process. Theories that say more about the way things appear are to be preferred over those that do not; the more generally applicable a theory is, the greater its value. Thus Newton's laws, with their wide general application, are to be preferred over the much more specific ""the solar system has seven planets"".[dubious – discuss] While there is some dispute as to the matter of influence, Popper had a long-standing and close friendship with economist Friedrich Hayek, who was also brought to the London School of Economics from Vienna. Each found support and similarities in the other's work, citing each other often, though not without qualification. In a letter to Hayek in 1944, Popper stated, ""I think I have learnt more from you than from any other living thinker, except perhaps Alfred Tarski."" Popper dedicated his Conjectures and Refutations to Hayek. For his part, Hayek dedicated a collection of papers, Studies in Philosophy, Politics, and Economics, to Popper, and in 1982 said, ""...ever since his Logik der Forschung first came out in 1934, I have been a complete adherent to his general theory of methodology."" Karl Popper's rejection of Marxism during his teenage years left a profound mark on his thought. He had at one point joined a socialist association, and for a few months in 1919 considered himself a communist. During this time he became familiar with the Marxist view of economics, class-war, and history. Although he quickly became disillusioned with the views expounded by Marxism, his flirtation with the ideology led him to distance himself from those who believed that spilling blood for the sake of a revolution was necessary. He came to realise that when it came to sacrificing human lives, one was to think and act with extreme prudence. Popper coined the term ""critical rationalism"" to describe his philosophy. Concerning the method of science, the term indicates his rejection of classical empiricism, and the classical observationalist-inductivist account of science that had grown out of it. Popper argued strongly against the latter, holding that scientific theories are abstract in nature, and can be tested only indirectly, by reference to their implications. He also held that scientific theory, and human knowledge generally, is irreducibly conjectural or hypothetical, and is generated by the creative imagination to solve problems that have arisen in specific historico-cultural settings. According to this theory, the conditions for the truth of a sentence as well as the sentences themselves are part of a metalanguage. So, for example, the sentence ""Snow is white"" is true if and only if snow is white. Although many philosophers have interpreted, and continue to interpret, Tarski's theory as a deflationary theory, Popper refers to it as a theory in which ""is true"" is replaced with ""corresponds to the facts"". He bases this interpretation on the fact that examples such as the one described above refer to two things: assertions and the facts to which they refer. He identifies Tarski's formulation of the truth conditions of sentences as the introduction of a ""metalinguistic predicate"" and distinguishes the following cases: Popper claimed to have recognised already in the 1934 version of his Logic of Discovery a fact later stressed by Kuhn, ""that scientists necessarily develop their ideas within a definite theoretical framework"", and to that extent to have anticipated Kuhn's central point about ""normal science"". (But Popper criticised what he saw as Kuhn's relativism.) Also, in his collection Conjectures and Refutations: The Growth of Scientific Knowledge (Harper & Row, 1963), Popper writes, ""Science must begin with myths, and with the criticism of myths; neither with the collection of observations, nor with the invention of experiments, but with the critical discussion of myths, and of magical techniques and practices. The scientific tradition is distinguished from the pre-scientific tradition in having two layers. Like the latter, it passes on its theories; but it also passes on a critical attitude towards them. The theories are passed on, not as dogmas, but rather with the challenge to discuss them and improve upon them."" Among his contributions to philosophy is his claim to have solved the philosophical problem of induction. He states that while there is no way to prove that the sun will rise, it is possible to formulate the theory that every day the sun will rise; if it does not rise on some particular day, the theory will be falsified and will have to be replaced by a different one. Until that day, there is no need to reject the assumption that the theory is true. Nor is it rational according to Popper to make instead the more complex assumption that the sun will rise until a given day, but will stop doing so the day after, or similar statements with additional conditions. The creation–evolution controversy in the United States raises the issue of whether creationistic ideas may be legitimately called science and whether evolution itself may be legitimately called science. In the debate, both sides and even courts in their decisions have frequently invoked Popper's criterion of falsifiability (see Daubert standard). In this context, passages written by Popper are frequently quoted in which he speaks about such issues himself. For example, he famously stated ""Darwinism is not a testable scientific theory, but a metaphysical research program—a possible framework for testable scientific theories."" He continued: According to John N. Gray, Popper held that ""a theory is scientific only in so far as it is falsifiable, and should be given up as soon as it is falsified."" By applying Popper's account of scientific method, Gray's Straw Dogs states that this would have ""killed the theories of Darwin and Einstein at birth."" When they were first advanced, Gray claims, each of them was ""at odds with some available evidence; only later did evidence become available that gave them crucial support."" Against this, Gray seeks to establish the irrationalist thesis that ""the progress of science comes from acting against reason."" Such a theory would be true with higher probability, because it cannot be attacked so easily: to falsify the first one, it is sufficient to find that the sun has stopped rising; to falsify the second one, one additionally needs the assumption that the given day has not yet been reached. Popper held that it is the least likely, or most easily falsifiable, or simplest theory (attributes which he identified as all the same thing) that explains known facts that one should rationally prefer. His opposition to positivism, which held that it is the theory most likely to be true that one should prefer, here becomes very apparent. It is impossible, Popper argues, to ensure a theory to be true; it is more important that its falsity can be detected as easily as possible. Instead, he formulated the spearhead model of evolution, a version of genetic pluralism. According to this model, living organisms themselves have goals, and act according to these goals, each guided by a central control. In its most sophisticated form, this is the brain of humans, but controls also exist in much less sophisticated ways for species of lower complexity, such as the amoeba. This control organ plays a special role in evolution—it is the ""spearhead of evolution"". The goals bring the purpose into the world. Mutations in the genes that determine the structure of the control may then cause drastic changes in behaviour, preferences and goals, without having an impact on the organism's phenotype. Popper postulates that such purely behavioural changes are less likely to be lethal for the organism compared to drastic changes of the phenotype. Popper is known for his rejection of the classical inductivist views on the scientific method, in favour of empirical falsification: A theory in the empirical sciences can never be proven, but it can be falsified, meaning that it can and should be scrutinized by decisive experiments. He used the black swan fallacy to discuss falsification. If the outcome of an experiment contradicts the theory, one should refrain from ad hoc manoeuvres that evade the contradiction merely by making it less falsifiable. Popper is also known for his opposition to the classical justificationist account of knowledge, which he replaced with critical rationalism, ""the first non-justificational philosophy of criticism in the history of philosophy."" This led Popper to conclude that what were regarded[by whom?] as the remarkable strengths of psychoanalytical theories were actually their weaknesses. Psychoanalytical theories were crafted in a way that made them able to refute any criticism and to give an explanation for every possible form of human behaviour. The nature of such theories made it impossible for any criticism or experiment - even in principle - to show them to be false. This realisation had an important consequence when Popper later tackled the problem of demarcation in the philosophy of science, as it led him to posit that the strength of a scientific theory lies in its both being susceptible to falsification, and not actually being falsified by criticism made of it. He considered that if a theory cannot, in principle, be falsified by criticism, it is not a scientific theory."
New_York_City,"New York is also a major center for non-commercial educational media. The oldest public-access television channel in the United States is the Manhattan Neighborhood Network, founded in 1971. WNET is the city's major public television station and a primary source of national Public Broadcasting Service (PBS) television programming. WNYC, a public radio station owned by the city until 1997, has the largest public radio audience in the United States. New York City also has an extensive web of expressways and parkways, which link the city's boroughs to each other as well as to northern New Jersey, Westchester County, Long Island, and southwestern Connecticut through various bridges and tunnels. Because these highways serve millions of outer borough and suburban residents who commute into Manhattan, it is quite common for motorists to be stranded for hours in traffic jams that are a daily occurrence, particularly during rush hour. On August 24, 1673, Dutch captain Anthonio Colve took over the colony of New York from England and rechristened it ""New Orange"" to honor the Prince of Orange, King William III. However, facing defeat from the British and French, who had teamed up to destroy Dutch trading routes, the Dutch returned the island to England in 1674. The character of New York's large residential districts is often defined by the elegant brownstone rowhouses and townhouses and shabby tenements that were built during a period of rapid expansion from 1870 to 1930. In contrast, New York City also has neighborhoods that are less densely populated and feature free-standing dwellings. In neighborhoods such as Riverdale (in the Bronx), Ditmas Park (in Brooklyn), and Douglaston (in Queens), large single-family homes are common in various architectural styles such as Tudor Revival and Victorian. The first documented visit by a European was in 1524 by Giovanni da Verrazzano, a Florentine explorer in the service of the French crown, who sailed his ship La Dauphine into New York Harbor. He claimed the area for France and named it ""Nouvelle Angoulême"" (New Angoulême). New York City's commuter rail network is the largest in North America. The rail network, connecting New York City to its suburbs, consists of the Long Island Rail Road, Metro-North Railroad, and New Jersey Transit. The combined systems converge at Grand Central Terminal and Pennsylvania Station and contain more than 250 stations and 20 rail lines. In Queens, the elevated AirTrain people mover system connects JFK International Airport to the New York City Subway and the Long Island Rail Road; a separate AirTrain system is planned alongside the Grand Central Parkway to connect LaGuardia Airport to these transit systems. For intercity rail, New York City is served by Amtrak, whose busiest station by a significant margin is Pennsylvania Station on the West Side of Manhattan, from which Amtrak provides connections to Boston, Philadelphia, and Washington, D.C. along the Northeast Corridor, as well as long-distance train service to other North American cities. The Great Irish Famine brought a large influx of Irish immigrants. Over 200,000 were living in New York by 1860, upwards of a quarter of the city's population. There was also extensive immigration from the German provinces, where revolutions had disrupted societies, and Germans comprised another 25% of New York's population by 1860. The city receives 49.9 inches (1,270 mm) of precipitation annually, which is fairly spread throughout the year. Average winter snowfall between 1981 and 2010 has been 25.8 inches (66 cm), but this varies considerably from year to year. Hurricanes and tropical storms are rare in the New York area, but are not unheard of and always have the potential to strike the area. Hurricane Sandy brought a destructive storm surge to New York City on the evening of October 29, 2012, flooding numerous streets, tunnels, and subway lines in Lower Manhattan and other areas of the city and cutting off electricity in many parts of the city and its suburbs. The storm and its profound impacts have prompted the discussion of constructing seawalls and other coastal barriers around the shorelines of the city and the metropolitan area to minimize the risk of destructive consequences from another such event in the future. Much of the scientific research in the city is done in medicine and the life sciences. New York City has the most post-graduate life sciences degrees awarded annually in the United States, with 127 Nobel laureates having roots in local institutions as of 2004; while in 2012, 43,523 licensed physicians were practicing in New York City. Major biomedical research institutions include Memorial Sloan–Kettering Cancer Center, Rockefeller University, SUNY Downstate Medical Center, Albert Einstein College of Medicine, Mount Sinai School of Medicine, and Weill Cornell Medical College, being joined by the Cornell University/Technion-Israel Institute of Technology venture on Roosevelt Island. Major tourist destinations include Times Square; Broadway theater productions; the Empire State Building; the Statue of Liberty; Ellis Island; the United Nations Headquarters; museums such as the Metropolitan Museum of Art; greenspaces such as Central Park and Washington Square Park; Rockefeller Center; the Manhattan Chinatown; luxury shopping along Fifth and Madison Avenues; and events such as the Halloween Parade in Greenwich Village; the Macy's Thanksgiving Day Parade; the lighting of the Rockefeller Center Christmas Tree; the St. Patrick's Day parade; seasonal activities such as ice skating in Central Park in the wintertime; the Tribeca Film Festival; and free performances in Central Park at Summerstage. Major attractions in the boroughs outside Manhattan include Flushing Meadows-Corona Park and the Unisphere in Queens; the Bronx Zoo; Coney Island, Brooklyn; and the New York Botanical Garden in the Bronx. The New York Wheel, a 630-foot ferris wheel, was under construction at the northern shore of Staten Island in 2015, overlooking the Statue of Liberty, New York Harbor, and the Lower Manhattan skyline. In the 1970s, job losses due to industrial restructuring caused New York City to suffer from economic problems and rising crime rates. While a resurgence in the financial industry greatly improved the city's economic health in the 1980s, New York's crime rate continued to increase through that decade and into the beginning of the 1990s. By the mid 1990s, crime rates started to drop dramatically due to revised police strategies, improving economic opportunities, gentrification, and new residents, both American transplants and new immigrants from Asia and Latin America. Important new sectors, such as Silicon Alley, emerged in the city's economy. New York's population reached all-time highs in the 2000 Census and then again in the 2010 Census. New York City has over 28,000 acres (110 km2) of municipal parkland and 14 miles (23 km) of public beaches. Parks in New York City include Central Park, Prospect Park, Flushing Meadows–Corona Park, Forest Park, and Washington Square Park. The largest municipal park in the city is Pelham Bay Park with 2,700 acres (1,093 ha). New York has been described as the ""Capital of Baseball"". There have been 35 Major League Baseball World Series and 73 pennants won by New York teams. It is one of only five metro areas (Los Angeles, Chicago, Baltimore–Washington, and the San Francisco Bay Area being the others) to have two baseball teams. Additionally, there have been 14 World Series in which two New York City teams played each other, known as a Subway Series and occurring most recently in 2000. No other metropolitan area has had this happen more than once (Chicago in 1906, St. Louis in 1944, and the San Francisco Bay Area in 1989). The city's two current Major League Baseball teams are the New York Mets, who play at Citi Field in Queens, and the New York Yankees, who play at Yankee Stadium in the Bronx. who compete in six games of interleague play every regular season that has also come to be called the Subway Series. The Yankees have won a record 27 championships, while the Mets have won the World Series twice. The city also was once home to the Brooklyn Dodgers (now the Los Angeles Dodgers), who won the World Series once, and the New York Giants (now the San Francisco Giants), who won the World Series five times. Both teams moved to California in 1958. There are also two Minor League Baseball teams in the city, the Brooklyn Cyclones and Staten Island Yankees. The New York Public Library, which has the largest collection of any public library system in the United States, serves Manhattan, the Bronx, and Staten Island. Queens is served by the Queens Borough Public Library, the nation's second largest public library system, while the Brooklyn Public Library serves Brooklyn. Organized crime has long been associated with New York City, beginning with the Forty Thieves and the Roach Guards in the Five Points in the 1820s. The 20th century saw a rise in the Mafia, dominated by the Five Families, as well as in gangs, including the Black Spades. The Mafia presence has declined in the city in the 21st century. New York City has a high degree of income disparity as indicated by its Gini Coefficient of 0.5 for the city overall and 0.6 for Manhattan. The disparity is driven by wage growth in high-income brackets, while wages have stagnated for middle and lower-income brackets. In the first quarter of 2014, the average weekly wage in New York County (Manhattan) was $2,749, representing the highest total among large counties in the United States. In 2013, New York City had the highest number of billionaires of any city in the world, higher than the next five U.S. cities combined, including former Mayor Michael R. Bloomberg. New York also had the highest density of millionaires per capita among major U.S. cities in 2014, at 4.6% of residents. Lower Manhattan has been experiencing a baby boom, with the area south of Canal Street witnessing 1,086 births in 2010, 12% greater than 2009 and over twice the number born in 2001. Forty of the city's theaters, with more than 500 seats each, are collectively known as Broadway, after the major thoroughfare that crosses the Times Square Theater District, sometimes referred to as ""The Great White Way"". According to The Broadway League, Broadway shows sold approximately US$1.27 billion worth of tickets in the 2013–2014 season, an 11.4% increase from US$1.139 billion in the 2012–2013 season. Attendance in 2013–2014 stood at 12.21 million, representing a 5.5% increase from the 2012–2013 season's 11.57 million. In the precolonial era, the area of present-day New York City was inhabited by various bands of Algonquian tribes of Native Americans, including the Lenape, whose homeland, known as Lenapehoking, included Staten Island; the western portion of Long Island, including the area that would become Brooklyn and Queens; Manhattan; the Bronx; and the Lower Hudson Valley. The Hudson River flows through the Hudson Valley into New York Bay. Between New York City and Troy, New York, the river is an estuary. The Hudson River separates the city from the U.S. state of New Jersey. The East River—a tidal strait—flows from Long Island Sound and separates the Bronx and Manhattan from Long Island. The Harlem River, another tidal strait between the East and Hudson Rivers, separates most of Manhattan from the Bronx. The Bronx River, which flows through the Bronx and Westchester County, is the only entirely fresh water river in the city. When one Republican presidential candidate for the 2016 election ridiculed the liberalism of ""New York values"" in January 2016, Donald Trump, leading in the polls, vigorously defended his city. The National Review, a conservative magazine published in the city since its founding by William F. Buckley, Jr. in 1955, commented, ""By hearkening back to New York's heart after 9/11, for a moment Trump transcended politics. How easily we forget, but for weeks after the terror attacks, New York was America."" Other important sectors include medical research and technology, non-profit institutions, and universities. Manufacturing accounts for a significant but declining share of employment, although the city's garment industry is showing a resurgence in Brooklyn. Food processing is a US$5 billion industry that employs more than 19,000 residents. The Staten Island Railway rapid transit system solely serves Staten Island, operating 24 hours a day. The Port Authority Trans-Hudson (PATH train) links Midtown and Lower Manhattan to northeastern New Jersey, primarily Hoboken, Jersey City, and Newark. Like the New York City Subway, the PATH operates 24 hours a day; meaning three of the six rapid transit systems in the world which operate on 24-hour schedules are wholly or partly in New York (the others are a portion of the Chicago 'L', the PATCO Speedline serving Philadelphia, and the Copenhagen Metro). Stone and brick became the city's building materials of choice after the construction of wood-frame houses was limited in the aftermath of the Great Fire of 1835. A distinctive feature of many of the city's buildings is the wooden roof-mounted water towers. In the 1800s, the city required their installation on buildings higher than six stories to prevent the need for excessively high water pressures at lower elevations, which could break municipal water pipes. Garden apartments became popular during the 1920s in outlying areas, such as Jackson Heights. The Queensboro Bridge is an important piece of cantilever architecture. The Manhattan Bridge, Throgs Neck Bridge, Triborough Bridge, and Verrazano-Narrows Bridge are all examples of Structural Expressionism. The traditional New York area accent is characterized as non-rhotic, so that the sound [ɹ] does not appear at the end of a syllable or immediately before a consonant; hence the pronunciation of the city name as ""New Yawk."" There is no [ɹ] in words like park [pɑək] or [pɒək] (with vowel backed and diphthongized due to the low-back chain shift), butter [bʌɾə], or here [hiə]. In another feature called the low back chain shift, the [ɔ] vowel sound of words like talk, law, cross, chocolate, and coffee and the often homophonous [ɔr] in core and more are tensed and usually raised more than in General American. In the most old-fashioned and extreme versions of the New York dialect, the vowel sounds of words like ""girl"" and of words like ""oil"" became a diphthong [ɜɪ]. This would often be misperceived by speakers of other accents as a reversal of the er and oy sounds, so that girl is pronounced ""goil"" and oil is pronounced ""erl""; this leads to the caricature of New Yorkers saying things like ""Joizey"" (Jersey), ""Toidy-Toid Street"" (33rd St.) and ""terlet"" (toilet). The character Archie Bunker from the 1970s sitcom All in the Family (played by Carroll O'Connor) was a notable example of having used this pattern of speech, which continues to fade in its overall presence. The city's population in 2010 was 44% white (33.3% non-Hispanic white), 25.5% black (23% non-Hispanic black), 0.7% Native American, and 12.7% Asian. Hispanics of any race represented 28.6% of the population, while Asians constituted the fastest-growing segment of the city's population between 2000 and 2010; the non-Hispanic white population declined 3 percent, the smallest recorded decline in decades; and for the first time since the Civil War, the number of blacks declined over a decade. Newtown Creek, a 3.5-mile (6-kilometer) a long estuary that forms part of the border between the boroughs of Brooklyn and Queens, has been designated a Superfund site for environmental clean-up and remediation of the waterway's recreational and economic resources for many communities. One of the most heavily used bodies of water in the Port of New York and New Jersey, it had been one of the most contaminated industrial sites in the country, containing years of discarded toxins, an estimated 30 million US gallons (110,000 m3) of spilled oil, including the Greenpoint oil spill, raw sewage from New York City's sewer system, and other accumulation. Manhattan's skyline, with its many skyscrapers, is universally recognized, and the city has been home to several of the tallest buildings in the world. As of 2011, New York City had 5,937 high-rise buildings, of which 550 completed structures were at least 330 feet (100 m) high, both second in the world after Hong Kong, with over 50 completed skyscrapers taller than 656 feet (200 m). These include the Woolworth Building (1913), an early gothic revival skyscraper built with massively scaled gothic detailing. Other features of the city's transportation infrastructure encompass more than 12,000 yellow taxicabs; various competing startup transportation network companies; and an aerial tramway that transports commuters between Roosevelt Island and Manhattan Island. New York City is situated in the Northeastern United States, in southeastern New York State, approximately halfway between Washington, D.C. and Boston. The location at the mouth of the Hudson River, which feeds into a naturally sheltered harbor and then into the Atlantic Ocean, has helped the city grow in significance as a trading port. Most of New York City is built on the three islands of Long Island, Manhattan, and Staten Island. The New York Islanders and the New York Rangers represent the city in the National Hockey League. Also within the metropolitan area are the New Jersey Devils, who play in nearby Newark, New Jersey. The New York City Fire Department (FDNY), provides fire protection, technical rescue, primary response to biological, chemical, and radioactive hazards, and emergency medical services for the five boroughs of New York City. The New York City Fire Department is the largest municipal fire department in the United States and the second largest in the world after the Tokyo Fire Department. The FDNY employs approximately 11,080 uniformed firefighters and over 3,300 uniformed EMTs and paramedics. The FDNY's motto is New York's Bravest. The FDNY headquarters is located at 9 MetroTech Center in Downtown Brooklyn, and the FDNY Fire Academy is located on Randalls Island. There are three Bureau of Fire Communications alarm offices which receive and dispatch alarms to appropriate units. One office, at 11 Metrotech Center in Brooklyn, houses Manhattan/Citywide, Brooklyn, and Staten Island Fire Communications. The Bronx and Queens offices are in separate buildings. The Mayor and council members are elected to four-year terms. The City Council is a unicameral body consisting of 51 council members whose districts are defined by geographic population boundaries. Each term for the mayor and council members lasts four years and has a three consecutive-term limit, but can resume after a four-year break. The New York City Administrative Code, the New York City Rules, and the City Record are the code of local laws, compilation of regulations, and official journal, respectively. New York's non-white population was 36,620 in 1890. New York City was a prime destination in the early twentieth century for African Americans during the Great Migration from the American South, and by 1916, New York City was home to the largest urban African diaspora in North America. The Harlem Renaissance of literary and cultural life flourished during the era of Prohibition. The larger economic boom generated construction of skyscrapers competing in height and creating an identifiable skyline. At the end of the Second Anglo-Dutch War, the English gained New Amsterdam (New York) in North America in exchange for Dutch control of Run, an Indonesian island. Several intertribal wars among the Native Americans and some epidemics brought on by contact with the Europeans caused sizable population losses for the Lenape between the years 1660 and 1670. By 1700, the Lenape population had diminished to 200. Real estate is a major force in the city's economy, as the total value of all New York City property was assessed at US$914.8 billion for the 2015 fiscal year. The Time Warner Center is the property with the highest-listed market value in the city, at US$1.1 billion in 2006. New York City is home to some of the nation's—and the world's—most valuable real estate. 450 Park Avenue was sold on July 2, 2007 for US$510 million, about $1,589 per square foot ($17,104/m²), breaking the barely month-old record for an American office building of $1,476 per square foot ($15,887/m²) set in the June 2007 sale of 660 Madison Avenue. According to Forbes, in 2014, Manhattan was home to six of the top ten zip codes in the United States by median housing price. In 1904, the steamship General Slocum caught fire in the East River, killing 1,021 people on board. In 1911, the Triangle Shirtwaist Factory fire, the city's worst industrial disaster, took the lives of 146 garment workers and spurred the growth of the International Ladies' Garment Workers' Union and major improvements in factory safety standards. Returning World War II veterans created a post-war economic boom and the development of large housing tracts in eastern Queens. New York emerged from the war unscathed as the leading city of the world, with Wall Street leading America's place as the world's dominant economic power. The United Nations Headquarters was completed in 1952, solidifying New York's global geopolitical influence, and the rise of abstract expressionism in the city precipitated New York's displacement of Paris as the center of the art world. A Spanish expedition led by captain Estêvão Gomes, a Portuguese sailing for Emperor Charles V, arrived in New York Harbor in January 1525 aboard the purpose-built caravel ""La Anunciada"" and charted the mouth of the Hudson River, which he named Rio de San Antonio. Heavy ice kept him from further exploration, and he returned to Spain in August. The first scientific map to show the North American East coast continuously, the 1527 world map known as the Padrón Real, was informed by Gomes' expedition, and labeled the Northeast as Tierra de Esteban Gómez in his honor. Throughout its history, the city has been a major port of entry for immigrants into the United States; more than 12 million European immigrants were received at Ellis Island between 1892 and 1924. The term ""melting pot"" was first coined to describe densely populated immigrant neighborhoods on the Lower East Side. By 1900, Germans constituted the largest immigrant group, followed by the Irish, Jews, and Italians. In 1940, whites represented 92% of the city's population. Christianity (59%), particularly Catholicism (33%), was the most prevalently practiced religion in New York as of 2014, followed by Judaism, with approximately 1.1 million Jews in New York City, over half living in Brooklyn. Islam ranks third in New York City, with official estimates ranging between 600,000 and 1,000,000 observers and including 10% of the city's public schoolchildren, followed by Hinduism, Buddhism, and a variety of other religions, as well as atheism. In 2014, 24% self-identified with no organized religious affiliation. The iconic New York City Subway system is the largest rapid transit system in the world when measured by stations in operation, with 469, and by length of routes. New York's subway is notable for nearly the entire system remaining open 24 hours a day, in contrast to the overnight shutdown common to systems in most cities, including Hong Kong, London, Paris, Seoul, and Tokyo. The New York City Subway is also the busiest metropolitan rail transit system in the Western Hemisphere, with 1.75 billion passengers rides in 2014, while Grand Central Terminal, also popularly referred to as ""Grand Central Station"", is the world's largest railway station by number of train platforms. The first non-Native American inhabitant of what would eventually become New York City was Dominican trader Juan Rodriguez (transliterated to Dutch as Jan Rodrigues). Born in Santo Domingo of Portuguese and African descent, he arrived in Manhattan during the winter of 1613–1614, trapping for pelts and trading with the local population as a representative of the Dutch. Broadway, from 159th Street to 218th Street, is named Juan Rodriguez Way in his honor. Under the Köppen climate classification, using the 0 °C (32 °F) coldest month (January) isotherm, New York City itself experiences a humid subtropical climate (Cfa) and is thus the northernmost major city on the North American continent with this categorization. The suburbs to the immediate north and west lie in the transition zone from a humid subtropical (Cfa) to a humid continental climate (Dfa). The area averages 234 days with at least some sunshine annually, and averages 57% of possible sunshine annually, accumulating 2,535 hours of sunshine per annum. The city falls under USDA 7b Plant Hardiness zone. New York is a global hub of international business and commerce. In 2012, New York City topped the first Global Economic Power Index, published by The Atlantic (to be differentiated from a namesake list published by the Martin Prosperity Institute), with cities ranked according to criteria reflecting their presence on similar lists as published by other entities. The city is a major center for banking and finance, retailing, world trade, transportation, tourism, real estate, new media as well as traditional media, advertising, legal services, accountancy, insurance, theater, fashion, and the arts in the United States; while Silicon Alley, metonymous for New York's broad-spectrum high technology sphere, continues to expand. The Port of New York and New Jersey is also a major economic engine, handling record cargo volume in the first half of 2014. New York grew in importance as a trading port while under British rule in the early 1700s. It also became a center of slavery, with 42% of households holding slaves by 1730, more than any other city other than Charleston, South Carolina. Most slaveholders held a few or several domestic slaves, but others hired them out to work at labor. Slavery became integrally tied to New York's economy through the labor of slaves throughout the port, and the banks and shipping tied to the South. Discovery of the African Burying Ground in the 1990s, during construction of a new federal courthouse near Foley Square, revealed that tens of thousands of Africans had been buried in the area in the colonial years. The biotechnology sector is also growing in New York City, based upon the city's strength in academic scientific research and public and commercial financial support. On December 19, 2011, then Mayor Michael R. Bloomberg announced his choice of Cornell University and Technion-Israel Institute of Technology to build a US$2 billion graduate school of applied sciences called Cornell Tech on Roosevelt Island with the goal of transforming New York City into the world's premier technology capital. By mid-2014, Accelerator, a biotech investment firm, had raised more than US$30 million from investors, including Eli Lilly and Company, Pfizer, and Johnson & Johnson, for initial funding to create biotechnology startups at the Alexandria Center for Life Science, which encompasses more than 700,000 square feet (65,000 m2) on East 29th Street and promotes collaboration among scientists and entrepreneurs at the center and with nearby academic, medical, and research institutions. The New York City Economic Development Corporation's Early Stage Life Sciences Funding Initiative and venture capital partners, including Celgene, General Electric Ventures, and Eli Lilly, committed a minimum of US$100 million to help launch 15 to 20 ventures in life sciences and biotechnology. Numerous major American cultural movements began in the city, such as the Harlem Renaissance, which established the African-American literary canon in the United States. The city was a center of jazz in the 1940s, abstract expressionism in the 1950s, and the birthplace of hip hop in the 1970s. The city's punk and hardcore scenes were influential in the 1970s and 1980s. New York has long had a flourishing scene for Jewish American literature. The city's total area is 468.9 square miles (1,214 km2). 164.1 sq mi (425 km2) of this is water and 304.8 sq mi (789 km2) is land. The highest point in the city is Todt Hill on Staten Island, which, at 409.8 feet (124.9 m) above sea level, is the highest point on the Eastern Seaboard south of Maine. The summit of the ridge is mostly covered in woodlands as part of the Staten Island Greenbelt. In 1898, the modern City of New York was formed with the consolidation of Brooklyn (until then a separate city), the County of New York (which then included parts of the Bronx), the County of Richmond, and the western portion of the County of Queens. The opening of the subway in 1904, first built as separate private systems, helped bind the new city together. Throughout the first half of the 20th century, the city became a world center for industry, commerce, and communication. In 2006, the Sister City Program of the City of New York, Inc. was restructured and renamed New York City Global Partners. New York City has expanded its international outreach via this program to a network of cities worldwide, promoting the exchange of ideas and innovation between their citizenry and policymakers, according to the city's website. New York's historic sister cities are denoted below by the year they joined New York City's partnership network. The television industry developed in New York and is a significant employer in the city's economy. The three major American broadcast networks are all headquartered in New York: ABC, CBS, and NBC. Many cable networks are based in the city as well, including MTV, Fox News, HBO, Showtime, Bravo, Food Network, AMC, and Comedy Central. The City of New York operates a public broadcast service, NYCTV, that has produced several original Emmy Award-winning shows covering music and culture in city neighborhoods and city government. Manhattan was on track to have an estimated 90,000 hotel rooms at the end of 2014, a 10% increase from 2013. In October 2014, the Anbang Insurance Group, based in China, purchased the Waldorf Astoria New York for US$1.95 billion, making it the world's most expensive hotel ever sold. New York City is home to Fort Hamilton, the U.S. military's only active duty installation within the city. Established in 1825 in Brooklyn on the site of a small battery utilized during the American Revolution, it is one of America's longest serving military forts. Today Fort Hamilton serves as the headquarters of the North Atlantic Division of the United States Army Corps of Engineers as well as for the New York City Recruiting Battalion. It also houses the 1179th Transportation Brigade, the 722nd Aeromedical Staging Squadron, and a military entrance processing station. Other formerly active military reservations still utilized for National Guard and military training or reserve operations in the city include Fort Wadsworth in Staten Island and Fort Totten in Queens. New York's airspace is the busiest in the United States and one of the world's busiest air transportation corridors. The three busiest airports in the New York metropolitan area include John F. Kennedy International Airport, Newark Liberty International Airport, and LaGuardia Airport; 109 million travelers used these three airports in 2012, and the city's airspace is the busiest in the nation. JFK and Newark Liberty were the busiest and fourth busiest U.S. gateways for international air passengers, respectively, in 2012; as of 2011, JFK was the busiest airport for international passengers in North America. Plans have advanced to expand passenger volume at a fourth airport, Stewart International Airport near Newburgh, New York, by the Port Authority of New York and New Jersey. Plans were announced in July 2015 to entirely rebuild LaGuardia Airport in a multibillion-dollar project to replace its aging facilities. The Staten Island Ferry is the world's busiest ferry route, carrying approximately 20 million passengers on the 5.2-mile (8.4 km) route between Staten Island and Lower Manhattan and running 24 hours a day. Other ferry systems shuttle commuters between Manhattan and other locales within the city and the metropolitan area. The Stonewall riots were a series of spontaneous, violent demonstrations by members of the gay community against a police raid that took place in the early morning hours of June 28, 1969, at the Stonewall Inn in the Greenwich Village neighborhood of Lower Manhattan. They are widely considered to constitute the single most important event leading to the gay liberation movement and the modern fight for LGBT rights in the United States. In 2014, the city had an estimated population density of 27,858 people per square mile (10,756/km²), rendering it the most densely populated of all municipalities housing over 100,000 residents in the United States; however, several small cities (of fewer than 100,000) in adjacent Hudson County, New Jersey are more dense overall, as per the 2000 Census. Geographically co-extensive with New York County, the borough of Manhattan's population density of 71,672 people per square mile (27,673/km²) makes it the highest of any county in the United States and higher than the density of any individual American city. New York is a prominent location for the American entertainment industry, with many films, television series, books, and other media being set there. As of 2012, New York City was the second largest center for filmmaking and television production in the United States, producing about 200 feature films annually, employing 130,000 individuals, and generating an estimated $7.1 billion in direct expenditures, and by volume, New York is the world leader in independent film production; one-third of all American independent films are produced in New York City. The Association of Independent Commercial Producers is also based in New York. In the first five months of 2014 alone, location filming for television pilots in New York City exceeded the record production levels for all of 2013, with New York surpassing Los Angeles as the top North American city for the same distinction during the 2013/2014 cycle. In its 2013 ParkScore ranking, The Trust for Public Land reported that the park system in New York City was the second best park system among the 50 most populous U.S. cities, behind the park system of Minneapolis. ParkScore ranks urban park systems by a formula that analyzes median park size, park acres as percent of city area, the percent of city residents within a half-mile of a park, spending of park services per resident, and the number of playgrounds per 10,000 residents. The annual United States Open Tennis Championships is one of the world's four Grand Slam tennis tournaments and is held at the National Tennis Center in Flushing Meadows-Corona Park, Queens. The New York Marathon is one of the world's largest, and the 2004–2006 events hold the top three places in the marathons with the largest number of finishers, including 37,866 finishers in 2006. The Millrose Games is an annual track and field meet whose featured event is the Wanamaker Mile. Boxing is also a prominent part of the city's sporting scene, with events like the Amateur Boxing Golden Gloves being held at Madison Square Garden each year. The city is also considered the host of the Belmont Stakes, the last, longest and oldest of horse racing's Triple Crown races, held just over the city's border at Belmont Park on the first or second Sunday of June. The city also hosted the 1932 U.S. Open golf tournament and the 1930 and 1939 PGA Championships, and has been host city for both events several times, most notably for nearby Winged Foot Golf Club. Uniquely among major American cities, New York is divided between, and is host to the main branches of, two different US district courts: the District Court for the Southern District of New York, whose main courthouse is on Foley Square near City Hall in Manhattan and whose jurisdiction includes Manhattan and the Bronx, and the District Court for the Eastern District of New York, whose main courthouse is in Brooklyn and whose jurisdiction includes Brooklyn, Queens, and Staten Island. The US Court of Appeals for the Second Circuit and US Court of International Trade are also based in New York, also on Foley Square in Manhattan. The city's land has been altered substantially by human intervention, with considerable land reclamation along the waterfronts since Dutch colonial times; reclamation is most prominent in Lower Manhattan, with developments such as Battery Park City in the 1970s and 1980s. Some of the natural relief in topography has been evened out, especially in Manhattan. Ecuador, Colombia, Guyana, Peru, and Brazil were the top source countries from South America for legal immigrants to the New York City region in 2013; the Dominican Republic, Jamaica, Haiti, and Trinidad and Tobago in the Caribbean; Egypt, Ghana, and Nigeria from Africa; and El Salvador, Honduras, and Guatemala in Central America. Amidst a resurgence of Puerto Rican migration to New York City, this population had increased to approximately 1.3 million in the metropolitan area as of 2013. New York City's most important economic sector lies in its role as the headquarters for the U.S.financial industry, metonymously known as Wall Street. The city's securities industry, enumerating 163,400 jobs in August 2013, continues to form the largest segment of the city's financial sector and an important economic engine, accounting in 2012 for 5 percent of the city's private sector jobs, 8.5 percent (US$3.8 billion) of its tax revenue, and 22 percent of the city's total wages, including an average salary of US$360,700. Many large financial companies are headquartered in New York City, and the city is also home to a burgeoning number of financial startup companies. New York City is additionally a center for the advertising, music, newspaper, digital media, and publishing industries and is also the largest media market in North America. Some of the city's media conglomerates and institutions include Time Warner, the Thomson Reuters Corporation, the Associated Press, Bloomberg L.P., the News Corporation, The New York Times Company, NBCUniversal, the Hearst Corporation, AOL, and Viacom. Seven of the world's top eight global advertising agency networks have their headquarters in New York. Two of the top three record labels' headquarters are in New York: Sony Music Entertainment and Warner Music Group. Universal Music Group also has offices in New York. New media enterprises are contributing an increasingly important component to the city's central role in the media sphere. More than 200 newspapers and 350 consumer magazines have an office in the city, and the publishing industry employs about 25,000 people. Two of the three national daily newspapers in the United States are New York papers: The Wall Street Journal and The New York Times, which has won the most Pulitzer Prizes for journalism. Major tabloid newspapers in the city include: The New York Daily News, which was founded in 1919 by Joseph Medill Patterson and The New York Post, founded in 1801 by Alexander Hamilton. The city also has a comprehensive ethnic press, with 270 newspapers and magazines published in more than 40 languages. El Diario La Prensa is New York's largest Spanish-language daily and the oldest in the nation. The New York Amsterdam News, published in Harlem, is a prominent African American newspaper. The Village Voice is the largest alternative newspaper. According to the United States Geological Survey, an updated analysis of seismic hazard in July 2014 revealed a ""slightly lower hazard for tall buildings"" in New York City than previously assessed. Scientists estimated this lessened risk based upon a lower likelihood than previously thought of slow shaking near the city, which would be more likely to cause damage to taller structures from an earthquake in the vicinity of the city. The city and surrounding area suffered the bulk of the economic damage and largest loss of human life in the aftermath of the September 11, 2001 attacks when 10 of the 19 terrorists associated with Al-Qaeda piloted American Airlines Flight 11 into the North Tower of the World Trade Center and United Airlines Flight 175 into the South Tower of the World Trade Center, and later destroyed them, killing 2,192 civilians, 343 firefighters, and 71 law enforcement officers who were in the towers and in the surrounding area. The rebuilding of the area, has created a new One World Trade Center, and a 9/11 memorial and museum along with other new buildings and infrastructure. The World Trade Center PATH station, which opened on July 19, 1909 as the Hudson Terminal, was also destroyed in the attack. A temporary station was built and opened on November 23, 2003. A permanent station, the World Trade Center Transportation Hub, is currently under construction. The new One World Trade Center is the tallest skyscraper in the Western Hemisphere and the fourth-tallest building in the world by pinnacle height, with its spire reaching a symbolic 1,776 feet (541.3 m) in reference to the year of American independence. In 1785, the assembly of the Congress of the Confederation made New York the national capital shortly after the war. New York was the last capital of the U.S. under the Articles of Confederation and the first capital under the Constitution of the United States. In 1789, the first President of the United States, George Washington, was inaugurated; the first United States Congress and the Supreme Court of the United States each assembled for the first time, and the United States Bill of Rights was drafted, all at Federal Hall on Wall Street. By 1790, New York had surpassed Philadelphia as the largest city in the United States. As of 2013, the global advertising agencies of Omnicom Group and Interpublic Group, both based in Manhattan, had combined annual revenues of approximately US$21 billion, reflecting New York City's role as the top global center for the advertising industry, which is metonymously referred to as ""Madison Avenue"". The city's fashion industry provides approximately 180,000 employees with $11 billion in annual wages. The New York City Police Department (NYPD) has been the largest police force in the United States by a significant margin, with over 35,000 sworn officers. Members of the NYPD are frequently referred to by politicians, the media, and their own police cars by the nickname, New York's Finest. Many of the world's largest media conglomerates are also based in the city. Manhattan contained over 500 million square feet (46.5 million m2) of office space in 2015, making it the largest office market in the United States, while Midtown Manhattan, with nearly 400 million square feet (37.2 million m2) in 2015, is the largest central business district in the world. New York City is located on one of the world's largest natural harbors, and the boroughs of Manhattan and Staten Island are (primarily) coterminous with islands of the same names, while Queens and Brooklyn are located at the west end of the larger Long Island, and The Bronx is located at the southern tip of New York State's mainland. This situation of boroughs separated by water led to the development of an extensive infrastructure of bridges and tunnels. Nearly all of the city's major bridges and tunnels are notable, and several have broken or set records. New York is the most important source of political fundraising in the United States, as four of the top five ZIP codes in the nation for political contributions are in Manhattan. The top ZIP code, 10021 on the Upper East Side, generated the most money for the 2004 presidential campaigns of George W. Bush and John Kerry. The city has a strong imbalance of payments with the national and state governments. It receives 83 cents in services for every $1 it sends to the federal government in taxes (or annually sends $11.4 billion more than it receives back). The city also sends an additional $11 billion more each year to the state of New York than it receives back. The 1916 Zoning Resolution required setbacks in new buildings, and restricted towers to a percentage of the lot size, to allow sunlight to reach the streets below. The Art Deco style of the Chrysler Building (1930) and Empire State Building (1931), with their tapered tops and steel spires, reflected the zoning requirements. The buildings have distinctive ornamentation, such as the eagles at the corners of the 61st floor on the Chrysler Building, and are considered some of the finest examples of the Art Deco style. A highly influential example of the international style in the United States is the Seagram Building (1957), distinctive for its façade using visible bronze-toned I-beams to evoke the building's structure. The Condé Nast Building (2000) is a prominent example of green design in American skyscrapers and has received an award from the American Institute of Architects as well as AIA New York State for its design. The most well-known hospital in the HHC system is Bellevue Hospital, the oldest public hospital in the United States. Bellevue is the designated hospital for treatment of the President of the United States and other world leaders if they become sick or injured while in New York City. The president of HHC is Ramanathan Raju, MD, a surgeon and former CEO of the Cook County health system in Illinois. Many sports are associated with New York's immigrant communities. Stickball, a street version of baseball, was popularized by youths in the 1930s, and a street in the Bronx was renamed Stickball Boulevard in the late 2000s to memorialize this. Each year HHC's facilities provide about 225,000 admissions, one million emergency room visits and five million clinic visits to New Yorkers. HHC facilities treat nearly one-fifth of all general hospital discharges and more than one third of emergency room and hospital-based clinic visits in New York City. New York—often called New York City or the City of New York to distinguish it from the State of New York, of which it is a part—is the most populous city in the United States and the center of the New York metropolitan area, the premier gateway for legal immigration to the United States and one of the most populous urban agglomerations in the world. A global power city, New York exerts a significant impact upon commerce, finance, media, art, fashion, research, technology, education, and entertainment, its fast pace defining the term New York minute. Home to the headquarters of the United Nations, New York is an important center for international diplomacy and has been described as the cultural and financial capital of the world. The New York City Health and Hospitals Corporation (HHC) operates the public hospitals and clinics in New York City. A public benefit corporation with $6.7 billion in annual revenues, HHC is the largest municipal healthcare system in the United States serving 1.4 million patients, including more than 475,000 uninsured city residents. HHC was created in 1969 by the New York State Legislature as a public benefit corporation (Chapter 1016 of the Laws 1969). It is similar to a municipal agency but has a Board of Directors. HHC operates 11 acute care hospitals, five nursing homes, six diagnostic and treatment centers, and more than 70 community-based primary care sites, serving primarily the poor and working class. HHC's MetroPlus Health Plan is one of the New York area's largest providers of government-sponsored health insurance and is the plan of choice for nearly half million New Yorkers. Situated on one of the world's largest natural harbors, New York City consists of five boroughs, each of which is a separate county of New York State. The five boroughs – Brooklyn, Queens, Manhattan, the Bronx, and Staten Island – were consolidated into a single city in 1898. With a census-estimated 2014 population of 8,491,079 distributed over a land area of just 305 square miles (790 km2), New York is the most densely populated major city in the United States. As many as 800 languages are spoken in New York, making it the most linguistically diverse city in the world. By 2014 census estimates, the New York City metropolitan region remains by a significant margin the most populous in the United States, as defined by both the Metropolitan Statistical Area (20.1 million residents) and the Combined Statistical Area (23.6 million residents). In 2013, the MSA produced a gross metropolitan product (GMP) of nearly US$1.39 trillion, while in 2012, the CSA generated a GMP of over US$1.55 trillion, both ranking first nationally by a wide margin and behind the GDP of only twelve and eleven countries, respectively. Public transport is essential in New York City. 54.6% of New Yorkers commuted to work in 2005 using mass transit. This is in contrast to the rest of the United States, where about 90% of commuters drive automobiles to their workplace. According to the US Census Bureau, New York City residents spend an average of 38.4 minutes a day getting to work, the longest commute time in the nation among large cities. New York is the only US city in which a majority (52%) of households do not have a car; only 22% of Manhattanites own a car. Due to their high usage of mass transit, New Yorkers spend less of their household income on transportation than the national average, saving $19 billion annually on transportation compared to other urban Americans. Despite New York's heavy reliance on its vast public transit system, streets are a defining feature of the city. Manhattan's street grid plan greatly influenced the city's physical development. Several of the city's streets and avenues, like Broadway, Wall Street, Madison Avenue, and Seventh Avenue are also used as metonyms for national industries there: the theater, finance, advertising, and fashion organizations, respectively. The city is represented in the National Football League by the New York Giants and the New York Jets, although both teams play their home games at MetLife Stadium in nearby East Rutherford, New Jersey, which hosted Super Bowl XLVIII in 2014. The trial in Manhattan of John Peter Zenger in 1735 helped to establish the freedom of the press in North America. In 1754, Columbia University was founded under charter by King George II as King's College in Lower Manhattan. The Stamp Act Congress met in New York in October 1765 as the Sons of Liberty organized in the city, skirmishing over the next ten years with British troops stationed there. Over 600,000 students are enrolled in New York City's over 120 higher education institutions, the highest number of any city in the United States, including over half million in the City University of New York (CUNY) system alone in 2014. In 2005, three out of five Manhattan residents were college graduates, and one out of four had a postgraduate degree, forming one of the highest concentrations of highly educated people in any American city. New York City is home to such notable private universities as Barnard College, Columbia University, Cooper Union, Fordham University, New York University, New York Institute of Technology, Pace University, and Yeshiva University. The public CUNY system is one of the largest universities in the nation, comprising 24 institutions across all five boroughs: senior colleges, community colleges, and other graduate/professional schools. The public State University of New York (SUNY) system also serves New York City, as well as the rest of the state. The city also has other smaller private colleges and universities, including many religious and special-purpose institutions, such as St. John's University, The Juilliard School, Manhattan College, The College of Mount Saint Vincent, The New School, Pratt Institute, The School of Visual Arts, The King's College, and Wagner College. New York became the most populous urbanized area in the world in the early 1920s, overtaking London. The metropolitan area surpassed the 10 million mark in the early 1930s, becoming the first megacity in human history. The difficult years of the Great Depression saw the election of reformer Fiorello La Guardia as mayor and the fall of Tammany Hall after eighty years of political dominance. Many Fortune 500 corporations are headquartered in New York City, as are a large number of foreign corporations. One out of ten private sector jobs in the city is with a foreign company. New York City has been ranked first among cities across the globe in attracting capital, business, and tourists. This ability to attract foreign investment helped New York City top the FDi Magazine American Cities of the Future ranking for 2013. Lincoln Center for the Performing Arts, anchoring Lincoln Square on the Upper West Side of Manhattan, is home to numerous influential arts organizations, including the Metropolitan Opera, New York City Opera, New York Philharmonic, and New York City Ballet, as well as the Vivian Beaumont Theater, the Juilliard School, Jazz at Lincoln Center, and Alice Tully Hall. The Lee Strasberg Theatre and Film Institute is in Union Square, and Tisch School of the Arts is based at New York University, while Central Park SummerStage presents performances of free plays and music in Central Park. The city is the birthplace of many cultural movements, including the Harlem Renaissance in literature and visual art; abstract expressionism (also known as the New York School) in painting; and hip hop, punk, salsa, disco, freestyle, Tin Pan Alley, and Jazz in music. New York City has been considered the dance capital of the world. The city is also widely celebrated in popular lore, frequently the setting for books, movies (see List of films set in New York City), and television programs. New York Fashion Week is one of the world's preeminent fashion events and is afforded extensive coverage by the media. New York has also frequently been ranked the top fashion capital of the world on the annual list compiled by the Global Language Monitor. The New York area is home to a distinctive regional speech pattern called the New York dialect, alternatively known as Brooklynese or New Yorkese. It has generally been considered one of the most recognizable accents within American English. The classic version of this dialect is centered on middle and working-class people of European descent. However, the influx of non-European immigrants in recent decades has led to changes in this distinctive dialect, and the traditional form of this speech pattern is no longer as prevalent among general New Yorkers as in the past. Multibillion US$ heavy-rail transit projects under construction in New York City include the Second Avenue Subway, the East Side Access project, and the 7 Subway Extension. New York City is home to hundreds of cultural institutions and historic sites, many of which are internationally known. Museum Mile is the name for a section of Fifth Avenue running from 82nd to 105th streets on the Upper East Side of Manhattan, in an area sometimes called Upper Carnegie Hill. The Mile, which contains one of the densest displays of culture in the world, is actually three blocks longer than one mile (1.6 km). Ten museums occupy the length of this section of Fifth Avenue. The tenth museum, the Museum for African Art, joined the ensemble in 2009, however its Museum at 110th Street, the first new museum constructed on the Mile since the Guggenheim in 1959, opened in late 2012. In addition to other programming, the museums collaborate for the annual Museum Mile Festival, held each year in June, to promote the museums and increase visitation. Many of the world's most lucrative art auctions are held in New York City. The Democratic Party holds the majority of public offices. As of November 2008, 67% of registered voters in the city are Democrats. New York City has not been carried by a Republican in a statewide or presidential election since President Calvin Coolidge won the five boroughs in 1924. In 2012, Democrat Barack Obama became the first presidential candidate of any party to receive more than 80% of the overall vote in New York City, sweeping all five boroughs. Party platforms center on affordable housing, education, and economic development, and labor politics are of importance in the city. The city's National Basketball Association teams are the Brooklyn Nets and the New York Knicks, while the New York Liberty is the city's Women's National Basketball Association. The first national college-level basketball championship, the National Invitation Tournament, was held in New York in 1938 and remains in the city. The city is well known for its links to basketball, which is played in nearly every park in the city by local youth, many of whom have gone on to play for major college programs and in the NBA. There are hundreds of distinct neighborhoods throughout the five boroughs of New York City, many with a definable history and character to call their own. If the boroughs were each independent cities, four of the boroughs (Brooklyn, Queens, Manhattan, and the Bronx) would be among the ten most populous cities in the United States. Winters are cold and damp, and prevailing wind patterns that blow offshore minimize the moderating effects of the Atlantic Ocean; yet the Atlantic and the partial shielding from colder air by the Appalachians keep the city warmer in the winter than inland North American cities at similar or lesser latitudes such as Pittsburgh, Cincinnati, and Indianapolis. The daily mean temperature in January, the area's coldest month, is 32.6 °F (0.3 °C); however, temperatures usually drop to 10 °F (−12 °C) several times per winter, and reach 50 °F (10 °C) several days each winter month. Spring and autumn are unpredictable and can range from chilly to warm, although they are usually mild with low humidity. Summers are typically warm to hot and humid, with a daily mean temperature of 76.5 °F (24.7 °C) in July and an average humidity level of 72%. Nighttime conditions are often exacerbated by the urban heat island phenomenon, while daytime temperatures exceed 90 °F (32 °C) on average of 17 days each summer and in some years exceed 100 °F (38 °C). In the warmer months, the dew point, a measure of atmospheric moisture, ranges from 57.3 °F (14.1 °C) in June to 62.0 °F (16.7 °C) in August. Extreme temperatures have ranged from −15 °F (−26 °C), recorded on February 9, 1934, up to 106 °F (41 °C) on July 9, 1936. Democratic Party candidates were consistently elected to local office, increasing the city's ties to the South and its dominant party. In 1861, Mayor Fernando Wood called on the aldermen to declare independence from Albany and the United States after the South seceded, but his proposal was not acted on. Anger at new military conscription laws during the American Civil War (1861–1865), which spared wealthier men who could afford to pay a $300 (equivalent to $5,766 in 2016) commutation fee to hire a substitute, led to the Draft Riots of 1863, whose most visible participants were ethnic Irish working class. The situation deteriorated into attacks on New York's elite, followed by attacks on black New Yorkers and their property after fierce competition for a decade between Irish immigrants and blacks for work. Rioters burned the Colored Orphan Asylum to the ground, but more than 200 children escaped harm due to efforts of the New York City Police Department, which was mainly made up of Irish immigrants. According to historian James M. McPherson (2001), at least 120 people were killed. In all, eleven black men were lynched over five days, and the riots forced hundreds of blacks to flee the city for Williamsburg, Brooklyn, as well as New Jersey; the black population in Manhattan fell below 10,000 by 1865, which it had last been in 1820. The white working class had established dominance. Violence by longshoremen against black men was especially fierce in the docks area. It was one of the worst incidents of civil unrest in American history. Sociologists and criminologists have not reached consensus on the explanation for the dramatic decrease in the city's crime rate. Some attribute the phenomenon to new tactics used by the NYPD, including its use of CompStat and the broken windows theory. Others cite the end of the crack epidemic and demographic changes, including from immigration. Another theory is that widespread exposure to lead pollution from automobile exhaust, which can lower intelligence and increase aggression levels, incited the initial crime wave in the mid-20th century, most acutely affecting heavily trafficked cities like New York. A strong correlation was found demonstrating that violent crime rates in New York and other big cities began to fall after lead was removed from American gasoline in the 1970s. Another theory cited to explain New York City's falling homicide rate is the inverse correlation between the number of murders and the increasingly wetter climate in the city. The George Washington Bridge is the world's busiest motor vehicle bridge, connecting Manhattan to Bergen County, New Jersey. The Verrazano-Narrows Bridge is the longest suspension bridge in the Americas and one of the world's longest. The Brooklyn Bridge is an icon of the city itself. The towers of the Brooklyn Bridge are built of limestone, granite, and Rosendale cement, and their architectural style is neo-Gothic, with characteristic pointed arches above the passageways through the stone towers. This bridge was also the longest suspension bridge in the world from its opening until 1903, and is the first steel-wire suspension bridge. New York City has been a metropolitan municipality with a mayor-council form of government since its consolidation in 1898. The government of New York is more centralized than that of most other U.S. cities. In New York City, the city government is responsible for public education, correctional institutions, public safety, recreational facilities, sanitation, water supply, and welfare services. The Occupy Wall Street protests in Zuccotti Park in the Financial District of Lower Manhattan began on September 17, 2011, receiving global attention and spawning the Occupy movement against social and economic inequality worldwide. New York City's public bus fleet is the largest in North America, and the Port Authority Bus Terminal, the main intercity bus terminal of the city, serves 7,000 buses and 200,000 commuters daily, making it the busiest bus station in the world. In 2012, New York City had the lowest overall crime rate and the second lowest murder rate among the largest U.S. cities, having become significantly safer after a spike in crime in the 1970s through 1990s. Violent crime in New York City decreased more than 75% from 1993 to 2005, and continued decreasing during periods when the nation as a whole saw increases. By 2002, New York City's crime rate was similar to that of Provo, Utah, and was ranked 197th in crime among the 216 U.S. cities with populations greater than 100,000. In 2005 the homicide rate was at its lowest level since 1966, and in 2007 the city recorded fewer than 500 homicides for the first time ever since crime statistics were first published in 1963. In the first six months of 2010, 95.1% of all murder victims and 95.9% of all shooting victims in New York City were black or Hispanic; additionally, 90.2 percent of those arrested for murder and 96.7 percent of those arrested for shooting someone were black or Hispanic. New York experienced a record low of 328 homicides in 2014 and has a far lower murder rate than other major American cities. New York's high rate of public transit use, over 200,000 daily cyclists as of 2014, and many pedestrian commuters make it the most energy-efficient major city in the United States. Walk and bicycle modes of travel account for 21% of all modes for trips in the city; nationally the rate for metro regions is about 8%. In both its 2011 and 2015 rankings, Walk Score named New York City the most walkable large city in the United States. Citibank sponsored the introduction of 10,000 public bicycles for the city's bike-share project in the summer of 2013. Research conducted by Quinnipiac University showed that a majority of New Yorkers support the initiative. New York City's numerical ""in-season cycling indicator"" of bicycling in the city hit an all-time high in 2013. The city government was a petitioner in the landmark Massachusetts v. Environmental Protection Agency Supreme Court case forcing the EPA to regulate greenhouse gases as pollutants. The city is also a leader in the construction of energy-efficient green office buildings, including the Hearst Tower among others. Mayor Bill de Blasio has committed to an 80% reduction in greenhouse gas emissions between 2014 and 2050 to reduce the city's contributions to climate change, beginning with a comprehensive ""Green Buildings"" plan. New York City is the most-populous city in the United States, with an estimated record high of 8,491,079 residents as of 2014, incorporating more immigration into the city than outmigration since the 2010 United States Census. More than twice as many people live in New York City as in the second-most populous U.S. city (Los Angeles), and within a smaller area. New York City gained more residents between April 2010 and July 2014 (316,000) than any other U.S. city. New York City's population amounts to about 40% of New York State's population and a similar percentage of the New York metropolitan area population. New York City's food culture includes a variety of international cuisines influenced by the city's immigrant history. Central European and Italian immigrants originally made the city famous for bagels, cheesecake, and New York-style pizza, while Chinese and other Asian restaurants, sandwich joints, trattorias, diners, and coffeehouses have become ubiquitous. Some 4,000 mobile food vendors licensed by the city, many immigrant-owned, have made Middle Eastern foods such as falafel and kebabs popular examples of modern New York street food. The city is also home to nearly one thousand of the finest and most diverse haute cuisine restaurants in the world, according to Michelin. The New York City Department of Health and Mental Hygiene assigns letter grades to the city's 24,000 restaurants based upon their inspection results. In 1609, English explorer Henry Hudson re-discovered the region when he sailed his ship the Halve Maen (""Half Moon"" in Dutch) into New York Harbor while searching for the Northwest Passage to the Orient for his employer, the Dutch East India Company. He proceeded to sail up what he named the North River, also called the Mauritis River, and now known as the Hudson River, to the site of the present-day New York State capital of Albany in the belief that it might represent an oceanic tributary. When the river narrowed and was no longer saline, he realized it was not a maritime passage and sailed back downriver. He made a ten-day exploration of the area and claimed the region for his employer. In 1614, the area between Cape Cod and Delaware Bay would be claimed by the Netherlands and called Nieuw-Nederland (New Netherland). The Statue of Liberty National Monument and Ellis Island Immigration Museum are managed by the National Park Service and are in both the states of New York and New Jersey. They are joined in the harbor by Governors Island National Monument, in New York. Historic sites under federal management on Manhattan Island include Castle Clinton National Monument; Federal Hall National Memorial; Theodore Roosevelt Birthplace National Historic Site; General Grant National Memorial (""Grant's Tomb""); African Burial Ground National Monument; and Hamilton Grange National Memorial. Hundreds of private properties are listed on the National Register of Historic Places or as a National Historic Landmark such as, for example, the Stonewall Inn in Greenwich Village as the catalyst of the modern gay rights movement. I Love New York (stylized I ❤ NY) is both a logo and a song that are the basis of an advertising campaign and have been used since 1977 to promote tourism in New York City, and later to promote New York State as well. The trademarked logo, owned by New York State Empire State Development, appears in souvenir shops and brochures throughout the city and state, some licensed, many not. The song is the state song of New York. New York City has more than 2,000 arts and cultural organizations and more than 500 art galleries of all sizes. The city government funds the arts with a larger annual budget than the National Endowment for the Arts. Wealthy business magnates in the 19th century built a network of major cultural institutions, such as the famed Carnegie Hall and The Metropolitan Museum of Art, that would become internationally established. The advent of electric lighting led to elaborate theater productions, and in the 1880s, New York City theaters on Broadway and along 42nd Street began featuring a new stage form that became known as the Broadway musical. Strongly influenced by the city's immigrants, productions such as those of Harrigan and Hart, George M. Cohan, and others used song in narratives that often reflected themes of hope and ambition. Mass transit in New York City, most of which runs 24 hours a day, accounts for one in every three users of mass transit in the United States, and two-thirds of the nation's rail riders live in the New York City Metropolitan Area. New York City is home to the headquarters of the National Football League, Major League Baseball, the National Basketball Association, the National Hockey League, and Major League Soccer. The New York metropolitan area hosts the most sports teams in these five professional leagues. Participation in professional sports in the city predates all professional leagues, and the city has been continuously hosting professional sports since the birth of the Brooklyn Dodgers in 1882. The city has played host to over forty major professional teams in the five sports and their respective competing leagues, both current and historic. Four of the ten most expensive stadiums ever built worldwide (MetLife Stadium, the new Yankee Stadium, Madison Square Garden, and Citi Field) are located in the New York metropolitan area. Madison Square Garden, its predecessor, as well as the original Yankee Stadium and Ebbets Field, are some of the most famous sporting venues in the world, the latter two having been commemorated on U.S. postage stamps. The New York City Charter School Center assists the setup of new charter schools. There are approximately 900 additional privately run secular and religious schools in the city. Asian Americans in New York City, according to the 2010 Census, number more than one million, greater than the combined totals of San Francisco and Los Angeles. New York contains the highest total Asian population of any U.S. city proper. The New York City borough of Queens is home to the state's largest Asian American population and the largest Andean (Colombian, Ecuadorian, Peruvian, and Bolivian) populations in the United States, and is also the most ethnically diverse urban area in the world. The Chinese population constitutes the fastest-growing nationality in New York State; multiple satellites of the original Manhattan Chinatown (紐約華埠), in Brooklyn (布鲁克林華埠), and around Flushing, Queens (法拉盛華埠), are thriving as traditionally urban enclaves, while also expanding rapidly eastward into suburban Nassau County (拿騷縣) on Long Island (長島), as the New York metropolitan region and New York State have become the top destinations for new Chinese immigrants, respectively, and large-scale Chinese immigration continues into New York City and surrounding areas. In 2012, 6.3% of New York City was of Chinese ethnicity, with nearly three-fourths living in either Queens or Brooklyn, geographically on Long Island. A community numbering 20,000 Korean-Chinese (Chaoxianzu (Chinese: 朝鲜族) or Joseonjok (Hangul: 조선족)) is centered in Flushing, Queens, while New York City is also home to the largest Tibetan population outside China, India, and Nepal, also centered in Queens. Koreans made up 1.2% of the city's population, and Japanese 0.3%. Filipinos were the largest Southeast Asian ethnic group at 0.8%, followed by Vietnamese, who made up 0.2% of New York City's population in 2010. Indians are the largest South Asian group, comprising 2.4% of the city's population, with Bangladeshis and Pakistanis at 0.7% and 0.5%, respectively. Queens is the preferred borough of settlement for Asian Indians, Koreans, and Filipinos, as well as Malaysians and other Southeast Asians; while Brooklyn is receiving large numbers of both West Indian as well as Asian Indian immigrants. The New York metropolitan area is home to a self-identifying gay and bisexual community estimated at 568,903 individuals, the largest in the United States and one of the world's largest. Same-sex marriages in New York were legalized on June 24, 2011 and were authorized to take place beginning 30 days thereafter. New York City has focused on reducing its environmental impact and carbon footprint. Mass transit use in New York City is the highest in the United States. Also, by 2010, the city had 3,715 hybrid taxis and other clean diesel vehicles, representing around 28% of New York's taxi fleet in service, the most of any city in North America. Each borough is coextensive with a judicial district of the state Unified Court System, of which the Criminal Court and the Civil Court are the local courts, while the New York Supreme Court conducts major trials and appeals. Manhattan hosts the First Department of the Supreme Court, Appellate Division while Brooklyn hosts the Second Department. There are also several extrajudicial administrative courts, which are executive agencies and not part of the state Unified Court System. During the Wisconsinan glaciation, the New York City region was situated at the edge of a large ice sheet over 1,000 feet in depth. The ice sheet scraped away large amounts of soil, leaving the bedrock that serves as the geologic foundation for much of New York City today. Later on, the ice sheet would help split apart what are now Long Island and Staten Island. New York City is supplied with drinking water by the protected Catskill Mountains watershed. As a result of the watershed's integrity and undisturbed natural water filtration system, New York is one of only four major cities in the United States the majority of whose drinking water is pure enough not to require purification by water treatment plants. The Croton Watershed north of the city is undergoing construction of a US$3.2 billion water purification plant to augment New York City's water supply by an estimated 290 million gallons daily, representing a greater than 20% addition to the city's current availability of water. The ongoing expansion of New York City Water Tunnel No. 3, an integral part of the New York City water supply system, is the largest capital construction project in the city's history. Many districts and landmarks in New York City have become well known, and the city received a record 56 million tourists in 2014, hosting three of the world's ten most visited tourist attractions in 2013. Several sources have ranked New York the most photographed city in the world. Times Square, iconic as the world's ""heart"" and its ""Crossroads"", is the brightly illuminated hub of the Broadway Theater District, one of the world's busiest pedestrian intersections, and a major center of the world's entertainment industry. The names of many of the city's bridges, skyscrapers, and parks are known around the world. Anchored by Wall Street in the Financial District of Lower Manhattan, New York City has been called both the most economically powerful city and the leading financial center of the world, and the city is home to the world's two largest stock exchanges by total market capitalization, the New York Stock Exchange and NASDAQ. Manhattan's real estate market is among the most expensive in the world. Manhattan's Chinatown incorporates the highest concentration of Chinese people in the Western Hemisphere, with multiple signature Chinatowns developing across the city. Providing continuous 24/7 service, the New York City Subway is one of the most extensive metro systems worldwide, with 469 stations in operation. New York City's higher education network comprises over 120 colleges and universities, including Columbia University, New York University, and Rockefeller University, which have been ranked among the top 35 in the world. A permanent European presence in New Netherland began in 1624 – making New York the 12th oldest continuously occupied European-established settlement in the continental United States  – with the founding of a Dutch fur trading settlement on Governors Island. In 1625, construction was started on a citadel and a Fort Amsterdam on Manhattan Island, later called New Amsterdam (Nieuw Amsterdam). The colony of New Amsterdam was centered at the site which would eventually become Lower Manhattan. The Dutch colonial Director-General Peter Minuit purchased the island of Manhattan from the Canarsie, a small band of the Lenape, in 1626 for a value of 60 guilders (about $1000 in 2006); a disproved legend says that Manhattan was purchased for $24 worth of glass beads. The New York City Fire Department faces highly multifaceted firefighting challenges in many ways unique to New York. In addition to responding to building types that range from wood-frame single family homes to high-rise structures, there are many secluded bridges and tunnels, as well as large parks and wooded areas that can give rise to brush fires. New York is also home to one of the largest subway systems in the world, consisting of hundreds of miles of tunnel with electrified track. Under New York State's gradual abolition act of 1799, children of slave mothers were born to be eventually liberated but were held in indentured servitude until their mid-to-late twenties. Together with slaves freed by their masters after the Revolutionary War and escaped slaves, a significant free-black population gradually developed in Manhattan. Under such influential United States founders as Alexander Hamilton and John Jay, the New York Manumission Society worked for abolition and established the African Free School to educate black children. It was not until 1827 that slavery was completely abolished in the state, and free blacks struggled afterward with discrimination. New York interracial abolitionist activism continued; among its leaders were graduates of the African Free School. The city's black population reached more than 16,000 in 1840. The Battle of Long Island, the largest battle of the American Revolutionary War, was fought in August 1776 entirely within the modern-day borough of Brooklyn. After the battle, in which the Americans were defeated, leaving subsequent smaller armed engagements following in its wake, the city became the British military and political base of operations in North America. The city was a haven for Loyalist refugees, as well as escaped slaves who joined the British lines for freedom newly promised by the Crown for all fighters. As many as 10,000 escaped slaves crowded into the city during the British occupation. When the British forces evacuated at the close of the war in 1783, they transported 3,000 freedmen for resettlement in Nova Scotia. They resettled other freedmen in England and the Caribbean. New York City has the largest European and non-Hispanic white population of any American city. At 2.7 million in 2012, New York's non-Hispanic white population is larger than the non-Hispanic white populations of Los Angeles (1.1 million), Chicago (865,000), and Houston (550,000) combined. The European diaspora residing in the city is very diverse. According to 2012 Census estimates, there were roughly 560,000 Italian Americans, 385,000 Irish Americans, 253,000 German Americans, 223,000 Russian Americans, 201,000 Polish Americans, and 137,000 English Americans. Additionally, Greek and French Americans numbered 65,000 each, with those of Hungarian descent estimated at 60,000 people. Ukrainian and Scottish Americans numbered 55,000 and 35,000, respectively. People identifying ancestry from Spain numbered 30,838 total in 2010. People of Norwegian and Swedish descent both stood at about 20,000 each, while people of Czech, Lithuanian, Portuguese, Scotch-Irish, and Welsh descent all numbered between 12,000–14,000 people. Arab Americans number over 160,000 in New York City, with the highest concentration in Brooklyn. Central Asians, primarily Uzbek Americans, are a rapidly growing segment of the city's non-Hispanic white population, enumerating over 30,000, and including over half of all Central Asian immigrants to the United States, most settling in Queens or Brooklyn. Albanian Americans are most highly concentrated in the Bronx. Approximately 37% of the city's population is foreign born. In New York, no single country or region of origin dominates. The ten largest sources of foreign-born individuals in the city as of 2011 were the Dominican Republic, China, Mexico, Guyana, Jamaica, Ecuador, Haiti, India, Russia, and Trinidad and Tobago, while the Bangladeshi immigrant population has since become one of the fastest growing in the city, counting over 74,000 by 2013. Silicon Alley, centered in Manhattan, has evolved into a metonym for the sphere encompassing the New York City metropolitan region's high technology industries involving the Internet, new media, telecommunications, digital media, software development, biotechnology, game design, financial technology (""fintech""), and other fields within information technology that are supported by its entrepreneurship ecosystem and venture capital investments. In the first half of 2015, Silicon Alley generated over US$3.7 billion in venture capital investment across a broad spectrum of high technology enterprises, most based in Manhattan, with others in Brooklyn, Queens, and elsewhere in the region. High technology startup companies and employment are growing in New York City and the region, bolstered by the city's position in North America as the leading Internet hub and telecommunications center, including its vicinity to several transatlantic fiber optic trunk lines, New York's intellectual capital, and its extensive outdoor wireless connectivity. Verizon Communications, headquartered at 140 West Street in Lower Manhattan, was at the final stages in 2014 of completing a US$3 billion fiberoptic telecommunications upgrade throughout New York City. As of 2014, New York City hosted 300,000 employees in the tech sector. In 1664, Peter Stuyvesant, the Director-General of the colony of New Netherland, surrendered New Amsterdam to the English without bloodshed. The English promptly renamed the fledgling city ""New York"" after the Duke of York (later King James II). The City of New York has a complex park system, with various lands operated by the National Park Service, the New York State Office of Parks, Recreation and Historic Preservation, and the New York City Department of Parks and Recreation. There are seven state parks within the confines of New York City, including Clay Pit Ponds State Park Preserve, a natural area which includes extensive riding trails, and Riverbank State Park, a 28-acre (110,000 m2) facility that rises 69 feet (21 m) over the Hudson River. The New York City Public Schools system, managed by the New York City Department of Education, is the largest public school system in the United States, serving about 1.1 million students in more than 1,700 separate primary and secondary schools. The city's public school system includes nine specialized high schools to serve academically and artistically gifted students. Tourism is a vital industry for New York City, which has witnessed a growing combined volume of international and domestic tourists – receiving approximately 51 million tourists in 2011, 54 million in 2013, and a record 56.4 million in 2014. Tourism generated an all-time high US$61.3 billion in overall economic impact for New York City in 2014. Manhattan Island is linked to New York City's outer boroughs and New Jersey by several tunnels as well. The Lincoln Tunnel, which carries 120,000 vehicles a day under the Hudson River between New Jersey and Midtown Manhattan, is the busiest vehicular tunnel in the world. The tunnel was built instead of a bridge to allow unfettered passage of large passenger and cargo ships that sailed through New York Harbor and up the Hudson River to Manhattan's piers. The Holland Tunnel, connecting Lower Manhattan to Jersey City, New Jersey, was the world's first mechanically ventilated vehicular tunnel when it opened in 1927. The Queens-Midtown Tunnel, built to relieve congestion on the bridges connecting Manhattan with Queens and Brooklyn, was the largest non-federal project in its time when it was completed in 1940. President Franklin D. Roosevelt was the first person to drive through it. The Hugh L. Carey Tunnel runs underneath Battery Park and connects the Financial District at the southern tip of Manhattan to Red Hook in Brooklyn. Chocolate is New York City's leading specialty-food export, with up to US$234 million worth of exports each year. Entrepreneurs were forming a ""Chocolate District"" in Brooklyn as of 2014, while Godiva, one of the world's largest chocolatiers, continues to be headquartered in Manhattan. In soccer, New York City is represented by New York City FC of Major League Soccer, who play their home games at Yankee Stadium. The New York Red Bulls play their home games at Red Bull Arena in nearby Harrison, New Jersey. Historically, the city is known for the New York Cosmos, the highly successful former professional soccer team which was the American home of Pelé, one of the world's most famous soccer players. A new version of the New York Cosmos was formed in 2010, and began play in the second division North American Soccer League in 2013. The Cosmos play their home games at James M. Shuart Stadium on the campus of Hofstra University, just outside the New York City limits in Hempstead, New York. New York City traces its roots to its 1624 founding as a trading post by colonists of the Dutch Republic and was named New Amsterdam in 1626. The city and its surroundings came under English control in 1664. New York served as the capital of the United States from 1785 until 1790. It has been the country's largest city since 1790. The Statue of Liberty greeted millions of immigrants as they came to the Americas by ship in the late 19th and early 20th centuries and is a globally recognized symbol of the United States and its democracy. Gateway National Recreation Area contains over 26,000 acres (10,521.83 ha) in total, most of it surrounded by New York City, including the Jamaica Bay Wildlife Refuge in Brooklyn and Queens, over 9,000 acres (36 km2) of salt marsh, islands, and water, including most of Jamaica Bay. Also in Queens, the park includes a significant portion of the western Rockaway Peninsula, most notably Jacob Riis Park and Fort Tilden. In Staten Island, the park includes Fort Wadsworth, with historic pre-Civil War era Battery Weed and Fort Tompkins, and Great Kills Park, with beaches, trails, and a marina. Several prominent American literary figures lived in New York during the 1830s and 1840s, including William Cullen Bryant, Washington Irving, Herman Melville, Rufus Wilmot Griswold, John Keese, Nathaniel Parker Willis, and Edgar Allan Poe. Public-minded members of the contemporaneous business elite lobbied for the establishment of Central Park, which in 1857 became the first landscaped park in an American city. New York City has been described as the cultural capital of the world by the diplomatic consulates of Iceland and Latvia and by New York's Baruch College. A book containing a series of essays titled New York, culture capital of the world, 1940–1965 has also been published as showcased by the National Library of Australia. In describing New York, author Tom Wolfe said, ""Culture just seems to be in the air, like part of the weather."" The wider New York City metropolitan area, with over 20 million people, about 50% greater than the second-place Los Angeles metropolitan area in the United States, is also ethnically diverse. The New York region continues to be by far the leading metropolitan gateway for legal immigrants admitted into the United States, substantially exceeding the combined totals of Los Angeles and Miami, the next most popular gateway regions. It is home to the largest Jewish as well as Israeli communities outside Israel, with the Jewish population in the region numbering over 1.5 million in 2012 and including many diverse Jewish sects from around the Middle East and Eastern Europe. The metropolitan area is also home to 20% of the nation's Indian Americans and at least 20 Little India enclaves, as well as 15% of all Korean Americans and four Koreatowns; the largest Asian Indian population in the Western Hemisphere; the largest Russian American, Italian American, and African American populations; the largest Dominican American, Puerto Rican American, and South American and second-largest overall Hispanic population in the United States, numbering 4.8 million; and includes at least 6 established Chinatowns within New York City alone, with the urban agglomeration comprising a population of 779,269 overseas Chinese as of 2013 Census estimates, the largest outside of Asia. The only attempt at a peaceful solution to the war took place at the Conference House on Staten Island between American delegates, including Benjamin Franklin, and British general Lord Howe on September 11, 1776. Shortly after the British occupation began, the Great Fire of New York occurred, a large conflagration on the West Side of Lower Manhattan, which destroyed about a quarter of the buildings in the city, including Trinity Church. Lower Manhattan is the third-largest central business district in the United States and is home to the New York Stock Exchange, on Wall Street, and the NASDAQ, at 165 Broadway, representing the world's largest and second largest stock exchanges, respectively, when measured both by overall average daily trading volume and by total market capitalization of their listed companies in 2013. Investment banking fees on Wall Street totaled approximately $40 billion in 2012, while in 2013, senior New York City bank officers who manage risk and compliance functions earned as much as $324,000 annually. In fiscal year 2013–14, Wall Street's securities industry generated 19% of New York State's tax revenue. New York City remains the largest global center for trading in public equity and debt capital markets, driven in part by the size and financial development of the U.S. economy.:31–32 In July 2013, NYSE Euronext, the operator of the New York Stock Exchange, took over the administration of the London interbank offered rate from the British Bankers Association. New York also leads in hedge fund management; private equity; and the monetary volume of mergers and acquisitions. Several investment banks and investment mangers headquartered in Manhattan are important participants in other global financial centers.:34–35 New York is also the principal commercial banking center of the United States. In the 19th century, the city was transformed by development relating to its status as a trading center, as well as by European immigration. The city adopted the Commissioners' Plan of 1811, which expanded the city street grid to encompass all of Manhattan. The 1825 completion of the Erie Canal through central New York connected the Atlantic port to the agricultural markets and commodities of the North American interior via the Hudson River and the Great Lakes. Local politics became dominated by Tammany Hall, a political machine supported by Irish and German immigrants. New York has architecturally noteworthy buildings in a wide range of styles and from distinct time periods, from the saltbox style Pieter Claesen Wyckoff House in Brooklyn, the oldest section of which dates to 1656, to the modern One World Trade Center, the skyscraper at Ground Zero in Lower Manhattan and currently the most expensive new office tower in the world."
Central_African_Republic,"Agriculture is dominated by the cultivation and sale of food crops such as cassava, peanuts, maize, sorghum, millet, sesame, and plantain. The annual real GDP growth rate is just above 3%. The importance of food crops over exported cash crops is indicated by the fact that the total production of cassava, the staple food of most Central Africans, ranges between 200,000 and 300,000 tonnes a year, while the production of cotton, the principal exported cash crop, ranges from 25,000 to 45,000 tonnes a year. Food crops are not exported in large quantities, but still constitute the principal cash crops of the country, because Central Africans derive far more income from the periodic sale of surplus food crops than from exported cash crops such as cotton or coffee.[citation needed] Much of the country is self-sufficient in food crops; however, livestock development is hindered by the presence of the tsetse fly.[citation needed] During the 16th and 17th centuries slave traders began to raid the region as part of the expansion of the Saharan and Nile River slave routes. Their captives were slaved and shipped to the Mediterranean coast, Europe, Arabia, the Western Hemisphere, or to the slave ports and factories along the West and North Africa or South the Ubanqui and Congo rivers. In the mid 19th century, the Bobangi people became major slave traders and sold their captives to the Americas using the Ubangi river to reach the coast. During the 18th century Bandia-Nzakara peoples established the Bangassou Kingdom along the Ubangi River. A new government was appointed on 31 March 2013, which consisted of members of Séléka and representatives of the opposition to Bozizé, one pro-Bozizé individual, and a number representatives of civil society. On 1 April, the former opposition parties declared that they would boycott the government. After African leaders in Chad refused to recognize Djotodia as President, proposing to form a transitional council and the holding of new elections, Djotodia signed a decree on 6 April for the formation of a council that would act as a transitional parliament. The council was tasked with electing a president to serve prior to elections in 18 months. In 1920 French Equatorial Africa was established and Ubangi-Shari was administered from Brazzaville. During the 1920s and 1930s the French introduced a policy of mandatory cotton cultivation, a network of roads was built, attempts were made to combat sleeping sickness and Protestant missions were established to spread Christianity. New forms of forced labor were also introduced and a large number of Ubangians were sent to work on the Congo-Ocean Railway. Many of these forced laborers died of exhaustion, illness, or the poor conditions which claimed between 20% and 25% of the 127,000 workers. In the aftermath of the failed coup, militias loyal to Patassé sought revenge against rebels in many neighborhoods of Bangui and incited unrest including the murder of many political opponents. Eventually, Patassé came to suspect that General François Bozizé was involved in another coup attempt against him, which led Bozizé to flee with loyal troops to Chad. In March 2003, Bozizé launched a surprise attack against Patassé, who was out of the country. Libyan troops and some 1,000 soldiers of Bemba's Congolese rebel organization failed to stop the rebels and Bozizé's forces succeeded in overthrowing Patassé.[citation needed] Michel Djotodia took over as president and in May 2013 Central African Republic's Prime Minister Nicolas Tiangaye requested a UN peacekeeping force from the UN Security Council and on 31 May former President Bozizé was indicted for crimes against humanity and incitement of genocide. The security situation did not improve during June–August 2013 and there were reports of over 200,000 internally displaced persons (IDPs) as well as human rights abuses and renewed fighting between Séléka and Bozizé supporters. The per capita income of the Republic is often listed as being approximately $400 a year, one of the lowest in the world, but this figure is based mostly on reported sales of exports and largely ignores the unregistered sale of foods, locally produced alcoholic beverages, diamonds, ivory, bushmeat, and traditional medicine. For most Central Africans, the informal economy of the CAR is more important than the formal economy.[citation needed] Export trade is hindered by poor economic development and the country's landlocked position.[citation needed] In September 1940, during the Second World War, pro-Gaullist French officers took control of Ubangi-Shari and General Leclerc established his headquarters for the Free French Forces in Bangui. In 1946 Barthélémy Boganda was elected with 9,000 votes to the French National Assembly, becoming the first representative for CAR in the French government. Boganda maintained a political stance against racism and the colonial regime but gradually became disheartened with the French political system and returned to CAR to establish the Movement for the Social Evolution of Black Africa (MESAN) in 1950. In April 1979, young students protested against Bokassa's decree that all school attendees would need to buy uniforms from a company owned by one of his wives. The government violently suppressed the protests, killing 100 children and teenagers. Bokassa himself may have been personally involved in some of the killings. In September 1979, France overthrew Bokassa and ""restored"" Dacko to power (subsequently restoring the name of the country to the Central African Republic). Dacko, in turn, was again overthrown in a coup by General André Kolingba on 1 September 1981. Patassé purged many of the Kolingba elements from the government and Kolingba supporters accused Patassé's government of conducting a ""witch hunt"" against the Yakoma. A new constitution was approved on 28 December 1994 but had little impact on the country's politics. In 1996–1997, reflecting steadily decreasing public confidence in the government's erratic behaviour, three mutinies against Patassé's administration were accompanied by widespread destruction of property and heightened ethnic tension. During this time (1996) the Peace Corps evacuated all its volunteers to neighboring Cameroon. To date, the Peace Corps has not returned to the Central African Republic. The Bangui Agreements, signed in January 1997, provided for the deployment of an inter-African military mission, to Central African Republic and re-entry of ex-mutineers into the government on 7 April 1997. The inter-African military mission was later replaced by a U.N. peacekeeping force (MINURCA). There are many missionary groups operating in the country, including Lutherans, Baptists, Catholics, Grace Brethren, and Jehovah's Witnesses. While these missionaries are predominantly from the United States, France, Italy, and Spain, many are also from Nigeria, the Democratic Republic of the Congo, and other African countries. Large numbers of missionaries left the country when fighting broke out between rebel and government forces in 2002–3, but many of them have now returned to continue their work. In 2006, due to ongoing violence, over 50,000 people in the country's northwest were at risk of starvation but this was averted due to assistance from the United Nations.[citation needed] On 8 January 2008, the UN Secretary-General Ban Ki-Moon declared that the Central African Republic was eligible to receive assistance from the Peacebuilding Fund. Three priority areas were identified: first, the reform of the security sector; second, the promotion of good governance and the rule of law; and third, the revitalization of communities affected by conflicts. On 12 June 2008, the Central African Republic requested assistance from the UN Peacebuilding Commission, which was set up in 2005 to help countries emerging from conflict avoid devolving back into war or chaos. What is today the Central African Republic has been inhabited for millennia; however, the country's current borders were established by France, which ruled the country as a colony starting in the late 19th century. After gaining independence from France in 1960, the Central African Republic was ruled by a series of autocratic leaders; by the 1990s, calls for democracy led to the first multi-party democratic elections in 1993. Ange-Félix Patassé became president, but was later removed by General François Bozizé in the 2003 coup. The Central African Republic Bush War began in 2004 and, despite a peace treaty in 2007 and another in 2011, fighting broke out between various factions in December 2012, leading to ethnic and religious cleansing of the Muslim minority and massive population displacement in 2013 and 2014. The Central African Republic (CAR; Sango: Ködörösêse tî Bêafrîka; French: République centrafricaine  pronounced: [ʁepyblik sɑ̃tʁafʁikɛn], or Centrafrique [sɑ̃tʀafʁik]) is a landlocked country in Central Africa. It is bordered by Chad to the north, Sudan to the northeast, South Sudan to the east, the Democratic Republic of the Congo and the Republic of the Congo to the south and Cameroon to the west. The CAR covers a land area of about 620,000 square kilometres (240,000 sq mi) and had an estimated population of around 4.7 million as of 2014[update]. Approximately 10,000 years ago, desertification forced hunter-gatherer societies south into the Sahel regions of northern Central Africa, where some groups settled and began farming as part of the Neolithic Revolution. Initial farming of white yam progressed into millet and sorghum, and before 3000 BC the domestication of African oil palm improved the groups' nutrition and allowed for expansion of the local populations. Bananas arrived in the region and added an important source of carbohydrates to the diet; they were also used in the production of alcoholic beverages.[when?] This Agricultural Revolution, combined with a ""Fish-stew Revolution"", in which fishing began to take place, and the use of boats, allowed for the transportation of goods. Products were often moved in ceramic pots, which are the first known examples of artistic expression from the region's inhabitants. The Syrte Agreement in February and the Birao Peace Agreement in April 2007 called for a cessation of hostilities, the billeting of FDPC fighters and their integration with FACA, the liberation of political prisoners, integration of FDPC into government, an amnesty for the UFDR, its recognition as a political party, and the integration of its fighters into the national army. Several groups continued to fight but other groups signed on to the agreement, or similar agreements with the government (e.g. UFR on 15 December 2008). The only major group not to sign an agreement at the time was the CPJP, which continued its activities and signed a peace agreement with the government on 25 August 2012. By 1990, inspired by the fall of the Berlin Wall, a pro-democracy movement arose. Pressure from the United States, France, and from a group of locally represented countries and agencies called GIBAFOR (France, the USA, Germany, Japan, the EU, the World Bank, and the UN) finally led Kolingba to agree, in principle, to hold free elections in October 1992 with help from the UN Office of Electoral Affairs. After using the excuse of alleged irregularities to suspend the results of the elections as a pretext for holding on to power, President Kolingba came under intense pressure from GIBAFOR to establish a ""Conseil National Politique Provisoire de la République"" (Provisional National Political Council, CNPPR) and to set up a ""Mixed Electoral Commission"", which included representatives from all political parties.[citation needed] In the Ubangi-Shari Territorial Assembly election in 1957, MESAN captured 347,000 out of the total 356,000 votes, and won every legislative seat, which led to Boganda being elected president of the Grand Council of French Equatorial Africa and vice-president of the Ubangi-Shari Government Council. Within a year, he declared the establishment of the Central African Republic and served as the country's first prime minister. MESAN continued to exist, but its role was limited. After Boganda's death in a plane crash on 29 March 1959, his cousin, David Dacko, took control of MESAN and became the country's first president after the CAR had formally received independence from France. Dacko threw out his political rivals, including former Prime Minister and Mouvement d'évolution démocratique de l'Afrique centrale (MEDAC), leader Abel Goumba, whom he forced into exile in France. With all opposition parties suppressed by November 1962, Dacko declared MESAN as the official party of the state. When a second round of elections were finally held in 1993, again with the help of the international community coordinated by GIBAFOR, Ange-Félix Patassé won in the second round of voting with 53% of the vote while Goumba won 45.6%. Patassé's party, the Mouvement pour la Libération du Peuple Centrafricain (MLPC) or Movement for the Liberation of the Central African People, gained a simple but not an absolute majority of seats in parliament, which meant Patassé's party required coalition partners.[citation needed] Presently, the Central African Republic has active television services, radio stations, internet service providers, and mobile phone carriers; Socatel is the leading provider for both internet and mobile phone access throughout the country. The primary governmental regulating bodies of telecommunications are the Ministère des Postes and Télécommunications et des Nouvelles Technologies. In addition, the Central African Republic receives international support on telecommunication related operations from ITU Telecommunication Development Sector (ITU-D) within the International Telecommunication Union to improve infrastructure. The 2009 Human Rights Report by the United States Department of State noted that human rights in CAR were poor and expressed concerns over numerous government abuses. The U.S. State Department alleged that major human rights abuses such as extrajudicial executions by security forces, torture, beatings and rape of suspects and prisoners occurred with impunity. It also alleged harsh and life-threatening conditions in prisons and detention centers, arbitrary arrest, prolonged pretrial detention and denial of a fair trial, restrictions on freedom of movement, official corruption, and restrictions on workers' rights. In 2004 the Central African Republic Bush War began as forces opposed to Bozizé took up arms against his government. In May 2005 Bozizé won a presidential election that excluded Patassé and in 2006 fighting continued between the government and the rebels. In November 2006, Bozizé's government requested French military support to help them repel rebels who had taken control of towns in the country's northern regions. Though the initially public details of the agreement pertained to logistics and intelligence, the French assistance eventually included strikes by Mirage jets against rebel positions. In the southwest, the Dzanga-Sangha National Park is located in a rain forest area. The country is noted for its population of forest elephants and western lowland gorillas. In the north, the Manovo-Gounda St Floris National Park is well-populated with wildlife, including leopards, lions, cheetahs and rhinos, and the Bamingui-Bangoran National Park is located in the northeast of CAR. The parks have been seriously affected by the activities of poachers, particularly those from Sudan, over the past two decades.[citation needed]"
Packet_switching,"Internet2 is a not-for-profit United States computer networking consortium led by members from the research and education communities, industry, and government. The Internet2 community, in partnership with Qwest, built the first Internet2 Network, called Abilene, in 1998 and was a prime investor in the National LambdaRail (NLR) project. In 2006, Internet2 announced a partnership with Level 3 Communications to launch a brand new nationwide network, boosting its capacity from 10 Gbit/s to 100 Gbit/s. In October, 2007, Internet2 officially retired Abilene and now refers to its new, higher capacity network as the Internet2 Network. Starting in 1965, Donald Davies at the National Physical Laboratory, UK, independently developed the same message routing methodology as developed by Baran. He called it packet switching, a more accessible name than Baran's, and proposed to build a nationwide network in the UK. He gave a talk on the proposal in 1966, after which a person from the Ministry of Defence (MoD) told him about Baran's work. A member of Davies' team (Roger Scantlebury) met Lawrence Roberts at the 1967 ACM Symposium on Operating System Principles and suggested it for use in the ARPANET. In connectionless mode each packet includes complete addressing information. The packets are routed individually, sometimes resulting in different paths and out-of-order delivery. Each packet is labeled with a destination address, source address, and port numbers. It may also be labeled with the sequence number of the packet. This precludes the need for a dedicated path to help the packet find its way to its destination, but means that much more information is needed in the packet header, which is therefore larger, and this information needs to be looked up in power-hungry content-addressable memory. Each packet is dispatched and may go via different routes; potentially, the system has to do as much work for every packet as the connection-oriented system has to do in connection set-up, but with less information as to the application's requirements. At the destination, the original message/data is reassembled in the correct order, based on the packet sequence number. Thus a virtual connection, also known as a virtual circuit or byte stream is provided to the end-user by a transport layer protocol, although intermediate network nodes only provides a connectionless network layer service. Connection-oriented transmission requires a setup phase in each involved node before any packet is transferred to establish the parameters of communication. The packets include a connection identifier rather than address information and are negotiated between endpoints so that they are delivered in order and with error checking. Address information is only transferred to each node during the connection set-up phase, when the route to the destination is discovered and an entry is added to the switching table in each network node through which the connection passes. The signaling protocols used allow the application to specify its requirements and discover link parameters. Acceptable values for service parameters may be negotiated. Routing a packet requires the node to look up the connection id in a table. The packet header can be small, as it only needs to contain this code and any information, such as length, timestamp, or sequence number, which is different for different packets. ARPANET and SITA HLN became operational in 1969. Before the introduction of X.25 in 1973, about twenty different network technologies had been developed. Two fundamental differences involved the division of functions and tasks between the hosts at the edge of the network and the network core. In the datagram system, the hosts have the responsibility to ensure orderly delivery of packets. The User Datagram Protocol (UDP) is an example of a datagram protocol. In the virtual call system, the network guarantees sequenced delivery of data to the host. This results in a simpler host interface with less functionality than in the datagram model. The X.25 protocol suite uses this network type. Datanet 1 was the public switched data network operated by the Dutch PTT Telecom (now known as KPN). Strictly speaking Datanet 1 only referred to the network and the connected users via leased lines (using the X.121 DNIC 2041), the name also referred to the public PAD service Telepad (using the DNIC 2049). And because the main Videotex service used the network and modified PAD devices as infrastructure the name Datanet 1 was used for these services as well. Although this use of the name was incorrect all these services were managed by the same people within one department of KPN contributed to the confusion. Baran developed the concept of distributed adaptive message block switching during his research at the RAND Corporation for the US Air Force into survivable communications networks, first presented to the Air Force in the summer of 1961 as briefing B-265, later published as RAND report P-2626 in 1962, and finally in report RM 3420 in 1964. Report P-2626 described a general architecture for a large-scale, distributed, survivable communications network. The work focuses on three key ideas: use of a decentralized network with multiple paths between any two points, dividing user messages into message blocks, later called packets, and delivery of these messages by store and forward switching. In 1965, at the instigation of Warner Sinback, a data network based on this voice-phone network was designed to connect GE's four computer sales and service centers (Schenectady, Phoenix, Chicago, and Phoenix) to facilitate a computer time-sharing service, apparently the world's first commercial online service. (In addition to selling GE computers, the centers were computer service bureaus, offering batch processing services. They lost money from the beginning, and Sinback, a high-level marketing manager, was given the job of turning the business around. He decided that a time-sharing system, based on Kemney's work at Dartmouth—which used a computer on loan from GE—could be profitable. Warner was right.) The National Science Foundation Network (NSFNET) was a program of coordinated, evolving projects sponsored by the National Science Foundation (NSF) beginning in 1985 to promote advanced research and education networking in the United States. NSFNET was also the name given to several nationwide backbone networks operating at speeds of 56 kbit/s, 1.5 Mbit/s (T1), and 45 Mbit/s (T3) that were constructed to support NSF's networking initiatives from 1985-1995. Initially created to link researchers to the nation's NSF-funded supercomputing centers, through further public funding and private industry partnerships it developed into a major part of the Internet backbone. The Very high-speed Backbone Network Service (vBNS) came on line in April 1995 as part of a National Science Foundation (NSF) sponsored project to provide high-speed interconnection between NSF-sponsored supercomputing centers and select access points in the United States. The network was engineered and operated by MCI Telecommunications under a cooperative agreement with the NSF. By 1998, the vBNS had grown to connect more than 100 universities and research and engineering institutions via 12 national points of presence with DS-3 (45 Mbit/s), OC-3c (155 Mbit/s), and OC-12c (622 Mbit/s) links on an all OC-12c backbone, a substantial engineering feat for that time. The vBNS installed one of the first ever production OC-48c (2.5 Gbit/s) IP links in February 1999 and went on to upgrade the entire backbone to OC-48c. Starting in the late 1950s, American computer scientist Paul Baran developed the concept Distributed Adaptive Message Block Switching with the goal to provide a fault-tolerant, efficient routing method for telecommunication messages as part of a research program at the RAND Corporation, funded by the US Department of Defense. This concept contrasted and contradicted the theretofore established principles of pre-allocation of network bandwidth, largely fortified by the development of telecommunications in the Bell System. The new concept found little resonance among network implementers until the independent work of Donald Davies at the National Physical Laboratory (United Kingdom) (NPL) in the late 1960s. Davies is credited with coining the modern name packet switching and inspiring numerous packet switching networks in Europe in the decade following, including the incorporation of the concept in the early ARPANET in the United States. Packet mode communication may be implemented with or without intermediate forwarding nodes (packet switches or routers). Packets are normally forwarded by intermediate network nodes asynchronously using first-in, first-out buffering, but may be forwarded according to some scheduling discipline for fair queuing, traffic shaping, or for differentiated or guaranteed quality of service, such as weighted fair queuing or leaky bucket. In case of a shared physical medium (such as radio or 10BASE5), the packets may be delivered according to a multiple access scheme. The CYCLADES packet switching network was a French research network designed and directed by Louis Pouzin. First demonstrated in 1973, it was developed to explore alternatives to the early ARPANET design and to support network research generally. It was the first network to make the hosts responsible for reliable delivery of data, rather than the network itself, using unreliable datagrams and associated end-to-end protocol mechanisms. Concepts of this network influenced later ARPANET architecture. AUSTPAC was an Australian public X.25 network operated by Telstra. Started by Telecom Australia in the early 1980s, AUSTPAC was Australia's first public packet-switched data network, supporting applications such as on-line betting, financial applications — the Australian Tax Office made use of AUSTPAC — and remote terminal access to academic institutions, who maintained their connections to AUSTPAC up until the mid-late 1990s in some cases. Access can be via a dial-up terminal to a PAD, or, by linking a permanent X.25 node to the network.[citation needed] DECnet is a suite of network protocols created by Digital Equipment Corporation, originally released in 1975 in order to connect two PDP-11 minicomputers. It evolved into one of the first peer-to-peer network architectures, thus transforming DEC into a networking powerhouse in the 1980s. Initially built with three layers, it later (1982) evolved into a seven-layer OSI-compliant networking protocol. The DECnet protocols were designed entirely by Digital Equipment Corporation. However, DECnet Phase II (and later) were open standards with published specifications, and several implementations were developed outside DEC, including one for Linux. Tymnet was an international data communications network headquartered in San Jose, CA that utilized virtual call packet switched technology and used X.25, SNA/SDLC, BSC and ASCII interfaces to connect host computers (servers)at thousands of large companies, educational institutions, and government agencies. Users typically connected via dial-up connections or dedicated async connections. The business consisted of a large public network that supported dial-up users and a private network business that allowed government agencies and large companies (mostly banks and airlines) to build their own dedicated networks. The private networks were often connected via gateways to the public network to reach locations not on the private network. Tymnet was also connected to dozens of other public networks in the U.S. and internationally via X.25/X.75 gateways. (Interesting note: Tymnet was not named after Mr. Tyme. Another employee suggested the name.)   AppleTalk was a proprietary suite of networking protocols developed by Apple Inc. in 1985 for Apple Macintosh computers. It was the primary protocol used by Apple devices through the 1980s and 90s. AppleTalk included features that allowed local area networks to be established ad hoc without the requirement for a centralized router or server. The AppleTalk system automatically assigned addresses, updated the distributed namespace, and configured any required inter-network routing. It was a plug-n-play system. Telenet was the first FCC-licensed public data network in the United States. It was founded by former ARPA IPTO director Larry Roberts as a means of making ARPANET technology public. He had tried to interest AT&T in buying the technology, but the monopoly's reaction was that this was incompatible with their future. Bolt, Beranack and Newman (BBN) provided the financing. It initially used ARPANET technology but changed the host interface to X.25 and the terminal interface to X.29. Telenet designed these protocols and helped standardize them in the CCITT. Telenet was incorporated in 1973 and started operations in 1975. It went public in 1979 and was then sold to GTE. There were two kinds of X.25 networks. Some such as DATAPAC and TRANSPAC were initially implemented with an X.25 external interface. Some older networks such as TELENET and TYMNET were modified to provide a X.25 host interface in addition to older host connection schemes. DATAPAC was developed by Bell Northern Research which was a joint venture of Bell Canada (a common carrier) and Northern Telecom (a telecommunications equipment supplier). Northern Telecom sold several DATAPAC clones to foreign PTTs including the Deutsche Bundespost. X.75 and X.121 allowed the interconnection of national X.25 networks. A user or host could call a host on a foreign network by including the DNIC of the remote network as part of the destination address.[citation needed] Both X.25 and Frame Relay provide connection-oriented operations. But X.25 does it at the network layer of the OSI Model. Frame Relay does it at level two, the data link layer. Another major difference between X.25 and Frame Relay is that X.25 requires a handshake between the communicating parties before any user packets are transmitted. Frame Relay does not define any such handshakes. X.25 does not define any operations inside the packet network. It only operates at the user-network-interface (UNI). Thus, the network provider is free to use any procedure it wishes inside the network. X.25 does specify some limited re-transmission procedures at the UNI, and its link layer protocol (LAPB) provides conventional HDLC-type link management procedures. Frame Relay is a modified version of ISDN's layer two protocol, LAPD and LAPB. As such, its integrity operations pertain only between nodes on a link, not end-to-end. Any retransmissions must be carried out by higher layer protocols. The X.25 UNI protocol is part of the X.25 protocol suite, which consists of the lower three layers of the OSI Model. It was widely used at the UNI for packet switching networks during the 1980s and early 1990s, to provide a standardized interface into and out of packet networks. Some implementations used X.25 within the network as well, but its connection-oriented features made this setup cumbersome and inefficient. Frame relay operates principally at layer two of the OSI Model. However, its address field (the Data Link Connection ID, or DLCI) can be used at the OSI network layer, with a minimum set of procedures. Thus, it rids itself of many X.25 layer 3 encumbrances, but still has the DLCI as an ID beyond a node-to-node layer two link protocol. The simplicity of Frame Relay makes it faster and more efficient than X.25. Because Frame relay is a data link layer protocol, like X.25 it does not define internal network routing operations. For X.25 its packet IDs---the virtual circuit and virtual channel numbers have to be correlated to network addresses. The same is true for Frame Relays DLCI. How this is done is up to the network provider. Frame Relay, by virtue of having no network layer procedures is connection-oriented at layer two, by using the HDLC/LAPD/LAPB Set Asynchronous Balanced Mode (SABM). X.25 connections are typically established for each communication session, but it does have a feature allowing a limited amount of traffic to be passed across the UNI without the connection-oriented handshake. For a while, Frame Relay was used to interconnect LANs across wide area networks. However, X.25 and well as Frame Relay have been supplanted by the Internet Protocol (IP) at the network layer, and the Asynchronous Transfer Mode (ATM) and or versions of Multi-Protocol Label Switching (MPLS) at layer two. A typical configuration is to run IP over ATM or a version of MPLS. <Uyless Black, X.25 and Related Protocols, IEEE Computer Society, 1991> <Uyless Black, Frame Relay Networks, McGraw-Hill, 1998> <Uyless Black, MPLS and Label Switching Networks, Prentice Hall, 2001> < Uyless Black, ATM, Volume I, Prentice Hall, 1995> Merit Network, Inc., an independent non-profit 501(c)(3) corporation governed by Michigan's public universities, was formed in 1966 as the Michigan Educational Research Information Triad to explore computer networking between three of Michigan's public universities as a means to help the state's educational and economic development. With initial support from the State of Michigan and the National Science Foundation (NSF), the packet-switched network was first demonstrated in December 1971 when an interactive host to host connection was made between the IBM mainframe computer systems at the University of Michigan in Ann Arbor and Wayne State University in Detroit. In October 1972 connections to the CDC mainframe at Michigan State University in East Lansing completed the triad. Over the next several years in addition to host to host interactive connections the network was enhanced to support terminal to host connections, host to host batch connections (remote job submission, remote printing, batch file transfer), interactive file transfer, gateways to the Tymnet and Telenet public data networks, X.25 host attachments, gateways to X.25 data networks, Ethernet attached hosts, and eventually TCP/IP and additional public universities in Michigan join the network. All of this set the stage for Merit's role in the NSFNET project starting in the mid-1980s. Packet switching contrasts with another principal networking paradigm, circuit switching, a method which pre-allocates dedicated network bandwidth specifically for each communication session, each having a constant bit rate and latency between nodes. In cases of billable services, such as cellular communication services, circuit switching is characterized by a fee per unit of connection time, even when no data is transferred, while packet switching may be characterized by a fee per unit of information transmitted, such as characters, packets, or messages. The Computer Science Network (CSNET) was a computer network funded by the U.S. National Science Foundation (NSF) that began operation in 1981. Its purpose was to extend networking benefits, for computer science departments at academic and research institutions that could not be directly connected to ARPANET, due to funding or authorization limitations. It played a significant role in spreading awareness of, and access to, national networking and was a major milestone on the path to development of the global Internet."
Hanover,"The Schnellweg (en: expressway) system, a number of Bundesstraße roads, forms a structure loosely resembling a large ring road together with A2 and A7. The roads are B 3, B 6 and B 65, called Westschnellweg (B6 on the northern part, B3 on the southern part), Messeschnellweg (B3, becomes A37 near Burgdorf, crosses A2, becomes B3 again, changes to B6 at Seelhorster Kreuz, then passes the Hanover fairground as B6 and becomes A37 again before merging into A7) and Südschnellweg (starts out as B65, becomes B3/B6/B65 upon crossing Westschnellweg, then becomes B65 again at Seelhorster Kreuz). After Napoleon imposed the Convention of Artlenburg (Convention of the Elbe) on July 5, 1803, about 30,000 French soldiers occupied Hanover. The Convention also required disbanding the army of Hanover. However, George III did not recognize the Convention of the Elbe. This resulted in a great number of soldiers from Hanover eventually emigrating to Great Britain, where the King's German Legion was formed. It was the only German army to fight against France throughout the entire Napoleonic wars. The Legion later played an important role in the Battle of Waterloo in 1815. The Congress of Vienna in 1815 elevated the electorate to the Kingdom of Hanover. The capital town Hanover expanded to the western bank of the Leine and since then has grown considerably. Another point of interest is the Old Town. In the centre are the large Marktkirche (Church St. Georgii et Jacobi, preaching venue of the bishop of the Lutheran Landeskirche Hannovers) and the Old Town Hall. Nearby are the Leibniz House, the Nolte House, and the Beguine Tower. A very nice quarter of the Old Town is the Kreuz-Church-Quarter around the Kreuz Church with many nice little lanes. Nearby is the old royal sports hall, now called the Ballhof theatre. On the edge of the Old Town are the Market Hall, the Leine Palace, and the ruin of the Aegidien Church which is now a monument to the victims of war and violence. Through the Marstall Gate you arrive at the bank of the river Leine, where the world-famous Nanas of Niki de Saint-Phalle are located. They are part of the Mile of Sculptures, which starts from Trammplatz, leads along the river bank, crosses Königsworther Square, and ends at the entrance of the Georgengarten. Near the Old Town is the district of Calenberger Neustadt where the Catholic Basilica Minor of St. Clemens, the Reformed Church and the Lutheran Neustädter Hof- und Stadtkirche St. Johannis are located. Hanover's leading cabaret-stage is the GOP Variety theatre which is located in the Georgs Palace. Some other famous cabaret-stages are the Variety Marlene, the Uhu-Theatre. the theatre Die Hinterbühne, the Rampenlich Variety and the revue-stage TAK. The most important Cabaret-Event is the Kleines Fest im Großen Garten (Little Festival in the Great Garden) which is the most successful Cabaret Festival in Germany. It features artists from around the world. Some other important events are the Calenberger Cabaret Weeks, the Hanover Cabaret Festival and the Wintervariety. The Hanover Zoo is one of the most spectacular and best zoos in Europe. The zoo received the Park Scout Award for the fourth year running in 2009/10, placing it among the best zoos in Germany. The zoo consists of several theme areas: Sambesi, Meyers Farm, Gorilla-Mountain, Jungle-Palace, and Mullewapp. Some smaller areas are Australia, the wooded area for wolves, and the so-called swimming area with many seabirds. There is also a tropical house, a jungle house, and a show arena. The new Canadian-themed area, Yukon Bay, opened in 2010. In 2010 the Hanover Zoo had over 1.6 million visitors. Hannover 96 (nickname Die Roten or 'The Reds') is the top local football team that plays in the Bundesliga top division. Home games are played at the HDI-Arena, which hosted matches in the 1974 and 2006 World Cups and the Euro 1988. Their reserve team Hannover 96 II plays in the fourth league. Their home games were played in the traditional Eilenriedestadium till they moved to the HDI Arena due to DFL directives. Arminia Hannover is another very traditional soccer team in Hanover that has played in the first league for years and plays now in the Niedersachsen-West Liga (Lower Saxony League West). Home matches are played in the Rudolf-Kalweit-Stadium. Around 40 theatres are located in Hanover. The Opera House, the Schauspielhaus (Play House), the Ballhofeins, the Ballhofzwei and the Cumberlandsche Galerie belong to the Lower Saxony State Theatre. The Theater am Aegi is Hanover's big theatre for musicals, shows and guest performances. The Neues Theater (New Theatre) is the Boulevard Theatre of Hanover. The Theater für Niedersachsen is another big theatre in Hanover, which also has an own Musical-Company. Some of the most important Musical-Productions are the rock musicals of the German rock musician Heinz Rudolph Kunze, which take place at the Garden-Theatre in the Great Garden. Some other popular sights are the Waterloo Column, the Laves House, the Wangenheim Palace, the Lower Saxony State Archives, the Hanover Playhouse, the Kröpcke Clock, the Anzeiger Tower Block, the Administration Building of the NORD/LB, the Cupola Hall of the Congress Centre, the Lower Saxony Stock, the Ministry of Finance, the Garten Church, the Luther Church, the Gehry Tower (designed by the American architect Frank O. Gehry), the specially designed Bus Stops, the Opera House, the Central Station, the Maschsee lake and the city forest Eilenriede, which is one of the largest of its kind in Europe. With around 40 parks, forests and gardens, a couple of lakes, two rivers and one canal, Hanover offers a large variety of leisure activities. A cabinet of coins is the Münzkabinett der TUI-AG. The Polizeigeschichtliche Sammlung Niedersachsen is the largest police museum in Germany. Textiles from all over the world can be visited in the Museum for textile art. The EXPOseeum is the museum of the world-exhibition ""EXPO 2000 Hannover"". Carpets and objects from the orient can be visited in the Oriental Carpet Museum. The Blind Man Museum is a rarity in Germany, another one is only in Berlin. The Museum of veterinary medicine is unique in Germany. The Museum for Energy History describes the 150 years old history of the application of energy. The Home Museum Ahlem shows the history of the district of Ahlem. The Mahn- und Gedenkstätte Ahlem describes the history of the Jewish people in Hanover and the Stiftung Ahlers Pro Arte / Kestner Pro Arte shows modern art. Modern art is also the main topic of the Kunsthalle Faust, the Nord/LB Art Gellery and of the Foro Artistico / Eisfabrik. After 1937 the Lord Mayor and the state commissioners of Hanover were members of the NSDAP (Nazi party). A large Jewish population then existed in Hanover. In October 1938, 484 Hanoverian Jews of Polish origin were expelled to Poland, including the Grynszpan family. However, Poland refused to accept them, leaving them stranded at the border with thousands of other Polish-Jewish deportees, fed only intermittently by the Polish Red Cross and Jewish welfare organisations. The Gryszpan's son Herschel Grynszpan was in Paris at the time. When he learned of what was happening, he drove to the German embassy in Paris and shot the German diplomat Eduard Ernst vom Rath, who died shortly afterwards. ""Hanover"" is the traditional English spelling. The German spelling (with a double n) is becoming more popular in English; recent editions of encyclopedias prefer the German spelling, and the local government uses the German spelling on English websites. The English pronunciation /ˈhænəvər/, with stress on the first syllable and a reduced second syllable, is applied to both the German and English spellings, which is different from German pronunciation [haˈnoːfɐ], with stress on the second syllable and a long second vowel. The traditional English spelling is still used in historical contexts, especially when referring to the British House of Hanover. In 1837, the personal union of the United Kingdom and Hanover ended because William IV's heir in the United Kingdom was female (Queen Victoria). Hanover could be inherited only by male heirs. Thus, Hanover passed to William IV's brother, Ernest Augustus, and remained a kingdom until 1866, when it was annexed by Prussia during the Austro-Prussian war. Despite being expected to defeat Prussia at the Battle of Langensalza, Prussia employed Moltke the Elder's Kesselschlacht order of battle to instead destroy the Hanoverian army. The city of Hanover became the capital of the Prussian Province of Hanover. After the annexation, the people of Hanover generally opposed the Prussian government. The Berggarten is an important European botanical garden.[citation needed] Some points of interest are the Tropical House, the Cactus House, the Canary House and the Orchid House, which hosts one of the world's biggest collection of orchids, and free-flying birds and butterflies. Near the entrance to the Berggarten is the historic Library Pavillon. The Mausoleum of the Guelphs is also located in the Berggarten. Like the Great Garden, the Berggarten also consists of several parts, for example the Paradies and the Prairie Garden. There is also the Sea Life Centre Hanover, which is the first tropical aquarium in Germany.[citation needed] In September 1941, through the ""Action Lauterbacher"" plan, a ghettoisation of the remaining Hanoverian Jewish families began. Even before the Wannsee Conference, on 15 December 1941, the first Jews from Hanover were deported to Riga. A total of 2,400 people were deported, and very few survived. During the war seven concentration camps were constructed in Hanover, in which many Jews were confined. Of the approximately 4,800 Jews who had lived in Hannover in 1938, fewer than 100 were still in the city when troops of the United States Army arrived on 10 April 1945 to occupy Hanover at the end of the war.[citation needed] Today, a memorial at the Opera Square is a reminder of the persecution of the Jews in Hanover. After the war a large group of Orthodox Jewish survivors of the nearby Bergen-Belsen concentration camp settled in Hanover. But Hanover is not only one of the most important Exhibition Cities in the world, it is also one of the German capitals for marksmen. The Schützenfest Hannover is the largest Marksmen's Fun Fair in the world and takes place once a year (late June to early July) (2014 - July 4th to the 13th). It consists of more than 260 rides and inns, five large beer tents and a big entertainment programme. The highlight of this fun fair is the 12 kilometres (7 mi) long Parade of the Marksmen with more than 12.000 participants from all over the world, among them around 5.000 marksmen, 128 bands and more than 70 wagons, carriages and big festival vehicles. It is the longest procession in Europe. Around 2 million people visit this fun fair every year. The landmark of this Fun Fair is the biggest transportable Ferris Wheel in the world (60 m or 197 ft high). The origins of this fun fair is located in the year 1529. In 1636 George, Duke of Brunswick-Lüneburg, ruler of the Brunswick-Lüneburg principality of Calenberg, moved his residence to Hanover. The Dukes of Brunswick-Lüneburg were elevated by the Holy Roman Emperor to the rank of Prince-Elector in 1692, and this elevation was confirmed by the Imperial Diet in 1708. Thus the principality was upgraded to the Electorate of Brunswick-Lüneburg, colloquially known as the Electorate of Hanover after Calenberg's capital (see also: House of Hanover). Its electors would later become monarchs of Great Britain (and from 1801, of the United Kingdom of Great Britain and Ireland). The first of these was George I Louis, who acceded to the British throne in 1714. The last British monarch who ruled in Hanover was William IV. Semi-Salic law, which required succession by the male line if possible, forbade the accession of Queen Victoria in Hanover. As a male-line descendant of George I, Queen Victoria was herself a member of the House of Hanover. Her descendants, however, bore her husband's titular name of Saxe-Coburg-Gotha. Three kings of Great Britain, or the United Kingdom, were concurrently also Electoral Princes of Hanover. With a population of 518,000, Hanover is a major centre of Northern Germany and the country's thirteenth largest city. Hanover also hosts annual commercial trade fairs such as the Hanover Fair and the CeBIT. Every year Hanover hosts the Schützenfest Hannover, the world's largest marksmen's festival, and the Oktoberfest Hannover, the second largest Oktoberfest in the world (beside Oktoberfest of Blumenau). In 2000, Hanover hosted the world fair Expo 2000. The Hanover fairground, due to numerous extensions, especially for the Expo 2000, is the largest in the world. Hanover is of national importance because of its universities and medical school, its international airport and its large zoo. The city is also a major crossing point of railway lines and highways (Autobahnen), connecting European main lines in both the east-west (Berlin–Ruhr area) and north-south (Hamburg–Munich, etc.) directions. As an important railroad and road junction and production center, Hanover was a major target for strategic bombing during World War II, including the Oil Campaign. Targets included the AFA (Stöcken), the Deurag-Nerag refinery (Misburg), the Continental plants (Vahrenwald and Limmer), the United light metal works (VLW) in Ricklingen and Laatzen (today Hanover fairground), the Hanover/Limmer rubber reclamation plant, the Hanomag factory (Linden) and the tank factory M.N.H. Maschinenfabrik Niedersachsen (Badenstedt). Forced labourers were sometimes used from the Hannover-Misburg subcamp of the Neuengamme concentration camp. Residential areas were also targeted, and more than 6,000 civilians were killed by the Allied bombing raids. More than 90% of the city center was destroyed in a total of 88 bombing raids. After the war, the Aegidienkirche was not rebuilt and its ruins were left as a war memorial. The Great Garden is an important European baroque garden. The palace itself, however, was largely destroyed by Allied bombing but is currently under reconstruction.[citation needed] Some points of interest are the Grotto (the interior was designed by the French artist Niki de Saint-Phalle), the Gallery Building, the Orangerie and the two pavilions by Remy de la Fosse. The Great Garden consists of several parts. The most popular ones are the Great Ground and the Nouveau Jardin. At the centre of the Nouveau Jardin is Europe's highest garden fountain. The historic Garden Theatre inter alia hosted the musicals of the German rock musician Heinz Rudolf Kunze.[citation needed] Various industrial businesses are located in Hannover. The Volkswagen Commercial Vehicles Transporter (VWN) factory at Hannover-Stöcken is the biggest employer in the region and operates a huge plant at the northern edge of town adjoining the Mittellandkanal and Motorway A2. Jointly with a factory of German tire and automobile parts manufacturer Continental AG, they have a coal-burning power plant. Continental AG, founded in Hanover in 1871, is one of the city's major companies, as is Sennheiser. Since 2008 a take-over is in progress: the Schaeffler Group from Herzogenaurach (Bavaria) holds the majority of the stock but were required due to the financial crisis to deposit the options as securities at banks. TUI AG has its HQ in Hanover. Hanover is home to many insurance companies, many of which operate only in Germany. One major global reinsurance company is Hannover Re, whose headquarters are east of the city centre. Hanover was founded in medieval times on the east bank of the River Leine. Its original name Honovere may mean ""high (river)bank"", though this is debated (cf. das Hohe Ufer). Hanover was a small village of ferrymen and fishermen that became a comparatively large town in the 13th century due to its position at a natural crossroads. As overland travel was relatively difficult, its position on the upper navigable reaches of the river helped it to grow by increasing trade. It was connected to the Hanseatic League city of Bremen by the Leine, and was situated near the southern edge of the wide North German Plain and north-west of the Harz mountains, so that east-west traffic such as mule trains passed through it. Hanover was thus a gateway to the Rhine, Ruhr and Saar river valleys, their industrial areas which grew up to the southwest and the plains regions to the east and north, for overland traffic skirting the Harz between the Low Countries and Saxony or Thuringia."
History_of_science,"Perhaps the most prominent, controversial and far-reaching theory in all of science has been the theory of evolution by natural selection put forward by the British naturalist Charles Darwin in his book On the Origin of Species in 1859. Darwin proposed that the features of all living things, including humans, were shaped by natural processes over long periods of time. The theory of evolution in its current form affects almost all areas of biology. Implications of evolution on fields outside of pure science have led to both opposition and support from different parts of society, and profoundly influenced the popular understanding of ""man's place in the universe"". In the early 20th century, the study of heredity became a major investigation after the rediscovery in 1900 of the laws of inheritance developed by the Moravian monk Gregor Mendel in 1866. Mendel's laws provided the beginnings of the study of genetics, which became a major field of research for both scientific and industrial research. By 1953, James D. Watson, Francis Crick and Maurice Wilkins clarified the basic structure of DNA, the genetic material for expressing life in all its forms. In the late 20th century, the possibilities of genetic engineering became practical for the first time, and a massive international effort began in 1990 to map out an entire human genome (the Human Genome Project). The earliest Greek philosophers, known as the pre-Socratics, provided competing answers to the question found in the myths of their neighbors: ""How did the ordered cosmos in which we live come to be?"" The pre-Socratic philosopher Thales (640-546 BC), dubbed the ""father of science"", was the first to postulate non-supernatural explanations for natural phenomena, for example, that land floats on water and that earthquakes are caused by the agitation of the water upon which the land floats, rather than the god Poseidon. Thales' student Pythagoras of Samos founded the Pythagorean school, which investigated mathematics for its own sake, and was the first to postulate that the Earth is spherical in shape. Leucippus (5th century BC) introduced atomism, the theory that all matter is made of indivisible, imperishable units called atoms. This was greatly expanded by his pupil Democritus. The renewal of learning in Europe, that began with 12th century Scholasticism, came to an end about the time of the Black Death, and the initial period of the subsequent Italian Renaissance is sometimes seen as a lull in scientific activity. The Northern Renaissance, on the other hand, showed a decisive shift in focus from Aristoteleian natural philosophy to chemistry and the biological sciences (botany, anatomy, and medicine). Thus modern science in Europe was resumed in a period of great upheaval: the Protestant Reformation and Catholic Counter-Reformation; the discovery of the Americas by Christopher Columbus; the Fall of Constantinople; but also the re-discovery of Aristotle during the Scholastic period presaged large social and political changes. Thus, a suitable environment was created in which it became possible to question scientific doctrine, in much the same way that Martin Luther and John Calvin questioned religious doctrine. The works of Ptolemy (astronomy) and Galen (medicine) were found not always to match everyday observations. Work by Vesalius on human cadavers found problems with the Galenic view of anatomy. An intellectual revitalization of Europe started with the birth of medieval universities in the 12th century. The contact with the Islamic world in Spain and Sicily, and during the Reconquista and the Crusades, allowed Europeans access to scientific Greek and Arabic texts, including the works of Aristotle, Ptolemy, Jābir ibn Hayyān, al-Khwarizmi, Alhazen, Avicenna, and Averroes. European scholars had access to the translation programs of Raymond of Toledo, who sponsored the 12th century Toledo School of Translators from Arabic to Latin. Later translators like Michael Scotus would learn Arabic in order to study these texts directly. The European universities aided materially in the translation and propagation of these texts and started a new infrastructure which was needed for scientific communities. In fact, European university put many works about the natural world and the study of nature at the center of its curriculum, with the result that the ""medieval university laid far greater emphasis on science than does its modern counterpart and descendent."" The above ""history of economics"" reflects modern economic textbooks and this means that the last stage of a science is represented as the culmination of its history (Kuhn, 1962). The ""invisible hand"" mentioned in a lost page in the middle of a chapter in the middle of the to ""Wealth of Nations"", 1776, advances as Smith's central message.[clarification needed] It is played down that this ""invisible hand"" acts only ""frequently"" and that it is ""no part of his [the individual's] intentions"" because competition leads to lower prices by imitating ""his"" invention. That this ""invisible hand"" prefers ""the support of domestic to foreign industry"" is cleansed—often without indication that part of the citation is truncated. The opening passage of the ""Wealth"" containing Smith's message is never mentioned as it cannot be integrated into modern theory: ""Wealth"" depends on the division of labour which changes with market volume and on the proportion of productive to Unproductive labor. Ibn Sina (Avicenna) is regarded as the most influential philosopher of Islam. He pioneered the science of experimental medicine and was the first physician to conduct clinical trials. His two most notable works in medicine are the Kitāb al-shifāʾ (""Book of Healing"") and The Canon of Medicine, both of which were used as standard medicinal texts in both the Muslim world and in Europe well into the 17th century. Amongst his many contributions are the discovery of the contagious nature of infectious diseases, and the introduction of clinical pharmacology. Humboldtian science refers to the early 19th century approach of combining scientific field work with the age of Romanticism sensitivity, ethics and aesthetic ideals. It helped to install natural history as a separate field, gave base for ecology and was based on the role model of scientist, naturalist and explorer Alexander von Humboldt. The later 19th century positivism asserted that all authentic knowledge allows verification and that all authentic knowledge assumes that the only valid knowledge is scientific. In Western culture, the study of politics is first found in Ancient Greece. The antecedents of European politics trace their roots back even earlier than Plato and Aristotle, particularly in the works of Homer, Hesiod, Thucydides, Xenophon, and Euripides. Later, Plato analyzed political systems, abstracted their analysis from more literary- and history- oriented studies and applied an approach we would understand as closer to philosophy. Similarly, Aristotle built upon Plato's analysis to include historical empirical evidence in his analysis. Astronomy: Astronomical observations from China constitute the longest continuous sequence from any civilisation and include records of sunspots (112 records from 364 BC), supernovas (1054), lunar and solar eclipses. By the 12th century, they could reasonably accurately make predictions of eclipses, but the knowledge of this was lost during the Ming dynasty, so that the Jesuit Matteo Ricci gained much favour in 1601 by his predictions. By 635 Chinese astronomers had observed that the tails of comets always point away from the sun. The development of writing enabled knowledge to be stored and communicated across generations with much greater fidelity. Combined with the development of agriculture, which allowed for a surplus of food, it became possible for early civilizations to develop, because more time and effort could be devoted to tasks (other than food production) than hunter-gatherers or early subsistence farmers had available. This surplus allowed a community to support individuals who did things other than work towards bare survival. These other tasks included systematic studies of nature, study of written information gathered and recorded by others, and often of adding to that body of information. In 1847, Hungarian physician Ignác Fülöp Semmelweis dramatically reduced the occurrency of puerperal fever by simply requiring physicians to wash their hands before attending to women in childbirth. This discovery predated the germ theory of disease. However, Semmelweis' findings were not appreciated by his contemporaries and came into use only with discoveries by British surgeon Joseph Lister, who in 1865 proved the principles of antisepsis. Lister's work was based on the important findings by French biologist Louis Pasteur. Pasteur was able to link microorganisms with disease, revolutionizing medicine. He also devised one of the most important methods in preventive medicine, when in 1880 he produced a vaccine against rabies. Pasteur invented the process of pasteurization, to help prevent the spread of disease through milk and other foods. Political science is a late arrival in terms of social sciences[citation needed]. However, the discipline has a clear set of antecedents such as moral philosophy, political philosophy, political economy, history, and other fields concerned with normative determinations of what ought to be and with deducing the characteristics and functions of the ideal form of government. The roots of politics are in prehistory. In each historic period and in almost every geographic area, we can find someone studying politics and increasing political understanding. The end of the 19th century marks the start of psychology as a scientific enterprise. The year 1879 is commonly seen as the start of psychology as an independent field of study. In that year Wilhelm Wundt founded the first laboratory dedicated exclusively to psychological research (in Leipzig). Other important early contributors to the field include Hermann Ebbinghaus (a pioneer in memory studies), Ivan Pavlov (who discovered classical conditioning), William James, and Sigmund Freud. Freud's influence has been enormous, though more as cultural icon than a force in scientific psychology. From their beginnings in Sumer (now Iraq) around 3500 BC, the Mesopotamian people began to attempt to record some observations of the world with numerical data. But their observations and measurements were seemingly taken for purposes other than for elucidating scientific laws. A concrete instance of Pythagoras' law was recorded, as early as the 18th century BC: the Mesopotamian cuneiform tablet Plimpton 322 records a number of Pythagorean triplets (3,4,5) (5,12,13). ..., dated 1900 BC, possibly millennia before Pythagoras,  but an abstract formulation of the Pythagorean theorem was not. In Babylonian astronomy, records of the motions of the stars, planets, and the moon are left on thousands of clay tablets created by scribes. Even today, astronomical periods identified by Mesopotamian proto-scientists are still widely used in Western calendars such as the solar year and the lunar month. Using these data they developed arithmetical methods to compute the changing length of daylight in the course of the year and to predict the appearances and disappearances of the Moon and planets and eclipses of the Sun and Moon. Only a few astronomers' names are known, such as that of Kidinnu, a Chaldean astronomer and mathematician. Kiddinu's value for the solar year is in use for today's calendars. Babylonian astronomy was ""the first and highly successful attempt at giving a refined mathematical description of astronomical phenomena."" According to the historian A. Aaboe, ""all subsequent varieties of scientific astronomy, in the Hellenistic world, in India, in Islam, and in the West—if not indeed all subsequent endeavour in the exact sciences—depend upon Babylonian astronomy in decisive and fundamental ways."" Geology did not undergo systematic restructuring during the Scientific Revolution, but individual theorists made important contributions. Robert Hooke, for example, formulated a theory of earthquakes, and Nicholas Steno developed the theory of superposition and argued that fossils were the remains of once-living creatures. Beginning with Thomas Burnet's Sacred Theory of the Earth in 1681, natural philosophers began to explore the idea that the Earth had changed over time. Burnet and his contemporaries interpreted Earth's past in terms of events described in the Bible, but their work laid the intellectual foundations for secular interpretations of Earth history. Indian astronomer and mathematician Aryabhata (476-550), in his Aryabhatiya (499) introduced a number of trigonometric functions (including sine, versine, cosine and inverse sine), trigonometric tables, and techniques and algorithms of algebra. In 628 AD, Brahmagupta suggested that gravity was a force of attraction. He also lucidly explained the use of zero as both a placeholder and a decimal digit, along with the Hindu-Arabic numeral system now used universally throughout the world. Arabic translations of the two astronomers' texts were soon available in the Islamic world, introducing what would become Arabic numerals to the Islamic World by the 9th century. During the 14th–16th centuries, the Kerala school of astronomy and mathematics made significant advances in astronomy and especially mathematics, including fields such as trigonometry and analysis. In particular, Madhava of Sangamagrama is considered the ""founder of mathematical analysis"". In Hellenistic Egypt, the mathematician Euclid laid down the foundations of mathematical rigor and introduced the concepts of definition, axiom, theorem and proof still in use today in his Elements, considered the most influential textbook ever written. Archimedes, considered one of the greatest mathematicians of all time, is credited with using the method of exhaustion to calculate the area under the arc of a parabola with the summation of an infinite series, and gave a remarkably accurate approximation of Pi. He is also known in physics for laying the foundations of hydrostatics, statics, and the explanation of the principle of the lever. Ancient Egypt made significant advances in astronomy, mathematics and medicine. Their development of geometry was a necessary outgrowth of surveying to preserve the layout and ownership of farmland, which was flooded annually by the Nile river. The 3-4-5 right triangle and other rules of thumb were used to build rectilinear structures, and the post and lintel architecture of Egypt. Egypt was also a center of alchemy research for much of the Mediterranean.The Edwin Smith papyrus is one of the first medical documents still extant, and perhaps the earliest document that attempts to describe and analyse the brain: it might be seen as the very beginnings of modern neuroscience. However, while Egyptian medicine had some effective practices, it was not without its ineffective and sometimes harmful practices. Medical historians believe that ancient Egyptian pharmacology, for example, was largely ineffective. Nevertheless, it applies the following components to the treatment of disease: examination, diagnosis, treatment, and prognosis, which display strong parallels to the basic empirical method of science and according to G. E. R. Lloyd played a significant role in the development of this methodology. The Ebers papyrus (c. 1550 BC) also contains evidence of traditional empiricism. Further studies, e.g. Jerome Ravetz 1971 Scientific Knowledge and its Social Problems referred to the role of the scientific community, as a social construct, in accepting or rejecting (objective) scientific knowledge. The Science wars of the 1990 were about the influence of especially French philosophers, which denied the objectivity of science in general or seemed to do so. They described as well differences between the idealized model of a pure science and the actual scientific practice; while scientism, a revival of the positivism approach, saw in precise measurement and rigorous calculation the basis for finally settling enduring metaphysical and moral controversies. However, more recently some of the leading critical theorists have recognized that their postmodern deconstructions have at times been counter-productive, and are providing intellectual ammunition for reactionary interests. Bruno Latour noted that ""dangerous extremists are using the very same argument of social construction to destroy hard-won evidence that could save our lives. Was I wrong to participate in the invention of this field known as science studies? Is it enough to say that we did not really mean what we meant?"" Modern chemistry emerged from the sixteenth through the eighteenth centuries through the material practices and theories promoted by alchemy, medicine, manufacturing and mining. A decisive moment came when 'chymistry' was distinguished from alchemy by Robert Boyle in his work The Sceptical Chymist, in 1661; although the alchemical tradition continued for some time after his work. Other important steps included the gravimetric experimental practices of medical chemists like William Cullen, Joseph Black, Torbern Bergman and Pierre Macquer and through the work of Antoine Lavoisier (Father of Modern Chemistry) on oxygen and the law of conservation of mass, which refuted phlogiston theory. The theory that all matter is made of atoms, which are the smallest constituents of matter that cannot be broken down without losing the basic chemical and physical properties of that matter, was provided by John Dalton in 1803, although the question took a hundred years to settle as proven. Dalton also formulated the law of mass relationships. In 1869, Dmitri Mendeleev composed his periodic table of elements on the basis of Dalton's discoveries. The synthesis of urea by Friedrich Wöhler opened a new research field, organic chemistry, and by the end of the 19th century, scientists were able to synthesize hundreds of organic compounds. The later part of the 19th century saw the exploitation of the Earth's petrochemicals, after the exhaustion of the oil supply from whaling. By the 20th century, systematic production of refined materials provided a ready supply of products which provided not only energy, but also synthetic materials for clothing, medicine, and everyday disposable resources. Application of the techniques of organic chemistry to living organisms resulted in physiological chemistry, the precursor to biochemistry. The 20th century also saw the integration of physics and chemistry, with chemical properties explained as the result of the electronic structure of the atom. Linus Pauling's book on The Nature of the Chemical Bond used the principles of quantum mechanics to deduce bond angles in ever-more complicated molecules. Pauling's work culminated in the physical modelling of DNA, the secret of life (in the words of Francis Crick, 1953). In the same year, the Miller–Urey experiment demonstrated in a simulation of primordial processes, that basic constituents of proteins, simple amino acids, could themselves be built up from simpler molecules. The basis for classical economics forms Adam Smith's An Inquiry into the Nature and Causes of the Wealth of Nations, published in 1776. Smith criticized mercantilism, advocating a system of free trade with division of labour. He postulated an ""invisible hand"" that regulated economic systems made up of actors guided only by self-interest. Karl Marx developed an alternative economic theory, called Marxian economics. Marxian economics is based on the labor theory of value and assumes the value of good to be based on the amount of labor required to produce it. Under this assumption, capitalism was based on employers not paying the full value of workers labor to create profit. The Austrian school responded to Marxian economics by viewing entrepreneurship as driving force of economic development. This replaced the labor theory of value by a system of supply and demand. Much of the study of the history of science has been devoted to answering questions about what science is, how it functions, and whether it exhibits large-scale patterns and trends. The sociology of science in particular has focused on the ways in which scientists work, looking closely at the ways in which they ""produce"" and ""construct"" scientific knowledge. Since the 1960s, a common trend in science studies (the study of the sociology and history of science) has been to emphasize the ""human component"" of scientific knowledge, and to de-emphasize the view that scientific data are self-evident, value-free, and context-free. The field of Science and Technology Studies, an area that overlaps and often informs historical studies of science, focuses on the social context of science in both contemporary and historical periods. At the beginning of the 13th century, there were reasonably accurate Latin translations of the main works of almost all the intellectually crucial ancient authors, allowing a sound transfer of scientific ideas via both the universities and the monasteries. By then, the natural philosophy contained in these texts began to be extended by notable scholastics such as Robert Grosseteste, Roger Bacon, Albertus Magnus and Duns Scotus. Precursors of the modern scientific method, influenced by earlier contributions of the Islamic world, can be seen already in Grosseteste's emphasis on mathematics as a way to understand nature, and in the empirical approach admired by Bacon, particularly in his Opus Majus. Pierre Duhem's provocative thesis of the Catholic Church's Condemnation of 1277 led to the study of medieval science as a serious discipline, ""but no one in the field any longer endorses his view that modern science started in 1277"". However, many scholars agree with Duhem's view that the Middle Ages were a period of important scientific developments. The scientific revolution is a convenient boundary between ancient thought and classical physics. Nicolaus Copernicus revived the heliocentric model of the solar system described by Aristarchus of Samos. This was followed by the first known model of planetary motion given by Johannes Kepler in the early 17th century, which proposed that the planets follow elliptical orbits, with the Sun at one focus of the ellipse. Galileo (""Father of Modern Physics"") also made use of experiments to validate physical theories, a key element of the scientific method. An ancient Indian treatise on statecraft, economic policy and military strategy by Kautilya and Viṣhṇugupta, who are traditionally identified with Chāṇakya (c. 350–-283 BCE). In this treatise, the behaviors and relationships of the people, the King, the State, the Government Superintendents, Courtiers, Enemies, Invaders, and Corporations are analysed and documented. Roger Boesche describes the Arthaśāstra as ""a book of political realism, a book analysing how the political world does work and not very often stating how it ought to work, a book that frequently discloses to a king what calculating and sometimes brutal measures he must carry out to preserve the state and the common good."" American sociology in the 1940s and 1950s was dominated largely by Talcott Parsons, who argued that aspects of society that promoted structural integration were therefore ""functional"". This structural functionalism approach was questioned in the 1960s, when sociologists came to see this approach as merely a justification for inequalities present in the status quo. In reaction, conflict theory was developed, which was based in part on the philosophies of Karl Marx. Conflict theorists saw society as an arena in which different groups compete for control over resources. Symbolic interactionism also came to be regarded as central to sociological thinking. Erving Goffman saw social interactions as a stage performance, with individuals preparing ""backstage"" and attempting to control their audience through impression management. While these theories are currently prominent in sociological thought, other approaches exist, including feminist theory, post-structuralism, rational choice theory, and postmodernism. Astronomy: The first textual mention of astronomical concepts comes from the Vedas, religious literature of India. According to Sarma (2008): ""One finds in the Rigveda intelligent speculations about the genesis of the universe from nonexistence, the configuration of the universe, the spherical self-supporting earth, and the year of 360 days divided into 12 equal parts of 30 days each with a periodical intercalary month."". The first 12 chapters of the Siddhanta Shiromani, written by Bhāskara in the 12th century, cover topics such as: mean longitudes of the planets; true longitudes of the planets; the three problems of diurnal rotation; syzygies; lunar eclipses; solar eclipses; latitudes of the planets; risings and settings; the moon's crescent; conjunctions of the planets with each other; conjunctions of the planets with the fixed stars; and the patas of the sun and moon. The 13 chapters of the second part cover the nature of the sphere, as well as significant astronomical and trigonometric calculations based on it. Modern geology, like modern chemistry, gradually evolved during the 18th and early 19th centuries. Benoît de Maillet and the Comte de Buffon saw the Earth as much older than the 6,000 years envisioned by biblical scholars. Jean-Étienne Guettard and Nicolas Desmarest hiked central France and recorded their observations on some of the first geological maps. Aided by chemical experimentation, naturalists such as Scotland's John Walker, Sweden's Torbern Bergman, and Germany's Abraham Werner created comprehensive classification systems for rocks and minerals—a collective achievement that transformed geology into a cutting edge field by the end of the eighteenth century. These early geologists also proposed a generalized interpretations of Earth history that led James Hutton, Georges Cuvier and Alexandre Brongniart, following in the steps of Steno, to argue that layers of rock could be dated by the fossils they contained: a principle first applied to the geology of the Paris Basin. The use of index fossils became a powerful tool for making geological maps, because it allowed geologists to correlate the rocks in one locality with those of similar age in other, distant localities. Over the first half of the 19th century, geologists such as Charles Lyell, Adam Sedgwick, and Roderick Murchison applied the new technique to rocks throughout Europe and eastern North America, setting the stage for more detailed, government-funded mapping projects in later decades. Medicine: Findings from Neolithic graveyards in what is now Pakistan show evidence of proto-dentistry among an early farming culture. Ayurveda is a system of traditional medicine that originated in ancient India before 2500 BC, and is now practiced as a form of alternative medicine in other parts of the world. Its most famous text is the Suśrutasamhitā of Suśruta, which is notable for describing procedures on various forms of surgery, including rhinoplasty, the repair of torn ear lobes, perineal lithotomy, cataract surgery, and several other excisions and other surgical procedures. The beginning of the 20th century brought the start of a revolution in physics. The long-held theories of Newton were shown not to be correct in all circumstances. Beginning in 1900, Max Planck, Albert Einstein, Niels Bohr and others developed quantum theories to explain various anomalous experimental results, by introducing discrete energy levels. Not only did quantum mechanics show that the laws of motion did not hold on small scales, but even more disturbingly, the theory of general relativity, proposed by Einstein in 1915, showed that the fixed background of spacetime, on which both Newtonian mechanics and special relativity depended, could not exist. In 1925, Werner Heisenberg and Erwin Schrödinger formulated quantum mechanics, which explained the preceding quantum theories. The observation by Edwin Hubble in 1929 that the speed at which galaxies recede positively correlates with their distance, led to the understanding that the universe is expanding, and the formulation of the Big Bang theory by Georges Lemaître. The final decades of the 20th century have seen the rise of a new interdisciplinary approach to studying human psychology, known collectively as cognitive science. Cognitive science again considers the mind as a subject for investigation, using the tools of psychology, linguistics, computer science, philosophy, and neurobiology. New methods of visualizing the activity of the brain, such as PET scans and CAT scans, began to exert their influence as well, leading some researchers to investigate the mind by investigating the brain, rather than cognition. These new forms of investigation assume that a wide understanding of the human mind is possible, and that such an understanding may be applied to other research domains, such as artificial intelligence. Subsequently, Plato and Aristotle produced the first systematic discussions of natural philosophy, which did much to shape later investigations of nature. Their development of deductive reasoning was of particular importance and usefulness to later scientific inquiry. Plato founded the Platonic Academy in 387 BC, whose motto was ""Let none unversed in geometry enter here"", and turned out many notable philosophers. Plato's student Aristotle introduced empiricism and the notion that universal truths can be arrived at via observation and induction, thereby laying the foundations of the scientific method. Aristotle also produced many biological writings that were empirical in nature, focusing on biological causation and the diversity of life. He made countless observations of nature, especially the habits and attributes of plants and animals in the world around him, classified more than 540 animal species, and dissected at least 50. Aristotle's writings profoundly influenced subsequent Islamic and European scholarship, though they were eventually superseded in the Scientific Revolution. In the 1920s, John Maynard Keynes prompted a division between microeconomics and macroeconomics. Under Keynesian economics macroeconomic trends can overwhelm economic choices made by individuals. Governments should promote aggregate demand for goods as a means to encourage economic expansion. Following World War II, Milton Friedman created the concept of monetarism. Monetarism focuses on using the supply and demand of money as a method for controlling economic activity. In the 1970s, monetarism has adapted into supply-side economics which advocates reducing taxes as a means to increase the amount of money available for economic expansion. The discipline of ecology typically traces its origin to the synthesis of Darwinian evolution and Humboldtian biogeography, in the late 19th and early 20th centuries. Equally important in the rise of ecology, however, were microbiology and soil science—particularly the cycle of life concept, prominent in the work Louis Pasteur and Ferdinand Cohn. The word ecology was coined by Ernst Haeckel, whose particularly holistic view of nature in general (and Darwin's theory in particular) was important in the spread of ecological thinking. In the 1930s, Arthur Tansley and others began developing the field of ecosystem ecology, which combined experimental soil science with physiological concepts of energy and the techniques of field biology. The history of ecology in the 20th century is closely tied to that of environmentalism; the Gaia hypothesis, first formulated in the 1960s, and spreading in the 1970s, and more recently the scientific-religious movement of Deep Ecology have brought the two closer together. The astronomer Aristarchus of Samos was the first known person to propose a heliocentric model of the solar system, while the geographer Eratosthenes accurately calculated the circumference of the Earth. Hipparchus (c. 190 – c. 120 BC) produced the first systematic star catalog. The level of achievement in Hellenistic astronomy and engineering is impressively shown by the Antikythera mechanism (150-100 BC), an analog computer for calculating the position of planets. Technological artifacts of similar complexity did not reappear until the 14th century, when mechanical astronomical clocks appeared in Europe. The Jesuit China missions of the 16th and 17th centuries ""learned to appreciate the scientific achievements of this ancient culture and made them known in Europe. Through their correspondence European scientists first learned about the Chinese science and culture."" Western academic thought on the history of Chinese technology and science was galvanized by the work of Joseph Needham and the Needham Research Institute. Among the technological accomplishments of China were, according to the British scholar Needham, early seismological detectors (Zhang Heng in the 2nd century), the water-powered celestial globe (Zhang Heng), matches, the independent invention of the decimal system, dry docks, sliding calipers, the double-action piston pump, cast iron, the blast furnace, the iron plough, the multi-tube seed drill, the wheelbarrow, the suspension bridge, the winnowing machine, the rotary fan, the parachute, natural gas as fuel, the raised-relief map, the propeller, the crossbow, and a solid fuel rocket, the multistage rocket, the horse collar, along with contributions in logic, astronomy, medicine, and other fields. In 1938 Otto Hahn and Fritz Strassmann discovered nuclear fission with radiochemical methods, and in 1939 Lise Meitner and Otto Robert Frisch wrote the first theoretical interpretation of the fission process, which was later improved by Niels Bohr and John A. Wheeler. Further developments took place during World War II, which led to the practical application of radar and the development and use of the atomic bomb. Though the process had begun with the invention of the cyclotron by Ernest O. Lawrence in the 1930s, physics in the postwar period entered into a phase of what historians have called ""Big Science"", requiring massive machines, budgets, and laboratories in order to test their theories and move into new frontiers. The primary patron of physics became state governments, who recognized that the support of ""basic"" research could often lead to technologies useful to both military and industrial applications. Currently, general relativity and quantum mechanics are inconsistent with each other, and efforts are underway to unify the two. The mid 20th century saw a series of studies relying to the role of science in a social context, starting from Thomas Kuhn's The Structure of Scientific Revolutions in 1962. It opened the study of science to new disciplines by suggesting that the evolution of science was in part sociologically determined and that positivism did not explain the actual interactions and strategies of the human participants in science. As Thomas Kuhn put it, the history of science may be seen in more nuanced terms, such as that of competing paradigms or conceptual systems in a wider matrix that includes intellectual, cultural, economic and political themes outside of science. ""Partly by selection and partly by distortion, the scientists of earlier ages are implicitly presented as having worked upon the same set of fixed problems and in accordance with the same set of fixed canons that the most recent revolution in scientific theory and method made seem scientific."" In astronomy, Al-Battani improved the measurements of Hipparchus, preserved in the translation of Ptolemy's Hè Megalè Syntaxis (The great treatise) translated as Almagest. Al-Battani also improved the precision of the measurement of the precession of the Earth's axis. The corrections made to the geocentric model by al-Battani, Ibn al-Haytham, Averroes and the Maragha astronomers such as Nasir al-Din al-Tusi, Mo'ayyeduddin Urdi and Ibn al-Shatir are similar to Copernican heliocentric model. Heliocentric theories may have also been discussed by several other Muslim astronomers such as Ja'far ibn Muhammad Abu Ma'shar al-Balkhi, Abu-Rayhan Biruni, Abu Said al-Sijzi, Qutb al-Din al-Shirazi, and Najm al-Dīn al-Qazwīnī al-Kātibī. As an academic field, history of science began with the publication of William Whewell's History of the Inductive Sciences (first published in 1837). A more formal study of the history of science as an independent discipline was launched by George Sarton's publications, Introduction to the History of Science (1927) and the Isis journal (founded in 1912). Sarton exemplified the early 20th-century view of the history of science as the history of great men and great ideas. He shared with many of his contemporaries a Whiggish belief in history as a record of the advances and delays in the march of progress. The history of science was not a recognized subfield of American history in this period, and most of the work was carried out by interested scientists and physicians rather than professional historians. With the work of I. Bernard Cohen at Harvard, the history of science became an established subdiscipline of history after 1945. The first half of the 14th century saw much important scientific work being done, largely within the framework of scholastic commentaries on Aristotle's scientific writings. William of Ockham introduced the principle of parsimony: natural philosophers should not postulate unnecessary entities, so that motion is not a distinct thing but is only the moving object and an intermediary ""sensible species"" is not needed to transmit an image of an object to the eye. Scholars such as Jean Buridan and Nicole Oresme started to reinterpret elements of Aristotle's mechanics. In particular, Buridan developed the theory that impetus was the cause of the motion of projectiles, which was a first step towards the modern concept of inertia. The Oxford Calculators began to mathematically analyze the kinematics of motion, making this analysis without considering the causes of motion. With the division of the Roman Empire, the Western Roman Empire lost contact with much of its past. In the Middle East, Greek philosophy was able to find some support under the newly created Arab Empire. With the spread of Islam in the 7th and 8th centuries, a period of Muslim scholarship, known as the Islamic Golden Age, lasted until the 13th century. This scholarship was aided by several factors. The use of a single language, Arabic, allowed communication without need of a translator. Access to Greek texts from the Byzantine Empire, along with Indian sources of learning, provided Muslim scholars a knowledge base to build upon. In Classical Antiquity, the inquiry into the workings of the universe took place both in investigations aimed at such practical goals as establishing a reliable calendar or determining how to cure a variety of illnesses and in those abstract investigations known as natural philosophy. The ancient people who are considered the first scientists may have thought of themselves as natural philosophers, as practitioners of a skilled profession (for example, physicians), or as followers of a religious tradition (for example, temple healers). Muslim scientists placed far greater emphasis on experiment than had the Greeks. This led to an early scientific method being developed in the Muslim world, where significant progress in methodology was made, beginning with the experiments of Ibn al-Haytham (Alhazen) on optics from c. 1000, in his Book of Optics. The law of refraction of light was known to the Persians. The most important development of the scientific method was the use of experiments to distinguish between competing scientific theories set within a generally empirical orientation, which began among Muslim scientists. Ibn al-Haytham is also regarded as the father of optics, especially for his empirical proof of the intromission theory of light. Some have also described Ibn al-Haytham as the ""first scientist"" for his development of the modern scientific method. Theophrastus wrote some of the earliest descriptions of plants and animals, establishing the first taxonomy and looking at minerals in terms of their properties such as hardness. Pliny the Elder produced what is one of the largest encyclopedias of the natural world in 77 AD, and must be regarded as the rightful successor to Theophrastus. For example, he accurately describes the octahedral shape of the diamond, and proceeds to mention that diamond dust is used by engravers to cut and polish other gems owing to its great hardness. His recognition of the importance of crystal shape is a precursor to modern crystallography, while mention of numerous other minerals presages mineralogy. He also recognises that other minerals have characteristic crystal shapes, but in one example, confuses the crystal habit with the work of lapidaries. He was also the first to recognise that amber was a fossilized resin from pine trees because he had seen samples with trapped insects within them. The Age of Enlightenment was a European affair. The 17th century ""Age of Reason"" opened the avenues to the decisive steps towards modern science, which took place during the 18th century ""Age of Enlightenment"". Directly based on the works of Newton, Descartes, Pascal and Leibniz, the way was now clear to the development of modern mathematics, physics and technology by the generation of Benjamin Franklin (1706–1790), Leonhard Euler (1707–1783), Mikhail Lomonosov (1711–1765) and Jean le Rond d'Alembert (1717–1783), epitomized in the appearance of Denis Diderot's Encyclopédie between 1751 and 1772. The impact of this process was not limited to science and technology, but affected philosophy (Immanuel Kant, David Hume), religion (the increasingly significant impact of science upon religion), and society and politics in general (Adam Smith, Voltaire), the French Revolution of 1789 setting a bloody cesura indicating the beginning of political modernity[citation needed]. The early modern period is seen as a flowering of the European Renaissance, in what is often known as the Scientific Revolution, viewed as a foundation of modern science. The Romantic Movement of the early 19th century reshaped science by opening up new pursuits unexpected in the classical approaches of the Enlightenment. Major breakthroughs came in biology, especially in Darwin's theory of evolution, as well as physics (electromagnetism), mathematics (non-Euclidean geometry, group theory) and chemistry (organic chemistry). The decline of Romanticism occurred because a new movement, Positivism, began to take hold of the ideals of the intellectuals after 1840 and lasted until about 1880. Mathematics: The earliest traces of mathematical knowledge in the Indian subcontinent appear with the Indus Valley Civilization (c. 4th millennium BC ~ c. 3rd millennium BC). The people of this civilization made bricks whose dimensions were in the proportion 4:2:1, considered favorable for the stability of a brick structure. They also tried to standardize measurement of length to a high degree of accuracy. They designed a ruler—the Mohenjo-daro ruler—whose unit of length (approximately 1.32 inches or 3.4 centimetres) was divided into ten equal parts. Bricks manufactured in ancient Mohenjo-daro often had dimensions that were integral multiples of this unit of length. Mathematics: From the earliest the Chinese used a positional decimal system on counting boards in order to calculate. To express 10, a single rod is placed in the second box from the right. The spoken language uses a similar system to English: e.g. four thousand two hundred seven. No symbol was used for zero. By the 1st century BC, negative numbers and decimal fractions were in use and The Nine Chapters on the Mathematical Art included methods for extracting higher order roots by Horner's method and solving linear equations and by Pythagoras' theorem. Cubic equations were solved in the Tang dynasty and solutions of equations of order higher than 3 appeared in print in 1245 AD by Ch'in Chiu-shao. Pascal's triangle for binomial coefficients was described around 1100 by Jia Xian. Seismology: To better prepare for calamities, Zhang Heng invented a seismometer in 132 CE which provided instant alert to authorities in the capital Luoyang that an earthquake had occurred in a location indicated by a specific cardinal or ordinal direction. Although no tremors could be felt in the capital when Zhang told the court that an earthquake had just occurred in the northwest, a message came soon afterwards that an earthquake had indeed struck 400 km (248 mi) to 500 km (310 mi) northwest of Luoyang (in what is now modern Gansu). Zhang called his device the 'instrument for measuring the seasonal winds and the movements of the Earth' (Houfeng didong yi 候风地动仪), so-named because he and others thought that earthquakes were most likely caused by the enormous compression of trapped air. See Zhang's seismometer for further details. In mathematics, the Persian mathematician Muhammad ibn Musa al-Khwarizmi gave his name to the concept of the algorithm, while the term algebra is derived from al-jabr, the beginning of the title of one of his publications. What is now known as Arabic numerals originally came from India, but Muslim mathematicians did make several refinements to the number system, such as the introduction of decimal point notation. Sabian mathematician Al-Battani (850-929) contributed to astronomy and mathematics, while Persian scholar Al-Razi contributed to chemistry and medicine. In 1348, the Black Death and other disasters sealed a sudden end to the previous period of massive philosophic and scientific development. Yet, the rediscovery of ancient texts was improved after the Fall of Constantinople in 1453, when many Byzantine scholars had to seek refuge in the West. Meanwhile, the introduction of printing was to have great effect on European society. The facilitated dissemination of the printed word democratized learning and allowed a faster propagation of new ideas. New ideas also helped to influence the development of European science at this point: not least the introduction of Algebra. These developments paved the way for the Scientific Revolution, which may also be understood as a resumption of the process of scientific inquiry, halted at the start of the Black Death. The English word scientist is relatively recent—first coined by William Whewell in the 19th century. Previously, people investigating nature called themselves natural philosophers. While empirical investigations of the natural world have been described since classical antiquity (for example, by Thales, Aristotle, and others), and scientific methods have been employed since the Middle Ages (for example, by Ibn al-Haytham, and Roger Bacon), the dawn of modern science is often traced back to the early modern period and in particular to the scientific revolution that took place in 16th- and 17th-century Europe. Scientific methods are considered to be so fundamental to modern science that some consider earlier inquiries into nature to be pre-scientific. Traditionally, historians of science have defined science sufficiently broadly to include those inquiries. The willingness to question previously held truths and search for new answers resulted in a period of major scientific advancements, now known as the Scientific Revolution. The Scientific Revolution is traditionally held by most historians to have begun in 1543, when the books De humani corporis fabrica (On the Workings of the Human Body) by Andreas Vesalius, and also De Revolutionibus, by the astronomer Nicolaus Copernicus, were first printed. The thesis of Copernicus' book was that the Earth moved around the Sun. The period culminated with the publication of the Philosophiæ Naturalis Principia Mathematica in 1687 by Isaac Newton, representative of the unprecedented growth of scientific publications throughout Europe. Historical linguistics emerged as an independent field of study at the end of the 18th century. Sir William Jones proposed that Sanskrit, Persian, Greek, Latin, Gothic, and Celtic languages all shared a common base. After Jones, an effort to catalog all languages of the world was made throughout the 19th century and into the 20th century. Publication of Ferdinand de Saussure's Cours de linguistique générale created the development of descriptive linguistics. Descriptive linguistics, and the related structuralism movement caused linguistics to focus on how language changes over time, instead of just describing the differences between languages. Noam Chomsky further diversified linguistics with the development of generative linguistics in the 1950s. His effort is based upon a mathematical model of language that allows for the description and prediction of valid syntax. Additional specialties such as sociolinguistics, cognitive linguistics, and computational linguistics have emerged from collaboration between linguistics and other disciplines. Ibn Khaldun can be regarded as the earliest scientific systematic sociologist. The modern sociology, emerged in the early 19th century as the academic response to the modernization of the world. Among many early sociologists (e.g., Émile Durkheim), the aim of sociology was in structuralism, understanding the cohesion of social groups, and developing an ""antidote"" to social disintegration. Max Weber was concerned with the modernization of society through the concept of rationalization, which he believed would trap individuals in an ""iron cage"" of rational thought. Some sociologists, including Georg Simmel and W. E. B. Du Bois, utilized more microsociological, qualitative analyses. This microlevel approach played an important role in American sociology, with the theories of George Herbert Mead and his student Herbert Blumer resulting in the creation of the symbolic interactionism approach to sociology. Geology existed as a cloud of isolated, disconnected ideas about rocks, minerals, and landforms long before it became a coherent science. Theophrastus' work on rocks, Peri lithōn, remained authoritative for millennia: its interpretation of fossils was not overturned until after the Scientific Revolution. Chinese polymath Shen Kua (1031–1095) first formulated hypotheses for the process of land formation. Based on his observation of fossils in a geological stratum in a mountain hundreds of miles from the ocean, he deduced that the land was formed by erosion of the mountains and by deposition of silt. The history of science is the study of the development of science and scientific knowledge, including both the natural sciences and social sciences. (The history of the arts and humanities is termed as the history of scholarship.) Science is a body of empirical, theoretical, and practical knowledge about the natural world, produced by scientists who emphasize the observation, explanation, and prediction of real world phenomena. Historiography of science, in contrast, often draws on the historical methods of both intellectual history and social history. With the fall of the Western Roman Empire, there arose a more diffuse arena for political studies. The rise of monotheism and, particularly for the Western tradition, Christianity, brought to light a new space for politics and political action[citation needed]. During the Middle Ages, the study of politics was widespread in the churches and courts. Works such as Augustine of Hippo's The City of God synthesized current philosophies and political traditions with those of Christianity, redefining the borders between what was religious and what was political. Most of the political questions surrounding the relationship between Church and State were clarified and contested in this period. Computer science, built upon a foundation of theoretical linguistics, discrete mathematics, and electrical engineering, studies the nature and limits of computation. Subfields include computability, computational complexity, database design, computer networking, artificial intelligence, and the design of computer hardware. One area in which advances in computing have contributed to more general scientific development is by facilitating large-scale archiving of scientific data. Contemporary computer science typically distinguishes itself by emphasising mathematical 'theory' in contrast to the practical emphasis of software engineering. The important legacy of this period included substantial advances in factual knowledge, especially in anatomy, zoology, botany, mineralogy, geography, mathematics and astronomy; an awareness of the importance of certain scientific problems, especially those related to the problem of change and its causes; and a recognition of the methodological importance of applying mathematics to natural phenomena and of undertaking empirical research. In the Hellenistic age scholars frequently employed the principles developed in earlier Greek thought: the application of mathematics and deliberate empirical research, in their scientific investigations. Thus, clear unbroken lines of influence lead from ancient Greek and Hellenistic philosophers, to medieval Muslim philosophers and scientists, to the European Renaissance and Enlightenment, to the secular sciences of the modern day. Neither reason nor inquiry began with the Ancient Greeks, but the Socratic method did, along with the idea of Forms, great advances in geometry, logic, and the natural sciences. According to Benjamin Farrington, former Professor of Classics at Swansea University: Midway through the 19th century, the focus of geology shifted from description and classification to attempts to understand how the surface of the Earth had changed. The first comprehensive theories of mountain building were proposed during this period, as were the first modern theories of earthquakes and volcanoes. Louis Agassiz and others established the reality of continent-covering ice ages, and ""fluvialists"" like Andrew Crombie Ramsay argued that river valleys were formed, over millions of years by the rivers that flow through them. After the discovery of radioactivity, radiometric dating methods were developed, starting in the 20th century. Alfred Wegener's theory of ""continental drift"" was widely dismissed when he proposed it in the 1910s, but new data gathered in the 1950s and 1960s led to the theory of plate tectonics, which provided a plausible mechanism for it. Plate tectonics also provided a unified explanation for a wide range of seemingly unrelated geological phenomena. Since 1970 it has served as the unifying principle in geology. In 1687, Isaac Newton published the Principia Mathematica, detailing two comprehensive and successful physical theories: Newton's laws of motion, which led to classical mechanics; and Newton's Law of Gravitation, which describes the fundamental force of gravity. The behavior of electricity and magnetism was studied by Faraday, Ohm, and others during the early 19th century. These studies led to the unification of the two phenomena into a single theory of electromagnetism, by James Clerk Maxwell (known as Maxwell's equations). From the 18th century through late 20th century, the history of science, especially of the physical and biological sciences, was often presented in a progressive narrative in which true theories replaced false beliefs. More recent historical interpretations, such as those of Thomas Kuhn, tend to portray the history of science in different terms, such as that of competing paradigms or conceptual systems in a wider matrix that includes intellectual, cultural, economic and political themes outside of science. There are many notable contributors to the field of Chinese science throughout the ages. One of the best examples would be Shen Kuo (1031–1095), a polymath scientist and statesman who was the first to describe the magnetic-needle compass used for navigation, discovered the concept of true north, improved the design of the astronomical gnomon, armillary sphere, sight tube, and clepsydra, and described the use of drydocks to repair boats. After observing the natural process of the inundation of silt and the find of marine fossils in the Taihang Mountains (hundreds of miles from the Pacific Ocean), Shen Kuo devised a theory of land formation, or geomorphology. He also adopted a theory of gradual climate change in regions over time, after observing petrified bamboo found underground at Yan'an, Shaanxi province. If not for Shen Kuo's writing, the architectural works of Yu Hao would be little known, along with the inventor of movable type printing, Bi Sheng (990-1051). Shen's contemporary Su Song (1020–1101) was also a brilliant polymath, an astronomer who created a celestial atlas of star maps, wrote a pharmaceutical treatise with related subjects of botany, zoology, mineralogy, and metallurgy, and had erected a large astronomical clocktower in Kaifeng city in 1088. To operate the crowning armillary sphere, his clocktower featured an escapement mechanism and the world's oldest known use of an endless power-transmitting chain drive."
George_VI,"During George's reign the break-up of the British Empire and its transition into the Commonwealth of Nations accelerated. The parliament of the Irish Free State removed direct mention of the monarch from the country's constitution on the day of his accession. From 1939, the Empire and Commonwealth, except Ireland, was at war with Nazi Germany. War with Italy and Japan followed in 1940 and 1941, respectively. Though Britain and its allies were ultimately victorious in 1945, the United States and the Soviet Union rose as pre-eminent world powers and the British Empire declined. After the independence of India and Pakistan in 1947, George remained as king of both countries, but the title Emperor of India was abandoned in June 1948. Ireland formally declared itself a republic and left the Commonwealth in 1949, and India became a republic within the Commonwealth the following year. George adopted the new title of Head of the Commonwealth. He was beset by health problems in the later years of his reign. His elder daughter, Elizabeth, succeeded him. In September 1939, Britain and the self-governing Dominions, but not Ireland, declared war on Nazi Germany. George VI and his wife resolved to stay in London, despite German bombing raids. They officially stayed in Buckingham Palace throughout the war, although they usually spent nights at Windsor Castle. The first German raid on London, on 7 September 1940, killed about one thousand civilians, mostly in the East End. On 13 September, the King and Queen narrowly avoided death when two German bombs exploded in a courtyard at Buckingham Palace while they were there. In defiance, the Queen famously declared: ""I am glad we have been bombed. It makes me feel we can look the East End in the face"". The royal family were portrayed as sharing the same dangers and deprivations as the rest of the country. They were subject to rationing restrictions, and U.S. First Lady Eleanor Roosevelt remarked on the rationed food served and the limited bathwater that was permitted during a stay at the unheated and boarded-up Palace. In August 1942, the King's brother, Prince George, Duke of Kent, was killed on active service. George VI's reign saw the acceleration of the dissolution of the British Empire. The Statute of Westminster 1931 had already acknowledged the evolution of the Dominions into separate sovereign states. The process of transformation from an empire to a voluntary association of independent states, known as the Commonwealth, gathered pace after the Second World War. During the ministry of Clement Attlee, British India became the two independent dominions of India and Pakistan in 1947. George relinquished the title of Emperor of India, and became King of India and King of Pakistan instead. In 1950 he ceased to be King of India when it became a republic within the Commonwealth of Nations, but he remained King of Pakistan until his death and India recognised his new title of Head of the Commonwealth. Other countries left the Commonwealth, such as Burma in January 1948, Palestine (divided between Israel and the Arab states) in May 1948 and the Republic of Ireland in 1949. From 9 February for two days his coffin rested in St. Mary Magdalene Church, Sandringham, before lying in state at Westminster Hall from 11 February. His funeral took place at St. George's Chapel, Windsor Castle, on the 15th. He was interred initially in the Royal Vault until he was transferred to the King George VI Memorial Chapel inside St. George's on 26 March 1969. In 2002, fifty years after his death, the remains of his widow, Queen Elizabeth The Queen Mother, and the ashes of his younger daughter Princess Margaret, who both died that year, were interred in the chapel alongside him. Throughout the war, the King and Queen provided morale-boosting visits throughout the United Kingdom, visiting bomb sites, munitions factories, and troops. The King visited military forces abroad in France in December 1939, North Africa and Malta in June 1943, Normandy in June 1944, southern Italy in July 1944, and the Low Countries in October 1944. Their high public profile and apparently indefatigable determination secured their place as symbols of national resistance. At a social function in 1944, Chief of the Imperial General Staff Sir Alan Brooke, revealed that every time he met Field Marshal Bernard Montgomery he thought he was after his job. The King replied: ""You should worry, when I meet him, I always think he's after mine!"" In May and June 1939, the King and Queen toured Canada and the United States. From Ottawa, the royal couple were accompanied throughout by Canadian Prime Minister William Lyon Mackenzie King, to present themselves in North America as King and Queen of Canada. George was the first reigning monarch of Canada to visit North America, although he had been to Canada previously as Prince Albert and as Duke of York. Both Governor General of Canada Lord Tweedsmuir and Mackenzie King hoped that the King's presence in Canada would demonstrate the principles of the Statute of Westminster 1931, which gave full sovereignty to the British Dominions. On 19 May, George VI personally accepted and approved the Letter of Credence of the new U.S. Ambassador to Canada, Daniel Calhoun Roper; gave Royal Assent to nine parliamentary bills; and ratified two international treaties with the Great Seal of Canada. The official royal tour historian, Gustave Lanctot, wrote ""the Statute of Westminster had assumed full reality"" and George gave a speech emphasising ""the free and equal association of the nations of the Commonwealth"". The growing likelihood of war in Europe dominated the early reign of George VI. The King was constitutionally bound to support Prime Minister Neville Chamberlain's appeasement of Hitler. However, when the King and Queen greeted Chamberlain on his return from negotiating the Munich Agreement in 1938, they invited him to appear on the balcony of Buckingham Palace with them. This public association of the monarchy with a politician was exceptional, as balcony appearances were traditionally restricted to the royal family. While broadly popular among the general public, Chamberlain's policy towards Hitler was the subject of some opposition in the House of Commons, which led historian John Grigg to describe the King's behaviour in associating himself so prominently with a politician as ""the most unconstitutional act by a British sovereign in the present century"". In a time when royals were expected to marry fellow royals, it was unusual that Albert had a great deal of freedom in choosing a prospective wife. An infatuation with the already-married Australian socialite Sheila, Lady Loughborough, came to an end in April 1920 when the King, with the promise of the dukedom of York, persuaded Albert to stop seeing her. That year, he met for the first time since childhood Lady Elizabeth Bowes-Lyon, the youngest daughter of the Earl and Countess of Strathmore and Kinghorne. He became determined to marry her. She rejected his proposal twice, in 1921 and 1922, reportedly because she was reluctant to make the sacrifices necessary to become a member of the royal family. In the words of Lady Elizabeth's mother, Albert would be ""made or marred"" by his choice of wife. After a protracted courtship, Elizabeth agreed to marry him. Albert spent the first six months of 1913 on the training ship HMS Cumberland in the West Indies and on the east coast of Canada. He was rated as a midshipman aboard HMS Collingwood on 15 September 1913, and spent three months in the Mediterranean. His fellow officers gave him the nickname ""Mr. Johnson"". One year after his commission, he began service in the First World War. He was mentioned in despatches for his action as a turret officer aboard Collingwood in the Battle of Jutland (31 May – 1 June 1916), an indecisive engagement with the German navy that was the largest naval action of the war. He did not see further combat, largely because of ill health caused by a duodenal ulcer, for which he had an operation in November 1917. On the day of the abdication, the Oireachtas, the parliament of the Irish Free State, removed all direct mention of the monarch from the Irish constitution. The next day, it passed the External Relations Act, which gave the monarch limited authority (strictly on the advice of the government) to appoint diplomatic representatives for Ireland and to be involved in the making of foreign treaties. The two acts made the Irish Free State a republic in essence without removing its links to the Commonwealth. The stress of the war had taken its toll on the King's health, exacerbated by his heavy smoking and subsequent development of lung cancer among other ailments, including arteriosclerosis and thromboangiitis obliterans. A planned tour of Australia and New Zealand was postponed after the King suffered an arterial blockage in his right leg, which threatened the loss of the leg and was treated with a right lumbar sympathectomy in March 1949. His elder daughter Elizabeth, the heir presumptive, took on more royal duties as her father's health deteriorated. The delayed tour was re-organised, with Elizabeth and her husband, the Duke of Edinburgh, taking the place of the King and Queen. The King was well enough to open the Festival of Britain in May 1951, but on 23 September 1951, his left lung was removed by Clement Price Thomas after a malignant tumour was found. In October 1951, Princess Elizabeth and the Duke of Edinburgh went on a month-long tour of Canada; the trip had been delayed for a week due to the King's illness. At the State Opening of Parliament in November, the King's speech from the throne was read for him by the Lord Chancellor, Lord Simonds. His Christmas broadcast of 1951 was recorded in sections, and then edited together. His birthday (14 December 1895) was the 34th anniversary of the death of his great-grandfather, Prince Albert, the Prince Consort. Uncertain of how the Prince Consort's widow, Queen Victoria, would take the news of the birth, the Prince of Wales wrote to the Duke of York that the Queen had been ""rather distressed"". Two days later, he wrote again: ""I really think it would gratify her if you yourself proposed the name Albert to her"". Queen Victoria was mollified by the proposal to name the new baby Albert, and wrote to the Duchess of York: ""I am all impatience to see the new one, born on such a sad day but rather more dear to me, especially as he will be called by that dear name which is a byword for all that is great and good"". Consequently, he was baptised ""Albert Frederick Arthur George"" at St. Mary Magdalene's Church near Sandringham three months later.[a] As a great-grandson of Queen Victoria, he was known formally as His Highness Prince Albert of York from birth. Within the family, he was known informally as ""Bertie"". His maternal grandmother, the Duchess of Teck, did not like the first name the baby had been given, and she wrote prophetically that she hoped the last name ""may supplant the less favoured one"". In October 1919, Albert went up to Trinity College, Cambridge, where he studied history, economics and civics for a year. On 4 June 1920, he was created Duke of York, Earl of Inverness and Baron Killarney. He began to take on more royal duties. He represented his father, and toured coal mines, factories, and railyards. Through such visits he acquired the nickname of the ""Industrial Prince"". His stammer, and his embarrassment over it, together with his tendency to shyness, caused him to appear much less impressive than his older brother, Edward. However, he was physically active and enjoyed playing tennis. He played at Wimbledon in the Men's Doubles with Louis Greig in 1926, losing in the first round. He developed an interest in working conditions, and was President of the Industrial Welfare Society. His series of annual summer camps for boys between 1921 and 1939 brought together boys from different social backgrounds. The trip was intended to soften the strong isolationist tendencies among the North American public with regard to the developing tensions in Europe. Although the aim of the tour was mainly political, to shore up Atlantic support for the United Kingdom in any future war, the King and Queen were enthusiastically received by the public. The fear that George would be compared unfavourably to his predecessor, Edward VIII, was dispelled. They visited the 1939 New York World's Fair and stayed with President Franklin D. Roosevelt at the White House and at his private estate at Hyde Park, New York. A strong bond of friendship was forged between the King and Queen and the President during the tour, which had major significance in the relations between the United States and the United Kingdom through the ensuing war years. Because of his stammer, Albert dreaded public speaking. After his closing speech at the British Empire Exhibition at Wembley on 31 October 1925, one which was an ordeal for both him and his listeners, he began to see Lionel Logue, an Australian-born speech therapist. The Duke and Logue practised breathing exercises, and the Duchess rehearsed with him patiently. Subsequently, he was able to speak with less hesitation. With his delivery improved, the Duke opened the new Parliament House in Canberra, Australia, during a tour of the empire in 1927. His journey by sea to Australia, New Zealand and Fiji took him via Jamaica, where Albert played doubles tennis partnered with a black man, which was unusual at the time and taken locally as a display of equality between races. The Duke and Duchess of York had two children: Elizabeth (called ""Lilibet"" by the family), and Margaret. The Duke and Duchess and their two daughters lived a relatively sheltered life at their London residence, 145 Piccadilly. They were a close and loving family. One of the few stirs arose when the Canadian Prime Minister, R. B. Bennett, considered the Duke for Governor General of Canada in 1931—a proposal that King George V rejected on the advice of the Secretary of State for Dominion Affairs, J. H. Thomas. In 1940, Winston Churchill replaced Neville Chamberlain as Prime Minister, though personally George would have preferred to appoint Lord Halifax. After the King's initial dismay over Churchill's appointment of Lord Beaverbrook to the Cabinet, he and Churchill developed ""the closest personal relationship in modern British history between a monarch and a Prime Minister"". Every Tuesday for four and a half years from September 1940, the two men met privately for lunch to discuss the war in secret and with frankness. In February 1918, he was appointed Officer in Charge of Boys at the Royal Naval Air Service's training establishment at Cranwell. With the establishment of the Royal Air Force two months later and the transfer of Cranwell from Navy to Air Force control, he transferred from the Royal Navy to the Royal Air Force. He was appointed Officer Commanding Number 4 Squadron of the Boys' Wing at Cranwell until August 1918, before reporting to the RAF's Cadet School at St Leonards-on-Sea where he completed a fortnight's training and took command of a squadron on the Cadet Wing. He was the first member of the royal family to be certified as a fully qualified pilot. During the closing weeks of the war, he served on the staff of the RAF's Independent Air Force at its headquarters in Nancy, France. Following the disbanding of the Independent Air Force in November 1918, he remained on the Continent for two months as a staff officer with the Royal Air Force until posted back to Britain. He accompanied the Belgian monarch King Albert on his triumphal reentry into Brussels on 22 November. Prince Albert qualified as an RAF pilot on 31 July 1919 and gained a promotion to squadron leader on the following day. As Edward was unmarried and had no children, Albert was the heir presumptive to the throne. Less than a year later, on 11 December 1936, Edward VIII abdicated in order to marry his mistress, Wallis Simpson, who was divorced from her first husband and divorcing her second. Edward had been advised by British Prime Minister Stanley Baldwin that he could not remain king and marry a divorced woman with two living ex-husbands. Edward chose abdication in preference to abandoning his marriage plans. Thus Albert became king, a position he was reluctant to accept. The day before the abdication, he went to London to see his mother, Queen Mary. He wrote in his diary, ""When I told her what had happened, I broke down and sobbed like a child."" Albert assumed the regnal name ""George VI"" to emphasise continuity with his father and restore confidence in the monarchy. The beginning of George VI's reign was taken up by questions surrounding his predecessor and brother, whose titles, style and position were uncertain. He had been introduced as ""His Royal Highness Prince Edward"" for the abdication broadcast, but George VI felt that by abdicating and renouncing the succession Edward had lost the right to bear royal titles, including ""Royal Highness"". In settling the issue, George's first act as king was to confer upon his brother the title and style ""His Royal Highness The Duke of Windsor"", but the Letters Patent creating the dukedom prevented any wife or children from bearing royal styles. George VI was also forced to buy from Edward the royal residences of Balmoral Castle and Sandringham House, as these were private properties and did not pass to George VI automatically. Three days after his accession, on his 41st birthday, he invested his wife, the new queen consort, with the Order of the Garter. In the words of Labour Member of Parliament George Hardie, the abdication crisis of 1936 did ""more for republicanism than fifty years of propaganda"". George VI wrote to his brother Edward that in the aftermath of the abdication he had reluctantly assumed ""a rocking throne"", and tried ""to make it steady again"". He became king at a point when public faith in the monarchy was at a low ebb. During his reign his people endured the hardships of war, and imperial power was eroded. However, as a dutiful family man and by showing personal courage, he succeeded in restoring the popularity of the monarchy. In 1947, the King and his family toured Southern Africa. The Prime Minister of the Union of South Africa, Jan Smuts, was facing an election and hoped to make political capital out of the visit. George was appalled, however, when instructed by the South African government to shake hands only with whites, and referred to his South African bodyguards as ""the Gestapo"". Despite the tour, Smuts lost the election the following year, and the new government instituted a strict policy of racial segregation. George VI's coronation took place on 12 May 1937, the date previously intended for Edward's coronation. In a break with tradition, Queen Mary attended the ceremony in a show of support for her son. There was no Durbar held in Delhi for George VI, as had occurred for his father, as the cost would have been a burden to the government of India. Rising Indian nationalism made the welcome that the royal couple would have received likely to be muted at best, and a prolonged absence from Britain would have been undesirable in the tense period before the Second World War. Two overseas tours were undertaken, to France and to North America, both of which promised greater strategic advantages in the event of war."
Cardinal_(Catholicism),"In 1630, Pope Urban VIII decreed their title to be Eminence (previously, it had been ""illustrissimo"" and ""reverendissimo"")  and decreed that their secular rank would equate to Prince, making them secondary only to the Pope and crowned monarchs. A cardinal named in pectore is known only to the pope; not even the cardinal so named is necessarily aware of his elevation, and in any event cannot function as a cardinal while his appointment is in pectore. Today, cardinals are named in pectore to protect them or their congregations from reprisals if their identities were known. In early modern times, cardinals often had important roles in secular affairs. In some cases, they took on powerful positions in government. In Henry VIII's England, his chief minister was Cardinal Wolsey. Cardinal Richelieu's power was so great that he was for many years effectively the ruler of France. Richelieu successor was also a cardinal, Jules Mazarin. Guillaume Dubois and André-Hercule de Fleury complete the list of the ""four great"" cardinals to have ruled France.[citation needed] In Portugal, due to a succession crisis, one cardinal, Henry, King of Portugal, was crowned king, the only example of a cardinal-king. In early times, the privilege of papal election was not reserved to the cardinals, and for centuries the person elected was customarily a Roman priest and never a bishop from elsewhere. To preserve apostolic succession the rite of consecrating him a bishop had to be performed by someone who was already a bishop. The rule remains that, if the person elected Pope is not yet a bishop, he is consecrated by the Dean of the College of Cardinals, the Cardinal Bishop of Ostia. The cardinal protodeacon, the senior cardinal deacon in order of appointment to the College of Cardinals, has the privilege of announcing a new pope's election and name (once he has been ordained to the Episcopate) from the central balcony at the Basilica of Saint Peter in Vatican City State. In the past, during papal coronations, the proto-deacon also had the honor of bestowing the pallium on the new pope and crowning him with the papal tiara. However, in 1978 Pope John Paul I chose not to be crowned and opted for a simpler papal inauguration ceremony, and his three successors followed that example. As a result, the Cardinal protodeacon's privilege of crowning a new pope has effectively ceased although it could be revived if a future Pope were to restore a coronation ceremony. However, the proto-deacon still has the privilege of bestowing the pallium on a new pope at his papal inauguration. “Acting in the place of the Roman Pontiff, he also confers the pallium upon metropolitan bishops or gives the pallium to their proxies.” The current cardinal proto-deacon is Renato Raffaele Martino. To symbolize their bond with the papacy, the pope gives each newly appointed cardinal a gold ring, which is traditionally kissed by Catholics when greeting a cardinal (as with a bishop's episcopal ring). The pope chooses the image on the outside: under Pope Benedict XVI it was a modern depiction of the crucifixion of Jesus, with Mary and John to each side. The ring includes the pope's coat of arms on the inside.[citation needed] The cardinal who is the longest-serving member of the order of cardinal priests is titled cardinal protopriest. He had certain ceremonial duties in the conclave that have effectively ceased because he would generally have already reached age 80, at which cardinals are barred from the conclave. The current cardinal protopriest is Paulo Evaristo Arns of Brazil. Cardinals elevated to the diaconal order are mainly officials of the Roman Curia holding various posts in the church administration. Their number and influence has varied through the years. While historically predominantly Italian the group has become much more internationally diverse in later years. While in 1939 about half were Italian by 1994 the number was reduced to one third. Their influence in the election of the Pope has been considered important, they are better informed and connected than the dislocated cardinals but their level of unity has been varied. Under the 1587 decree of Pope Sixtus V, which fixed the maximum size of the College of Cardinals, there were 14 cardinal deacons. Later the number increased. As late as 1939 almost half of the cardinals were members of the curia. Pius XII reduced this percentage to 24 percent. John XXIII brought it back up to 37 percent but Paul VI brought it down to 27 percent where John Paul II has maintained this ratio. In cities other than Rome, the name cardinal began to be applied to certain church men as a mark of honour. The earliest example of this occurs in a letter sent by Pope Zacharias in 747 to Pippin III (the Short), ruler of the Franks, in which Zacharias applied the title to the priests of Paris to distinguish them from country clergy. This meaning of the word spread rapidly, and from the 9th century various episcopal cities had a special class among the clergy known as cardinals. The use of the title was reserved for the cardinals of Rome in 1567 by Pius V. In 1059, the right of electing the pope was reserved to the principal clergy of Rome and the bishops of the seven suburbicarian sees. In the 12th century the practice of appointing ecclesiastics from outside Rome as cardinals began, with each of them assigned a church in Rome as his titular church or linked with one of the suburbicarian dioceses, while still being incardinated in a diocese other than that of Rome.[citation needed] During the Western Schism, many cardinals were created by the contending popes. Beginning with the reign of Pope Martin V, cardinals were created without publishing their names until later, termed creati et reservati in pectore. In the year 1563 the influential Ecumenical Council of Trent, headed by Pope Pius IV, wrote about the importance of selecting good Cardinals. According to this historic council ""nothing is more necessary to the Church of God than that the holy Roman pontiff apply that solicitude which by the duty of his office he owes the universal Church in a very special way by associating with himself as cardinals the most select persons only, and appoint to each church most eminently upright and competent shepherds; and this the more so, because our Lord Jesus Christ will require at his hands the blood of the sheep of Christ that perish through the evil government of shepherds who are negligent and forgetful of their office."" At various times, there have been cardinals who had only received first tonsure and minor orders but not yet been ordained as deacons or priests. Though clerics, they were inaccurately called ""lay cardinals"" and were permitted to marry. Teodolfo Mertel was among the last of the lay cardinals. When he died in 1899 he was the last surviving cardinal who was not at least ordained a priest. With the revision of the Code of Canon Law promulgated in 1917 by Pope Benedict XV, only those who are already priests or bishops may be appointed cardinals. Since the time of Pope John XXIII a priest who is appointed a cardinal must be consecrated a bishop, unless he obtains a dispensation. Since 1962, the cardinal bishops have only a titular relationship with the suburbicarian sees, with no powers of governance over them. Each see has its own bishop, with the exception of Ostia, in which the Cardinal Vicar of the see of Rome is apostolic administrator. In 1965, Pope Paul VI decreed in his motu proprio Ad Purpuratorum Patrum that patriarchs of the Eastern Catholic Churches who were named cardinals (i.e., patriarch cardinals) would also be part of the episcopal order, ranking after the six cardinal bishops of the suburbicarian sees (who had been relieved of direct responsibilities for those sees by Pope John XXIII three years earlier). Patriarch cardinals do not receive title of a suburbicarian see, and as such they cannot elect the dean or become dean. There are currently three Eastern Patriarchs who are cardinal bishops: In previous times, at the consistory at which the pope named a new cardinal, he would bestow upon him a distinctive wide-brimmed hat called a galero. This custom was discontinued in 1969 and the investiture now takes place with the scarlet biretta. In ecclesiastical heraldry, however, the scarlet galero is still displayed on the cardinal's coat of arms. Cardinals had the right to display the galero in their cathedral, and when a cardinal died, it would be suspended from the ceiling above his tomb. Some cardinals will still have a galero made, even though it is not officially part of their apparel.[citation needed] While the cardinalate has long been expanded beyond the Roman pastoral clergy and Roman Curia, every cardinal priest has a titular church in Rome, though they may be bishops or archbishops elsewhere, just as cardinal bishops are given one of the suburbicarian dioceses around Rome. Pope Paul VI abolished all administrative rights cardinals had with regard to their titular churches, though the cardinal's name and coat of arms are still posted in the church, and they are expected to celebrate mass and preach there if convenient when they are in Rome. While the number of cardinals was small from the times of the Roman Empire to the Renaissance, and frequently smaller than the number of recognized churches entitled to a cardinal priest, in the 16th century the College expanded markedly. In 1587, Pope Sixtus V sought to arrest this growth by fixing the maximum size of the College at 70, including 50 cardinal priests, about twice the historical number. This limit was respected until 1958, and the list of titular churches modified only on rare occasions, generally when a building fell into disrepair. When Pope John XXIII abolished the limit, he began to add new churches to the list, which Popes Paul VI and John Paul II continued to do. Today there are close to 150 titular churches, out of over 300 churches in Rome. In modern times, the name ""cardinal priest"" is interpreted as meaning a cardinal who is of the order of priests. Originally, however, this referred to certain key priests of important churches of the Diocese of Rome, who were recognized as the cardinal priests, the important priests chosen by the pope to advise him in his duties as Bishop of Rome (the Latin cardo means ""hinge""). Certain clerics in many dioceses at the time, not just that of Rome, were said to be the key personnel — the term gradually became exclusive to Rome to indicate those entrusted with electing the bishop of Rome, the pope. In Latin, on the other hand, the [First name] Cardinal [Surname] order is used in the proclamation of the election of a new pope by the cardinal protodeacon: ""Annuntio vobis gaudium magnum; habemus Papam: Eminentissimum ac Reverendissimum Dominum, Dominum (first name) Sanctae Romanae Ecclesiae Cardinalem (last name), ..."" (Meaning: ""I announce to you a great joy; we have a Pope: The Most Eminent and Most Reverend Lord, Lord (first name) Cardinal of the Holy Roman Church (last name), ..."") This assumes that the new pope had been a cardinal just before becoming pope; the most recent election of a non-cardinal as pope was in 1378. As of 2005, there were over 50 churches recognized as cardinalatial deaconries, though there were only 30 cardinals of the order of deacons. Cardinal deacons have long enjoyed the right to ""opt for the order of cardinal priests"" (optazione) after they have been cardinal deacons for 10 years. They may on such elevation take a vacant ""title"" (a church allotted to a cardinal priest as the church in Rome with which he is associated) or their diaconal church may be temporarily elevated to a cardinal priest's ""title"" for that occasion. When elevated to cardinal priests, they take their precedence according to the day they were first made cardinal deacons (thus ranking above cardinal priests who were elevated to the college after them, regardless of order). The Cardinal Camerlengo of the Holy Roman Church, assisted by the Vice-Camerlengo and the other prelates of the office known as the Apostolic Camera, has functions that in essence are limited to a period of sede vacante of the papacy. He is to collate information about the financial situation of all administrations dependent on the Holy See and present the results to the College of Cardinals, as they gather for the papal conclave. Cardinal bishops (cardinals of the episcopal order) are among the most senior prelates of the Catholic Church. Though in modern times most cardinals are also bishops, the term ""cardinal bishop"" only refers to the cardinals who are titular bishops of one of the ""suburbicarian"" sees. When not celebrating Mass but still serving a liturgical function, such as the semiannual Urbi et Orbi papal blessing, some Papal Masses and some events at Ecumenical Councils, cardinal deacons can be recognized by the dalmatics they would don with the simple white mitre (so called mitra simplex). Until 1917, it was possible for someone who was not a priest, but only in minor orders, to become a cardinal (see ""lay cardinals"", below), but they were enrolled only in the order of cardinal deacons. For example, in the 16th century, Reginald Pole was a cardinal for 18 years before he was ordained a priest. In 1917 it was established that all cardinals, even cardinal deacons, had to be priests, and, in 1962, Pope John XXIII set the norm that all cardinals be ordained as bishops, even if they are only priests at the time of appointment. As a consequence of these two changes, canon 351 of the 1983 Code of Canon Law requires that a cardinal be at least in the order of priesthood at his appointment, and that those who are not already bishops must receive episcopal consecration. Several cardinals aged over 80 or close to it when appointed have obtained dispensation from the rule of having to be a bishop. These were all appointed cardinal-deacons, but one of them, Roberto Tucci, lived long enough to exercise the right of option and be promoted to the rank of cardinal-priest. A cardinal who is not a bishop is still entitled to wear and use the episcopal vestments and other pontificalia (episcopal regalia: mitre, crozier, zucchetto, pectoral cross and ring). Even if not a bishop, any cardinal has both actual and honorary precedence over non-cardinal patriarchs, as well as the archbishops and bishops who are not cardinals, but he cannot perform the functions reserved solely to bishops, such as ordination. The prominent priests who since 1962 were not ordained bishops on their elevation to the cardinalate were over the age of 80 or near to it, and so no cardinal who was not a bishop has participated in recent papal conclaves. Cardinals have in canon law a ""privilege of forum"" (i.e., exemption from being judged by ecclesiastical tribunals of ordinary rank): only the pope is competent to judge them in matters subject to ecclesiastical jurisdiction (cases that refer to matters that are spiritual or linked with the spiritual, or with regard to infringement of ecclesiastical laws and whatever contains an element of sin, where culpability must be determined and the appropriate ecclesiastical penalty imposed). The pope either decides the case himself or delegates the decision to a tribunal, usually one of the tribunals or congregations of the Roman Curia. Without such delegation, no ecclesiastical court, even the Roman Rota, is competent to judge a canon law case against a cardinal. Cardinals are, however, subject to the civil and criminal law like everybody else. If conditions change, so that the pope judges it safe to make the appointment public, he may do so at any time. The cardinal in question then ranks in precedence with those raised to the cardinalate at the time of his in pectore appointment. If a pope dies before revealing the identity of an in pectore cardinal, the cardinalate expires. Cardinal priests are the most numerous of the three orders of cardinals in the Catholic Church, ranking above the cardinal deacons and below the cardinal bishops. Those who are named cardinal priests today are generally bishops of important dioceses throughout the world, though some hold Curial positions. The earlier influence of temporal rulers, notably the French kings, reasserted itself through the influence of cardinals of certain nationalities or politically significant movements. Traditions even developed entitling certain monarchs, including those of Austria, Spain, and Portugal, to nominate one of their trusted clerical subjects to be created cardinal, a so-called crown-cardinal.[citation needed] A cardinal (Latin: sanctae romanae ecclesiae cardinalis, literally cardinal of the Holy Roman Church) is a senior ecclesiastical leader, an ecclesiastical prince, and usually (now always for those created when still within the voting age-range) an ordained bishop of the Roman Catholic Church. The cardinals of the Church are collectively known as the College of Cardinals. The duties of the cardinals include attending the meetings of the College and making themselves available individually or in groups to the Pope as requested. Most have additional duties, such as leading a diocese or archdiocese or managing a department of the Roman Curia. A cardinal's primary duty is electing the pope when the see becomes vacant. During the sede vacante (the period between a pope's death or resignation and the election of his successor), the day-to-day governance of the Holy See is in the hands of the College of Cardinals. The right to enter the conclave of cardinals where the pope is elected is limited to those who have not reached the age of 80 years by the day the vacancy occurs. There is disagreement about the origin of the term, but general consensus that ""cardinalis"" from the word cardo (meaning 'pivot' or 'hinge') was first used in late antiquity to designate a bishop or priest who was incorporated into a church for which he had not originally been ordained. In Rome the first persons to be called cardinals were the deacons of the seven regions of the city at the beginning of the 6th century, when the word began to mean “principal,” “eminent,” or ""superior."" The name was also given to the senior priest in each of the ""title"" churches (the parish churches) of Rome and to the bishops of the seven sees surrounding the city. By the 8th century the Roman cardinals constituted a privileged class among the Roman clergy. They took part in the administration of the church of Rome and in the papal liturgy. By decree of a synod of 769, only a cardinal was eligible to become pope. In 1059, during the pontificate of Nicholas II, cardinals were given the right to elect the pope under the Papal Bull In nomine Domini. For a time this power was assigned exclusively to the cardinal bishops, but the Third Lateran Council in 1179 gave back the right to the whole body of cardinals. Cardinals were granted the privilege of wearing the red hat by Pope Innocent IV in 1244. The cardinal deacons are the lowest-ranking cardinals. Cardinals elevated to the diaconal order are either officials of the Roman Curia or priests elevated after their 80th birthday. Bishops with diocesan responsibilities, however, are created cardinal priests. For a period ending in the mid-20th century, long-serving cardinal priests were entitled to fill vacancies that arose among the cardinal bishops, just as cardinal deacons of ten years' standing are still entitled to become cardinal priests. Since then, cardinals have been advanced to cardinal bishop exclusively by papal appointment. Cardinal deacons derive originally from the seven deacons in the Papal Household and the seven deacons who supervised the Church's works in the districts of Rome during the early Middle Ages, when church administration was effectively the government of Rome and provided all social services. Cardinal deacons are given title to one of these deaconries. Pope Sixtus V limited the number of cardinals to 70, comprising six cardinal bishops, 50 cardinal priests, and 14 cardinal deacons. Starting in the pontificate of Pope John XXIII, that limit has been exceeded. At the start of 1971, Pope Paul VI set the number of cardinal electors at a maximum of 120, but set no limit on the number of cardinals generally. He also established a maximum age of eighty years for electors. His action deprived twenty-five living cardinals, including the three living cardinals elevated by Pope Pius XI, of the right to participate in a conclave.[citation needed] Popes can dispense from church laws and have sometimes brought the number of cardinals under the age of 80 to more than 120. Pope Paul VI also increased the number of cardinal bishops by giving that rank to patriarchs of the Eastern Catholic Churches. In accordance with tradition, they sign by placing the title ""Cardinal"" (abbreviated Card.) after their personal name and before their surname as, for instance, ""John Card(inal) Doe"" or, in Latin, ""Ioannes Card(inalis) Cognomen"". Some writers, such as James-Charles Noonan, hold that, in the case of cardinals, the form used for signatures should be used also when referring to them in English. Official sources such as the Archdiocese of Milwaukee and Catholic News Service say that the correct form for referring to a cardinal in English is normally as ""Cardinal [First name] [Surname]"". This is the rule given also in stylebooks not associated with the Catholic Church. This style is also generally followed on the websites of the Holy See and episcopal conferences. Oriental Patriarchs who are created Cardinals customarily use ""Sanctae Ecclesiae Cardinalis"" as their full title, probably because they do not belong to the Roman clergy. The Dean of the College of Cardinals in addition to such a titular church also receives the titular bishopric of Ostia, the primary suburbicarian see. Cardinals governing a particular Church retain that church. There are seven suburbicarian sees: Ostia, Albano, Porto and Santa Rufina, Palestrina, Sabina and Mentana, Frascati and Velletri. Velletri was united with Ostia from 1150 until 1914, when Pope Pius X separated them again, but decreed that whatever cardinal bishop became Dean of the College of Cardinals would keep the suburbicarian see he already held, adding to it that of Ostia, with the result that there continued to be only six cardinal bishops. Eastern Catholic cardinals continue to wear the normal dress appropriate to their liturgical tradition, though some may line their cassocks with scarlet and wear scarlet fascias, or in some cases, wear Eastern-style cassocks entirely of scarlet. When in choir dress, a Latin-rite cardinal wears scarlet garments — the blood-like red symbolizes a cardinal's willingness to die for his faith. Excluding the rochet — which is always white — the scarlet garments include the cassock, mozzetta, and biretta (over the usual scarlet zucchetto). The biretta of a cardinal is distinctive not merely for its scarlet color, but also for the fact that it does not have a pompon or tassel on the top as do the birettas of other prelates. Until the 1460s, it was customary for cardinals to wear a violet or blue cape unless granted the privilege of wearing red when acting on papal business. His normal-wear cassock is black but has scarlet piping and a scarlet fascia (sash). Occasionally, a cardinal wears a scarlet ferraiolo which is a cape worn over the shoulders, tied at the neck in a bow by narrow strips of cloth in the front, without any 'trim' or piping on it. It is because of the scarlet color of cardinals' vesture that the bird of the same name has become known as such.[citation needed] The term cardinal at one time applied to any priest permanently assigned or incardinated to a church, or specifically to the senior priest of an important church, based on the Latin cardo (hinge), meaning ""principal"" or ""chief"". The term was applied in this sense as early as the ninth century to the priests of the tituli (parishes) of the diocese of Rome. The Church of England retains an instance of this origin of the title, which is held by the two senior members of the College of Minor Canons of St Paul's Cathedral. The Dean of the College of Cardinals, or Cardinal-dean, is the primus inter pares of the College of Cardinals, elected by the cardinal bishops holding suburbicarian sees from among their own number, an election, however, that must be approved by the Pope. Formerly the position of dean belonged by right to the longest-serving of the cardinal bishops. While the incumbents of some sees are regularly made cardinals, and some countries are entitled to at least one cardinal by concordate (usually earning its primate the cardinal's hat), no see carries an actual right to the cardinalate, not even if its bishop is a Patriarch. Each cardinal takes on a titular church, either a church in the city of Rome or one of the suburbicarian sees. The only exception is for patriarchs of Eastern Catholic Churches. Nevertheless, cardinals possess no power of governance nor are they to intervene in any way in matters which pertain to the administration of goods, discipline, or the service of their titular churches. They are allowed to celebrate Mass and hear confessions and lead visits and pilgrimages to their titular churches, in coordination with the staff of the church. They often support their churches monetarily, and many Cardinals do keep in contact with the pastoral staffs of their titular churches."
The_Legend_of_Zelda:_Twilight_Princess,"Following the discovery of a buffer overflow vulnerability in the Wii version of Twilight Princess, an exploit known as the ""Twilight Hack"" was developed, allowing the execution of custom code from a Secure Digital (SD) card on the console. A properly designed save file would cause the game to load unsigned code, which could include Executable and Linkable Format (ELF) programs and homebrew Wii applications. Versions 3.3 and 3.4 of the Wii Menu prevented copying exploited save files onto the console until circumvention methods were discovered, and version 4.0 of the Wii Menu patched the vulnerability. The Legend of Zelda: Twilight Princess (Japanese: ゼルダの伝説 トワイライトプリンセス, Hepburn: Zeruda no Densetsu: Towairaito Purinsesu?) is an action-adventure game developed and published by Nintendo for the GameCube and Wii home video game consoles. It is the thirteenth installment in the The Legend of Zelda series. Originally planned for release on the GameCube in November 2005, Twilight Princess was delayed by Nintendo to allow its developers to refine the game, add more content, and port it to the Wii. The Wii version was released alongside the console in North America in November 2006, and in Japan, Europe, and Australia the following month. The GameCube version was released worldwide in December 2006.[b] Special bundles of the game contain a Wolf Link Amiibo figurine, which unlocks a Wii U-exclusive dungeon called the ""Cave of Shadows"" and can carry data over to the upcoming 2016 Zelda game. Other Zelda-related Amiibo figurines have distinct functions: Link and Toon Link replenish arrows, Zelda and Sheik restore Link's health, and Ganondorf causes Link to take twice as much damage. The game's score was composed by Toru Minegishi and Asuka Ohta, with series regular Koji Kondo serving as the sound supervisor. Minegishi took charge of composition and sound design in Twilight Princess, providing all field and dungeon music under the supervision of Kondo. For the trailers, three pieces were written by different composers, two of which were created by Mahito Yokota and Kondo. Michiru Ōshima created orchestral arrangements for the three compositions, later to be performed by an ensemble conducted by Yasuzo Takemoto. Kondo's piece was later chosen as music for the E3 2005 trailer and for the demo movie after the game's title screen. Twilight Princess received the awards for Best Artistic Design, Best Original Score, and Best Use of Sound from IGN for its GameCube version. Both IGN and Nintendo Power gave Twilight Princess the awards for Best Graphics and Best Story. Twilight Princess received Game of the Year awards from GameTrailers, 1UP.com, Electronic Gaming Monthly, Game Informer, Games Radar, GameSpy, Spacey Awards, X-Play and Nintendo Power. It was also given awards for Best Adventure Game from the Game Critics Awards, X-Play, IGN, GameTrailers, 1UP.com, and Nintendo Power. The game was considered the Best Console Game by the Game Critics Awards and GameSpy. The game placed 16th in Official Nintendo Magazine's list of the 100 Greatest Nintendo Games of All Time. IGN ranked the game as the 4th-best Wii game. Nintendo Power ranked the game as the third-best game to be released on a Nintendo system in the 2000s decade. The context-sensitive button mechanic allows one button to serve a variety of functions, such as talking, opening doors, and pushing, pulling, and throwing objects.[e] The on-screen display shows what action, if any, the button will trigger, determined by the situation. For example, if Link is holding a rock, the context-sensitive button will cause Link to throw the rock if he is moving or targeting an object or enemy, or place the rock on the ground if he is standing still.[f] In the PAL region, which covers most of Africa, Asia, Europe, and Oceania, Twilight Princess is the best-selling entry in the Zelda series. During its first week, the game was sold with three of every four Wii purchases. The game had sold 5.82 million copies on the Wii as of March 31, 2011[update], and 1.32 million on the GameCube as of March 31, 2007[update]. Aonuma had anticipated creating a Zelda game for what would later be called the Wii, but had assumed that he would need to complete Twilight Princess first. His team began work developing a pointing-based interface for the bow and arrow, and Aonuma found that aiming directly at the screen gave the game a new feel, just like the DS control scheme for Phantom Hourglass. Aonuma felt confident this was the only way to proceed, but worried about consumers who had been anticipating a GameCube release. Developing two versions would mean delaying the previously announced 2005 release, still disappointing the consumer. Satoru Iwata felt that having both versions would satisfy users in the end, even though they would have to wait for the finished product. Aonuma then started working on both versions in parallel.[o] Media requests at the trade show prompted Kondo to consider using orchestral music for the other tracks in the game as well, a notion reinforced by his preference for live instruments. He originally envisioned a full 50-person orchestra for action sequences and a string quartet for more ""lyrical moments"", though the final product used sequenced music instead. Kondo later cited the lack of interactivity that comes with orchestral music as one of the main reasons for the decision. Both six- and seven-track versions of the game's soundtrack were released on November 19, 2006, as part of a Nintendo Power promotion and bundled with replicas of the Master Sword and the Hylian Shield. The story focuses on series protagonist Link, who tries to prevent Hyrule from being engulfed by a corrupted parallel dimension known as the Twilight Realm. To do so, he takes the form of both a Hylian and a wolf, and is assisted by a mysterious creature named Midna. The game takes place hundreds of years after Ocarina of Time and Majora's Mask, in an alternate timeline from The Wind Waker. There is very little voice acting in the game, as is the case in most Zelda titles to date. Link remains silent in conversation, but grunts when attacking or injured and gasps when surprised. His emotions and responses are largely indicated visually by nods and facial expressions. Other characters have similar language-independent verbalizations, including laughter, surprised or fearful exclamations, and screams. The character of Midna has the most voice acting—her on-screen dialog is often accompanied by a babble of pseudo-speech, which was produced by scrambling the phonemes of English phrases[better source needed] sampled by Japanese voice actress Akiko Kōmoto. During this time, Link also helps Midna find the Fused Shadows, fragments of a relic containing powerful dark magic. In return, she helps Link find Ordon Village's children while helping the monkeys of Faron, the Gorons of Eldin, and the Zoras of Lanayru. Once Link has restored the Light Spirits and Midna has all the Fused Shadows, they are ambushed by Zant. After he relieves Midna of the Fused Shadow fragments, she ridicules him for abusing his tribe's magic, but Zant reveals that his power comes from another source as he uses it to turn Link back into a wolf, and then leaves Midna in Hyrule to die from the world's light. Bringing a dying Midna to Zelda, Link learns he needs the Master Sword to return to human form. Zelda sacrifices herself to heal Midna with her power before vanishing mysteriously. Midna is moved by Zelda's sacrifice, and begins to care more about Link and the fate of the light world. Nintendo staff members reported that demo users complained about the difficulty of the control scheme. Aonuma realized that his team had implemented Wii controls under the mindset of ""forcing"" users to adapt, instead of making the system intuitive and easy to use. He began rethinking the controls with Miyamoto to focus on comfort and ease.[q] The camera movement was reworked and item controls were changed to avoid accidental button presses.[r] In addition, the new item system required use of the button that had previously been used for the sword. To solve this, sword controls were transferred back to gestures—something E3 attendees had commented they would like to see. This reintroduced the problem of using a right-handed swing to control a left-handed sword attack. The team did not have enough time before release to rework Link's character model, so they instead flipped the entire game—everything was made a mirror image.[s] Link was now right-handed, and references to ""east"" and ""west"" were switched around. The GameCube version, however, was left with the original orientation. The Twilight Princess player's guide focuses on the Wii version, but has a section in the back with mirror-image maps for GameCube users.[t] On release, Twilight Princess was considered to be the greatest Zelda game ever made by many critics including writers for 1UP.com, Computer and Video Games, Electronic Gaming Monthly, Game Informer, GamesRadar, IGN and The Washington Post. Game Informer called it ""so creative that it rivals the best that Hollywood has to offer"". GamesRadar praised Twilight Princess as ""a game that deserves nothing but the absolute highest recommendation"". Cubed3 hailed Twilight Princess as ""the single greatest videogame experience"". Twilight Princess's graphics were praised for the art style and animation, although the game was designed for the GameCube, which is technically lacking compared to the next generation consoles. Both IGN and GameSpy pointed out the existence of blurry textures and low-resolution characters. Despite these complaints, Computer and Video Games felt the game's atmosphere was superior to that of any previous Zelda game, and regarded Twilight Princess's Hyrule as the best version ever created. PALGN praised the game's cinematics, noting that ""the cutscenes are the best ever in Zelda games"". Regarding the Wii version, GameSpot's Jeff Gerstmann said the Wii controls felt ""tacked-on"", although 1UP.com said the remote-swinging sword attacks were ""the most impressive in the entire series"". Gaming Nexus considered Twilight Princess's soundtrack to be the best of this generation, though IGN criticized its MIDI-formatted songs for lacking ""the punch and crispness"" of their orchestrated counterparts. Hyper's Javier Glickman commended the game for its ""very long quests, superb Wii controls and being able to save anytime"". However, he criticised it for ""no voice acting, no orchestral score and slightly outdated graphics"". The team worked on a Wii control scheme, adapting camera control and the fighting mechanics to the new interface. A prototype was created that used a swinging gesture to control the sword from a first-person viewpoint, but was unable to show the variety of Link's movements. When the third-person view was restored, Aonuma thought it felt strange to swing the Wii Remote with the right hand to control the sword in Link's left hand, so the entire Wii version map was mirrored.[p] Details about Wii controls began to surface in December 2005 when British publication NGC Magazine claimed that when a GameCube copy of Twilight Princess was played on the Revolution, it would give the player the option of using the Revolution controller. Miyamoto confirmed the Revolution controller-functionality in an interview with Nintendo of Europe and Time reported this soon after. However, support for the Wii controller did not make it into the GameCube release. At E3 2006, Nintendo announced that both versions would be available at the Wii launch, and had a playable version of Twilight Princess for the Wii.[p] Later, the GameCube release was pushed back to a month after the launch of the Wii. Twilight Princess takes place several centuries after Ocarina of Time and Majora's Mask, and begins with a youth named Link who is working as a ranch hand in Ordon Village. One day, the village is attacked by Bulblins, who carry off the village's children with Link in pursuit before he encounters a wall of Twilight. A Shadow Beast pulls him beyond the wall into the Realm of Twilight, where he is transformed into a wolf and imprisoned. Link is soon freed by an imp-like Twilight being named Midna, who dislikes Link but agrees to help him if he obeys her unconditionally. She guides him to Princess Zelda. Zelda explains that Zant, the King of the Twilight, has stolen the light from three of the four Light Spirits and conquered Hyrule. In order to save Hyrule, Link must first restore the Light Spirits by entering the Twilight-covered areas and, as a wolf, recover the Spirits' lost light. He must do this by collecting the multiple ""Tears of Light""; once all the Tears of Light are collected for one area, he restores that area's Light Spirit. As he restores them, the Light Spirits return Link to his Hylian form. In four months, Aonuma's team managed to present realistic horseback riding,[l] which Nintendo later revealed to the public with a trailer at Electronic Entertainment Expo 2004. The game was scheduled to be released the next year, and was no longer a follow-up to The Wind Waker; a true sequel to it was released for the Nintendo DS in 2007, in the form of Phantom Hourglass. Miyamoto explained in interviews that the graphical style was chosen to satisfy demand, and that it better fit the theme of an older incarnation of Link. The game runs on a modified The Wind Waker engine. When Link enters the Twilight Realm, the void that corrupts parts of Hyrule, he transforms into a wolf.[h] He is eventually able to transform between his Hylian and wolf forms at will. As a wolf, Link loses the ability to use his sword, shield, or any secondary items; he instead attacks by biting, and defends primarily by dodging attacks. However, ""Wolf Link"" gains several key advantages in return—he moves faster than he does as a human (though riding Epona is still faster) and digs holes to create new passages and uncover buried items, and has improved senses, including the ability to follow scent trails.[i] He also carries Midna, a small imp-like creature who gives him hints, uses an energy field to attack enemies, helps him jump long distances, and eventually allows Link to ""warp"" to any of several preset locations throughout the overworld.[j] Using Link's wolf senses, the player can see and listen to the wandering spirits of those affected by the Twilight, as well as hunt for enemy ghosts named Poes.[k] Transferring GameCube development to the Wii was relatively simple, since the Wii was being created to be compatible with the GameCube.[o] At E3 2005, Nintendo released a small number of Nintendo DS game cards containing a preview trailer for Twilight Princess. They also announced that Zelda would appear on the Wii (then codenamed ""Revolution""), but it was not clear to the media if this meant Twilight Princess or a different game. The GameCube and Wii versions feature several minor differences in their controls. The Wii version of the game makes use of the motion sensors and built-in speaker of the Wii Remote. The speaker emits the sounds of a bowstring when shooting an arrow, Midna's laugh when she gives advice to Link, and the series' trademark ""chime"" when discovering secrets. The player controls Link's sword by swinging the Wii Remote. Other attacks are triggered using similar gestures with the Nunchuk. Unique to the GameCube version is the ability for the player to control the camera freely, without entering a special ""lookaround"" mode required by the Wii; however, in the GameCube version, only two of Link's secondary weapons can be equipped at a time, as opposed to four in the Wii version.[g] A high-definition remaster of the game, The Legend of Zelda: Twilight Princess HD, is being developed by Tantalus Media for the Wii U. Officially announced during a Nintendo Direct presentation on November 12, 2015, it features enhanced graphics and Amiibo functionality. The game will be released in North America and Europe on March 4, 2016; in Australia on March 5, 2016; and in Japan on March 10, 2016. A Japan-exclusive manga series based on Twilight Princess, penned and illustrated by Akira Himekawa, was first released on February 8, 2016. The series is available solely via publisher Shogakukan's MangaOne mobile application. While the manga adaptation began almost ten years after the initial release of the game on which it is based, it launched only a month before the release of the high-definition remake. At the time of its release, Twilight Princess was considered the greatest entry in the Zelda series by many critics, including writers for 1UP.com, Computer and Video Games, Electronic Gaming Monthly, Game Informer, GamesRadar, IGN, and The Washington Post. It received several Game of the Year awards, and was the most critically acclaimed game of 2006. In 2011, the Wii version was rereleased under the Nintendo Selects label. A high-definition port for the Wii U, The Legend of Zelda: Twilight Princess HD, will be released in March 2016. The Legend of Zelda: Twilight Princess is an action-adventure game focused on combat, exploration, and item collection. It uses the basic control scheme introduced in Ocarina of Time, including context-sensitive action buttons and L-targeting (Z-targeting on the Wii), a system that allows the player to keep Link's view focused on an enemy or important object while moving and attacking. Link can walk, run, and attack, and will automatically jump when running off of or reaching for a ledge.[c] Link uses a sword and shield in combat, complemented with secondary weapons and items, including a bow and arrows, a boomerang, bombs, and the Clawshot (similar to the Hookshot introduced earlier in the The Legend of Zelda series).[d] While L-targeting, projectile-based weapons can be fired at a target without the need for manual aiming.[c] Ganondorf then revives, and Midna teleports Link and Zelda outside the castle so she can hold him off with the Fused Shadows. However, as Hyrule Castle collapses, it is revealed that Ganondorf was victorious as he crushes Midna's helmet. Ganondorf engages Link on horseback, and, assisted by Zelda and the Light Spirits, Link eventually knocks Ganondorf off his horse and they duel on foot before Link strikes down Ganondorf and plunges the Master Sword into his chest. With Ganondorf dead, the Light Spirits not only bring Midna back to life, but restore her to her true form. After bidding farewell to Link and Zelda, Midna returns home before destroying the Mirror of Twilight with a tear to maintain balance between Hyrule and the Twilight Realm. Near the end, as Hyrule Castle is rebuilt, Link is shown leaving Ordon Village heading to parts unknown. Prior Zelda games have employed a theme of two separate, yet connected, worlds. In A Link to the Past, Link travels between a ""Light World"" and a ""Dark World""; in Ocarina of Time, as well as in Oracle of Ages, Link travels between two different time periods. The Zelda team sought to reuse this motif in the series' latest installment. It was suggested that Link transform into a wolf, much like he metamorphoses into a rabbit in the Dark World of A Link to the Past.[m] The story of the game was created by Aonuma, and later underwent several changes by scenario writers Mitsuhiro Takano and Aya Kyogoku. Takano created the script for the story scenes, while Kyogoku and Takayuki Ikkaku handled the actual in-game script. Aonuma left his team working on the new idea while he directed The Minish Cap for the Game Boy Advance. When he returned, he found the Twilight Princess team struggling. Emphasis on the parallel worlds and the wolf transformation had made Link's character unbelievable. Aonuma also felt the gameplay lacked the caliber of innovation found in Phantom Hourglass, which was being developed with touch controls for the Nintendo DS. At the same time, the Wii was under development with the code name ""Revolution"". Miyamoto thought that the Revolution's pointing device, the Wii Remote, was well suited for aiming arrows in Zelda, and suggested that Aonuma consider using it.[n] The artificial intelligence (AI) of enemies in Twilight Princess is more advanced than that of enemies in The Wind Waker. Enemies react to defeated companions and to arrows or slingshot pellets that pass by, and can detect Link from a greater distance than was possible in previous games. A CD containing 20 musical selections from the game was available as a GameStop preorder bonus in the United States; it is included in all bundles in Japan, Europe, and Australia.[citation needed] Twilight Princess was released to universal critical acclaim and commercial success. It received perfect scores from major publications such as 1UP.com, Computer and Video Games, Electronic Gaming Monthly, Game Informer, GamesRadar, and GameSpy. On the review aggregators GameRankings and Metacritic, Twilight Princess has average scores of 95% and 95 for the Wii version and scores of 95% and 96 for the GameCube version. GameTrailers in their review called it one of the greatest games ever created. In 2003, Nintendo announced that a new The Legend of Zelda game was in the works for the GameCube by the same team that had created the cel-shaded The Wind Waker. At the following year's Game Developers Conference, director Eiji Aonuma unintentionally revealed that the game's sequel was in development under the working title The Wind Waker 2; it was set to use a similar graphical style to that of its predecessor. Nintendo of America told Aonuma that North American sales of The Wind Waker were sluggish because its cartoon appearance created the impression that the game was designed for a young audience. Concerned that the sequel would have the same problem, Aonuma expressed to producer Shigeru Miyamoto that he wanted to create a realistic Zelda game that would appeal to the North American market. Miyamoto, hesitant about solely changing the game's presentation, suggested the team's focus should instead be on coming up with gameplay innovations. He advised that Aonuma should start by doing what could not be done in Ocarina of Time, particularly horseback combat.[l] After gaining the Master Sword, Link is cleansed of the magic that kept him in wolf form, obtaining the Shadow Crystal. Now able to use it to switch between both forms at will, Link is led by Midna to the Mirror of Twilight located deep within the Gerudo Desert, the only known gateway between the Twilight Realm and Hyrule. However, they discover that the mirror is broken. The Sages there explain that Zant tried to destroy it, but he was only able to shatter it into fragments; only the true ruler of the Twili can completely destroy the Mirror of Twilight. They also reveal that they used it a century ago to banish Ganondorf, the Gerudo leader who attempted to steal the Triforce, to the Twilight Realm when executing him failed. Assisted by an underground resistance group they meet in Castle Town, Link and Midna set out to retrieve the missing shards of the Mirror, defeating those they infected. Once the portal has been restored, Midna is revealed to be the true ruler of the Twilight Realm, usurped by Zant when he cursed her into her current form. Confronting Zant, Link and Midna learn that Zant's coup was made possible when he forged a pact with Ganondorf, who asked for Zant's assistance in conquering Hyrule. After Link defeats Zant, Midna recovers the Fused Shadows, but destroys Zant after learning that only Ganondorf's death can release her from her curse. Returning to Hyrule, Link and Midna find Ganondorf in Hyrule Castle, with a lifeless Zelda suspended above his head. Ganondorf fights Link by possessing Zelda's body and eventually by transforming into a beast, but Link defeats him and Midna is able to resurrect Zelda. The game features nine dungeons—large, contained areas where Link battles enemies, collects items, and solves puzzles. Link navigates these dungeons and fights a boss at the end in order to obtain an item or otherwise advance the plot. The dungeons are connected by a large overworld, across which Link can travel on foot; on his horse, Epona; or by teleporting."
Web_browser,"The most recent major entrant to the browser market is Chrome, first released in September 2008. Chrome's take-up has increased significantly year by year, by doubling its usage share from 8% to 16% by August 2011. This increase seems largely to be at the expense of Internet Explorer, whose share has tended to decrease from month to month. In December 2011, Chrome overtook Internet Explorer 8 as the most widely used web browser but still had lower usage than all versions of Internet Explorer combined. Chrome's user-base continued to grow and in May 2012, Chrome's usage passed the usage of all versions of Internet Explorer combined. By April 2014, Chrome's usage had hit 45%. Although browsers are primarily intended to use the World Wide Web, they can also be used to access information provided by web servers in private networks or files in file systems. In 1993, browser software was further innovated by Marc Andreessen with the release of Mosaic, ""the world's first popular browser"", which made the World Wide Web system easy to use and more accessible to the average person. Andreesen's browser sparked the internet boom of the 1990s. The introduction of Mosaic in 1993 – one of the first graphical web browsers – led to an explosion in web use. Andreessen, the leader of the Mosaic team at National Center for Supercomputing Applications (NCSA), soon started his own company, named Netscape, and released the Mosaic-influenced Netscape Navigator in 1994, which quickly became the world's most popular browser, accounting for 90% of all web use at its peak (see usage share of web browsers). In 1998, Netscape launched what was to become the Mozilla Foundation in an attempt to produce a competitive browser using the open source software model. That browser would eventually evolve into Firefox, which developed a respectable following while still in the beta stage of development; shortly after the release of Firefox 1.0 in late 2004, Firefox (all versions) accounted for 7% of browser use. As of August 2011, Firefox has a 28% usage share. A web browser (commonly referred to as a browser) is a software application for retrieving, presenting, and traversing information resources on the World Wide Web. An information resource is identified by a Uniform Resource Identifier (URI/URL) and may be a web page, image, video or other piece of content. Hyperlinks present in resources enable users easily to navigate their browsers to related resources. A browser extension is a computer program that extends the functionality of a web browser. Every major web browser supports the development of browser extensions. This process begins when the user inputs a Uniform Resource Locator (URL), for example http://en.wikipedia.org/, into the browser. The prefix of the URL, the Uniform Resource Identifier or URI, determines how the URL will be interpreted. The most commonly used kind of URI starts with http: and identifies a resource to be retrieved over the Hypertext Transfer Protocol (HTTP). Many browsers also support a variety of other prefixes, such as https: for HTTPS, ftp: for the File Transfer Protocol, and file: for local files. Prefixes that the web browser cannot directly handle are often handed off to another application entirely. For example, mailto: URIs are usually passed to the user's default e-mail application, and news: URIs are passed to the user's default newsgroup reader. Information resources may contain hyperlinks to other information resources. Each link contains the URI of a resource to go to. When a link is clicked, the browser navigates to the resource indicated by the link's target URI, and the process of bringing content to the user begins again. Early web browsers supported only a very simple version of HTML. The rapid development of proprietary web browsers led to the development of non-standard dialects of HTML, leading to problems with interoperability. Modern web browsers support a combination of standards-based and de facto HTML and XHTML, which should be rendered in the same way by all browsers. Opera debuted in 1996; it has never achieved widespread use, having less than 2% browser usage share as of February 2012 according to Net Applications. Its Opera-mini version has an additive share, in April 2011 amounting to 1.1% of overall browser use, but focused on the fast-growing mobile phone web browser market, being preinstalled on over 40 million phones. It is also available on several other embedded systems, including Nintendo's Wii video game console. In January 2009, the European Commission announced it would investigate the bundling of Internet Explorer with Windows operating systems from Microsoft, saying ""Microsoft's tying of Internet Explorer to the Windows operating system harms competition between web browsers, undermines product innovation and ultimately reduces consumer choice."" Microsoft Corp v Commission The primary purpose of a web browser is to bring information resources to the user (""retrieval"" or ""fetching""), allowing them to view the information (""display"", ""rendering""), and then access other information (""navigation"", ""following links""). Safari and Mobile Safari were likewise always included with OS X and iOS respectively, so, similarly, they were originally funded by sales of Apple computers and mobile devices, and formed part of the overall Apple experience to customers. Microsoft responded with its Internet Explorer in 1995, also heavily influenced by Mosaic, initiating the industry's first browser war. Bundled with Windows, Internet Explorer gained dominance in the web browser market; Internet Explorer usage share peaked at over 95% by 2002. Today, most commercial web browsers are paid by search engine companies to make their engine default, or to include them as another option. For example, Google pays Mozilla, the maker of Firefox, to make Google Search the default search engine in Firefox. Mozilla makes enough money from this deal that it does not need to charge users for Firefox. In addition, Google Search is also (as one would expect) the default search engine in Google Chrome. Users searching for websites or items on the Internet would be led to Google's search results page, increasing ad revenue and which funds development at Google and of Google Chrome. Web browsers consist of a user interface, layout engine, rendering engine, JavaScript interpreter, UI backend, networking component and data persistence component. These components achieve different functionalities of a web browser and together provide all capabilities of a web browser. The first web browser was invented in 1990 by Sir Tim Berners-Lee. Berners-Lee is the director of the World Wide Web Consortium (W3C), which oversees the Web's continued development, and is also the founder of the World Wide Web Foundation. His browser was called WorldWideWeb and later renamed Nexus. Available web browsers range in features from minimal, text-based user interfaces with bare-bones support for HTML to rich user interfaces supporting a wide variety of file formats and protocols. Browsers which include additional components to support e-mail, Usenet news, and Internet Relay Chat (IRC), are sometimes referred to as ""Internet suites"" rather than merely ""web browsers"". Most browsers support HTTP Secure and offer quick and easy ways to delete the web cache, download history, form and search history, cookies, and browsing history. For a comparison of the current security vulnerabilities of browsers, see comparison of web browsers. Apple's Safari had its first beta release in January 2003; as of April 2011, it had a dominant share of Apple-based web browsing, accounting for just over 7% of the entire browser market. Most web browsers can display a list of web pages that the user has bookmarked so that the user can quickly return to them. Bookmarks are also called ""Favorites"" in Internet Explorer. In addition, all major web browsers have some form of built-in web feed aggregator. In Firefox, web feeds are formatted as ""live bookmarks"" and behave like a folder of bookmarks corresponding to recent entries in the feed. In Opera, a more traditional feed reader is included which stores and displays the contents of the feed. All major web browsers allow the user to open multiple information resources at the same time, either in different browser windows or in different tabs of the same window. Major browsers also include pop-up blockers to prevent unwanted windows from ""popping up"" without the user's consent. Internet Explorer, on the other hand, was bundled free with the Windows operating system (and was also downloadable free), and therefore it was funded partly by the sales of Windows to computer manufacturers and direct to users. Internet Explorer also used to be available for the Mac. It is likely that releasing IE for the Mac was part of Microsoft's overall strategy to fight threats to its quasi-monopoly platform dominance - threats such as web standards and Java - by making some web developers, or at least their managers, assume that there was ""no need"" to develop for anything other than Internet Explorer. In this respect, IE may have contributed to Windows and Microsoft applications sales in another way, through ""lock-in"" to Microsoft's browser. In the case of http, https, file, and others, once the resource has been retrieved the web browser will display it. HTML and associated content (image files, formatting information such as CSS, etc.) is passed to the browser's layout engine to be transformed from markup to an interactive document, a process known as ""rendering"". Aside from HTML, web browsers can generally display any kind of content that can be part of a web page. Most browsers can display images, audio, video, and XML files, and often have plug-ins to support Flash applications and Java applets. Upon encountering a file of an unsupported type or a file that is set up to be downloaded rather than displayed, the browser prompts the user to save the file to disk."
Indigenous_peoples_of_the_Americas,"Morales began work on his ""indigenous autonomy"" policy, which he launched in the eastern lowlands department on August 3, 2009, making Bolivia the first country in the history of South America to affirm the right of indigenous people to govern themselves. Speaking in Santa Cruz Department, the President called it ""a historic day for the peasant and indigenous movement"", saying that, though he might make errors, he would ""never betray the fight started by our ancestors and the fight of the Bolivian people"". A vote on further autonomy will take place in referendums which are expected to be held in December 2009. The issue has divided the country. The Maya writing system (often called hieroglyphs from a superficial resemblance to the Ancient Egyptian writing) was a combination of phonetic symbols and logograms. It is most often classified as a logographic or (more properly) a logosyllabic writing system, in which syllabic signs play a significant role. It is the only pre-Columbian writing system known to represent completely the spoken language of its community. In total, the script has more than one thousand different glyphs although a few are variations of the same sign or meaning and many appear only rarely or are confined to particular localities. At any one time, no more than about five hundred glyphs were in use, some two hundred of which (including variations) had a phonetic or syllabic interpretation. Large numbers of Bolivian highland peasants retained indigenous language, culture, customs, and communal organization throughout the Spanish conquest and the post-independence period. They mobilized to resist various attempts at the dissolution of communal landholdings and used legal recognition of ""empowered caciques"" to further communal organization. Indigenous revolts took place frequently until 1953. While the National Revolutionary Movement government begun in 1952 discouraged self-identification as indigenous (reclassifying rural people as campesinos, or peasants), renewed ethnic and class militancy re-emerged in the Katarista movement beginning in the 1970s. Lowland indigenous peoples, mostly in the east, entered national politics through the 1990 March for Territory and Dignity organized by the CIDOB confederation. That march successfully pressured the national government to sign the ILO Convention 169 and to begin the still-ongoing process of recognizing and titling indigenous territories. The 1994 Law of Popular Participation granted ""grassroots territorial organizations"" that are recognized by the state certain rights to govern local areas. Many parts of the Americas are still populated by indigenous peoples; some countries have sizable populations, especially Belize, Bolivia, Chile, Ecuador, Greenland, Guatemala, Mexico, and Peru. At least a thousand different indigenous languages are spoken in the Americas. Some, such as the Quechuan languages, Aymara, Guaraní, Mayan languages, and Nahuatl, count their speakers in millions. Many also maintain aspects of indigenous cultural practices to varying degrees, including religion, social organization, and subsistence practices. Like most cultures, over time, cultures specific to many indigenous peoples have evolved to incorporate traditional aspects, but also cater to modern needs. Some indigenous peoples still live in relative isolation from Western culture and a few are still counted as uncontacted peoples. According to both indigenous American and European accounts and documents, American civilizations at the time of European encounter had achieved many accomplishments. For instance, the Aztecs built one of the largest cities in the world, Tenochtitlan, the ancient site of Mexico City, with an estimated population of 200,000. American civilizations also displayed impressive accomplishments in astronomy and mathematics. The domestication of maize or corn required thousands of years of selective breeding. Spanish mendicants in the sixteenth century taught indigenous scribes in their communities to write their languages in Latin letters and there is a large number of local-level documents in Nahuatl, Zapotec, Mixtec, and Yucatec Maya from the colonial era, many of which were part of lawsuits and other legal matters. Although Spaniards initially taught indigenous scribes alphabetic writing, the tradition became self-perpetuating at the local level. The Spanish crown gathered such documentation and contemporary Spanish translations were made for legal cases. Scholars have translated and analyzed these documents in what is called the New Philology to write histories of indigenous peoples from indigenous viewpoints. Native American music in North America is almost entirely monophonic, but there are notable exceptions. Traditional Native American music often centers around drumming. Rattles, clappersticks, and rasps were also popular percussive instruments. Flutes were made of rivercane, cedar, and other woods. The tuning of these flutes is not precise and depends on the length of the wood used and the hand span of the intended player, but the finger holes are most often around a whole step apart and, at least in Northern California, a flute was not used if it turned out to have an interval close to a half step. The Apache fiddle is a single stringed instrument. Although not without conflict, European/Canadian early interactions with First Nations and Inuit populations were relatively peaceful compared to the experience of native peoples in the United States. Combined with a late economic development in many regions, this relatively peaceful history has allowed Canadian Indigenous peoples to have a fairly strong influence on the early national culture while preserving their own identity. From the late 18th century, European Canadians encouraged Aboriginals to assimilate into their own culture, referred to as ""Canadian culture"". These attempts reached a climax in the late 19th and early 20th centuries with forced integration. National Aboriginal Day recognises the cultures and contributions of Aboriginal peoples of Canada. There are currently over 600 recognized First Nations governments or bands encompassing 1,172,790 2006 people spread across Canada with distinctive Aboriginal cultures, languages, art, and music. The music of the indigenous peoples of Central Mexico and Central America was often pentatonic. Before the arrival of the Spaniards and other Europeans, music was inseparable from religious festivities and included a large variety of percussion and wind instruments such as drums, flutes, sea snail shells (used as a trumpet) and ""rain"" tubes. No remnants of pre-Columbian stringed instruments were found until archaeologists discovered a jar in Guatemala, attributed to the Maya of the Late Classic Era (600–900 CE), which depicts a stringed musical instrument which has since been reproduced. This instrument is one of the very few stringed instruments known in the Americas prior to the introduction of European musical instruments; when played it produces a sound virtually identical to a jaguar's growl. Ecuador was the site of many indigenous cultures, and civilizations of different proportions. An early sedentary culture, known as the Valdivia culture, developed in the coastal region, while the Caras and the Quitus unified to form an elaborate civilization that ended at the birth of the Capital Quito. The Cañaris near Cuenca were the most advanced, and most feared by the Inca, due to their fierce resistance to the Incan expansion. Their architecture remains were later destroyed by Spaniards and the Incas. The first indigenous group encountered by Columbus were the 250,000 Taínos of Hispaniola who represented the dominant culture in the Greater Antilles and the Bahamas. Within thirty years about 70% of the Taínos had died. They had no immunity to European diseases, so outbreaks of measles and smallpox ravaged their population. Increasing punishment of the Taínos for revolting against forced labour, despite measures put in place by the encomienda, which included religious education and protection from warring tribes, eventually led to the last great Taíno rebellion. The Native American name controversy is an ongoing dispute over the acceptable ways to refer to the indigenous peoples of the Americas and to broad subsets thereof, such as those living in a specific country or sharing certain cultural attributes. When discussing broader subsets of peoples, naming may be based on shared language, region, or historical relationship. Many English exonyms have been used to refer to the indigenous peoples of the Americas. Some of these names were based on foreign-language terms used by earlier explorers and colonists, while others resulted from the colonists' attempt to translate endonyms from the native language into their own, and yet others were pejorative terms arising out of prejudice and fear, during periods of conflict. Genetic history of indigenous peoples of the Americas primarily focus on Human Y-chromosome DNA haplogroups and Human mitochondrial DNA haplogroups. ""Y-DNA"" is passed solely along the patrilineal line, from father to son, while ""mtDNA"" is passed down the matrilineal line, from mother to offspring of both sexes. Neither recombines, and thus Y-DNA and mtDNA change only by chance mutation at each generation with no intermixture between parents' genetic material. Autosomal ""atDNA"" markers are also used, but differ from mtDNA or Y-DNA in that they overlap significantly. AtDNA is generally used to measure the average continent-of-ancestry genetic admixture in the entire human genome and related isolated populations. About five percent of the population are of full-blooded indigenous descent, but upwards to eighty percent more or the majority of Hondurans are mestizo or part-indigenous with European admixture, and about ten percent are of indigenous or African descent. The main concentration of indigenous in Honduras are in the rural westernmost areas facing Guatemala and to the Caribbean Sea coastline, as well on the Nicaraguan border. The majority of indigenous people are Lencas, Miskitos to the east, Mayans, Pech, Sumos, and Tolupan. Indigenous peoples of Brazil make up 0.4% of Brazil's population, or about 700,000 people, even though millions of Brazilians have some indigenous ancestry. Indigenous peoples are found in the entire territory of Brazil, although the majority of them live in Indian reservations in the North and Center-Western part of the country. On January 18, 2007, FUNAI reported that it had confirmed the presence of 67 different uncontacted tribes in Brazil, up from 40 in 2005. With this addition Brazil has now overtaken the island of New Guinea as the country having the largest number of uncontacted tribes. Indigenous peoples in what is now the contiguous United States, including their descendants, are commonly called ""American Indians"", or simply ""Indians"" domestically, or ""Native Americans"" by the USCB. In Alaska, indigenous peoples belong to 11 cultures with 11 languages. These include the St. Lawrence Island Yupik, Iñupiat, Athabaskan, Yup'ik, Cup'ik, Unangax, Alutiiq, Eyak, Haida, Tsimshian, and Tlingit, who are collectively called Alaska Natives. Indigenous Polynesian peoples, which include Marshallese, Samoan, Tahitian, and Tongan, are politically considered Pacific Islands American but are geographically and culturally distinct from indigenous peoples of the Americas. However, since the 20th century, indigenous peoples in the Americas have been more vocal about the ways they wish to be referred to, pressing for the elimination of terms widely considered to be obsolete, inaccurate, or racist. During the latter half of the 20th century and the rise of the Indian rights movement, the United States government responded by proposing the use of the term ""Native American,"" to recognize the primacy of indigenous peoples' tenure in the nation, but this term was not fully accepted. Other naming conventions have been proposed and used, but none are accepted by all indigenous groups. While technically referring to the era before Christopher Columbus' voyages of 1492 to 1504, in practice the term usually includes the history of American indigenous cultures until Europeans either conquered or significantly influenced them, even if this happened decades or even centuries after Columbus' initial landing. ""Pre-Columbian"" is used especially often in the context of discussing the great indigenous civilizations of the Americas, such as those of Mesoamerica (the Olmec, the Toltec, the Teotihuacano, the Zapotec, the Mixtec, the Aztec, and the Maya civilizations) and those of the Andes (Inca Empire, Moche culture, Muisca Confederation, Cañaris). Application of the term ""Indian"" originated with Christopher Columbus, who, in his search for Asia, thought that he had arrived in the East Indies. The Americas came to be known as the ""West Indies"", a name still used to refer to the islands of the Caribbean Sea. This led to the names ""Indies"" and ""Indian"", which implied some kind of racial or cultural unity among the aboriginal peoples of the Americas. This unifying concept, codified in law, religion, and politics, was not originally accepted by indigenous peoples but has been embraced by many over the last two centuries.[citation needed] Even though the term ""Indian"" does not include the Aleuts, Inuit, or Yupik peoples, these groups are considered indigenous peoples of the Americas. About 5% of the Nicaraguan population are indigenous. The largest indigenous group in Nicaragua is the Miskito people. Their territory extended from Cape Camarón, Honduras, to Rio Grande, Nicaragua along the Mosquito Coast. There is a native Miskito language, but large groups speak Miskito Coast Creole, Spanish, Rama and other languages. The Creole English came about through frequent contact with the British who colonized the area. Many are Christians. Traditional Miskito society was highly structured with a defined political structure. There was a king, but he did not have total power. Instead, the power was split between himself, a governor, a general, and by the 1750s, an admiral. Historical information on kings is often obscured by the fact that many of the kings were semi-mythical. Another major group is the Mayangna (or Sumu) people, counting some 10,000 people. The territory of modern-day Mexico was home to numerous indigenous civilizations prior to the arrival of the Spanish conquistadores: The Olmecs, who flourished from between 1200 BCE to about 400 BCE in the coastal regions of the Gulf of Mexico; the Zapotecs and the Mixtecs, who held sway in the mountains of Oaxaca and the Isthmus of Tehuantepec; the Maya in the Yucatan (and into neighbouring areas of contemporary Central America); the Purépecha in present-day Michoacán and surrounding areas, and the Aztecs/Mexica, who, from their central capital at Tenochtitlan, dominated much of the centre and south of the country (and the non-Aztec inhabitants of those areas) when Hernán Cortés first landed at Veracruz. Native Americans in the United States make up 0.97% to 2% of the population. In the 2010 census, 2.9 million people self-identified as Native American, Native Hawaiian, and Alaska Native alone, and 5.2 million people identified as U.S. Native Americans, either alone or in combination with one or more ethnicity or other races. 1.8 million are recognized as enrolled tribal members.[citation needed] Tribes have established their own criteria for membership, which are often based on blood quantum, lineal descent, or residency. A minority of US Native Americans live in land units called Indian reservations. Some California and Southwestern tribes, such as the Kumeyaay, Cocopa, Pascua Yaqui and Apache span both sides of the US–Mexican border. Haudenosaunee people have the legal right to freely cross the US–Canadian border. Athabascan, Tlingit, Haida, Tsimshian, Iñupiat, Blackfeet, Nakota, Cree, Anishinaabe, Huron, Lenape, Mi'kmaq, Penobscot, and Haudenosaunee, among others live in both Canada and the US. The indigenous peoples of the Americas are the descendants of the pre-Columbian inhabitants of the Americas. Pueblos indígenas (indigenous peoples) is a common term in Spanish-speaking countries. Aborigen (aboriginal/native) is used in Argentina, whereas ""Amerindian"" is used in Quebec, The Guianas, and the English-speaking Caribbean. Indigenous peoples are commonly known in Canada as Aboriginal peoples, which include First Nations, Inuit, and Métis peoples. Indigenous peoples of the United States are commonly known as Native Americans or American Indians, and Alaska Natives. Indigenous population in Peru make up around 45%. Native Peruvian traditions and customs have shaped the way Peruvians live and see themselves today. Cultural citizenship—or what Renato Rosaldo has called, ""the right to be different and to belong, in a democratic, participatory sense"" (1996:243)—is not yet very well developed in Peru. This is perhaps no more apparent than in the country's Amazonian regions where indigenous societies continue to struggle against state-sponsored economic abuses, cultural discrimination, and pervasive violence. Genetic studies of mitochondrial DNA (mtDNA) of Amerindians and some Siberian and Central Asian peoples also revealed that the gene pool of the Turkic-speaking peoples of Siberia such as Altaians, Khakas, Shors and Soyots, living between the Altai and Lake Baikal along the Sayan mountains, are genetically closest to Amerindians.[citation needed] This view is shared by other researchers who argue that ""the ancestors of the American Indians were the first to separate from the great Asian population in the Middle Paleolithic. 2012 research found evidence for a recent common ancestry between Native Americans and indigenous Altaians based on mitochondrial DNA and Y-Chromosome analysis. The paternal lineages of Altaians mostly belong to the subclades of haplogroup P-M45 (xR1a 38-93%; xQ1a 4-32%). Aboriginal peoples in Canada comprise the First Nations, Inuit and Métis; the descriptors ""Indian"" and ""Eskimo"" are falling into disuse, and other than in neighboring Alaska. ""Eskimo"" is considered derogatory in many other places because it was given by non-Inuit people and was said to mean ""eater of raw meat."" Hundreds of Aboriginal nations evolved trade, spiritual and social hierarchies. The Métis culture of mixed blood originated in the mid-17th century when First Nation and native Inuit married European settlers. The Inuit had more limited interaction with European settlers during that early period. Various laws, treaties, and legislation have been enacted between European immigrants and First Nations across Canada. Aboriginal Right to Self-Government provides opportunity to manage historical, cultural, political, health care and economic control aspects within first people's communities. Following years of mistreatment, the Taínos began to adopt suicidal behaviors, with women aborting or killing their infants and men jumping from the cliffs or ingesting untreated cassava, a violent poison. Eventually, a Taíno Cacique named Enriquillo managed to hold out in the Baoruco Mountain Range for thirteen years, causing serious damage to the Spanish, Carib-held plantations and their Indian auxiliaries. Hearing of the seriousness of the revolt, Emperor Charles V (also King of Spain) sent captain Francisco Barrionuevo to negotiate a peace treaty with the ever-increasing number of rebels. Two months later, after consultation with the Audencia of Santo Domingo, Enriquillo was offered any part of the island to live in peace. The Spanish Empire and other Europeans brought horses to the Americas. Some of these animals escaped and began to breed and increase their numbers in the wild. The re-introduction of the horse, extinct in the Americas for over 7500 years, had a profound impact on Native American culture in the Great Plains of North America and of Patagonia in South America. By domesticating horses, some tribes had great success: horses enabled them to expand their territories, exchange more goods with neighboring tribes, and more easily capture game, especially bison. In recent years, there has been a rise of indigenous movements in the Americas (mainly South America). These are rights-driven groups that organize themselves in order to achieve some sort of self-determination and the preservation of their culture for their peoples. Organizations like the Coordinator of Indigenous Organizations of the Amazon River Basin and the Indian Council of South America are examples of movements that are breaking the barrier of borders in order to obtain rights for Amazonian indigenous populations everywhere. Similar movements for indigenous rights can also be seen in Canada and the United States, with movements like the International Indian Treaty Council and the accession of native Indian group into the Unrepresented Nations and Peoples Organization. In Bolivia, a 62% majority of residents over the age of 15 self-identify as belonging to an indigenous people, while another 3.7% grew up with an indigenous mother tongue yet do not self-identify as indigenous. Including both of these categories, and children under 15, some 66.4% of Bolivia's population was registered as indigenous in the 2001 Census. The largest indigenous ethnic groups are: Quechua, about 2.5 million people; Aymara, 2.0 million; Chiquitano, 181,000; Guaraní, 126,000; and Mojeño, 69,000. Some 124,000 belong to smaller indigenous groups. The Constitution of Bolivia, enacted in 2009, recognizes 36 cultures, each with its own language, as part of a plurinational state. Some groups, including CONAMAQ (the National Council of Ayllus and Markas of Qullasuyu) draw ethnic boundaries within the Quechua- and Aymara-speaking population, resulting in a total of fifty indigenous peoples native to Bolivia. Many crops first domesticated by indigenous Americans are now produced and used globally. Chief among these is maize or ""corn"", arguably the most important crop in the world. Other significant crops include cassava, chia, squash (pumpkins, zucchini, marrow, acorn squash, butternut squash), the pinto bean, Phaseolus beans including most common beans, tepary beans and lima beans, tomatoes, potatoes, avocados, peanuts, cocoa beans (used to make chocolate), vanilla, strawberries, pineapples, Peppers (species and varieties of Capsicum, including bell peppers, jalapeños, paprika and chili peppers) sunflower seeds, rubber, brazilwood, chicle, tobacco, coca, manioc and some species of cotton. Human settlement of the New World occurred in stages from the Bering sea coast line, with an initial 15,000 to 20,000-year layover on Beringia for the small founding population. The micro-satellite diversity and distributions of the Y lineage specific to South America indicates that certain indigenous peoples of the Americas populations have been isolated since the initial colonization of the region. The Na-Dené, Inuit and Indigenous Alaskan populations exhibit haplogroup Q (Y-DNA) mutations, however are distinct from other indigenous peoples of the Americas with various mtDNA and atDNA mutations. This suggests that the earliest migrants into the northern extremes of North America and Greenland derived from later migrant populations. Many pre-Columbian civilizations established characteristics and hallmarks which included permanent or urban settlements, agriculture, civic and monumental architecture, and complex societal hierarchies. Some of these civilizations had long faded by the time of the first significant European and African arrivals (ca. late 15th–early 16th centuries), and are known only through oral history and through archaeological investigations. Others were contemporary with this period, and are also known from historical accounts of the time. A few, such as the Mayan, Olmec, Mixtec, and Nahua peoples, had their own written records. However, the European colonists of the time worked to eliminate non-Christian beliefs, and Christian pyres destroyed many pre-Columbian written records. Only a few documents remained hidden and survived, leaving contemporary historians with glimpses of ancient culture and knowledge. The South American highlands were a center of early agriculture. Genetic testing of the wide variety of cultivars and wild species suggests that the potato has a single origin in the area of southern Peru, from a species in the Solanum brevicaule complex. Over 99% of all modern cultivated potatoes worldwide are descendants of a subspecies indigenous to south-central Chile, Solanum tuberosum ssp. tuberosum, where it was cultivated as long as 10,000 years ago. According to George Raudzens, ""It is clear that in pre-Columbian times some groups struggled to survive and often suffered food shortages and famines, while others enjoyed a varied and substantial diet."" The persistent drought around 850 AD coincided with the collapse of Classic Maya civilization, and the famine of One Rabbit (AD 1454) was a major catastrophe in Mexico. In 2005, Argentina's indigenous population (known as pueblos originarios) numbered about 600,329 (1.6% of total population); this figure includes 457,363 people who self-identified as belonging to an indigenous ethnic group and 142,966 who identified themselves as first-generation descendants of an indigenous people. The ten most populous indigenous peoples are the Mapuche (113,680 people), the Kolla (70,505), the Toba (69,452), the Guaraní (68,454), the Wichi (40,036), the Diaguita-Calchaquí (31,753), the Mocoví (15,837), the Huarpe (14,633), the Comechingón (10,863) and the Tehuelche (10,590). Minor but important peoples are the Quechua (6,739), the Charrúa (4,511), the Pilagá (4,465), the Chané (4,376), and the Chorote (2,613). The Selknam (Ona) people are now virtually extinct in its pure form. The languages of the Diaguita, Tehuelche, and Selknam nations have become extinct or virtually extinct: the Cacán language (spoken by Diaguitas) in the 18th century and the Selknam language in the 20th century; one Tehuelche language (Southern Tehuelche) is still spoken by a handful of elderly people. Over the course of thousands of years, American indigenous peoples domesticated, bred and cultivated a large array of plant species. These species now constitute 50–60% of all crops in cultivation worldwide. In certain cases, the indigenous peoples developed entirely new species and strains through artificial selection, as was the case in the domestication and breeding of maize from wild teosinte grasses in the valleys of southern Mexico. Numerous such agricultural products retain their native names in the English and Spanish lexicons. Much of El Salvador was home to the Pipil, the Lenca, Xinca, and Kakawira. The Pipil lived in western El Salvador, spoke Nawat, and had many settlements there, most noticeably Cuzcatlan. The Pipil had no precious mineral resources, but they did have rich and fertile land that was good for farming. The Spaniards were disappointed not to find gold or jewels in El Salvador as they had in other lands like Guatemala or Mexico, but upon learning of the fertile land in El Salvador, they attempted to conquer it. Noted Meso-American indigenous warriors to rise militarily against the Spanish included Princes Atonal and Atlacatl of the Pipil people in central El Salvador and Princess Antu Silan Ulap of the Lenca people in eastern El Salvador, who saw the Spanish not as gods but as barbaric invaders. After fierce battles, the Pipil successfully fought off the Spanish army led by Pedro de Alvarado along with their Mexican Indian allies (the Tlaxcalas), sending them back to Guatemala. After many other attacks with an army reinforced with Guatemalan Indian allies, the Spanish were able to conquer Cuzcatlan. After further attacks, the Spanish also conquered the Lenca people. Eventually, the Spaniards intermarried with Pipil and Lenca women, resulting in the Mestizo population which would become the majority of the Salvadoran people. Today many Pipil and other indigenous populations live in the many small towns of El Salvador like Izalco, Panchimalco, Sacacoyo, and Nahuizalco. The specifics of Paleo-Indian migration to and throughout the Americas, including the exact dates and routes traveled, provide the subject of ongoing research and discussion. According to archaeological and genetic evidence, North and South America were the last continents in the world with human habitation. During the Wisconsin glaciation, 50–17,000 years ago, falling sea levels allowed people to move across the land bridge of Beringia that joined Siberia to north west North America (Alaska). Alaska was a glacial refugia because it had low snowfall, allowing a small population to exist. The Laurentide Ice Sheet covered most of North America, blocking nomadic inhabitants and confining them to Alaska (East Beringia) for thousands of years. Approximately 96.4% of Ecuador's Indigenous population are Highland Quichuas living in the valleys of the Sierra region. Primarily consisting of the descendents of Incans, they are Kichwa speakers and include the Caranqui, the Otavalos, the Cayambi, the Quitu-Caras, the Panzaleo, the Chimbuelo, the Salasacan, the Tugua, the Puruhá, the Cañari, and the Saraguro. Linguistic evidence suggests that the Salascan and the Saraguro may have been the descendants of Bolivian ethnic groups transplanted to Ecuador as mitimaes. The development of writing is counted among the many achievements and innovations of pre-Columbian American cultures. Independent from the development of writing in other areas of the world, the Mesoamerican region produced several indigenous writing systems beginning in the 1st millennium BCE. What may be the earliest-known example in the Americas of an extensive text thought to be writing is by the Cascajal Block. The Olmec hieroglyphs tablet has been indirectly dated from ceramic shards found in the same context to approximately 900 BCE, around the time that Olmec occupation of San Lorenzo Tenochtitlán began to wane. Although some indigenous peoples of the Americas were traditionally hunter-gatherers—and many, especially in Amazonia, still are—many groups practiced aquaculture and agriculture. The impact of their agricultural endowment to the world is a testament to their time and work in reshaping and cultivating the flora indigenous to the Americas. Although some societies depended heavily on agriculture, others practiced a mix of farming, hunting, and gathering. In some regions the indigenous peoples created monumental architecture, large-scale organized cities, chiefdoms, states, and empires. Visual arts by indigenous peoples of the Americas comprise a major category in the world art collection. Contributions include pottery, paintings, jewellery, weavings, sculptures, basketry, carvings, and beadwork. Because too many artists were posing as Native Americans and Alaska Natives in order to profit from the caché of Indigenous art in the United States, the U.S. passed the Indian Arts and Crafts Act of 1990, requiring artists to prove that they are enrolled in a state or federally recognized tribe. To support the ongoing practice of American Indian, Alaska Native and Native Hawaiian arts and cultures in the United States, the Ford Foundation, arts advocates and American Indian tribes created an endowment seed fund and established a national Native Arts and Cultures Foundation in 2007. Most Venezuelans have some indigenous heritage, but the indigenous population make up only around 2% of the total population. They speak around 29 different languages and many more dialects, but some of the ethnic groups are very small and their languages are in danger of becoming extinct in the next decades. The most important indigenous groups are the Ye'kuana, the Wayuu, the Pemon and the Warao. The most advanced native people to have lived in present-day Venezuela is thought to have been the Timoto-cuicas, who mainly lived in the Venezuelan Andes. In total it is estimated that there were between 350 thousand and 500 thousand inhabitants, the most densely populated area being the Andean region (Timoto-cuicas), thanks to the advanced agricultural techniques used. A route through Beringia is seen as more likely than the Solutrean hypothesis. Kashani et al. 2012 state that ""The similarities in ages and geographical distributions for C4c and the previously analyzed X2a lineage provide support to the scenario of a dual origin for Paleo-Indians. Taking into account that C4c is deeply rooted in the Asian portion of the mtDNA phylogeny and is indubitably of Asian origin, the finding that C4c and X2a are characterized by parallel genetic histories definitively dismisses the controversial hypothesis of an Atlantic glacial entry route into North America."" Contact with European diseases such as smallpox and measles killed between 50 and 67 per cent of the Aboriginal population of North America in the first hundred years after the arrival of Europeans. Some 90 per cent of the native population near Massachusetts Bay Colony died of smallpox in an epidemic in 1617–1619. In 1633, in Plymouth, the Native Americans there were exposed to smallpox because of contact with Europeans. As it had done elsewhere, the virus wiped out entire population groups of Native Americans. It reached Lake Ontario in 1636, and the lands of the Iroquois by 1679. During the 1770s, smallpox killed at least 30% of the West Coast Native Americans. The 1775–82 North American smallpox epidemic and 1837 Great Plains smallpox epidemic brought devastation and drastic population depletion among the Plains Indians. In 1832, the federal government of the United States established a smallpox vaccination program for Native Americans (The Indian Vaccination Act of 1832). Indigenous genetic studies suggest that the first inhabitants of the Americas share a single ancestral population, one that developed in isolation, conjectured to be Beringia. The isolation of these peoples in Beringia might have lasted 10–20,000 years. Around 16,500 years ago, the glaciers began melting, allowing people to move south and east into Canada and beyond. These people are believed to have followed herds of now-extinct Pleistocene megafauna along ice-free corridors that stretched between the Laurentide and Cordilleran Ice Sheets. Various theories for the decline of the Native American populations emphasize epidemic diseases, conflicts with Europeans, and conflicts among warring tribes. Scholars now believe that, among the various contributing factors, epidemic disease was the overwhelming cause of the population decline of the American natives. Some believe that after first contacts with Europeans and Africans, Old World diseases caused the death of 90 to 95% of the native population of the New World in the following 150 years. Smallpox killed up to one third of the native population of Hispaniola in 1518. By killing the Incan ruler Huayna Capac, smallpox caused the Inca Civil War. Smallpox was only the first epidemic. Typhus (probably) in 1546, influenza and smallpox together in 1558, smallpox again in 1589, diphtheria in 1614, measles in 1618—all ravaged the remains of Inca culture. According to the prevailing theories of the settlement of the Americas, migrations of humans from Asia (in particular North Asia) to the Americas took place via Beringia, a land bridge which connected the two continents across what is now the Bering Strait. The majority of experts agree that the earliest pre-modern human migration via Beringia took place at least 13,500 years ago, with disputed evidence that people had migrated into the Americas much earlier, up to 40,000 years ago. These early Paleo-Indians spread throughout the Americas, diversifying into many hundreds of culturally distinct nations and tribes. According to the oral histories of many of the indigenous peoples of the Americas, they have been living there since their genesis, described by a wide range of creation myths. According to the 2002 Census, 4.6% of the Chilean population, including the Rapanui (a Polynesian people) of Easter Island, was indigenous, although most show varying degrees of mixed heritage. Many are descendants of the Mapuche, and live in Santiago, Araucanía and the lake district. The Mapuche successfully fought off defeat in the first 300–350 years of Spanish rule during the Arauco War. Relations with the new Chilean Republic were good until the Chilean state decided to occupy their lands. During the Occupation of Araucanía the Mapuche surrendered to the country's army in the 1880s. Their land was opened to settlement by Chileans and Europeans. Conflict over Mapuche land rights continues to the present. The ""General Law of Linguistic Rights of the Indigenous Peoples"" grants all indigenous languages spoken in Mexico, regardless of the number of speakers, the same validity as Spanish in all territories in which they are spoken, and indigenous peoples are entitled to request some public services and documents in their native languages. Along with Spanish, the law has granted them — more than 60 languages — the status of ""national languages"". The law includes all indigenous languages of the Americas regardless of origin; that is, it includes the indigenous languages of ethnic groups non-native to the territory. As such the National Commission for the Development of Indigenous Peoples recognizes the language of the Kickapoo, who immigrated from the United States, and recognizes the languages of the Guatemalan indigenous refugees. The Mexican government has promoted and established bilingual primary and secondary education in some indigenous rural communities. Nonetheless, of the indigenous peoples in Mexico, only about 67% of them (or 5.4% of the country's population) speak an indigenous language and about a sixth do not speak Spanish (1.2% of the country's population). Natives of North America began practicing farming approximately 4,000 years ago, late in the Archaic period of North American cultures. Technology had advanced to the point that pottery was becoming common and the small-scale felling of trees had become feasible. Concurrently, the Archaic Indians began using fire in a controlled manner. Intentional burning of vegetation was used to mimic the effects of natural fires that tended to clear forest understories. It made travel easier and facilitated the growth of herbs and berry-producing plants, which were important for both food and medicines. A 2013 study in Nature reported that DNA found in the 24,000-year-old remains of a young boy from the archaeological Mal'ta-Buret' culture suggest that up to one-third of the indigenous Americans may have ancestry that can be traced back to western Eurasians, who may have ""had a more north-easterly distribution 24,000 years ago than commonly thought"". ""We estimate that 14 to 38 percent of Native American ancestry may originate through gene flow from this ancient population,"" the authors wrote. Professor Kelly Graf said, The European colonization of the Americas forever changed the lives and cultures of the peoples of the continents. Although the exact pre-contact population of the Americas is unknown, scholars estimate that Native American populations diminished by between 80 and 90% within the first centuries of contact with Europeans. The leading cause was disease. The continent was ravaged by epidemics of diseases such as smallpox, measles, and cholera, which were brought from Europe by the early explorers and spread quickly into new areas even before later explorers and colonists reached them. Native Americans suffered high mortality rates due to their lack of prior exposure to these diseases. The loss of lives was exacerbated by conflict between colonists and indigenous people. Colonists also frequently perpetrated massacres on the indigenous groups and enslaved them. According to the U.S. Bureau of the Census (1894), the North American Indian Wars of the 19th century cost the lives of about 19,000 whites and 30,000 Native Americans. Representatives from indigenous and rural organizations from major South American countries, including Bolivia, Ecuador, Colombia, Chile and Brazil, started a forum in support of Morales' legal process of change. The meeting condemned plans by the European ""foreign power elite"" to destabilize the country. The forum also expressed solidarity with the Morales and his economic and social changes in the interest of historically marginalized majorities. Furthermore, in a cathartic blow to the US-backed elite, it questioned US interference through diplomats and NGOs. The forum was suspicious of plots against Bolivia and other countries, including Cuba, Venezuela, Ecuador, Paraguay and Nicaragua. Cultural practices in the Americas seem to have been shared mostly within geographical zones where unrelated peoples adopted similar technologies and social organizations. An example of such a cultural area is Mesoamerica, where millennia of coexistence and shared development among the peoples of the region produced a fairly homogeneous culture with complex agricultural and social patterns. Another well-known example is the North American plains where until the 19th century several peoples shared the traits of nomadic hunter-gatherers based primarily on buffalo hunting. The data indicate that the individual was from a population directly ancestral to present South American and Central American Native American populations, and closely related to present North American Native American populations. The implication is that there was an early divergence between North American and Central American plus South American populations. Hypotheses which posit that invasions subsequent to the Clovis culture overwhelmed or assimilated previous migrants into the Americas were ruled out."
Endangered_Species_Act,"A species can be listed in two ways. The United States Fish and Wildlife Service (FWS) or NOAA Fisheries (also called the National Marine Fisheries Service) can directly list a species through its candidate assessment program, or an individual or organizational petition may request that the FWS or NMFS list a species. A ""species"" under the act can be a true taxonomic species, a subspecies, or in the case of vertebrates, a ""distinct population segment."" The procedures are the same for both types except with the person/organization petition, there is a 90-day screening period. Fish and Wildlife Service (FWS) and National Marine Fisheries Service (NMFS) are required to create an Endangered Species Recovery Plan outlining the goals, tasks required, likely costs, and estimated timeline to recover endangered species (i.e., increase their numbers and improve their management to the point where they can be removed from the endangered list). The ESA does not specify when a recovery plan must be completed. The FWS has a policy specifying completion within three years of the species being listed, but the average time to completion is approximately six years. The annual rate of recovery plan completion increased steadily from the Ford administration (4) through Carter (9), Reagan (30), Bush I (44), and Clinton (72), but declined under Bush II (16 per year as of 9/1/06). Section 6 of the Endangered Species Act provided funding for development of programs for management of threatened and endangered species by state wildlife agencies. Subsequently, lists of endangered and threatened species within their boundaries have been prepared by each state. These state lists often include species which are considered endangered or threatened within a specific state but not within all states, and which therefore are not included on the national list of endangered and threatened species. Examples include Florida, Minnesota, Maine, and California. Opponents of the Endangered Species Act argue that with over 2,000 endangered species listed, and only 28 delisted due to recovery, the success rate of 1% over nearly three decades proves that there needs to be serious reform in their methods to actually help the endangered animals and plants. Others argue that the ESA may encourage preemptive habitat destruction by landowners who fear losing the use of their land because of the presence of an endangered species; known colloquially as ""Shoot, Shovel and Shut-Up."" One example of such perverse incentives is the case of a forest owner who, in response to ESA listing of the red-cockaded woodpecker, increased harvesting and shortened the age at which he harvests his trees to ensure that they do not become old enough to become suitable habitat. While no studies have shown that the Act's negative effects, in total, exceed the positive effects, many economists believe that finding a way to reduce such perverse incentives would lead to more effective protection of endangered species. One species in particular received widespread attention—the whooping crane. The species' historical range extended from central Canada South to Mexico, and from Utah to the Atlantic coast. Unregulated hunting and habitat loss contributed to a steady decline in the whooping crane population until, by 1890, it had disappeared from its primary breeding range in the north central United States. It would be another eight years before the first national law regulating wildlife commerce was signed, and another two years before the first version of the endangered species act was passed. The whooping crane population by 1941 was estimated at about only 16 birds still in the wild. The ""Safe Harbor"" agreement is a voluntary agreement between the private landowner and FWS. The landowner agrees to alter the property to benefit or even attract a listed or proposed species in exchange for assurances that the FWS will permit future ""takes"" above a pre-determined level. The policy relies on the ""enhancement of survival"" provision of Section §1539(a)(1)(A). A landowner can have either a ""Safe Harbor"" agreement or an Incidental Take Permit, or both. The policy was developed by the Clinton Administration in 1999. Green and the CPI further noted another exploit of the ESA in their discussion of the critically endangered cotton-top tamarin (Saguinus oedipus). Not only had they found documentation that 151 of these primates had inadvertently made their way from the Harvard-affiliated New England Regional Primate Research Center into the exotic pet trade through the aforementioned loophole, but in October 1976, over 800 cotton-top tamarins were imported into the United States in order to beat the official listing of the species under the ESA. Long before the exemption is considered by the Endangered Species Committee, the Forest Service, and either the FWS or the NMFS will have consulted on the biological implications of the timber harvest. The consultation can be informal, to determine if harm may occur; and then formal if the harm is believed to be likely. The questions to be answered in these consultations are whether the species will be harmed, whether the habitat will be harmed and if the action will aid or hinder the recovery of the listed species. More than half of habitat for listed species is on non-federal property, owned by citizens, states, local governments, tribal governments and private organizations. Before the law was amended in 1982, a listed species could be taken only for scientific or research purposes. The amendment created a permit process to circumvent the take prohibition called a Habitat Conservation Plan or HCP to give incentives to non-federal land managers and private landowners to help protect listed and unlisted species, while allowing economic development that may harm (""take"") the species. A reward will be paid to any person who furnishes information which leads to an arrest, conviction, or revocation of a license, so long as they are not a local, state, or federal employee in the performance of official duties. The Secretary may also provide reasonable and necessary costs incurred for the care of fish, wildlife, and forest service or plant pending the violation caused by the criminal. If the balance ever exceeds $500,000 the Secretary of the Treasury is required to deposit an amount equal to the excess into the cooperative endangered species conservation fund. The ESA requires that critical habitat be designated at the time of or within one year of a species being placed on the endangered list. In practice, most designations occur several years after listing. Between 1978 and 1986 the FWS regularly designated critical habitat. In 1986 the Reagan Administration issued a regulation limiting the protective status of critical habitat. As a result, few critical habitats were designated between 1986 and the late 1990s. In the late 1990s and early 2000s, a series of court orders invalidated the Reagan regulations and forced the FWS and NMFS to designate several hundred critical habitats, especially in Hawaii, California and other western states. Midwest and Eastern states received less critical habitat, primarily on rivers and coastlines. As of December, 2006, the Reagan regulation has not yet been replaced though its use has been suspended. Nonetheless, the agencies have generally changed course and since about 2005 have tried to designate critical habitat at or near the time of listing. The person or organization submits a HCP and if approved by the agency (FWS or NMFS), will be issued an Incidental Take Permit (ITP) which allows a certain number of ""takes"" of the listed species. The permit may be revoked at any time and can allow incidental takes for varying amounts of time. For instance, the San Bruno Habitat Conservation Plan/ Incidental Take Permit is good for 30 years and the Wal-Mart store (in Florida) permit expires after one year. Because the permit is issued by a federal agency to a private party, it is a federal action-which means other federal laws can apply, such as the National Environmental Policy Act or NEPA. A notice of the permit application action is published in the Federal Register and a public comment period of 30 to 90 days begins. The Candidate Conservation Agreement is closely related to the ""Safe Harbor"" agreement, the main difference is that the Candidate Conservation Agreements With Assurances(CCA) are meant to protect unlisted species by providing incentives to private landowners and land managing agencies to restore, enhance or maintain habitat of unlisted species which are declining and have the potential to become threatened or endangered if critical habitat is not protected. The FWS will then assure that if, in the future the unlisted species becomes listed, the landowner will not be required to do more than already agreed upon in the CCA. This first list is referred to as the ""Class of '67"" in The Endangered Species Act at Thirty, Volume 1, which concludes that habitat destruction, the biggest threat to those 78 species, is still the same threat to the currently listed species. It included only vertebrates because the Department of Interior's definition of ""fish and wildlife"" was limited to vertebrates. However, with time, researchers noticed that the animals on the endangered species list still were not getting enough protection, thus further threatening their extinction. The endangered species program was expanded by the Endangered Species Act of 1969. The Lacey Act of 1900 was the first federal law that regulated commercial animal markets. It prohibited interstate commerce of animals killed in violation of state game laws, and covered all fish and wildlife and their parts or products, as well as plants. Other legislation followed, including the Migratory Bird Conservation Act of 1929, a 1937 treaty prohibiting the hunting of right and gray whales, and the Bald Eagle Protection Act of 1940. These later laws had a low cost to society–the species were relatively rare–and little opposition was raised. According to research published in 1999 by Alan Green and the Center for Public Integrity (CPI), loopholes in the ESA are commonly exploited in the exotic pet trade. Although the legislation prohibits interstate and foreign transactions for list species, no provisions are made for in-state commerce, allowing these animals to be sold to roadside zoos and private collectors. Additionally, the ESA allows listed species to be shipped across state lines as long as they are not sold. According to Green and the CPI, this allows dealers to ""donate"" listed species through supposed ""breeding loans"" to anyone, and in return they can legally receive a reciprocal monetary ""donation"" from the receiving party. Furthermore, an interview with an endangered species specialist at the US Fish and Wildlife Service revealed that the agency does not have sufficient staff to perform undercover investigations, which would catch these false ""donations"" and other mislabeled transactions. The provision of the law in Section 4 that establishes critical habitat is a regulatory link between habitat protection and recovery goals, requiring the identification and protection of all lands, water and air necessary to recover endangered species. To determine what exactly is critical habitat, the needs of open space for individual and population growth, food, water, light or other nutritional requirements, breeding sites, seed germination and dispersal needs, and lack of disturbances are considered. The Endangered Species Conservation Act (P. L. 91-135), passed in December, 1969, amended the original law to provide additional protection to species in danger of ""worldwide extinction"" by prohibiting their importation and subsequent sale in the United States. It expanded the Lacey Act's ban on interstate commerce to include mammals, reptiles, amphibians, mollusks and crustaceans. Reptiles were added mainly to reduce the rampant poaching of alligators and crocodiles. This law was the first time that invertebrates were included for protection. Two examples of animal species recently delisted are: the Virginia northern flying squirrel (subspecies) on August, 2008, which had been listed since 1985, and the gray wolf (Northern Rocky Mountain DPS). On April 15, 2011, President Obama signed the Department of Defense and Full-Year Appropriations Act of 2011. A section of that Appropriations Act directed the Secretary of the Interior to reissue within 60 days of enactment the final rule published on April 2, 2009, that identified the Northern Rocky Mountain population of gray wolf (Canis lupus) as a distinct population segment (DPS) and to revise the List of Endangered and Threatened Wildlife by removing most of the gray wolves in the DPS. Growing scientific recognition of the role of private lands for endangered species recovery and the landmark 1981 court decision in Palila v. Hawaii Department of Land and Natural Resources both contributed to making Habitat Conservation Plans/ Incidental Take Permits ""a major force for wildlife conservation and a major headache to the development community"", wrote Robert D.Thornton in the 1991 Environmental Law article, Searching for Consensus and Predictability: Habitat Conservation Planning under the Endangered Species Act of 1973. The question to be answered is whether a listed species will be harmed by the action and, if so, how the harm can be minimized. If harm cannot be avoided, the project agency can seek an exemption from the Endangered Species Committee, an ad hoc panel composed of members from the executive branch and at least one appointee from the state where the project is to occur. Five of the seven committee members must vote for the exemption to allow taking (to harass, harm, pursue, hunt, shoot, wound, kill, trap, capture, or collect, or significant habitat modification, or to attempt to engage in any such conduct) of listed species. It authorized the Secretary of the Interior to list endangered domestic fish and wildlife and allowed the United States Fish and Wildlife Service to spend up to $15 million per year to buy habitats for listed species. It also directed federal land agencies to preserve habitat on their lands. The Act also consolidated and even expanded authority for the Secretary of the Interior to manage and administer the National Wildlife Refuge System. Other public agencies were encouraged, but not required, to protect species. The act did not address the commerce in endangered species and parts. The US Congress was urged to create the exemption by proponents of a conservation plan on San Bruno Mountain, California that was drafted in the early 1980s and is the first HCP in the nation. In the conference report on the 1982 amendments, Congress specified that it intended the San Bruno plan to act ""as a model"" for future conservation plans developed under the incidental take exemption provision and that ""the adequacy of similar conservation plans should be measured against the San Bruno plan"". Congress further noted that the San Bruno plan was based on ""an independent exhaustive biological study"" and protected at least 87% of the habitat of the listed butterflies that led to the development of the HCP. The ""No Surprises"" rule is meant to protect the landowner if ""unforeseen circumstances"" occur which make the landowner's efforts to prevent or mitigate harm to the species fall short. The ""No Surprises"" policy may be the most controversial of the recent reforms of the law, because once an Incidental Take Permit is granted, the Fish and Wildlife Service (FWS) loses much ability to further protect a species if the mitigation measures by the landowner prove insufficient. The landowner or permittee would not be required to set aside additional land or pay more in conservation money. The federal government would have to pay for additional protection measures. President Richard Nixon declared current species conservation efforts to be inadequate and called on the 93rd United States Congress to pass comprehensive endangered species legislation. Congress responded with a completely rewritten law, the Endangered Species Act of 1973 which was signed by Nixon on December 28, 1973 (Pub.L. 93–205). It was written by a team of lawyers and scientists, including the first appointed head of the Council on Environmental Quality (CEQ),an outgrowth of NEPA (The ""National Environmental Policy Act of 1969"") Dr. Russell E. Train. Dr. Train was assisted by a core group of staffers, including Dr. Earl Baysinger at EPA (currently Assistant Chief, Office of Endangered Species and International. Activities), Dick Gutting (U.S. Commerce Dept. lawyer, currently joined NOAA the previous year (1972), and Dr. Gerard A. ""Jerry"" Bertrand, a marine biologist (Ph.D, Oregon State University) by training, who had transferred from his post as the Scientific Adviser to the U.S Army Corps of Engineers, office of the Commandant of the Corp. to join the newly formed White House office. The staff, under Dr. Train's leadership, incorporated dozens of new principles and ideas into the landmark legislation; crafting a document that completely changed the direction of environmental conservation in the United States. Dr. Bertrand is credited with writing the most challenged section of the Act, the ""takings"" clause - Section 2. All federal agencies are prohibited from authorizing, funding or carrying out actions that ""destroy or adversely modify"" critical habitats (Section 7(a) (2)). While the regulatory aspect of critical habitat does not apply directly to private and other non-federal landowners, large-scale development, logging and mining projects on private and state land typically require a federal permit and thus become subject to critical habitat regulations. Outside or in parallel with regulatory processes, critical habitats also focus and encourage voluntary actions such as land purchases, grant making, restoration, and establishment of reserves. There have been six instances as of 2009 in which the exemption process was initiated. Of these six, one was granted, one was partially granted, one was denied and three were withdrawn. Donald Baur, in The Endangered Species Act: law, policy, and perspectives, concluded,"" ... the exemption provision is basically a nonfactor in the administration of the ESA. A major reason, of course, is that so few consultations result in jeopardy opinions, and those that do almost always result in the identification of reasonable and prudent alternatives to avoid jeopardy."" Public notice is given through legal notices in newspapers, and communicated to state and county agencies within the species' area. Foreign nations may also receive notice of a listing. A public hearing is mandatory if any person has requested one within 45 days of the published notice. ""The purpose of the notice and comment requirement is to provide for meaningful public participation in the rulemaking process."" summarized the Ninth Circuit court in the case of Idaho Farm Bureau Federation v. Babbitt. The Endangered Species Act of 1973 (ESA; 16 U.S.C. § 1531 et seq.) is one of the few dozens of United States environmental laws passed in the 1970s, and serves as the enacting legislation to carry out the provisions outlined in The Convention on International Trade in Endangered Species of Wild Fauna and Flora (CITES). The ESA was signed into law by President Richard Nixon on December 28, 1973, it was designed to protect critically imperiled species from extinction as a ""consequence of economic growth and development untempered by adequate concern and conservation."" The U.S. Supreme Court found that ""the plain intent of Congress in enacting"" the ESA ""was to halt and reverse the trend toward species extinction, whatever the cost."" The Act is administered by two federal agencies, the United States Fish and Wildlife Service (FWS) and the National Oceanic and Atmospheric Administration (NOAA). During the listing process, economic factors cannot be considered, but must be "" based solely on the best scientific and commercial data available."" The 1982 amendment to the ESA added the word ""solely"" to prevent any consideration other than the biological status of the species. Congress rejected President Ronald Reagan's Executive Order 12291 which required economic analysis of all government agency actions. The House committee's statement was ""that economic considerations have no relevance to determinations regarding the status of species."" As of September 2012, fifty-six species have been delisted; twenty-eight due to recovery, ten due to extinction (seven of which are believed to have been extinct prior to being listed), ten due to changes in taxonomic classification practices, six due to discovery of new populations, one due to an error in the listing rule, and one due to an amendment to the Endangered Species Act specifically requiring the species delisting. Twenty-five others have been down listed from ""endangered"" to ""threatened"" status."
Central_Intelligence_Agency,"Unlike the Federal Bureau of Investigation (FBI), which is a domestic security service, CIA has no law enforcement function and is mainly focused on overseas intelligence gathering, with only limited domestic collection. Though it is not the only U.S. government agency specializing in HUMINT, CIA serves as the national manager for coordination and deconfliction of HUMINT activities across the entire intelligence community. Moreover, CIA is the only agency authorized by law to carry out and oversee covert action on behalf of the President, unless the President determines that another agency is better suited for carrying out such action. It can, for example, exert foreign political influence through its tactical divisions, such as the Special Activities Division. Details of the overall United States intelligence budget are classified. Under the Central Intelligence Agency Act of 1949, the Director of Central Intelligence is the only federal government employee who can spend ""un-vouchered"" government money. The government has disclosed a total figure for all non-military intelligence spending since 2007; the fiscal 2013 figure is $52.6 billion. According to the 2013 mass surveillance disclosures, the CIA's fiscal 2013 budget is $14.7 billion, 28% of the total and almost 50% more than the budget of the National Security Agency. CIA's HUMINT budget is $2.3 billion, the SIGINT budget is $1.7 billion, and spending for security and logistics of CIA missions is $2.5 billion. ""Covert action programs"", including a variety of activities such as the CIA's drone fleet and anti-Iranian nuclear program activities, accounts for $2.6 billion. There were numerous previous attempts to obtain general information about the budget. As a result, it was revealed that CIA's annual budget in Fiscal Year 1963 was US $550 million (inflation-adjusted US$ 4.3 billion in 2016), and the overall intelligence budget in FY 1997 was US $26.6 billion (inflation-adjusted US$ 39.2 billion in 2016). There have been accidental disclosures; for instance, Mary Margaret Graham, a former CIA official and deputy director of national intelligence for collection in 2005, said that the annual intelligence budget was $44 billion, and in 1994 Congress accidentally published a budget of $43.4 billion (in 2012 dollars) in 1994 for the non-military National Intelligence Program, including $4.8 billion for the CIA. After the Marshall Plan was approved, appropriating $13.7 billion over five years, 5% of those funds or $685 million were made available to the CIA. US army general Hoyt Vandenberg, the CIG's second director, created the Office of Special Operations (OSO), as well as the Office of Reports and Estimates (ORE). Initially the OSO was tasked with spying and subversion overseas with a budget of $15 million, the largesse of a small number of patrons in congress. Vandenberg's goals were much like the ones set out by his predecessor; finding out ""everything about the Soviet forces in Eastern and Central Europe - their movements, their capabilities, and their intentions."" This task fell to the 228 overseas personnel covering Germany, Austria, Switzerland, Poland, Czechoslovakia, and Hungary. The Executive Office also supports the U.S. military by providing it with information it gathers, receiving information from military intelligence organizations, and cooperating on field activities. The Executive Director is in charge of the day to day operation of the CIA, and each branch of the service has its own Director. The Associate Director of military affairs, a senior military officer, manages the relationship between the CIA and the Unified Combatant Commands, who produce regional/operational intelligence and consume national intelligence. The early track record of the CIA was poor, with the agency unable to provide sufficient intelligence about the Soviet takeovers of Romania and Czechoslovakia, the Soviet blockade of Berlin, and the Soviet atomic bomb project. In particular, the agency failed to predict the Chinese entry into the Korean War with 300,000 troops. The famous double agent Kim Philby was the British liaison to American Central Intelligence. Through him the CIA coordinated hundreds of airdrops inside the iron curtain, all compromised by Philby. Arlington Hall, the nerve center of CIA cryptanalysisl was compromised by Bill Weisband, a Russian translator and Soviet spy. The CIA would reuse the tactic of dropping plant agents behind enemy lines by parachute again on China, and North Korea. This too would be fruitless. The Directorate of Analysis produces all-source intelligence investigation on key foreign and intercontinental issues relating to powerful and sometimes anti-government sensitive topics. It has four regional analytic groups, six groups for transnational issues, and three focus on policy, collection, and staff support. There is an office dedicated to Iraq, and regional analytical Offices covering the Near Eastern and South Asian Analysis, the Office of Russian and European Analysis, and the Office of Asian Pacific, Asian Pacific, Latin American, and African Analysis and African Analysis. The CIA established its first training facility, the Office of Training and Education, in 1950. Following the end of the Cold War, the CIA's training budget was slashed, which had a negative effect on employee retention. In response, Director of Central Intelligence George Tenet established CIA University in 2002. CIA University holds between 200 and 300 courses each year, training both new hires and experienced intelligence officers, as well as CIA support staff. The facility works in partnership with the National Intelligence University, and includes the Sherman Kent School for Intelligence Analysis, the Directorate of Analysis' component of the university. The CIA had different demands placed on it by the different bodies overseeing it. Truman wanted a centralized group to organize the information that reached him, the Department of Defense wanted military intelligence and covert action, and the State Department wanted to create global political change favorable to the US. Thus the two areas of responsibility for the CIA were covert action and covert intelligence. One of the main targets for intelligence gathering was the Soviet Union, which had also been a priority of the CIA's predecessors. The success of the British Commandos during World War II prompted U.S. President Franklin D. Roosevelt to authorize the creation of an intelligence service modeled after the British Secret Intelligence Service (MI6), and Special Operations Executive. This led to the creation of the Office of Strategic Services (OSS). On September 20, 1945, shortly after the end of World War II, Harry S. Truman signed an executive order dissolving the OSS, and by October 1945 its functions had been divided between the Departments of State and War. The division lasted only a few months. The first public mention of the ""Central Intelligence Agency"" appeared on a command-restructuring proposal presented by Jim Forrestal and Arthur Radford to the U.S. Senate Military Affairs Committee at the end of 1945. Despite opposition from the military establishment, the United States Department of State and the Federal Bureau of Investigation (FBI), Truman established the National Intelligence Authority in January 1946, which was the direct predecessor of the CIA. Its operational extension was known as the Central Intelligence Group (CIG) On 18 June 1948, the National Security Council issued Directive 10/2 calling for covert action against the USSR, and granting the authority to carry out covert operations against ""hostile foreign states or groups"" that could, if needed, be denied by the U.S. government. To this end, the Office of Policy Coordination was created inside the new CIA. The OPC was quite unique; Frank Wisner, the head of the OPC, answered not to the CIA Director, but to the secretaries of defense, state, and the NSC, and the OPC's actions were a secret even from the head of the CIA. Most CIA stations had two station chiefs, one working for the OSO, and one working for the OPC. The Directorate of Operations is responsible for collecting foreign intelligence, mainly from clandestine HUMINT sources, and covert action. The name reflects its role as the coordinator of human intelligence activities among other elements of the wider U.S. intelligence community with their own HUMINT operations. This Directorate was created in an attempt to end years of rivalry over influence, philosophy and budget between the United States Department of Defense (DOD) and the CIA. In spite of this, the Department of Defense recently organized its own global clandestine intelligence service, the Defense Clandestine Service (DCS), under the Defense Intelligence Agency (DIA). Lawrence Houston, head counsel of the SSU, CIG, and, later CIA, was a principle draftsman of the National Security Act of 1947 which dissolved the NIA and the CIG, and established both the National Security Council and the Central Intelligence Agency. In 1949, Houston would help draft the Central Intelligence Agency Act, (Public law 81-110) which authorized the agency to use confidential fiscal and administrative procedures, and exempted it from most limitations on the use of Federal funds. It also exempted the CIA from having to disclose its ""organization, functions, officials, titles, salaries, or numbers of personnel employed."" It created the program ""PL-110"", to handle defectors and other ""essential aliens"" who fell outside normal immigration procedures. The closest links of the U.S. IC to other foreign intelligence agencies are to Anglophone countries: Australia, Canada, New Zealand, and the United Kingdom. There is a special communications marking that signals that intelligence-related messages can be shared with these four countries. An indication of the United States' close operational cooperation is the creation of a new message distribution label within the main U.S. military communications network. Previously, the marking of NOFORN (i.e., No Foreign Nationals) required the originator to specify which, if any, non-U.S. countries could receive the information. A new handling caveat, USA/AUS/CAN/GBR/NZL Five Eyes, used primarily on intelligence messages, gives an easier way to indicate that the material can be shared with Australia, Canada, United Kingdom, and New Zealand. At the outset of the Korean War the CIA still only had a few thousand employees, a thousand of whom worked in analysis. Intelligence primarily came from the Office of Reports and Estimates, which drew its reports from a daily take of State Department telegrams, military dispatches, and other public documents. The CIA still lacked its own intelligence gathering abilities. On 21 August 1950, shortly after the invasion of South Korea, Truman announced Walter Bedell Smith as the new Director of the CIA to correct what was seen as a grave failure of Intelligence.[clarification needed] The role and functions of the CIA are roughly equivalent to those of the United Kingdom's Secret Intelligence Service (the SIS or MI6), the Australian Secret Intelligence Service (ASIS), the Egyptian General Intelligence Service, the Russian Foreign Intelligence Service (Sluzhba Vneshney Razvedki) (SVR), the Indian Research and Analysis Wing (RAW), the Pakistani Inter-Services Intelligence (ISI), the French foreign intelligence service Direction Générale de la Sécurité Extérieure (DGSE) and Israel's Mossad. While the preceding agencies both collect and analyze information, some like the U.S. State Department's Bureau of Intelligence and Research are purely analytical agencies.[citation needed]"
British_Empire,"At the end of the 16th century, England and the Netherlands began to challenge Portugal's monopoly of trade with Asia, forming private joint-stock companies to finance the voyages—the English, later British, East India Company and the Dutch East India Company, chartered in 1600 and 1602 respectively. The primary aim of these companies was to tap into the lucrative spice trade, an effort focused mainly on two regions; the East Indies archipelago, and an important hub in the trade network, India. There, they competed for trade supremacy with Portugal and with each other. Although England ultimately eclipsed the Netherlands as a colonial power, in the short term the Netherlands' more advanced financial system and the three Anglo-Dutch Wars of the 17th century left it with a stronger position in Asia. Hostilities ceased after the Glorious Revolution of 1688 when the Dutch William of Orange ascended the English throne, bringing peace between the Netherlands and England. A deal between the two nations left the spice trade of the East Indies archipelago to the Netherlands and the textiles industry of India to England, but textiles soon overtook spices in terms of profitability, and by 1720, in terms of sales, the British company had overtaken the Dutch. The British and French struggles in India became but one theatre of the global Seven Years' War (1756–1763) involving France, Britain and the other major European powers. The signing of the Treaty of Paris (1763) had important consequences for the future of the British Empire. In North America, France's future as a colonial power there was effectively ended with the recognition of British claims to Rupert's Land, and the ceding of New France to Britain (leaving a sizeable French-speaking population under British control) and Louisiana to Spain. Spain ceded Florida to Britain. Along with its victory over France in India, the Seven Years' War therefore left Britain as the world's most powerful maritime power. Since 1718, transportation to the American colonies had been a penalty for various criminal offences in Britain, with approximately one thousand convicts transported per year across the Atlantic. Forced to find an alternative location after the loss of the 13 Colonies in 1783, the British government turned to the newly discovered lands of Australia. The western coast of Australia had been discovered for Europeans by the Dutch explorer Willem Jansz in 1606 and was later named New Holland by the Dutch East India Company, but there was no attempt to colonise it. In 1770 James Cook discovered the eastern coast of Australia while on a scientific voyage to the South Pacific Ocean, claimed the continent for Britain, and named it New South Wales. In 1778, Joseph Banks, Cook's botanist on the voyage, presented evidence to the government on the suitability of Botany Bay for the establishment of a penal settlement, and in 1787 the first shipment of convicts set sail, arriving in 1788. Britain continued to transport convicts to New South Wales until 1840. The Australian colonies became profitable exporters of wool and gold, mainly because of gold rushes in the colony of Victoria, making its capital Melbourne the richest city in the world and the largest city after London in the British Empire. From its base in India, the Company had also been engaged in an increasingly profitable opium export trade to China since the 1730s. This trade, illegal since it was outlawed by the Qing dynasty in 1729, helped reverse the trade imbalances resulting from the British imports of tea, which saw large outflows of silver from Britain to China. In 1839, the confiscation by the Chinese authorities at Canton of 20,000 chests of opium led Britain to attack China in the First Opium War, and resulted in the seizure by Britain of Hong Kong Island, at that time a minor settlement. By the start of the 20th century, Germany and the United States challenged Britain's economic lead. Subsequent military and economic tensions between Britain and Germany were major causes of the First World War, during which Britain relied heavily upon its empire. The conflict placed enormous strain on the military, financial and manpower resources of Britain. Although the British Empire achieved its largest territorial extent immediately after World War I, Britain was no longer the world's pre-eminent industrial or military power. In the Second World War, Britain's colonies in South-East Asia were occupied by Imperial Japan. Despite the final victory of Britain and its allies, the damage to British prestige helped to accelerate the decline of the empire. India, Britain's most valuable and populous possession, achieved independence as part of a larger decolonisation movement in which Britain granted independence to most territories of the Empire. The transfer of Hong Kong to China in 1997 marked for many the end of the British Empire. Fourteen overseas territories remain under British sovereignty. After independence, many former British colonies joined the Commonwealth of Nations, a free association of independent states. The United Kingdom is now one of 16 Commonwealth nations, a grouping known informally as the Commonwealth realms, that share one monarch—Queen Elizabeth II. Between 1815 and 1914, a period referred to as Britain's ""imperial century"" by some historians, around 10,000,000 square miles (26,000,000 km2) of territory and roughly 400 million people were added to the British Empire. Victory over Napoleon left Britain without any serious international rival, other than Russia in central Asia. Unchallenged at sea, Britain adopted the role of global policeman, a state of affairs later known as the Pax Britannica, and a foreign policy of ""splendid isolation"". Alongside the formal control it exerted over its own colonies, Britain's dominant position in world trade meant that it effectively controlled the economies of many countries, such as China, Argentina and Siam, which has been characterised by some historians as ""Informal Empire"". A similar struggle began in India when the Government of India Act 1919 failed to satisfy demand for independence. Concerns over communist and foreign plots following the Ghadar Conspiracy ensured that war-time strictures were renewed by the Rowlatt Acts. This led to tension, particularly in the Punjab region, where repressive measures culminated in the Amritsar Massacre. In Britain public opinion was divided over the morality of the event, between those who saw it as having saved India from anarchy, and those who viewed it with revulsion. The subsequent Non-Co-Operation movement was called off in March 1922 following the Chauri Chaura incident, and discontent continued to simmer for the next 25 years. Under the terms of the concluding Treaty of Versailles signed in 1919, the empire reached its greatest extent with the addition of 1,800,000 square miles (4,700,000 km2) and 13 million new subjects. The colonies of Germany and the Ottoman Empire were distributed to the Allied powers as League of Nations mandates. Britain gained control of Palestine, Transjordan, Iraq, parts of Cameroon and Togo, and Tanganyika. The Dominions themselves also acquired mandates of their own: the Union of South Africa gained South-West Africa (modern-day Namibia), Australia gained German New Guinea, and New Zealand Western Samoa. Nauru was made a combined mandate of Britain and the two Pacific Dominions. Following the defeat of Japan in the Second World War, anti-Japanese resistance movements in Malaya turned their attention towards the British, who had moved to quickly retake control of the colony, valuing it as a source of rubber and tin. The fact that the guerrillas were primarily Malayan-Chinese Communists meant that the British attempt to quell the uprising was supported by the Muslim Malay majority, on the understanding that once the insurgency had been quelled, independence would be granted. The Malayan Emergency, as it was called, began in 1948 and lasted until 1960, but by 1957, Britain felt confident enough to grant independence to the Federation of Malaya within the Commonwealth. In 1963, the 11 states of the federation together with Singapore, Sarawak and North Borneo joined to form Malaysia, but in 1965 Chinese-majority Singapore was expelled from the union following tensions between the Malay and Chinese populations. Brunei, which had been a British protectorate since 1888, declined to join the union and maintained its status until independence in 1984. Britain's fears of war with Germany were realised in 1914 with the outbreak of the First World War. Britain quickly invaded and occupied most of Germany's overseas colonies in Africa. In the Pacific, Australia and New Zealand occupied German New Guinea and Samoa respectively. Plans for a post-war division of the Ottoman Empire, which had joined the war on Germany's side, were secretly drawn up by Britain and France under the 1916 Sykes–Picot Agreement. This agreement was not divulged to the Sharif of Mecca, who the British had been encouraging to launch an Arab revolt against their Ottoman rulers, giving the impression that Britain was supporting the creation of an independent Arab state. In 1695, the Scottish Parliament granted a charter to the Company of Scotland, which established a settlement in 1698 on the isthmus of Panama. Besieged by neighbouring Spanish colonists of New Granada, and afflicted by malaria, the colony was abandoned two years later. The Darien scheme was a financial disaster for Scotland—a quarter of Scottish capital was lost in the enterprise—and ended Scottish hopes of establishing its own overseas empire. The episode also had major political consequences, persuading the governments of both England and Scotland of the merits of a union of countries, rather than just crowns. This occurred in 1707 with the Treaty of Union, establishing the Kingdom of Great Britain. In 1919, the frustrations caused by delays to Irish home rule led members of Sinn Féin, a pro-independence party that had won a majority of the Irish seats at Westminster in the 1918 British general election, to establish an Irish assembly in Dublin, at which Irish independence was declared. The Irish Republican Army simultaneously began a guerrilla war against the British administration. The Anglo-Irish War ended in 1921 with a stalemate and the signing of the Anglo-Irish Treaty, creating the Irish Free State, a Dominion within the British Empire, with effective internal independence but still constitutionally linked with the British Crown. Northern Ireland, consisting of six of the 32 Irish counties which had been established as a devolved region under the 1920 Government of Ireland Act, immediately exercised its option under the treaty to retain its existing status within the United Kingdom. During the late 18th and early 19th centuries the British Crown began to assume an increasingly large role in the affairs of the Company. A series of Acts of Parliament were passed, including the Regulating Act of 1773, Pitt's India Act of 1784 and the Charter Act of 1813 which regulated the Company's affairs and established the sovereignty of the Crown over the territories that it had acquired. The Company's eventual end was precipitated by the Indian Rebellion, a conflict that had begun with the mutiny of sepoys, Indian troops under British officers and discipline. The rebellion took six months to suppress, with heavy loss of life on both sides. The following year the British government dissolved the Company and assumed direct control over India through the Government of India Act 1858, establishing the British Raj, where an appointed governor-general administered India and Queen Victoria was crowned the Empress of India. India became the empire's most valuable possession, ""the Jewel in the Crown"", and was the most important source of Britain's strength. Political boundaries drawn by the British did not always reflect homogeneous ethnicities or religions, contributing to conflicts in formerly colonised areas. The British Empire was also responsible for large migrations of peoples. Millions left the British Isles, with the founding settler populations of the United States, Canada, Australia and New Zealand coming mainly from Britain and Ireland. Tensions remain between the white settler populations of these countries and their indigenous minorities, and between white settler minorities and indigenous majorities in South Africa and Zimbabwe. Settlers in Ireland from Great Britain have left their mark in the form of divided nationalist and unionist communities in Northern Ireland. Millions of people moved to and from British colonies, with large numbers of Indians emigrating to other parts of the empire, such as Malaysia and Fiji, and Chinese people to Malaysia, Singapore and the Caribbean. The demographics of Britain itself was changed after the Second World War owing to immigration to Britain from its former colonies. The changing world order that the war had brought about, in particular the growth of the United States and Japan as naval powers, and the rise of independence movements in India and Ireland, caused a major reassessment of British imperial policy. Forced to choose between alignment with the United States or Japan, Britain opted not to renew its Japanese alliance and instead signed the 1922 Washington Naval Treaty, where Britain accepted naval parity with the United States. This decision was the source of much debate in Britain during the 1930s as militaristic governments took hold in Japan and Germany helped in part by the Great Depression, for it was feared that the empire could not survive a simultaneous attack by both nations. Although the issue of the empire's security was a serious concern in Britain, at the same time the empire was vital to the British economy. Most of the UK's Caribbean territories achieved independence after the departure in 1961 and 1962 of Jamaica and Trinidad from the West Indies Federation, established in 1958 in an attempt to unite the British Caribbean colonies under one government, but which collapsed following the loss of its two largest members. Barbados achieved independence in 1966 and the remainder of the eastern Caribbean islands in the 1970s and 1980s, but Anguilla and the Turks and Caicos Islands opted to revert to British rule after they had already started on the path to independence. The British Virgin Islands, Cayman Islands and Montserrat opted to retain ties with Britain, while Guyana achieved independence in 1966. Britain's last colony on the American mainland, British Honduras, became a self-governing colony in 1964 and was renamed Belize in 1973, achieving full independence in 1981. A dispute with Guatemala over claims to Belize was left unresolved. With support from the British abolitionist movement, Parliament enacted the Slave Trade Act in 1807, which abolished the slave trade in the empire. In 1808, Sierra Leone was designated an official British colony for freed slaves. The Slavery Abolition Act passed in 1833 abolished slavery in the British Empire on 1 August 1834 (with the exception of St. Helena, Ceylon and the territories administered by the East India Company, though these exclusions were later repealed). Under the Act, slaves were granted full emancipation after a period of 4 to 6 years of ""apprenticeship"". By the turn of the 20th century, fears had begun to grow in Britain that it would no longer be able to defend the metropole and the entirety of the empire while at the same time maintaining the policy of ""splendid isolation"". Germany was rapidly rising as a military and industrial power and was now seen as the most likely opponent in any future war. Recognising that it was overstretched in the Pacific and threatened at home by the Imperial German Navy, Britain formed an alliance with Japan in 1902 and with its old enemies France and Russia in 1904 and 1907, respectively. In 1980, Rhodesia, Britain's last African colony, became the independent nation of Zimbabwe. The New Hebrides achieved independence (as Vanuatu) in 1980, with Belize following suit in 1981. The passage of the British Nationality Act 1981, which reclassified the remaining Crown colonies as ""British Dependent Territories"" (renamed British Overseas Territories in 2002) meant that, aside from a scattering of islands and outposts the process of decolonisation that had begun after the Second World War was largely complete. In 1982, Britain's resolve in defending its remaining overseas territories was tested when Argentina invaded the Falkland Islands, acting on a long-standing claim that dated back to the Spanish Empire. Britain's ultimately successful military response to retake the islands during the ensuing Falklands War was viewed by many to have contributed to reversing the downward trend in Britain's status as a world power. The same year, the Canadian government severed its last legal link with Britain by patriating the Canadian constitution from Britain. The 1982 Canada Act passed by the British parliament ended the need for British involvement in changes to the Canadian constitution. Similarly, the Constitution Act 1986 reformed the constitution of New Zealand to sever its constitutional link with Britain, and the Australia Act 1986 severed the constitutional link between Britain and the Australian states. In 1984, Brunei, Britain's last remaining Asian protectorate, gained its independence. During the Age of Discovery in the 15th and 16th centuries, Portugal and Spain pioneered European exploration of the globe, and in the process established large overseas empires. Envious of the great wealth these empires generated, England, France, and the Netherlands began to establish colonies and trade networks of their own in the Americas and Asia. A series of wars in the 17th and 18th centuries with the Netherlands and France left England (and then, following union between England and Scotland in 1707, Great Britain) the dominant colonial power in North America and India. The loss of such a large portion of British America, at the time Britain's most populous overseas possession, is seen by some historians as the event defining the transition between the ""first"" and ""second"" empires, in which Britain shifted its attention away from the Americas to Asia, the Pacific and later Africa. Adam Smith's Wealth of Nations, published in 1776, had argued that colonies were redundant, and that free trade should replace the old mercantilist policies that had characterised the first period of colonial expansion, dating back to the protectionism of Spain and Portugal. The growth of trade between the newly independent United States and Britain after 1783 seemed to confirm Smith's view that political control was not necessary for economic success. In December 1941, Japan launched, in quick succession, attacks on British Malaya, the United States naval base at Pearl Harbor, and Hong Kong. Churchill's reaction to the entry of the United States into the war was that Britain was now assured of victory and the future of the empire was safe, but the manner in which British forces were rapidly defeated in the Far East irreversibly harmed Britain's standing and prestige as an imperial power. Most damaging of all was the fall of Singapore, which had previously been hailed as an impregnable fortress and the eastern equivalent of Gibraltar. The realisation that Britain could not defend its entire empire pushed Australia and New Zealand, which now appeared threatened by Japanese forces, into closer ties with the United States. This resulted in the 1951 ANZUS Pact between Australia, New Zealand and the United States of America. In 1578, Elizabeth I granted a patent to Humphrey Gilbert for discovery and overseas exploration. That year, Gilbert sailed for the West Indies with the intention of engaging in piracy and establishing a colony in North America, but the expedition was aborted before it had crossed the Atlantic. In 1583 he embarked on a second attempt, on this occasion to the island of Newfoundland whose harbour he formally claimed for England, although no settlers were left behind. Gilbert did not survive the return journey to England, and was succeeded by his half-brother, Walter Raleigh, who was granted his own patent by Elizabeth in 1584. Later that year, Raleigh founded the colony of Roanoke on the coast of present-day North Carolina, but lack of supplies caused the colony to fail. The Napoleonic Wars were therefore ones in which Britain invested large amounts of capital and resources to win. French ports were blockaded by the Royal Navy, which won a decisive victory over a Franco-Spanish fleet at Trafalgar in 1805. Overseas colonies were attacked and occupied, including those of the Netherlands, which was annexed by Napoleon in 1810. France was finally defeated by a coalition of European armies in 1815. Britain was again the beneficiary of peace treaties: France ceded the Ionian Islands, Malta (which it had occupied in 1797 and 1798 respectively), Mauritius, St Lucia, and Tobago; Spain ceded Trinidad; the Netherlands Guyana, and the Cape Colony. Britain returned Guadeloupe, Martinique, French Guiana, and Réunion to France, and Java and Suriname to the Netherlands, while gaining control of Ceylon (1795–1815). In July 1956, Nasser unilaterally nationalised the Suez Canal. The response of Anthony Eden, who had succeeded Churchill as Prime Minister, was to collude with France to engineer an Israeli attack on Egypt that would give Britain and France an excuse to intervene militarily and retake the canal. Eden infuriated US President Dwight D. Eisenhower, by his lack of consultation, and Eisenhower refused to back the invasion. Another of Eisenhower's concerns was the possibility of a wider war with the Soviet Union after it threatened to intervene on the Egyptian side. Eisenhower applied financial leverage by threatening to sell US reserves of the British pound and thereby precipitate a collapse of the British currency. Though the invasion force was militarily successful in its objectives, UN intervention and US pressure forced Britain into a humiliating withdrawal of its forces, and Eden resigned. During the middle decades of the 18th century, there were several outbreaks of military conflict on the Indian subcontinent, the Carnatic Wars, as the English East India Company (the Company) and its French counterpart, the Compagnie française des Indes orientales, struggled alongside local rulers to fill the vacuum that had been left by the decline of the Mughal Empire. The Battle of Plassey in 1757, in which the British, led by Robert Clive, defeated the Nawab of Bengal and his French allies, left the Company in control of Bengal and as the major military and political power in India. France was left control of its enclaves but with military restrictions and an obligation to support British client states, ending French hopes of controlling India. In the following decades the Company gradually increased the size of the territories under its control, either ruling directly or via local rulers under the threat of force from the British Indian Army, the vast majority of which was composed of Indian sepoys. After the German occupation of France in 1940, Britain and the empire stood alone against Germany, until the entry of the Soviet Union to the war in 1941. British Prime Minister Winston Churchill successfully lobbied President Franklin D. Roosevelt for military aid from the United States, but Roosevelt was not yet ready to ask Congress to commit the country to war. In August 1941, Churchill and Roosevelt met and signed the Atlantic Charter, which included the statement that ""the rights of all peoples to choose the form of government under which they live"" should be respected. This wording was ambiguous as to whether it referred to European countries invaded by Germany, or the peoples colonised by European nations, and would later be interpreted differently by the British, Americans, and nationalist movements. The British Empire comprised the dominions, colonies, protectorates, mandates and other territories ruled or administered by the United Kingdom. It originated with the overseas possessions and trading posts established by England between the late 16th and early 18th centuries. At its height, it was the largest empire in history and, for over a century, was the foremost global power. By 1922 the British Empire held sway over about 458 million people, one-fifth of the world's population at the time, and covered more than 13,000,000 sq mi (33,670,000 km2), almost a quarter of the Earth's total land area. As a result, its political, legal, linguistic and cultural legacy is widespread. At the peak of its power, the phrase ""the empire on which the sun never sets"" was often used to describe the British Empire, because its expanse around the globe meant that the sun was always shining on at least one of its territories. During his voyage, Cook also visited New Zealand, first discovered by Dutch explorer Abel Tasman in 1642, and claimed the North and South islands for the British crown in 1769 and 1770 respectively. Initially, interaction between the indigenous Māori population and Europeans was limited to the trading of goods. European settlement increased through the early decades of the 19th century, with numerous trading stations established, especially in the North. In 1839, the New Zealand Company announced plans to buy large tracts of land and establish colonies in New Zealand. On 6 February 1840, Captain William Hobson and around 40 Maori chiefs signed the Treaty of Waitangi. This treaty is considered by many to be New Zealand's founding document, but differing interpretations of the Maori and English versions of the text have meant that it continues to be a source of dispute. In 1951, the Conservative Party returned to power in Britain, under the leadership of Winston Churchill. Churchill and the Conservatives believed that Britain's position as a world power relied on the continued existence of the empire, with the base at the Suez Canal allowing Britain to maintain its pre-eminent position in the Middle East in spite of the loss of India. However, Churchill could not ignore Gamal Abdul Nasser's new revolutionary government of Egypt that had taken power in 1952, and the following year it was agreed that British troops would withdraw from the Suez Canal zone and that Sudan would be granted self-determination by 1955, with independence to follow. Sudan was granted independence on 1 January 1956. The path to independence for the white colonies of the British Empire began with the 1839 Durham Report, which proposed unification and self-government for Upper and Lower Canada, as a solution to political unrest there. This began with the passing of the Act of Union in 1840, which created the Province of Canada. Responsible government was first granted to Nova Scotia in 1848, and was soon extended to the other British North American colonies. With the passage of the British North America Act, 1867 by the British Parliament, Upper and Lower Canada, New Brunswick and Nova Scotia were formed into the Dominion of Canada, a confederation enjoying full self-government with the exception of international relations. Australia and New Zealand achieved similar levels of self-government after 1900, with the Australian colonies federating in 1901. The term ""dominion status"" was officially introduced at the Colonial Conference of 1907. The British declaration of war on Germany and its allies also committed the colonies and Dominions, which provided invaluable military, financial and material support. Over 2.5 million men served in the armies of the Dominions, as well as many thousands of volunteers from the Crown colonies. The contributions of Australian and New Zealand troops during the 1915 Gallipoli Campaign against the Ottoman Empire had a great impact on the national consciousness at home, and marked a watershed in the transition of Australia and New Zealand from colonies to nations in their own right. The countries continue to commemorate this occasion on Anzac Day. Canadians viewed the Battle of Vimy Ridge in a similar light. The important contribution of the Dominions to the war effort was recognised in 1917 by the British Prime Minister David Lloyd George when he invited each of the Dominion Prime Ministers to join an Imperial War Cabinet to co-ordinate imperial policy. No further attempts to establish English colonies in the Americas were made until well into the reign of Queen Elizabeth I, during the last decades of the 16th century. In the meantime the Protestant Reformation had turned England and Catholic Spain into implacable enemies . In 1562, the English Crown encouraged the privateers John Hawkins and Francis Drake to engage in slave-raiding attacks against Spanish and Portuguese ships off the coast of West Africa with the aim of breaking into the Atlantic trade system. This effort was rebuffed and later, as the Anglo-Spanish Wars intensified, Elizabeth I gave her blessing to further privateering raids against Spanish ports in the Americas and shipping that was returning across the Atlantic, laden with treasure from the New World. At the same time, influential writers such as Richard Hakluyt and John Dee (who was the first to use the term ""British Empire"") were beginning to press for the establishment of England's own empire. By this time, Spain had become the dominant power in the Americas and was exploring the Pacific ocean, Portugal had established trading posts and forts from the coasts of Africa and Brazil to China, and France had begun to settle the Saint Lawrence River area, later to become New France. Britain's remaining colonies in Africa, except for self-governing Southern Rhodesia, were all granted independence by 1968. British withdrawal from the southern and eastern parts of Africa was not a peaceful process. Kenyan independence was preceded by the eight-year Mau Mau Uprising. In Rhodesia, the 1965 Unilateral Declaration of Independence by the white minority resulted in a civil war that lasted until the Lancaster House Agreement of 1979, which set the terms for recognised independence in 1980, as the new nation of Zimbabwe. Events in America influenced British policy in Canada, where between 40,000 and 100,000 defeated Loyalists had migrated from America following independence. The 14,000 Loyalists who went to the Saint John and Saint Croix river valleys, then part of Nova Scotia, felt too far removed from the provincial government in Halifax, so London split off New Brunswick as a separate colony in 1784. The Constitutional Act of 1791 created the provinces of Upper Canada (mainly English-speaking) and Lower Canada (mainly French-speaking) to defuse tensions between the French and British communities, and implemented governmental systems similar to those employed in Britain, with the intention of asserting imperial authority and not allowing the sort of popular control of government that was perceived to have led to the American Revolution. Britain retains sovereignty over 14 territories outside the British Isles, which were renamed the British Overseas Territories in 2002. Some are uninhabited except for transient military or scientific personnel; the remainder are self-governing to varying degrees and are reliant on the UK for foreign relations and defence. The British government has stated its willingness to assist any Overseas Territory that wishes to proceed to independence, where that is an option. British sovereignty of several of the overseas territories is disputed by their geographical neighbours: Gibraltar is claimed by Spain, the Falkland Islands and South Georgia and the South Sandwich Islands are claimed by Argentina, and the British Indian Ocean Territory is claimed by Mauritius and Seychelles. The British Antarctic Territory is subject to overlapping claims by Argentina and Chile, while many countries do not recognise any territorial claims in Antarctica. The Suez Crisis very publicly exposed Britain's limitations to the world and confirmed Britain's decline on the world stage, demonstrating that henceforth it could no longer act without at least the acquiescence, if not the full support, of the United States. The events at Suez wounded British national pride, leading one MP to describe it as ""Britain's Waterloo"" and another to suggest that the country had become an ""American satellite"". Margaret Thatcher later described the mindset she believed had befallen the British political establishment as ""Suez syndrome"", from which Britain did not recover until the successful recapture of the Falkland Islands from Argentina in 1982. The British Mandate of Palestine, where an Arab majority lived alongside a Jewish minority, presented the British with a similar problem to that of India. The matter was complicated by large numbers of Jewish refugees seeking to be admitted to Palestine following the Holocaust, while Arabs were opposed to the creation of a Jewish state. Frustrated by the intractability of the problem, attacks by Jewish paramilitary organisations and the increasing cost of maintaining its military presence, Britain announced in 1947 that it would withdraw in 1948 and leave the matter to the United Nations to solve. The UN General Assembly subsequently voted for a plan to partition Palestine into a Jewish and an Arab state. The Dutch East India Company had founded the Cape Colony on the southern tip of Africa in 1652 as a way station for its ships travelling to and from its colonies in the East Indies. Britain formally acquired the colony, and its large Afrikaner (or Boer) population in 1806, having occupied it in 1795 to prevent its falling into French hands, following the invasion of the Netherlands by France. British immigration began to rise after 1820, and pushed thousands of Boers, resentful of British rule, northwards to found their own—mostly short-lived—independent republics, during the Great Trek of the late 1830s and early 1840s. In the process the Voortrekkers clashed repeatedly with the British, who had their own agenda with regard to colonial expansion in South Africa and with several African polities, including those of the Sotho and the Zulu nations. Eventually the Boers established two republics which had a longer lifespan: the South African Republic or Transvaal Republic (1852–77; 1881–1902) and the Orange Free State (1854–1902). In 1902 Britain occupied both republics, concluding a treaty with the two Boer Republics following the Second Boer War (1899–1902). In 1603, James VI, King of Scots, ascended (as James I) to the English throne and in 1604 negotiated the Treaty of London, ending hostilities with Spain. Now at peace with its main rival, English attention shifted from preying on other nations' colonial infrastructures to the business of establishing its own overseas colonies. The British Empire began to take shape during the early 17th century, with the English settlement of North America and the smaller islands of the Caribbean, and the establishment of private companies, most notably the English East India Company, to administer colonies and overseas trade. This period, until the loss of the Thirteen Colonies after the American War of Independence towards the end of the 18th century, has subsequently been referred to by some historians as the ""First British Empire"". The foundations of the British Empire were laid when England and Scotland were separate kingdoms. In 1496 King Henry VII of England, following the successes of Spain and Portugal in overseas exploration, commissioned John Cabot to lead a voyage to discover a route to Asia via the North Atlantic. Cabot sailed in 1497, five years after the European discovery of America, and although he successfully made landfall on the coast of Newfoundland (mistakenly believing, like Christopher Columbus, that he had reached Asia), there was no attempt to found a colony. Cabot led another voyage to the Americas the following year but nothing was heard of his ships again. During the 1760s and early 1770s, relations between the Thirteen Colonies and Britain became increasingly strained, primarily because of resentment of the British Parliament's attempts to govern and tax American colonists without their consent. This was summarised at the time by the slogan ""No taxation without representation"", a perceived violation of the guaranteed Rights of Englishmen. The American Revolution began with rejection of Parliamentary authority and moves towards self-government. In response Britain sent troops to reimpose direct rule, leading to the outbreak of war in 1775. The following year, in 1776, the United States declared independence. The entry of France to the war in 1778 tipped the military balance in the Americans' favour and after a decisive defeat at Yorktown in 1781, Britain began negotiating peace terms. American independence was acknowledged at the Peace of Paris in 1783. The last decades of the 19th century saw concerted political campaigns for Irish home rule. Ireland had been united with Britain into the United Kingdom of Great Britain and Ireland with the Act of Union 1800 after the Irish Rebellion of 1798, and had suffered a severe famine between 1845 and 1852. Home rule was supported by the British Prime minister, William Gladstone, who hoped that Ireland might follow in Canada's footsteps as a Dominion within the empire, but his 1886 Home Rule bill was defeated in Parliament. Although the bill, if passed, would have granted Ireland less autonomy within the UK than the Canadian provinces had within their own federation, many MPs feared that a partially independent Ireland might pose a security threat to Great Britain or mark the beginning of the break-up of the empire. A second Home Rule bill was also defeated for similar reasons. A third bill was passed by Parliament in 1914, but not implemented because of the outbreak of the First World War leading to the 1916 Easter Rising. At the concluding Treaty of Utrecht, Philip renounced his and his descendants' right to the French throne and Spain lost its empire in Europe. The British Empire was territorially enlarged: from France, Britain gained Newfoundland and Acadia, and from Spain, Gibraltar and Minorca. Gibraltar became a critical naval base and allowed Britain to control the Atlantic entry and exit point to the Mediterranean. Spain also ceded the rights to the lucrative asiento (permission to sell slaves in Spanish America) to Britain. The ability of the Dominions to set their own foreign policy, independent of Britain, was recognised at the 1923 Imperial Conference. Britain's request for military assistance from the Dominions at the outbreak of the Chanak Crisis the previous year had been turned down by Canada and South Africa, and Canada had refused to be bound by the 1923 Treaty of Lausanne. After pressure from Ireland and South Africa, the 1926 Imperial Conference issued the Balfour Declaration, declaring the Dominions to be ""autonomous Communities within the British Empire, equal in status, in no way subordinate one to another"" within a ""British Commonwealth of Nations"". This declaration was given legal substance under the 1931 Statute of Westminster. The parliaments of Canada, Australia, New Zealand, the Union of South Africa, the Irish Free State and Newfoundland were now independent of British legislative control, they could nullify British laws and Britain could no longer pass laws for them without their consent. Newfoundland reverted to colonial status in 1933, suffering from financial difficulties during the Great Depression. Ireland distanced itself further from Britain with the introduction of a new constitution in 1937, making it a republic in all but name. In September 1982, Prime minister Margaret Thatcher travelled to Beijing to negotiate with the Chinese government on the future of Britain's last major and most populous overseas territory, Hong Kong. Under the terms of the 1842 Treaty of Nanking, Hong Kong Island itself had been ceded to Britain in perpetuity, but the vast majority of the colony was constituted by the New Territories, which had been acquired under a 99-year lease in 1898, due to expire in 1997. Thatcher, seeing parallels with the Falkland Islands, initially wished to hold Hong Kong and proposed British administration with Chinese sovereignty, though this was rejected by China. A deal was reached in 1984—under the terms of the Sino-British Joint Declaration, Hong Kong would become a special administrative region of the People's Republic of China, maintaining its way of life for at least 50 years. The handover ceremony in 1997 marked for many, including Charles, Prince of Wales, who was in attendance, ""the end of Empire"". During the 19th century, Britain and the Russian Empire vied to fill the power vacuums that had been left by the declining Ottoman Empire, Qajar dynasty and Qing Dynasty. This rivalry in Eurasia came to be known as the ""Great Game"". As far as Britain was concerned, defeats inflicted by Russia on Persia and Turkey demonstrated its imperial ambitions and capabilities and stoked fears in Britain of an overland invasion of India. In 1839, Britain moved to pre-empt this by invading Afghanistan, but the First Anglo-Afghan War was a disaster for Britain. While the Suez Crisis caused British power in the Middle East to weaken, it did not collapse. Britain again deployed its armed forces to the region, intervening in Oman (1957), Jordan (1958) and Kuwait (1961), though on these occasions with American approval, as the new Prime Minister Harold Macmillan's foreign policy was to remain firmly aligned with the United States. Britain maintained a military presence in the Middle East for another decade. In January 1968, a few weeks after the devaluation of the pound, Prime Minister Harold Wilson and his Defence Secretary Denis Healey announced that British troops would be withdrawn from major military bases East of Suez, which included the ones in the Middle East, and primarily from Malaysia and Singapore. The British withdrew from Aden in 1967, Bahrain in 1971, and Maldives in 1976. The independence of the Thirteen Colonies in North America in 1783 after the American War of Independence caused Britain to lose some of its oldest and most populous colonies. British attention soon turned towards Asia, Africa, and the Pacific. After the defeat of France in the Revolutionary and Napoleonic Wars (1792–1815), Britain emerged as the principal naval and imperial power of the 19th century (with London the largest city in the world from about 1830). Unchallenged at sea, British dominance was later described as Pax Britannica (""British Peace""), a period of relative peace in Europe and the world (1815–1914) during which the British Empire became the global hegemon and adopted the role of global policeman. In the early 19th century, the Industrial Revolution began to transform Britain; by the time of the Great Exhibition in 1851 the country was described as the ""workshop of the world"". The British Empire expanded to include India, large parts of Africa and many other territories throughout the world. Alongside the formal control it exerted over its own colonies, British dominance of much of world trade meant that it effectively controlled the economies of many regions, such as Asia and Latin America. Domestically, political attitudes favoured free trade and laissez-faire policies and a gradual widening of the voting franchise. During this century, the population increased at a dramatic rate, accompanied by rapid urbanisation, causing significant social and economic stresses. To seek new markets and sources of raw materials, the Conservative Party under Disraeli launched a period of imperialist expansion in Egypt, South Africa, and elsewhere. Canada, Australia, and New Zealand became self-governing dominions. The Caribbean initially provided England's most important and lucrative colonies, but not before several attempts at colonisation failed. An attempt to establish a colony in Guiana in 1604 lasted only two years, and failed in its main objective to find gold deposits. Colonies in St Lucia (1605) and Grenada (1609) also rapidly folded, but settlements were successfully established in St. Kitts (1624), Barbados (1627) and Nevis (1628). The colonies soon adopted the system of sugar plantations successfully used by the Portuguese in Brazil, which depended on slave labour, and—at first—Dutch ships, to sell the slaves and buy the sugar. To ensure that the increasingly healthy profits of this trade remained in English hands, Parliament decreed in 1651 that only English ships would be able to ply their trade in English colonies. This led to hostilities with the United Dutch Provinces—a series of Anglo-Dutch Wars—which would eventually strengthen England's position in the Americas at the expense of the Dutch. In 1655, England annexed the island of Jamaica from the Spanish, and in 1666 succeeded in colonising the Bahamas. When Russia invaded the Turkish Balkans in 1853, fears of Russian dominance in the Mediterranean and Middle East led Britain and France to invade the Crimean Peninsula to destroy Russian naval capabilities. The ensuing Crimean War (1854–56), which involved new techniques of modern warfare, and was the only global war fought between Britain and another imperial power during the Pax Britannica, was a resounding defeat for Russia. The situation remained unresolved in Central Asia for two more decades, with Britain annexing Baluchistan in 1876 and Russia annexing Kirghizia, Kazakhstan, and Turkmenistan. For a while it appeared that another war would be inevitable, but the two countries reached an agreement on their respective spheres of influence in the region in 1878 and on all outstanding matters in 1907 with the signing of the Anglo-Russian Entente. The destruction of the Russian Navy by the Japanese at the Battle of Port Arthur during the Russo-Japanese War of 1904–05 also limited its threat to the British. England's first permanent settlement in the Americas was founded in 1607 in Jamestown, led by Captain John Smith and managed by the Virginia Company. Bermuda was settled and claimed by England as a result of the 1609 shipwreck there of the Virginia Company's flagship, and in 1615 was turned over to the newly formed Somers Isles Company. The Virginia Company's charter was revoked in 1624 and direct control of Virginia was assumed by the crown, thereby founding the Colony of Virginia. The London and Bristol Company was created in 1610 with the aim of creating a permanent settlement on Newfoundland, but was largely unsuccessful. In 1620, Plymouth was founded as a haven for puritan religious separatists, later known as the Pilgrims. Fleeing from religious persecution would become the motive of many English would-be colonists to risk the arduous trans-Atlantic voyage: Maryland was founded as a haven for Roman Catholics (1634), Rhode Island (1636) as a colony tolerant of all religions and Connecticut (1639) for Congregationalists. The Province of Carolina was founded in 1663. With the surrender of Fort Amsterdam in 1664, England gained control of the Dutch colony of New Netherland, renaming it New York. This was formalised in negotiations following the Second Anglo-Dutch War, in exchange for Suriname. In 1681, the colony of Pennsylvania was founded by William Penn. The American colonies were less financially successful than those of the Caribbean, but had large areas of good agricultural land and attracted far larger numbers of English emigrants who preferred their temperate climates. In 1869 the Suez Canal opened under Napoleon III, linking the Mediterranean with the Indian Ocean. Initially the Canal was opposed by the British; but once opened, its strategic value was quickly recognised and became the ""jugular vein of the Empire"". In 1875, the Conservative government of Benjamin Disraeli bought the indebted Egyptian ruler Isma'il Pasha's 44 percent shareholding in the Suez Canal for £4 million (£340 million in 2013). Although this did not grant outright control of the strategic waterway, it did give Britain leverage. Joint Anglo-French financial control over Egypt ended in outright British occupation in 1882. The French were still majority shareholders and attempted to weaken the British position, but a compromise was reached with the 1888 Convention of Constantinople, which made the Canal officially neutral territory. With French, Belgian and Portuguese activity in the lower Congo River region undermining orderly incursion of tropical Africa, the Berlin Conference of 1884–85 was held to regulate the competition between the European powers in what was called the ""Scramble for Africa"" by defining ""effective occupation"" as the criterion for international recognition of territorial claims. The scramble continued into the 1890s, and caused Britain to reconsider its decision in 1885 to withdraw from Sudan. A joint force of British and Egyptian troops defeated the Mahdist Army in 1896, and rebuffed a French attempted invasion at Fashoda in 1898. Sudan was nominally made an Anglo-Egyptian Condominium, but a British colony in reality. Though Britain and the empire emerged victorious from the Second World War, the effects of the conflict were profound, both at home and abroad. Much of Europe, a continent that had dominated the world for several centuries, was in ruins, and host to the armies of the United States and the Soviet Union, who now held the balance of global power. Britain was left essentially bankrupt, with insolvency only averted in 1946 after the negotiation of a $US 4.33 billion loan (US$56 billion in 2012) from the United States, the last instalment of which was repaid in 2006. At the same time, anti-colonial movements were on the rise in the colonies of European nations. The situation was complicated further by the increasing Cold War rivalry of the United States and the Soviet Union. In principle, both nations were opposed to European colonialism. In practice, however, American anti-communism prevailed over anti-imperialism, and therefore the United States supported the continued existence of the British Empire to keep Communist expansion in check. The ""wind of change"" ultimately meant that the British Empire's days were numbered, and on the whole, Britain adopted a policy of peaceful disengagement from its colonies once stable, non-Communist governments were available to transfer power to. This was in contrast to other European powers such as France and Portugal, which waged costly and ultimately unsuccessful wars to keep their empires intact. Between 1945 and 1965, the number of people under British rule outside the UK itself fell from 700 million to five million, three million of whom were in Hong Kong. The pro-decolonisation Labour government, elected at the 1945 general election and led by Clement Attlee, moved quickly to tackle the most pressing issue facing the empire: that of Indian independence. India's two major political parties—the Indian National Congress and the Muslim League—had been campaigning for independence for decades, but disagreed as to how it should be implemented. Congress favoured a unified secular Indian state, whereas the League, fearing domination by the Hindu majority, desired a separate Islamic state for Muslim-majority regions. Increasing civil unrest and the mutiny of the Royal Indian Navy during 1946 led Attlee to promise independence no later than 1948. When the urgency of the situation and risk of civil war became apparent, the newly appointed (and last) Viceroy, Lord Mountbatten, hastily brought forward the date to 15 August 1947. The borders drawn by the British to broadly partition India into Hindu and Muslim areas left tens of millions as minorities in the newly independent states of India and Pakistan. Millions of Muslims subsequently crossed from India to Pakistan and Hindus vice versa, and violence between the two communities cost hundreds of thousands of lives. Burma, which had been administered as part of the British Raj, and Sri Lanka gained their independence the following year in 1948. India, Pakistan and Sri Lanka became members of the Commonwealth, while Burma chose not to join. In 1922, Egypt, which had been declared a British protectorate at the outbreak of the First World War, was granted formal independence, though it continued to be a British client state until 1954. British troops remained stationed in Egypt until the signing of the Anglo-Egyptian Treaty in 1936, under which it was agreed that the troops would withdraw but continue to occupy and defend the Suez Canal zone. In return, Egypt was assisted to join the League of Nations. Iraq, a British mandate since 1920, also gained membership of the League in its own right after achieving independence from Britain in 1932. In Palestine, Britain was presented with the problem of mediating between the Arab and Jewish communities. The 1917 Balfour Declaration, which had been incorporated into the terms of the mandate, stated that a national home for the Jewish people would be established in Palestine, and Jewish immigration allowed up to a limit that would be determined by the mandatory power. This led to increasing conflict with the Arab population, who openly revolted in 1936. As the threat of war with Germany increased during the 1930s, Britain judged the support of the Arab population in the Middle East as more important than the establishment of a Jewish homeland, and shifted to a pro-Arab stance, limiting Jewish immigration and in turn triggering a Jewish insurgency. Peace between England and the Netherlands in 1688 meant that the two countries entered the Nine Years' War as allies, but the conflict—waged in Europe and overseas between France, Spain and the Anglo-Dutch alliance—left the English a stronger colonial power than the Dutch, who were forced to devote a larger proportion of their military budget on the costly land war in Europe. The 18th century saw England (after 1707, Britain) rise to be the world's dominant colonial power, and France becoming its main rival on the imperial stage. Most former British colonies and protectorates are among the 53 member states of the Commonwealth of Nations, a non-political, voluntary association of equal members, comprising a population of around 2.2 billion people. Sixteen Commonwealth realms voluntarily continue to share the British monarch, Queen Elizabeth II, as their head of state. These sixteen nations are distinct and equal legal entities – the United Kingdom, Australia, Canada, New Zealand, Papua New Guinea, Antigua and Barbuda, The Bahamas, Barbados, Belize, Grenada, Jamaica, Saint Kitts and Nevis, Saint Lucia, Saint Vincent and the Grenadines, Solomon Islands and Tuvalu. Two years later, the Royal African Company was inaugurated, receiving from King Charles a monopoly of the trade to supply slaves to the British colonies of the Caribbean. From the outset, slavery was the basis of the British Empire in the West Indies. Until the abolition of the slave trade in 1807, Britain was responsible for the transportation of 3.5 million African slaves to the Americas, a third of all slaves transported across the Atlantic. To facilitate this trade, forts were established on the coast of West Africa, such as James Island, Accra and Bunce Island. In the British Caribbean, the percentage of the population of African descent rose from 25 percent in 1650 to around 80 percent in 1780, and in the 13 Colonies from 10 percent to 40 percent over the same period (the majority in the southern colonies). For the slave traders, the trade was extremely profitable, and became a major economic mainstay for such western British cities as Bristol and Liverpool, which formed the third corner of the so-called triangular trade with Africa and the Americas. For the transported, harsh and unhygienic conditions on the slaving ships and poor diets meant that the average mortality rate during the Middle Passage was one in seven."
Miami,"Black labor played a crucial role in Miami's early development. During the beginning of the 20th century, migrants from the Bahamas and African-Americans constituted 40 percent of the city's population. Whatever their role in the city's growth, their community's growth was limited to a small space. When landlords began to rent homes to African-Americans in neighborhoods close to Avenue J (what would later become NW Fifth Avenue), a gang of white man with torches visited the renting families and warned them to move or be bombed. The northern side of Miami includes Midtown, a district with a great mix of diversity with many West Indians, Hispanics, European Americans, bohemians, and artists. Edgewater, and Wynwood, are neighborhoods of Midtown and are made up mostly of high-rise residential towers and are home to the Adrienne Arsht Center for the Performing Arts. The wealthier residents usually live in the northeastern part, in Midtown, the Design District, and the Upper East Side, with many sought after 1920s homes and home of the MiMo Historic District, a style of architecture originated in Miami in the 1950s. The northern side of Miami also has notable African American and Caribbean immigrant communities such as Little Haiti, Overtown (home of the Lyric Theater), and Liberty City. During the early 20th century, northerners were attracted to the city, and Miami prospered during the 1920s with an increase in population and infrastructure. The legacy of Jim Crow was embedded in these developments. Miami's chief of police, H. Leslie Quigg, did not hide the fact that he, like many other white Miami police officers, was a member of the Ku Klux Klan. Unsurprisingly, these officers enforced social codes far beyond the written law. Quigg, for example, ""personally and publicly beat a colored bellboy to death for speaking directly to a white woman."" Miami is the southern terminus of Amtrak's Atlantic Coast services, running two lines, the Silver Meteor and the Silver Star, both terminating in New York City. The Miami Amtrak Station is located in the suburb of Hialeah near the Tri-Rail/Metrorail Station on NW 79 St and NW 38 Ave. Current construction of the Miami Central Station will move all Amtrak operations from its current out-of-the-way location to a centralized location with Metrorail, MIA Mover, Tri-Rail, Miami International Airport, and the Miami Intermodal Center all within the same station closer to Downtown. The station was expected to be completed by 2012, but experienced several delays and was later expected to be completed in late 2014, again pushed back to early 2015. Miami is a major television production center, and the most important city in the U.S. for Spanish language media. Univisión, Telemundo and UniMÁS have their headquarters in Miami, along with their production studios. The Telemundo Television Studios produces much of the original programming for Telemundo, such as their telenovelas and talk shows. In 2011, 85% of Telemundo's original programming was filmed in Miami. Miami is also a major music recording center, with the Sony Music Latin and Universal Music Latin Entertainment headquarters in the city, along with many other smaller record labels. The city also attracts many artists for music video and film shootings. Miami has six major causeways that span over Biscayne Bay connecting the western mainland, with the eastern barrier islands along the Atlantic Ocean. The Rickenbacker Causeway is the southernmost causeway and connects Brickell to Virginia Key and Key Biscayne. The Venetian Causeway and MacArthur Causeway connect Downtown with South Beach. The Julia Tuttle Causeway connects Midtown and Miami Beach. The 79th Street Causeway connects the Upper East Side with North Beach. The northernmost causeway, the Broad Causeway, is the smallest of Miami's six causeways, and connects North Miami with Bal Harbour. Cuban immigrants in the 1960s brought the Cuban sandwich, medianoche, Cuban espresso, and croquetas, all of which have grown in popularity to all Miamians, and have become symbols of the city's varied cuisine. Today, these are part of the local culture, and can be found throughout the city in window cafés, particularly outside of supermarkets and restaurants. Restaurants such as Versailles restaurant in Little Havana is a landmark eatery of Miami. Located on the Atlantic Ocean, and with a long history as a seaport, Miami is also known for its seafood, with many seafood restaurants located along the Miami River, and in and around Biscayne Bay. Miami is also the home of restaurant chains such as Burger King, Tony Roma's and Benihana. Miami is also considered a ""hot spot"" for dance music, Freestyle, a style of dance music popular in the 80's and 90's heavily influenced by Electro, hip-hop, and disco. Many popular Freestyle acts such as Pretty Tony, Debbie Deb, Stevie B, and Exposé, originated in Miami. Indie/folk acts Cat Power and Iron & Wine are based in the city, while alternative hip hop artist Sage Francis, electro artist Uffie, and the electroclash duo Avenue D were born in Miami, but musically based elsewhere. Also, ska punk band Against All Authority is from Miami, and rock/metal bands Nonpoint and Marilyn Manson each formed in neighboring Fort Lauderdale. Cuban American female recording artist, Ana Cristina, was born in Miami in 1985. Miami is home to one of the largest ports in the United States, the PortMiami. It is the largest cruise ship port in the world. The port is often called the ""Cruise Capital of the World"" and the ""Cargo Gateway of the Americas"". It has retained its status as the number one cruise/passenger port in the world for well over a decade accommodating the largest cruise ships and the major cruise lines. In 2007, the port served 3,787,410 passengers. Additionally, the port is one of the nation's busiest cargo ports, importing 7.8 million tons of cargo in 2007. Among North American ports, it ranks second only to the Port of South Louisiana in New Orleans in terms of cargo tonnage imported/exported from Latin America. The port is on 518 acres (2 km2) and has 7 passenger terminals. China is the port's number one import country, and Honduras is the number one export country. Miami has the world's largest amount of cruise line headquarters, home to: Carnival Cruise Lines, Celebrity Cruises, Norwegian Cruise Line, Oceania Cruises, and Royal Caribbean International. In 2014, the Port of Miami Tunnel was completed and will serve the PortMiami. Tourism is also an important industry in Miami. Along with finance and business, the beaches, conventions, festivals and events draw over 38 million visitors annually into the city, from across the country and around the world, spending $17.1 billion. The Art Deco District in South Beach, is reputed as one of the most glamorous in the world for its nightclubs, beaches, historical buildings, and shopping. Annual events such as the Sony Ericsson Open, Art Basel, Winter Music Conference, South Beach Wine & Food Festival, and Mercedes-Benz Fashion Week Miami attract millions to the metropolis every year. In 1960, non-Hispanic whites represented 80% of Miami-Dade county's population. In 1970, the Census Bureau reported Miami's population as 45.3% Hispanic, 32.9% non-Hispanic White, and 22.7% Black. Miami's explosive population growth has been driven by internal migration from other parts of the country, primarily up until the 1980s, as well as by immigration, primarily from the 1960s to the 1990s. Today, immigration to Miami has slowed significantly and Miami's growth today is attributed greatly to its fast urbanization and high-rise construction, which has increased its inner city neighborhood population densities, such as in Downtown, Brickell, and Edgewater, where one area in Downtown alone saw a 2,069% increase in population in the 2010 Census. Miami is regarded as more of a multicultural mosaic, than it is a melting pot, with residents still maintaining much of, or some of their cultural traits. The overall culture of Miami is heavily influenced by its large population of Hispanics and blacks mainly from the Caribbean islands. Miami is a major center, and a leader in finance, commerce, culture, media, entertainment, the arts, and international trade. In 2012, Miami was classified as an Alpha−World City in the World Cities Study Group's inventory. In 2010, Miami ranked seventh in the United States in terms of finance, commerce, culture, entertainment, fashion, education, and other sectors. It ranked 33rd among global cities. In 2008, Forbes magazine ranked Miami ""America's Cleanest City"", for its year-round good air quality, vast green spaces, clean drinking water, clean streets, and city-wide recycling programs. According to a 2009 UBS study of 73 world cities, Miami was ranked as the richest city in the United States, and the world's fifth-richest city in terms of purchasing power. Miami is nicknamed the ""Capital of Latin America"", is the second largest U.S. city with a Spanish-speaking majority, and the largest city with a Cuban-American plurality. After Fidel Castro rose to power in Cuba in 1959, many wealthy Cubans sought refuge in Miami, further increasing the population. The city developed businesses and cultural amenities as part of the New South. In the 1980s and 1990s, South Florida weathered social problems related to drug wars, immigration from Haiti and Latin America, and the widespread destruction of Hurricane Andrew. Racial and cultural tensions were sometimes sparked, but the city developed in the latter half of the 20th century as a major international, financial, and cultural center. It is the second-largest U.S. city (after El Paso, Texas) with a Spanish-speaking majority, and the largest city with a Cuban-American plurality. Miami's main four sports teams are the Miami Dolphins of the National Football League, the Miami Heat of the National Basketball Association, the Miami Marlins of Major League Baseball, and the Florida Panthers of the National Hockey League. As well as having all four major professional teams, Miami is also home to the Major League Soccer expansion team led by David Beckham, Sony Ericsson Open for professional tennis, numerous greyhound racing tracks, marinas, jai alai venues, and golf courses. The city streets has hosted professional auto races, the Miami Indy Challenge and later the Grand Prix Americas. The Homestead-Miami Speedway oval hosts NASCAR national races. Construction is currently underway on the Miami Intermodal Center and Miami Central Station, a massive transportation hub servicing Metrorail, Amtrak, Tri-Rail, Metrobus, Greyhound Lines, taxis, rental cars, MIA Mover, private automobiles, bicycles and pedestrians adjacent to Miami International Airport. Completion of the Miami Intermodal Center is expected to be completed by winter 2011, and will serve over 150,000 commuters and travelers in the Miami area. Phase I of Miami Central Station is scheduled to begin service in the spring of 2012, and Phase II in 2013. The government of the City of Miami (proper) uses the mayor-commissioner type of system. The city commission consists of five commissioners which are elected from single member districts. The city commission constitutes the governing body with powers to pass ordinances, adopt regulations, and exercise all powers conferred upon the city in the city charter. The mayor is elected at large and appoints a city manager. The City of Miami is governed by Mayor Tomás Regalado and 5 City commissioners which oversee the five districts in the City. The commission's regular meetings are held at Miami City Hall, which is located at 3500 Pan American Drive on Dinner Key in the neighborhood of Coconut Grove . Florida High Speed Rail was a proposed government backed high-speed rail system that would have connected Miami, Orlando, and Tampa. The first phase was planned to connect Orlando and Tampa and was offered federal funding, but it was turned down by Governor Rick Scott in 2011. The second phase of the line was envisioned to connect Miami. By 2014, a private project known as All Aboard Florida by a company of the historic Florida East Coast Railway began construction of a higher-speed rail line in South Florida that is planned to eventually terminate at Orlando International Airport. Miami is partitioned into many different sections, roughly into North, South, West and Downtown. The heart of the city is Downtown Miami and is technically on the eastern side of the city. This area includes Brickell, Virginia Key, Watson Island, and PortMiami. Downtown is South Florida's central business district, and Florida's largest and most influential central business district. Downtown has the largest concentration of international banks in the U.S. along Brickell Avenue. Downtown is home to many major banks, courthouses, financial headquarters, cultural and tourist attractions, schools, parks and a large residential population. East of Downtown, across Biscayne Bay is South Beach. Just northwest of Downtown, is the Civic Center, which is Miami's center for hospitals, research institutes and biotechnology with hospitals such as Jackson Memorial Hospital, Miami VA Hospital, and the University of Miami's Leonard M. Miller School of Medicine. Miami has a tropical monsoon climate (Köppen climate classification Am) with hot and humid summers and short, warm winters, with a marked drier season in the winter. Its sea-level elevation, coastal location, position just above the Tropic of Cancer, and proximity to the Gulf Stream shapes its climate. With January averaging 67.2 °F (19.6 °C), winter features mild to warm temperatures; cool air usually settles after the passage of a cold front, which produces much of the little amount of rainfall during the season. Lows occasionally fall below 50 °F (10 °C), but very rarely below 35 °F (2 °C). Highs generally range between 70–77 °F (21–25 °C). This was also a period of alternatives to nightclubs, the warehouse party, acid house, rave and outdoor festival scenes of the late 1980s and early 1990s were havens for the latest trends in electronic dance music, especially house and its ever-more hypnotic, synthetic offspring techno and trance, in clubs like the infamous Warsaw Ballroom better known as Warsaw and The Mix where DJs like david padilla (who was the resident DJ for both) and radio. The new sound fed back into mainstream clubs across the country. The scene in SoBe, along with a bustling secondhand market for electronic instruments and turntables, had a strong democratizing effect, offering amateur, ""bedroom"" DJs the opportunity to become proficient and popular as both music players and producers, regardless of the whims of the professional music and club industries. Some of these notable DJs are John Benetiz (better known as JellyBean Benetiz), Danny Tenaglia, and David Padilla. Miami and its suburbs are located on a broad plain between the Florida Everglades to the west and Biscayne Bay to the east, which also extends from Florida Bay north to Lake Okeechobee. The elevation of the area never rises above 40 ft (12 m) and averages at around 6 ft (1.8 m) above mean sea level in most neighborhoods, especially near the coast. The highest undulations are found along the coastal Miami Rock Ridge, whose substrate underlies most of the eastern Miami metropolitan region. The main portion of the city lies on the shores of Biscayne Bay which contains several hundred natural and artificially created barrier islands, the largest of which contains Miami Beach and South Beach. The Gulf Stream, a warm ocean current, runs northward just 15 miles (24 km) off the coast, allowing the city's climate to stay warm and mild all year. In the early 1970s, the Miami disco sound came to life with TK Records, featuring the music of KC and the Sunshine Band, with such hits as ""Get Down Tonight"", ""(Shake, Shake, Shake) Shake Your Booty"" and ""That's the Way (I Like It)""; and the Latin-American disco group, Foxy (band), with their hit singles ""Get Off"" and ""Hot Number"". Miami-area natives George McCrae and Teri DeSario were also popular music artists during the 1970s disco era. The Bee Gees moved to Miami in 1975 and have lived here ever since then. Miami-influenced, Gloria Estefan and the Miami Sound Machine, hit the popular music scene with their Cuban-oriented sound and had hits in the 1980s with ""Conga"" and ""Bad Boys"". According to the U.S. Census Bureau, in 2004, Miami had the third highest incidence of family incomes below the federal poverty line in the United States, making it the third poorest city in the USA, behind only Detroit, Michigan (ranked #1) and El Paso, Texas (ranked #2). Miami is also one of the very few cities where its local government went bankrupt, in 2001. However, since that time, Miami has experienced a revival: in 2008, Miami was ranked as ""America's Cleanest City"" according to Forbes for its year-round good air quality, vast green spaces, clean drinking water, clean streets and city-wide recycling programs. In a 2009 UBS study of 73 world cities, Miami was ranked as the richest city in the United States (of four U.S. cities included in the survey) and the world's fifth-richest city, in terms of purchasing power. The city proper is home to less than one-thirteenth of the population of South Florida. Miami is the 42nd-most populous city in the United States. The Miami metropolitan area, which includes Miami-Dade, Broward and Palm Beach counties, had a combined population of more than 5.5 million people, ranked seventh largest in the United States, and is the largest metropolitan area in the Southeastern United States. As of 2008[update], the United Nations estimates that the Miami Urban Agglomeration is the 44th-largest in the world. The Miami area has a unique dialect, (commonly called the ""Miami accent"") which is widely spoken. The dialect developed among second- or third-generation Hispanics, including Cuban-Americans, whose first language was English (though some non-Hispanic white, black, and other races who were born and raised the Miami area tend to adopt it as well.) It is based on a fairly standard American accent but with some changes very similar to dialects in the Mid-Atlantic (especially the New York area dialect, Northern New Jersey English, and New York Latino English.) Unlike Virginia Piedmont, Coastal Southern American, and Northeast American dialects and Florida Cracker dialect (see section below), ""Miami accent"" is rhotic; it also incorporates a rhythm and pronunciation heavily influenced by Spanish (wherein rhythm is syllable-timed). However, this is a native dialect of English, not learner English or interlanguage; it is possible to differentiate this variety from an interlanguage spoken by second-language speakers in that ""Miami accent"" does not generally display the following features: there is no addition of /ɛ/ before initial consonant clusters with /s/, speakers do not confuse of /dʒ/ with /j/, (e.g., Yale with jail), and /r/ and /rr/ are pronounced as alveolar approximant [ɹ] instead of alveolar tap [ɾ] or alveolar trill [r] in Spanish. Other major newspapers include Miami Today, headquartered in Brickell, Miami New Times, headquartered in Midtown, Miami Sun Post, South Florida Business Journal, Miami Times, and Biscayne Boulevard Times. An additional Spanish-language newspapers, Diario Las Americas also serve Miami. The Miami Herald is Miami's primary newspaper with over a million readers and is headquartered in Downtown in Herald Plaza. Several other student newspapers from the local universities, such as the oldest, the University of Miami's The Miami Hurricane, Florida International University's The Beacon, Miami-Dade College's The Metropolis, Barry University's The Buccaneer, amongst others. Many neighborhoods and neighboring areas also have their own local newspapers such as the Aventura News, Coral Gables Tribune, Biscayne Bay Tribune, and the Palmetto Bay News. Miami (/maɪˈæmi/; Spanish pronunciation: [maiˈami]) is a city located on the Atlantic coast in southeastern Florida and the seat of Miami-Dade County. The 44th-most populated city proper in the United States, with a population of 430,332, it is the principal, central, and most populous city of the Miami metropolitan area, and the second most populous metropolis in the Southeastern United States after Washington, D.C. According to the U.S. Census Bureau, Miami's metro area is the eighth-most populous and fourth-largest urban area in the United States, with a population of around 5.5 million. Miami International Airport and PortMiami are among the nation's busiest ports of entry, especially for cargo from South America and the Caribbean. The Port of Miami is the world's busiest cruise port, and MIA is the busiest airport in Florida, and the largest gateway between the United States and Latin America. Additionally, the city has the largest concentration of international banks in the country, primarily along Brickell Avenue in Brickell, Miami's financial district. Due to its strength in international business, finance and trade, many international banks have offices in Downtown such as Espírito Santo Financial Group, which has its U.S. headquarters in Miami. Miami was also the host city of the 2003 Free Trade Area of the Americas negotiations, and is one of the leading candidates to become the trading bloc's headquarters. Since 2001, Miami has been undergoing a large building boom with more than 50 skyscrapers rising over 400 feet (122 m) built or currently under construction in the city. Miami's skyline is ranked third-most impressive in the U.S., behind New York City and Chicago, and 19th in the world according to the Almanac of Architecture and Design. The city currently has the eight tallest (as well as thirteen of the fourteen tallest) skyscrapers in the state of Florida, with the tallest being the 789-foot (240 m) Four Seasons Hotel & Tower. The southern side of Miami includes Coral Way, The Roads and Coconut Grove. Coral Way is a historic residential neighborhood built in 1922 connecting Downtown with Coral Gables, and is home to many old homes and tree-lined streets. Coconut Grove was established in 1825 and is the location of Miami's City Hall in Dinner Key, the Coconut Grove Playhouse, CocoWalk, many nightclubs, bars, restaurants and bohemian shops, and as such, is very popular with local college students. It is a historic neighborhood with narrow, winding roads, and a heavy tree canopy. Coconut Grove has many parks and gardens such as Villa Vizcaya, The Kampong, The Barnacle Historic State Park, and is the home of the Coconut Grove Convention Center and numerous historic homes and estates. Miami has one of the largest television markets in the nation and the second largest in the state of Florida. Miami has several major newspapers, the main and largest newspaper being The Miami Herald. El Nuevo Herald is the major and largest Spanish-language newspaper. The Miami Herald and El Nuevo Herald are Miami's and South Florida's main, major and largest newspapers. The papers left their longtime home in downtown Miami in 2013. The newspapers are now headquartered at the former home of U.S. Southern Command in Doral. Beneath the plain lies the Biscayne Aquifer, a natural underground source of fresh water that extends from southern Palm Beach County to Florida Bay, with its highest point peaking around the cities of Miami Springs and Hialeah. Most of the Miami metropolitan area obtains its drinking water from this aquifer. As a result of the aquifer, it is not possible to dig more than 15 to 20 ft (5 to 6 m) beneath the city without hitting water, which impedes underground construction, though some underground parking garages exist. For this reason, the mass transit systems in and around Miami are elevated or at-grade.[citation needed] The surface bedrock under the Miami area is called Miami oolite or Miami limestone. This bedrock is covered by a thin layer of soil, and is no more than 50 feet (15 m) thick. Miami limestone formed as the result of the drastic changes in sea level associated with recent glaciations or ice ages. Beginning some 130,000 years ago the Sangamonian Stage raised sea levels to approximately 25 feet (8 m) above the current level. All of southern Florida was covered by a shallow sea. Several parallel lines of reef formed along the edge of the submerged Florida plateau, stretching from the present Miami area to what is now the Dry Tortugas. The area behind this reef line was in effect a large lagoon, and the Miami limestone formed throughout the area from the deposition of oolites and the shells of bryozoans. Starting about 100,000 years ago the Wisconsin glaciation began lowering sea levels, exposing the floor of the lagoon. By 15,000 years ago, the sea level had dropped to 300 to 350 feet (90 to 110 m) below the contemporary level. The sea level rose quickly after that, stabilizing at the current level about 4000 years ago, leaving the mainland of South Florida just above sea level. Miami's road system is based along the numerical ""Miami Grid"" where Flagler Street forms the east-west baseline and Miami Avenue forms the north-south meridian. The corner of Flagler Street and Miami Avenue is in the middle of Downtown in front of the Downtown Macy's (formerly the Burdine's headquarters). The Miami grid is primarily numerical so that, for example, all street addresses north of Flagler Street and west of Miami Avenue have ""NW"" in their address. Because its point of origin is in Downtown, which is close to the coast, therefore, the ""NW"" and ""SW"" quadrants are much larger than the ""SE"" and ""NE"" quadrants. Many roads, especially major ones, are also named (e.g., Tamiami Trail/SW 8th St), although, with exceptions, the number is in more common usage among locals. Miami is noted as ""the only major city in the United States conceived by a woman, Julia Tuttle"", a local citrus grower and a wealthy Cleveland native. The Miami area was better known as ""Biscayne Bay Country"" in the early years of its growth. In the late 19th century, reports described the area as a promising wilderness. The area was also characterized as ""one of the finest building sites in Florida."" The Great Freeze of 1894–95 hastened Miami's growth, as the crops of the Miami area were the only ones in Florida that survived. Julia Tuttle subsequently convinced Henry Flagler, a railroad tycoon, to expand his Florida East Coast Railway to the region, for which she became known as ""the mother of Miami."" Miami was officially incorporated as a city on July 28, 1896 with a population of just over 300. It was named for the nearby Miami River, derived from Mayaimi, the historic name of Lake Okeechobee. Miami International Airport serves as the primary international airport of the Greater Miami Area. One of the busiest international airports in the world, Miami International Airport caters to over 35 million passengers a year. The airport is a major hub and the single largest international gateway for American Airlines. Miami International is the busiest airport in Florida, and is the United States' second-largest international port of entry for foreign air passengers behind New York's John F. Kennedy International Airport, and is the seventh-largest such gateway in the world. The airport's extensive international route network includes non-stop flights to over seventy international cities in North and South America, Europe, Asia, and the Middle East. Several large companies are headquartered in or around Miami, including but not limited to: Akerman Senterfitt, Alienware, Arquitectonica, Arrow Air, Bacardi, Benihana, Brightstar Corporation, Burger King, Celebrity Cruises, Carnival Corporation, Carnival Cruise Lines, Crispin Porter + Bogusky, Duany Plater-Zyberk & Company, Espírito Santo Financial Group, Fizber.com, Greenberg Traurig, Holland & Knight, Inktel Direct, Interval International, Lennar, Navarro Discount Pharmacies, Norwegian Cruise Lines, Oceania Cruises, Perry Ellis International, RCTV International, Royal Caribbean Cruise Lines, Ryder Systems, Seabourn Cruise Line, Sedano's, Telefónica USA, UniMÁS, Telemundo, Univision, U.S. Century Bank, Vector Group and World Fuel Services. Because of its proximity to Latin America, Miami serves as the headquarters of Latin American operations for more than 1400 multinational corporations, including AIG, American Airlines, Cisco, Disney, Exxon, FedEx, Kraft Foods, LEO Pharma Americas, Microsoft, Yahoo, Oracle, SBC Communications, Sony, Symantec, Visa International, and Wal-Mart. In recent years the city government, under Mayor Manny Diaz, has taken an ambitious stance in support of bicycling in Miami for both recreation and commuting. Every month, the city hosts ""Bike Miami"", where major streets in Downtown and Brickell are closed to automobiles, but left open for pedestrians and bicyclists. The event began in November 2008, and has doubled in popularity from 1,500 participants to about 3,000 in the October 2009 Bike Miami. This is the longest-running such event in the US. In October 2009, the city also approved an extensive 20-year plan for bike routes and paths around the city. The city has begun construction of bike routes as of late 2009, and ordinances requiring bike parking in all future construction in the city became mandatory as of October 2009. Miami's tropical weather allows for year-round outdoors activities. The city has numerous marinas, rivers, bays, canals, and the Atlantic Ocean, which make boating, sailing, and fishing popular outdoors activities. Biscayne Bay has numerous coral reefs which make snorkeling and scuba diving popular. There are over 80 parks and gardens in the city. The largest and most popular parks are Bayfront Park and Bicentennial Park (located in the heart of Downtown and the location of the American Airlines Arena and Bayside Marketplace), Tropical Park, Peacock Park, Morningside Park, Virginia Key, and Watson Island. During the mid-2000s, the city witnessed its largest real estate boom since the Florida land boom of the 1920s. During this period, the city had well over a hundred approved high-rise construction projects in which 50 were actually built. In 2007, however, the housing market crashed causing lots of foreclosures on houses. This rapid high-rise construction, has led to fast population growth in the city's inner neighborhoods, primarily in Downtown, Brickell and Edgewater, with these neighborhoods becoming the fastest-growing areas in the city. The Miami area ranks 8th in the nation in foreclosures. In 2011, Forbes magazine named Miami the second-most miserable city in the United States due to its high foreclosure rate and past decade of corruption among public officials. In 2012, Forbes magazine named Miami the most miserable city in the United States because of a crippling housing crisis that has cost multitudes of residents their homes and jobs. The metro area has one of the highest violent crime rates in the country and workers face lengthy daily commutes. Downtown Miami is home to the largest concentration of international banks in the United States, and many large national and international companies. The Civic Center is a major center for hospitals, research institutes, medical centers, and biotechnology industries. For more than two decades, the Port of Miami, known as the ""Cruise Capital of the World"", has been the number one cruise passenger port in the world. It accommodates some of the world's largest cruise ships and operations, and is the busiest port in both passenger traffic and cruise lines. In addition to such annual festivals like Calle Ocho Festival and Carnaval Miami, Miami is home to many entertainment venues, theaters, museums, parks and performing arts centers. The newest addition to the Miami arts scene is the Adrienne Arsht Center for the Performing Arts, the second-largest performing arts center in the United States after the Lincoln Center in New York City, and is the home of the Florida Grand Opera. Within it are the Ziff Ballet Opera House, the center's largest venue, the Knight Concert Hall, the Carnival Studio Theater and the Peacock Rehearsal Studio. The center attracts many large-scale operas, ballets, concerts, and musicals from around the world and is Florida's grandest performing arts center. Other performing arts venues in Miami include the Gusman Center for the Performing Arts, Coconut Grove Playhouse, Colony Theatre, Lincoln Theatre, New World Center, Actor's Playhouse at the Miracle Theatre, Jackie Gleason Theatre, Manuel Artime Theater, Ring Theatre, Playground Theatre, Wertheim Performing Arts Center, the Fair Expo Center and the Bayfront Park Amphitheater for outdoor music events. Miami's heavy-rail rapid transit system, Metrorail, is an elevated system comprising two lines and 23 stations on a 24.4-mile (39.3 km)-long line. Metrorail connects the urban western suburbs of Hialeah, Medley, and inner-city Miami with suburban The Roads, Coconut Grove, Coral Gables, South Miami and urban Kendall via the central business districts of Miami International Airport, the Civic Center, and Downtown. A free, elevated people mover, Metromover, operates 21 stations on three different lines in greater Downtown Miami, with a station at roughly every two blocks of Downtown and Brickell. Several expansion projects are being funded by a transit development sales tax surcharge throughout Miami-Dade County. Miami is also the headquarters and main production city of many of the world's largest television networks, record label companies, broadcasting companies and production facilities, such as Telemundo, TeleFutura, Galavisión, Mega TV, Univisión, Univision Communications, Inc., Universal Music Latin Entertainment, RCTV International and Sunbeam Television. In 2009, Univisión announced plans to build a new production studio in Miami, dubbed 'Univisión Studios'. Univisión Studios is currently headquartered in Miami, and will produce programming for all of Univisión Communications' television networks. The wet season begins some time in May, ending in mid-October. During this period, temperatures are in the mid 80s to low 90s (29–35 °C), accompanied by high humidity, though the heat is often relieved by afternoon thunderstorms or a sea breeze that develops off the Atlantic Ocean, which then allow lower temperatures, but conditions still remain very muggy. Much of the year's 55.9 inches (1,420 mm) of rainfall occurs during this period. Dewpoints in the warm months range from 71.9 °F (22.2 °C) in June to 73.7 °F (23.2 °C) in August."
Film_speed,"Upon exposure, the amount of light energy that reaches the film determines the effect upon the emulsion. If the brightness of the light is multiplied by a factor and the exposure of the film decreased by the same factor by varying the camera's shutter speed and aperture, so that the energy received is the same, the film will be developed to the same density. This rule is called reciprocity. The systems for determining the sensitivity for an emulsion are possible because reciprocity holds. In practice, reciprocity works reasonably well for normal photographic films for the range of exposures between 1/1000 second to 1/2 second. However, this relationship breaks down outside these limits, a phenomenon known as reciprocity failure. For digital photo cameras (""digital still cameras""), an exposure index (EI) rating—commonly called ISO setting—is specified by the manufacturer such that the sRGB image files produced by the camera will have a lightness similar to what would be obtained with film of the same EI rating at the same exposure. The usual design is that the camera's parameters for interpreting the sensor data values into sRGB values are fixed, and a number of different EI choices are accommodated by varying the sensor's signal gain in the analog realm, prior to conversion to digital. Some camera designs provide at least some EI choices by adjusting the sensor's signal gain in the digital realm. A few camera designs also provide EI adjustment through a choice of lightness parameters for the interpretation of sensor data values into sRGB; this variation allows different tradeoffs between the range of highlights that can be captured and the amount of noise introduced into the shadow areas of the photo. where  is the maximum possible exposure that does not lead to a clipped or bloomed camera output. Typically, the lower limit of the saturation speed is determined by the sensor itself, but with the gain of the amplifier between the sensor and the analog-to-digital converter, the saturation speed can be increased. The factor 78 is chosen such that exposure settings based on a standard light meter and an 18-percent reflective surface will result in an image with a grey level of 18%/√2 = 12.7% of saturation. The factor √2 indicates that there is half a stop of headroom to deal with specular reflections that would appear brighter than a 100% reflecting white surface. As in the Scheiner system, speeds were expressed in 'degrees'. Originally the sensitivity was written as a fraction with 'tenths' (for example ""18/10° DIN""), where the resultant value 1.8 represented the relative base 10 logarithm of the speed. 'Tenths' were later abandoned with DIN 4512:1957-11, and the example above would be written as ""18° DIN"". The degree symbol was finally dropped with DIN 4512:1961-10. This revision also saw significant changes in the definition of film speeds in order to accommodate then-recent changes in the American ASA PH2.5-1960 standard, so that film speeds of black-and-white negative film effectively would become doubled, that is, a film previously marked as ""18° DIN"" would now be labeled as ""21 DIN"" without emulsion changes. The Warnerke Standard Sensitometer consisted of a frame holding an opaque screen with an array of typically 25 numbered, gradually pigmented squares brought into contact with the photographic plate during a timed test exposure under a phosphorescent tablet excited before by the light of a burning Magnesium ribbon. The speed of the emulsion was then expressed in 'degrees' Warnerke (sometimes seen as Warn. or °W.) corresponding with the last number visible on the exposed plate after development and fixation. Each number represented an increase of 1/3 in speed, typical plate speeds were between 10° and 25° Warnerke at the time. The Recommended Exposure Index (REI) technique, new in the 2006 version of the standard, allows the manufacturer to specify a camera model’s EI choices arbitrarily. The choices are based solely on the manufacturer’s opinion of what EI values produce well-exposed sRGB images at the various sensor sensitivity settings. This is the only technique available under the standard for output formats that are not in the sRGB color space. This is also the only technique available under the standard when multi-zone metering (also called pattern metering) is used. The ASA standard underwent a major revision in 1960 with ASA PH2.5-1960, when the method to determine film speed was refined and previously applied safety factors against under-exposure were abandoned, effectively doubling the nominal speed of many black-and-white negative films. For example, an Ilford HP3 that had been rated at 200 ASA before 1960 was labeled 400 ASA afterwards without any change to the emulsion. Similar changes were applied to the DIN system with DIN 4512:1961-10 and the BS system with BS 1380:1963 in the following years. Digital cameras have far surpassed film in terms of sensitivity to light, with ISO equivalent speeds of up to 409,600, a number that is unfathomable in the realm of conventional film photography. Faster processors, as well as advances in software noise reduction techniques allow this type of processing to be executed the moment the photo is captured, allowing photographers to store images that have a higher level of refinement and would have been prohibitively time consuming to process with earlier generations of digital camera hardware. Despite these detailed standard definitions, cameras typically do not clearly indicate whether the user ""ISO"" setting refers to the noise-based speed, saturation-based speed, or the specified output sensitivity, or even some made-up number for marketing purposes. Because the 1998 version of ISO 12232 did not permit measurement of camera output that had lossy compression, it was not possible to correctly apply any of those measurements to cameras that did not produce sRGB files in an uncompressed format such as TIFF. Following the publication of CIPA DC-004 in 2006, Japanese manufacturers of digital still cameras are required to specify whether a sensitivity rating is REI or SOS.[citation needed] The system was later extended to cover larger ranges and some of its practical shortcomings were addressed by the Austrian scientist Josef Maria Eder (1855–1944) and Flemish-born botanist Walter Hecht (de) (1896–1960), (who, in 1919/1920, jointly developed their Eder–Hecht neutral wedge sensitometer measuring emulsion speeds in Eder–Hecht grades). Still, it remained difficult for manufactures to reliably determine film speeds, often only by comparing with competing products, so that an increasing number of modified semi-Scheiner-based systems started to spread, which no longer followed Scheiner's original procedures and thereby defeated the idea of comparability. The ISO standard ISO 12232:2006 gives digital still camera manufacturers a choice of five different techniques for determining the exposure index rating at each sensitivity setting provided by a particular camera model. Three of the techniques in ISO 12232:2006 are carried over from the 1998 version of the standard, while two new techniques allowing for measurement of JPEG output files are introduced from CIPA DC-004. Depending on the technique selected, the exposure index rating can depend on the sensor sensitivity, the sensor noise, and the appearance of the resulting image. The standard specifies the measurement of light sensitivity of the entire digital camera system and not of individual components such as digital sensors, although Kodak has reported using a variation to characterize the sensitivity of two of their sensors in 2001. The noise-based speed is defined as the exposure that will lead to a given signal-to-noise ratio on individual pixels. Two ratios are used, the 40:1 (""excellent image quality"") and the 10:1 (""acceptable image quality"") ratio. These ratios have been subjectively determined based on a resolution of 70 pixels per cm (178 DPI) when viewed at 25 cm (9.8 inch) distance. The signal-to-noise ratio is defined as the standard deviation of a weighted average of the luminance and color of individual pixels. The noise-based speed is mostly determined by the properties of the sensor and somewhat affected by the noise in the electronic gain and AD converter. General Electric switched to use the ASA scale in 1946. Meters manufactured since February 1946 were equipped with the ASA scale (labeled ""Exposure Index"") already. For some of the older meters with scales in ""Film Speed"" or ""Film Value"" (e.g. models DW-48, DW-49 as well as early DW-58 and GW-68 variants), replaceable hoods with ASA scales were available from the manufacturer. The company continued to publish recommended film values after that date, however, they were now aligned to the ASA scale. Based on earlier research work by Loyd Ancile Jones (1884–1954) of Kodak and inspired by the systems of Weston film speed ratings and General Electric film values, the American Standards Association (now named ANSI) defined a new method to determine and specify film speeds of black-and-white negative films in 1943. ASA Z38.2.1-1943 was revised in 1946 and 1947 before the standard grew into ASA PH2.5-1954. Originally, ASA values were frequently referred to as American standard speed numbers or ASA exposure-index numbers. (See also: Exposure Index (EI).) On an international level the German DIN 4512 system has been effectively superseded in the 1980s by ISO 6:1974, ISO 2240:1982, and ISO 5800:1979 where the same sensitivity is written in linear and logarithmic form as ""ISO 100/21°"" (now again with degree symbol). These ISO standards were subsequently adopted by DIN as well. Finally, the latest DIN 4512 revisions were replaced by corresponding ISO standards, DIN 4512-1:1993-05 by DIN ISO 6:1996-02 in September 2000, DIN 4512-4:1985-08 by DIN ISO 2240:1998-06 and DIN 4512-5:1990-11 by DIN ISO 5800:1998-06 both in July 2002. The CIPA DC-004 standard requires that Japanese manufacturers of digital still cameras use either the REI or SOS techniques, and DC-008 updates the Exif specification to differentiate between these values. Consequently, the three EI techniques carried over from ISO 12232:1998 are not widely used in recent camera models (approximately 2007 and later). As those earlier techniques did not allow for measurement from images produced with lossy compression, they cannot be used at all on cameras that produce images only in JPEG format. The standard specifies how speed ratings should be reported by the camera. If the noise-based speed (40:1) is higher than the saturation-based speed, the noise-based speed should be reported, rounded downwards to a standard value (e.g. 200, 250, 320, or 400). The rationale is that exposure according to the lower saturation-based speed would not result in a visibly better image. In addition, an exposure latitude can be specified, ranging from the saturation-based speed to the 10:1 noise-based speed. If the noise-based speed (40:1) is lower than the saturation-based speed, or undefined because of high noise, the saturation-based speed is specified, rounded upwards to a standard value, because using the noise-based speed would lead to overexposed images. The camera may also report the SOS-based speed (explicitly as being an SOS speed), rounded to the nearest standard speed rating. The Weston Cadet (model 852 introduced in 1949), Direct Reading (model 853 introduced 1954) and Master III (models 737 and S141.3 introduced in 1956) were the first in their line of exposure meters to switch and utilize the meanwhile established ASA scale instead. Other models used the original Weston scale up until ca. 1955. The company continued to publish Weston film ratings after 1955, but while their recommended values often differed slightly from the ASA film speeds found on film boxes, these newer Weston values were based on the ASA system and had to be converted for use with older Weston meters by subtracting 1/3 exposure stop as per Weston's recommendation. Vice versa, ""old"" Weston film speed ratings could be converted into ""new"" Westons and the ASA scale by adding the same amount, that is, a film rating of 100 Weston (up to 1955) corresponded with 125 ASA (as per ASA PH2.5-1954 and before). This conversion was not necessary on Weston meters manufactured and Weston film ratings published since 1956 due to their inherent use of the ASA system; however the changes of the ASA PH2.5-1960 revision may be taken into account when comparing with newer ASA or ISO values. Some high-speed black-and-white films, such as Ilford Delta 3200 and Kodak T-MAX P3200, are marketed with film speeds in excess of their true ISO speed as determined using the ISO testing method. For example, the Ilford product is actually an ISO 1000 film, according to its data sheet. The manufacturers do not indicate that the 3200 number is an ISO rating on their packaging. Kodak and Fuji also marketed E6 films designed for pushing (hence the ""P"" prefix), such as Ektachrome P800/1600 and Fujichrome P1600, both with a base speed of ISO 400. The DIN system, officially DIN standard 4512 by Deutsches Institut für Normung (but still named Deutscher Normenausschuß (DNA) at this time), was published in January 1934. It grew out of drafts for a standardized method of sensitometry put forward by Deutscher Normenausschuß für Phototechnik as proposed by the committee for sensitometry of the Deutsche Gesellschaft für photographische Forschung since 1930 and presented by Robert Luther (de) (1868–1945) and Emanuel Goldberg (1881–1970) at the influential VIII. International Congress of Photography (German: Internationaler Kongreß für wissenschaftliche und angewandte Photographie) held in Dresden from August 3 to 8, 1931. The Standard Output Sensitivity (SOS) technique, also new in the 2006 version of the standard, effectively specifies that the average level in the sRGB image must be 18% gray plus or minus 1/3 stop when the exposure is controlled by an automatic exposure control system calibrated per ISO 2721 and set to the EI with no exposure compensation. Because the output level is measured in the sRGB output from the camera, it is only applicable to sRGB images—typically JPEG—and not to output files in raw image format. It is not applicable when multi-zone metering is used. Film speed is found from a plot of optical density vs. log of exposure for the film, known as the D–log H curve or Hurter–Driffield curve. There typically are five regions in the curve: the base + fog, the toe, the linear region, the shoulder, and the overexposed region. For black-and-white negative film, the “speed point” m is the point on the curve where density exceeds the base + fog density by 0.1 when the negative is developed so that a point n where the log of exposure is 1.3 units greater than the exposure at point m has a density 0.8 greater than the density at point m. The exposure Hm, in lux-s, is that for point m when the specified contrast condition is satisfied. The ISO arithmetic speed is determined from: Before the advent of the ASA system, the system of Weston film speed ratings was introduced by Edward Faraday Weston (1878–1971) and his father Dr. Edward Weston (1850–1936), a British-born electrical engineer, industrialist and founder of the US-based Weston Electrical Instrument Corporation, with the Weston model 617, one of the earliest photo-electric exposure meters, in August 1932. The meter and film rating system were invented by William Nelson Goodwin, Jr., who worked for them and later received a Howard N. Potts Medal for his contributions to engineering. Film speed is used in the exposure equations to find the appropriate exposure parameters. Four variables are available to the photographer to obtain the desired effect: lighting, film speed, f-number (aperture size), and shutter speed (exposure time). The equation may be expressed as ratios, or, by taking the logarithm (base 2) of both sides, by addition, using the APEX system, in which every increment of 1 is a doubling of exposure; this increment is commonly known as a ""stop"". The effective f-number is proportional to the ratio between the lens focal length and aperture diameter, the diameter itself being proportional to the square root of the aperture area. Thus, a lens set to f/1.4 allows twice as much light to strike the focal plane as a lens set to f/2. Therefore, each f-number factor of the square root of two (approximately 1.4) is also a stop, so lenses are typically marked in that progression: f/1.4, 2, 2.8, 4, 5.6, 8, 11, 16, 22, 32, etc. Relatively insensitive film, with a correspondingly lower speed index, requires more exposure to light to produce the same image density as a more sensitive film, and is thus commonly termed a slow film. Highly sensitive films are correspondingly termed fast films. In both digital and film photography, the reduction of exposure corresponding to use of higher sensitivities generally leads to reduced image quality (via coarser film grain or higher image noise of other types). In short, the higher the sensitivity, the grainier the image will be. Ultimately sensitivity is limited by the quantum efficiency of the film or sensor. The Scheinergrade (Sch.) system was devised by the German astronomer Julius Scheiner (1858–1913) in 1894 originally as a method of comparing the speeds of plates used for astronomical photography. Scheiner's system rated the speed of a plate by the least exposure to produce a visible darkening upon development. Speed was expressed in degrees Scheiner, originally ranging from 1° Sch. to 20° Sch., where an increment of 19° Sch. corresponded to a hundredfold increase in sensitivity, which meant that an increment of 3° Sch. came close to a doubling of sensitivity."
Normans,"Before Rollo's arrival, its populations did not differ from Picardy or the Île-de-France, which were considered ""Frankish"". Earlier Viking settlers had begun arriving in the 880s, but were divided between colonies in the east (Roumois and Pays de Caux) around the low Seine valley and in the west in the Cotentin Peninsula, and were separated by traditional pagii, where the population remained about the same with almost no foreign settlers. Rollo's contingents who raided and ultimately settled Normandy and parts of the Atlantic coast included Danes, Norwegians, Norse–Gaels, Orkney Vikings, possibly Swedes, and Anglo-Danes from the English Danelaw under Norse control. The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (""Norman"" comes from ""Norseman"") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries. Between 1402 and 1405, the expedition led by the Norman noble Jean de Bethencourt and the Poitevine Gadifer de la Salle conquered the Canarian islands of Lanzarote, Fuerteventura and El Hierro off the Atlantic coast of Africa. Their troops were gathered in Normandy, Gascony and were later reinforced by Castilian colonists. The English name ""Normans"" comes from the French words Normans/Normanz, plural of Normant, modern French normand, which is itself borrowed from Old Low Franconian Nortmann ""Northman"" or directly from Old Norse Norðmaðr, Latinized variously as Nortmannus, Normannus, or Nordmannus (recorded in Medieval Latin, 9th century) to mean ""Norseman, Viking"". The Normans had a profound effect on Irish culture and history after their invasion at Bannow Bay in 1169. Initially the Normans maintained a distinct culture and ethnicity. Yet, with time, they came to be subsumed into Irish culture to the point that it has been said that they became ""more Irish than the Irish themselves."" The Normans settled mostly in an area in the east of Ireland, later known as the Pale, and also built many fine castles and settlements, including Trim Castle and Dublin Castle. Both cultures intermixed, borrowing from each other's language, culture and outlook. Norman descendants today can be recognised by their surnames. Names such as French, (De) Roche, Devereux, D'Arcy, Treacy and Lacy are particularly common in the southeast of Ireland, especially in the southern part of County Wexford where the first Norman settlements were established. Other Norman names such as Furlong predominate there. Another common Norman-Irish name was Morell (Murrell) derived from the French Norman name Morel. Other names beginning with Fitz (from the Norman for son) indicate Norman ancestry. These included Fitzgerald, FitzGibbons (Gibbons) dynasty, Fitzmaurice. Other families bearing such surnames as Barry (de Barra) and De Búrca (Burke) are also of Norman extraction. The customary law of Normandy was developed between the 10th and 13th centuries and survives today through the legal systems of Jersey and Guernsey in the Channel Islands. Norman customary law was transcribed in two customaries in Latin by two judges for use by them and their colleagues: These are the Très ancien coutumier (Very ancient customary), authored between 1200 and 1245; and the Grand coutumier de Normandie (Great customary of Normandy, originally Summa de legibus Normanniae in curia laïcali), authored between 1235 and 1245. In the visual arts, the Normans did not have the rich and distinctive traditions of the cultures they conquered. However, in the early 11th century the dukes began a programme of church reform, encouraging the Cluniac reform of monasteries and patronising intellectual pursuits, especially the proliferation of scriptoria and the reconstitution of a compilation of lost illuminated manuscripts. The church was utilised by the dukes as a unifying force for their disparate duchy. The chief monasteries taking part in this ""renaissance"" of Norman art and scholarship were Mont-Saint-Michel, Fécamp, Jumièges, Bec, Saint-Ouen, Saint-Evroul, and Saint-Wandrille. These centres were in contact with the so-called ""Winchester school"", which channeled a pure Carolingian artistic tradition to Normandy. In the final decade of the 11th and first of the 12th century, Normandy experienced a golden age of illustrated manuscripts, but it was brief and the major scriptoria of Normandy ceased to function after the midpoint of the century. The legendary religious zeal of the Normans was exercised in religious wars long before the First Crusade carved out a Norman principality in Antioch. They were major foreign participants in the Reconquista in Iberia. In 1018, Roger de Tosny travelled to the Iberian Peninsula to carve out a state for himself from Moorish lands, but failed. In 1064, during the War of Barbastro, William of Montreuil led the papal army and took a huge booty. Bethencourt took the title of King of the Canary Islands, as vassal to Henry III of Castile. In 1418, Jean's nephew Maciot de Bethencourt sold the rights to the islands to Enrique Pérez de Guzmán, 2nd Count de Niebla. In April 1191 Richard the Lion-hearted left Messina with a large fleet in order to reach Acre. But a storm dispersed the fleet. After some searching, it was discovered that the boat carrying his sister and his fiancée Berengaria was anchored on the south coast of Cyprus, together with the wrecks of several other ships, including the treasure ship. Survivors of the wrecks had been taken prisoner by the island's despot Isaac Komnenos. On 1 May 1191, Richard's fleet arrived in the port of Limassol on Cyprus. He ordered Isaac to release the prisoners and the treasure. Isaac refused, so Richard landed his troops and took Limassol. The further decline of Byzantine state-of-affairs paved the road to a third attack in 1185, when a large Norman army invaded Dyrrachium, owing to the betrayal of high Byzantine officials. Some time later, Dyrrachium—one of the most important naval bases of the Adriatic—fell again to Byzantine hands. Various princes of the Holy Land arrived in Limassol at the same time, in particular Guy de Lusignan. All declared their support for Richard provided that he support Guy against his rival Conrad of Montferrat. The local barons abandoned Isaac, who considered making peace with Richard, joining him on the crusade, and offering his daughter in marriage to the person named by Richard. But Isaac changed his mind and tried to escape. Richard then proceeded to conquer the whole island, his troops being led by Guy de Lusignan. Isaac surrendered and was confined with silver chains, because Richard had promised that he would not place him in irons. By 1 June, Richard had conquered the whole island. His exploit was well publicized and contributed to his reputation; he also derived significant financial gains from the conquest of the island. Richard left for Acre on 5 June, with his allies. Before his departure, he named two of his Norman generals, Richard de Camville and Robert de Thornham, as governors of Cyprus. Robert Guiscard, an other Norman adventurer previously elevated to the dignity of count of Apulia as the result of his military successes, ultimately drove the Byzantines out of southern Italy. Having obtained the consent of pope Gregory VII and acting as his vassal, Robert continued his campaign conquering the Balkan peninsula as a foothold for western feudal lords and the Catholic Church. After allying himself with Croatia and the Catholic cities of Dalmatia, in 1081 he led an army of 30,000 men in 300 ships landing on the southern shores of Albania, capturing Valona, Kanina, Jericho (Orikumi), and reaching Butrint after numerous pillages. They joined the fleet that had previously conquered Corfu and attacked Dyrrachium from land and sea, devastating everything along the way. Under these harsh circumstances, the locals accepted the call of emperor Alexius I Comnenus to join forces with the Byzantines against the Normans. The Albanian forces could not take part in the ensuing battle because it had started before their arrival. Immediately before the battle, the Venetian fleet had secured a victory in the coast surrounding the city. Forced to retreat, Alexius ceded the command to a high Albanian official named Comiscortes in the service of Byzantium. The city's garrison resisted until February 1082, when Dyrrachium was betrayed to the Normans by the Venetian and Amalfitan merchants who had settled there. The Normans were now free to penetrate into the hinterland; they took Ioannina and some minor cities in southwestern Macedonia and Thessaly before appearing at the gates of Thessalonica. Dissension among the high ranks coerced the Normans to retreat to Italy. They lost Dyrrachium, Valona, and Butrint in 1085, after the death of Robert. Eventually, the Normans merged with the natives, combining languages and traditions. In the course of the Hundred Years' War, the Norman aristocracy often identified themselves as English. The Anglo-Norman language became distinct from the Latin language, something that was the subject of some humour by Geoffrey Chaucer. The Anglo-Norman language was eventually absorbed into the Anglo-Saxon language of their subjects (see Old English) and influenced it, helping (along with the Norse language of the earlier Anglo-Norse settlers and the Latin used by the church) in the development of Middle English. It in turn evolved into Modern English. The Normans thereafter adopted the growing feudal doctrines of the rest of France, and worked them into a functional hierarchical system in both Normandy and in England. The new Norman rulers were culturally and ethnically distinct from the old French aristocracy, most of whom traced their lineage to Franks of the Carolingian dynasty. Most Norman knights remained poor and land-hungry, and by 1066 Normandy had been exporting fighting horsemen for more than a generation. Many Normans of Italy, France and England eventually served as avid Crusaders under the Italo-Norman prince Bohemund I and the Anglo-Norman king Richard the Lion-Heart. At Saint Evroul, a tradition of singing had developed and the choir achieved fame in Normandy. Under the Norman abbot Robert de Grantmesnil, several monks of Saint-Evroul fled to southern Italy, where they were patronised by Robert Guiscard and established a Latin monastery at Sant'Eufemia. There they continued the tradition of singing. The Normans were in contact with England from an early date. Not only were their original Viking brethren still ravaging the English coasts, they occupied most of the important ports opposite England across the English Channel. This relationship eventually produced closer ties of blood through the marriage of Emma, sister of Duke Richard II of Normandy, and King Ethelred II of England. Because of this, Ethelred fled to Normandy in 1013, when he was forced from his kingdom by Sweyn Forkbeard. His stay in Normandy (until 1016) influenced him and his sons by Emma, who stayed in Normandy after Cnut the Great's conquest of the isle. Normandy was the site of several important developments in the history of classical music in the 11th century. Fécamp Abbey and Saint-Evroul Abbey were centres of musical production and education. At Fécamp, under two Italian abbots, William of Volpiano and John of Ravenna, the system of denoting notes by letters was developed and taught. It is still the most common form of pitch representation in English- and German-speaking countries today. Also at Fécamp, the staff, around which neumes were oriented, was first developed and taught in the 11th century. Under the German abbot Isembard, La Trinité-du-Mont became a centre of musical composition. In England, the period of Norman architecture immediately succeeds that of the Anglo-Saxon and precedes the Early Gothic. In southern Italy, the Normans incorporated elements of Islamic, Lombard, and Byzantine building techniques into their own, initiating a unique style known as Norman-Arab architecture within the Kingdom of Sicily. The French Wars of Religion in the 16th century and French Revolution in the 18th successively destroyed much of what existed in the way of the architectural and artistic remnant of this Norman creativity. The former, with their violence, caused the wanton destruction of many Norman edifices; the latter, with its assault on religion, caused the purposeful destruction of religious objects of any type, and its destabilisation of society resulted in rampant pillaging. Some Normans joined Turkish forces to aid in the destruction of the Armenians vassal-states of Sassoun and Taron in far eastern Anatolia. Later, many took up service with the Armenian state further south in Cilicia and the Taurus Mountains. A Norman named Oursel led a force of ""Franks"" into the upper Euphrates valley in northern Syria. From 1073 to 1074, 8,000 of the 20,000 troops of the Armenian general Philaretus Brachamius were Normans—formerly of Oursel—led by Raimbaud. They even lent their ethnicity to the name of their castle: Afranji, meaning ""Franks."" The known trade between Amalfi and Antioch and between Bari and Tarsus may be related to the presence of Italo-Normans in those cities while Amalfi and Bari were under Norman rule in Italy. The conquest of Cyprus by the Anglo-Norman forces of the Third Crusade opened a new chapter in the history of the island, which would be under Western European domination for the following 380 years. Although not part of a planned operation, the conquest had much more permanent results than initially expected. Normans came into Scotland, building castles and founding noble families who would provide some future kings, such as Robert the Bruce, as well as founding a considerable number of the Scottish clans. King David I of Scotland, whose elder brother Alexander I had married Sybilla of Normandy, was instrumental in introducing Normans and Norman culture to Scotland, part of the process some scholars call the ""Davidian Revolution"". Having spent time at the court of Henry I of England (married to David's sister Maud of Scotland), and needing them to wrestle the kingdom from his half-brother Máel Coluim mac Alaxandair, David had to reward many with lands. The process was continued under David's successors, most intensely of all under William the Lion. The Norman-derived feudal system was applied in varying degrees to most of Scotland. Scottish families of the names Bruce, Gray, Ramsay, Fraser, Ogilvie, Montgomery, Sinclair, Pollock, Burnard, Douglas and Gordon to name but a few, and including the later royal House of Stewart, can all be traced back to Norman ancestry. In 1066, Duke William II of Normandy conquered England killing King Harold II at the Battle of Hastings. The invading Normans and their descendants replaced the Anglo-Saxons as the ruling class of England. The nobility of England were part of a single Normans culture and many had lands on both sides of the channel. Early Norman kings of England, as Dukes of Normandy, owed homage to the King of France for their land on the continent. They considered England to be their most important holding (it brought with it the title of King—an important status symbol). Even before the Norman Conquest of England, the Normans had come into contact with Wales. Edward the Confessor had set up the aforementioned Ralph as earl of Hereford and charged him with defending the Marches and warring with the Welsh. In these original ventures, the Normans failed to make any headway into Wales. When finally Edward the Confessor returned from his father's refuge in 1041, at the invitation of his half-brother Harthacnut, he brought with him a Norman-educated mind. He also brought many Norman counsellors and fighters, some of whom established an English cavalry force. This concept never really took root, but it is a typical example of the attitudes of Edward. He appointed Robert of Jumièges archbishop of Canterbury and made Ralph the Timid earl of Hereford. He invited his brother-in-law Eustace II, Count of Boulogne to his court in 1051, an event which resulted in the greatest of early conflicts between Saxon and Norman and ultimately resulted in the exile of Earl Godwin of Wessex. Subsequent to the Conquest, however, the Marches came completely under the dominance of William's most trusted Norman barons, including Bernard de Neufmarché, Roger of Montgomery in Shropshire and Hugh Lupus in Cheshire. These Normans began a long period of slow conquest during which almost all of Wales was at some point subject to Norman interference. Norman words, such as baron (barwn), first entered Welsh at that time. By far the most famous work of Norman art is the Bayeux Tapestry, which is not a tapestry but a work of embroidery. It was commissioned by Odo, the Bishop of Bayeux and first Earl of Kent, employing natives from Kent who were learned in the Nordic traditions imported in the previous half century by the Danish Vikings. One of the first Norman mercenaries to serve as a Byzantine general was Hervé in the 1050s. By then however, there were already Norman mercenaries serving as far away as Trebizond and Georgia. They were based at Malatya and Edessa, under the Byzantine duke of Antioch, Isaac Komnenos. In the 1060s, Robert Crispin led the Normans of Edessa against the Turks. Roussel de Bailleul even tried to carve out an independent state in Asia Minor with support from the local population, but he was stopped by the Byzantine general Alexius Komnenos. In Britain, Norman art primarily survives as stonework or metalwork, such as capitals and baptismal fonts. In southern Italy, however, Norman artwork survives plentifully in forms strongly influenced by its Greek, Lombard, and Arab forebears. Of the royal regalia preserved in Palermo, the crown is Byzantine in style and the coronation cloak is of Arab craftsmanship with Arabic inscriptions. Many churches preserve sculptured fonts, capitals, and more importantly mosaics, which were common in Norman Italy and drew heavily on the Greek heritage. Lombard Salerno was a centre of ivorywork in the 11th century and this continued under Norman domination. Finally should be noted the intercourse between French Crusaders traveling to the Holy Land who brought with them French artefacts with which to gift the churches at which they stopped in southern Italy amongst their Norman cousins. For this reason many south Italian churches preserve works from France alongside their native pieces. In the course of the 10th century, the initially destructive incursions of Norse war bands into the rivers of France evolved into more permanent encampments that included local women and personal property. The Duchy of Normandy, which began in 911 as a fiefdom, was established by the treaty of Saint-Clair-sur-Epte between King Charles III of West Francia and the famed Viking ruler Rollo, and was situated in the former Frankish kingdom of Neustria. The treaty offered Rollo and his men the French lands between the river Epte and the Atlantic coast in exchange for their protection against further Viking incursions. The area corresponded to the northern part of present-day Upper Normandy down to the river Seine, but the Duchy would eventually extend west beyond the Seine. The territory was roughly equivalent to the old province of Rouen, and reproduced the Roman administrative structure of Gallia Lugdunensis II (part of the former Gallia Lugdunensis). The descendants of Rollo's Vikings and their Frankish wives would replace the Norse religion and Old Norse language with Catholicism (Christianity) and the Gallo-Romance language of the local people, blending their maternal Frankish heritage with Old Norse traditions and customs to synthesize a unique ""Norman"" culture in the north of France. The Norman language was forged by the adoption of the indigenous langue d'oïl branch of Romance by a Norse-speaking ruling class, and it developed into the regional language that survives today. Soon after the Normans began to enter Italy, they entered the Byzantine Empire and then Armenia, fighting against the Pechenegs, the Bulgars, and especially the Seljuk Turks. Norman mercenaries were first encouraged to come to the south by the Lombards to act against the Byzantines, but they soon fought in Byzantine service in Sicily. They were prominent alongside Varangian and Lombard contingents in the Sicilian campaign of George Maniaces in 1038–40. There is debate whether the Normans in Greek service actually were from Norman Italy, and it now seems likely only a few came from there. It is also unknown how many of the ""Franks"", as the Byzantines called them, were Normans and not other Frenchmen. One of the claimants of the English throne opposing William the Conqueror, Edgar Atheling, eventually fled to Scotland. King Malcolm III of Scotland married Edgar's sister Margaret, and came into opposition to William who had already disputed Scotland's southern borders. William invaded Scotland in 1072, riding as far as Abernethy where he met up with his fleet of ships. Malcolm submitted, paid homage to William and surrendered his son Duncan as a hostage, beginning a series of arguments as to whether the Scottish Crown owed allegiance to the King of England. The Norman dynasty had a major political, cultural and military impact on medieval Europe and even the Near East. The Normans were famed for their martial spirit and eventually for their Christian piety, becoming exponents of the Catholic orthodoxy into which they assimilated. They adopted the Gallo-Romance language of the Frankish land they settled, their dialect becoming known as Norman, Normaund or Norman French, an important literary language. The Duchy of Normandy, which they formed by treaty with the French crown, was a great fief of medieval France, and under Richard I of Normandy was forged into a cohesive and formidable principality in feudal tenure. The Normans are noted both for their culture, such as their unique Romanesque architecture and musical traditions, and for their significant military accomplishments and innovations. Norman adventurers founded the Kingdom of Sicily under Roger II after conquering southern Italy on the Saracens and Byzantines, and an expedition on behalf of their duke, William the Conqueror, led to the Norman conquest of England at the Battle of Hastings in 1066. Norman cultural and military influence spread from these new European centres to the Crusader states of the Near East, where their prince Bohemond I founded the Principality of Antioch in the Levant, to Scotland and Wales in Great Britain, to Ireland, and to the coasts of north Africa and the Canary Islands. In 1096, Crusaders passing by the siege of Amalfi were joined by Bohemond of Taranto and his nephew Tancred with an army of Italo-Normans. Bohemond was the de facto leader of the Crusade during its passage through Asia Minor. After the successful Siege of Antioch in 1097, Bohemond began carving out an independent principality around that city. Tancred was instrumental in the conquest of Jerusalem and he worked for the expansion of the Crusader kingdom in Transjordan and the region of Galilee.[citation needed] Norman architecture typically stands out as a new stage in the architectural history of the regions they subdued. They spread a unique Romanesque idiom to England and Italy, and the encastellation of these regions with keeps in their north French style fundamentally altered the military landscape. Their style was characterised by rounded arches, particularly over windows and doorways, and massive proportions. A few years after the First Crusade, in 1107, the Normans under the command of Bohemond, Robert's son, landed in Valona and besieged Dyrrachium using the most sophisticated military equipment of the time, but to no avail. Meanwhile, they occupied Petrela, the citadel of Mili at the banks of the river Deabolis, Gllavenica (Ballsh), Kanina and Jericho. This time, the Albanians sided with the Normans, dissatisfied by the heavy taxes the Byzantines had imposed upon them. With their help, the Normans secured the Arbanon passes and opened their way to Dibra. The lack of supplies, disease and Byzantine resistance forced Bohemond to retreat from his campaign and sign a peace treaty with the Byzantines in the city of Deabolis. Several families of Byzantine Greece were of Norman mercenary origin during the period of the Comnenian Restoration, when Byzantine emperors were seeking out western European warriors. The Raoulii were descended from an Italo-Norman named Raoul, the Petraliphae were descended from a Pierre d'Aulps, and that group of Albanian clans known as the Maniakates were descended from Normans who served under George Maniaces in the Sicilian expedition of 1038."
Canon_law,"In Presbyterian and Reformed churches, canon law is known as ""practice and procedure"" or ""church order"", and includes the church's laws respecting its government, discipline, legal practice and worship. It is a fully developed legal system, with all the necessary elements: courts, lawyers, judges, a fully articulated legal code principles of legal interpretation, and coercive penalties, though it lacks civilly-binding force in most secular jurisdictions. The academic degrees in canon law are the J.C.B. (Juris Canonici Baccalaureatus, Bachelor of Canon Law, normally taken as a graduate degree), J.C.L. (Juris Canonici Licentiatus, Licentiate of Canon Law) and the J.C.D. (Juris Canonici Doctor, Doctor of Canon Law). Because of its specialized nature, advanced degrees in civil law or theology are normal prerequisites for the study of canon law. Canonical jurisprudential theory generally follows the principles of Aristotelian-Thomistic legal philosophy. While the term ""law"" is never explicitly defined in the Code, the Catechism of the Catholic Church cites Aquinas in defining law as ""...an ordinance of reason for the common good, promulgated by the one who is in charge of the community"" and reformulates it as ""...a rule of conduct enacted by competent authority for the sake of the common good."" The Greek-speaking Orthodox have collected canons and commentaries upon them in a work known as the Pēdálion (Greek: Πηδάλιον, ""Rudder""), so named because it is meant to ""steer"" the Church. The Orthodox Christian tradition in general treats its canons more as guidelines than as laws, the bishops adjusting them to cultural and other local circumstances. Some Orthodox canon scholars point out that, had the Ecumenical Councils (which deliberated in Greek) meant for the canons to be used as laws, they would have called them nómoi/νόμοι (laws) rather than kanónes/κανόνες (rules), but almost all Orthodox conform to them. The dogmatic decisions of the Councils, though, are to be obeyed rather than to be treated as guidelines, since they are essential for the Church's unity. The institutions and practices of canon law paralleled the legal development of much of Europe, and consequently both modern civil law and common law (legal system) bear the influences of canon law. Edson Luiz Sampel, a Brazilian expert in canon law, says that canon law is contained in the genesis of various institutes of civil law, such as the law in continental Europe and Latin American countries. Sampel explains that canon law has significant influence in contemporary society. In the Church of England, the ecclesiastical courts that formerly decided many matters such as disputes relating to marriage, divorce, wills, and defamation, still have jurisdiction of certain church-related matters (e.g. discipline of clergy, alteration of church property, and issues related to churchyards). Their separate status dates back to the 12th century when the Normans split them off from the mixed secular/religious county and local courts used by the Saxons. In contrast to the other courts of England the law used in ecclesiastical matters is at least partially a civil law system, not common law, although heavily governed by parliamentary statutes. Since the Reformation, ecclesiastical courts in England have been royal courts. The teaching of canon law at the Universities of Oxford and Cambridge was abrogated by Henry VIII; thereafter practitioners in the ecclesiastical courts were trained in civil law, receiving a Doctor of Civil Law (D.C.L.) degree from Oxford, or a Doctor of Laws (LL.D.) degree from Cambridge. Such lawyers (called ""doctors"" and ""civilians"") were centered at ""Doctors Commons"", a few streets south of St Paul's Cathedral in London, where they monopolized probate, matrimonial, and admiralty cases until their jurisdiction was removed to the common law courts in the mid-19th century. The Roman Catholic Church canon law also includes the main five rites (groups) of churches which are in full union with the Roman Catholic Church and the Supreme Pontiff: Canon law is the body of laws and regulations made by ecclesiastical authority (Church leadership), for the government of a Christian organization or church and its members. It is the internal ecclesiastical law governing the Catholic Church (both Latin Church and Eastern Catholic Churches), the Eastern and Oriental Orthodox churches, and the individual national churches within the Anglican Communion. The way that such church law is legislated, interpreted and at times adjudicated varies widely among these three bodies of churches. In all three traditions, a canon was originally a rule adopted by a church council; these canons formed the foundation of canon law. The first Code of Canon Law, 1917, was mostly for the Roman Rite, with limited application to the Eastern Churches. After the Second Vatican Council, (1962 - 1965), another edition was published specifically for the Roman Rite in 1983. Most recently, 1990, the Vatican produced the Code of Canons of the Eastern Churches which became the 1st code of Eastern Catholic Canon Law. Currently, (2004), there are principles of canon law common to the churches within the Anglican Communion; their existence can be factually established; each province or church contributes through its own legal system to the principles of canon law common within the Communion; these principles have a strong persuasive authority and are fundamental to the self-understanding of each of the churches of the Communion; these principles have a living force, and contain in themselves the possibility of further development; and the existence of these principles both demonstrates unity and promotes unity within the Anglican Communion.  The law of the Eastern Catholic Churches in full union with Rome was in much the same state as that of the Latin or Western Church before 1917; much more diversity in legislation existed in the various Eastern Catholic Churches. Each had its own special law, in which custom still played an important part. In 1929 Pius XI informed the Eastern Churches of his intention to work out a Code for the whole of the Eastern Church. The publication of these Codes for the Eastern Churches regarding the law of persons was made between 1949 through 1958 but finalized nearly 30 years later. The Book of Concord is the historic doctrinal statement of the Lutheran Church, consisting of ten credal documents recognized as authoritative in Lutheranism since the 16th century. However, the Book of Concord is a confessional document (stating orthodox belief) rather than a book of ecclesiastical rules or discipline, like canon law. Each Lutheran national church establishes its own system of church order and discipline, though these are referred to as ""canons."" Greek kanon / Ancient Greek: κανών, Arabic Qanun / قانون, Hebrew kaneh / קנה, ""straight""; a rule, code, standard, or measure; the root meaning in all these languages is ""reed"" (cf. the Romance-language ancestors of the English word ""cane""). Much of the legislative style was adapted from the Roman Law Code of Justinian. As a result, Roman ecclesiastical courts tend to follow the Roman Law style of continental Europe with some variation, featuring collegiate panels of judges and an investigative form of proceeding, called ""inquisitorial"", from the Latin ""inquirere"", to enquire. This is in contrast to the adversarial form of proceeding found in the common law system of English and U.S. law, which features such things as juries and single judges. The Apostolic Canons or Ecclesiastical Canons of the Same Holy Apostles is a collection of ancient ecclesiastical decrees (eighty-five in the Eastern, fifty in the Western Church) concerning the government and discipline of the Early Christian Church, incorporated with the Apostolic Constitutions which are part of the Ante-Nicene Fathers In the fourth century the First Council of Nicaea (325) calls canons the disciplinary measures of the Church: the term canon, κανὠν, means in Greek, a rule. There is a very early distinction between the rules enacted by the Church and the legislative measures taken by the State called leges, Latin for laws. Other churches in the Anglican Communion around the world (e.g., the Episcopal Church in the United States, and the Anglican Church of Canada) still function under their own private systems of canon law. In the Catholic Church, canon law is the system of laws and legal principles made and enforced by the Church's hierarchical authorities to regulate its external organization and government and to order and direct the activities of Catholics toward the mission of the Church. In the Roman Church, universal positive ecclesiastical laws, based upon either immutable divine and natural law, or changeable circumstantial and merely positive law, derive formal authority and promulgation from the office of pope, who as Supreme Pontiff possesses the totality of legislative, executive, and judicial power in his person. The actual subject material of the canons is not just doctrinal or moral in nature, but all-encompassing of the human condition. The history of Latin canon law can be divided into four periods: the jus antiquum, the jus novum, the jus novissimum and the Code of Canon Law. In relation to the Code, history can be divided into the jus vetus (all law before the Code) and the jus novum (the law of the Code, or jus codicis). The Catholic Church has what is claimed to be the oldest continuously functioning internal legal system in Western Europe, much later than Roman law but predating the evolution of modern European civil law traditions. What began with rules (""canons"") adopted by the Apostles at the Council of Jerusalem in the first century has developed into a highly complex legal system encapsulating not just norms of the New Testament, but some elements of the Hebrew (Old Testament), Roman, Visigothic, Saxon, and Celtic legal traditions. Roman canon law had been criticized by the Presbyterians as early as 1572 in the Admonition to Parliament. The protest centered on the standard defense that canon law could be retained so long as it did not contradict the civil law. According to Polly Ha, the Reformed Church Government refuted this claiming that the bishops had been enforcing canon law for 1500 years. The canon law of the Eastern Catholic Churches, which had developed some different disciplines and practices, underwent its own process of codification, resulting in the Code of Canons of the Eastern Churches promulgated in 1990 by Pope John Paul II."
Royal_Institute_of_British_Architects,"In the nineteenth and twentieth centuries the RIBA and its members had a leading part in the promotion of architectural education in the United Kingdom, including the establishment of the Architects' Registration Council of the United Kingdom (ARCUK) and the Board of Architectural Education under the Architects (Registration) Acts, 1931 to 1938. A member of the RIBA, Lionel Bailey Budden, then Associate Professor in the Liverpool University School of Architecture, had contributed the article on Architectural Education published in the fourteenth edition of the Encyclopædia Britannica (1929). His School, Liverpool, was one of the twenty schools named for the purpose of constituting the statutory Board of Architectural Education when the 1931 Act was passed. In 2007, RIBA called for minimum space standards in newly built British houses after research was published suggesting that British houses were falling behind other European countries. ""The average new home sold to people today is significantly smaller than that built in the 1920s... We're way behind the rest of Europe—even densely populated Holland has better proportioned houses than are being built in the country. So let's see minimum space standards for all new homes,"" said RIBA president Jack Pringle. It was granted its Royal Charter in 1837 under King William IV. Supplemental Charters of 1887, 1909 and 1925 were replaced by a single Charter in 1971, and there have been minor amendments since then. The Institute also maintains a dozen regional offices around the United Kingdom, it opened its first regional office for the East of England at Cambridge in 1966. Since 2004, through the V&A + RIBA Architecture Partnership, the RIBA and V&A have worked together to promote the understanding and enjoyment of architecture. Soon after the passing of the 1931 Act, in the book published on the occasion of the Institute's centenary celebration in 1934, Harry Barnes, FRIBA, Chairman of the Registration Committee, mentioned that ARCUK could not be a rival of any architectural association, least of all the RIBA, given the way ARCUK was constituted. Barnes commented that the Act's purpose was not protecting the architectural profession, and that the legitimate interests of the profession were best served by the (then) architectural associations in which some 80 per cent of those practising architecture were to be found. The content of the acts, particularly section 1 (1) of the amending act of 1938, shows the importance which was then attached to giving architects the responsibility of superintending or supervising the building works of local authorities (for housing and other projects), rather than persons professionally qualified only as municipal or other engineers. By the 1970s another issue had emerged affecting education for qualification and registration for practice as an architect, due to the obligation imposed on the United Kingdom and other European governments to comply with European Union Directives concerning mutual recognition of professional qualifications in favour of equal standards across borders, in furtherance of the policy for a single market of the European Union. This led to proposals for reconstituting ARCUK. Eventually, in the 1990s, before proceeding, the government issued a consultation paper ""Reform of Architects Registration"" (1994). The change of name to ""Architects Registration Board"" was one of the proposals which was later enacted in the Housing Grants, Construction and Regeneration Act 1996 and reenacted as the Architects Act 1997; another was the abolition of the ARCUK Board of Architectural Education. Its services include RIBA Insight, RIBA Appointments, and RIBA Publishing. It publishes the RIBA Product Selector and RIBA Journal. In Newcastle is the NBS, the National Building Specification, which has 130 staff and deals with the building regulations and the Construction Information Service. RIBA Bookshops, which operates online and at 66 Portland Place, is also part of RIBA Enterprises. RIBA Visiting Boards continue to assess courses for exemption from the RIBA's examinations in architecture. Under arrangements made in 2011 the validation criteria are jointly held by the RIBA and the Architects Registration Board, but unlike the ARB, the RIBA also validates courses outside the UK. The design of the Institute's Mycenean lions medal and the motto ‘Usui civium, decori urbium' has been attributed to Thomas Leverton Donaldson, who had been honorary secretary until 1839. The RIBA Guide to its Archive and History (Angela Mace,1986) records that the first official version of this badge was used as a bookplate for the Institute's library and publications from 1835 to 1891, when it was redesigned by J.H.Metcalfe. It was again redesigned in 1931 by Eric Gill and in 1960 by Joan Hassall. The description in the 1837 by-laws was: ""gules, two lions rampant guardant or, supporting a column marked with lines chevron, proper, all standing on a base of the same; a garter surrounding the whole with the inscription Institute of British Architects, anno salutis MDCCCXXXIV; above a mural crown proper, and beneath the motto Usui civium decori urbium "". The library is based at two public sites: the Reading Room at the RIBA's headquarters, 66 Portland Place, London; and the RIBA Architecture Study Rooms in the Henry Cole Wing of the V&A. The Reading Room, designed by the building's architect George Grey Wornum and his wife Miriam, retains its original 1934 Art Deco interior with open bookshelves, original furniture and double-height central space. The study rooms, opened in 2004, were designed by Wright & Wright. The library is funded entirely by the RIBA but it is open to the public without charge. It operates a free education programme aimed at students, education groups and families, and an information service for RIBA members and the public through the RIBA Information Centre. RIBA runs many awards including the Stirling Prize for the best new building of the year, the Royal Gold Medal (first awarded in 1848), which honours a distinguished body of work, and the Stephen Lawrence Prize for projects with a construction budget of less than £500,000. The RIBA also awards the President's Medals for student work, which are regarded as the most prestigious awards in architectural education, and the RIBA President's Awards for Research. The RIBA European Award was inaugurated in 2005 for work in the European Union, outside the UK. The RIBA National Award and the RIBA International Award were established in 2007. Since 1966, the RIBA also judges regional awards which are presented locally in the UK regions (East, East Midlands, London, North East, North West, Northern Ireland, Scotland, South/South East, South West/Wessex, Wales, West Midlands and Yorkshire). In addition to the Architects Registration Board, the RIBA provides accreditation to architecture schools in the UK under a course validation procedure. It also provides validation to international courses without input from the ARB. After the grant of the royal charter it had become known as the Royal Institute of British Architects in London, eventually dropping the reference to London in 1892. In 1934, it moved to its current headquarters on Portland Place, with the building being opened by King George V and Queen Mary. The RIBA is a member organisation, with 44,000 members. Chartered Members are entitled to call themselves chartered architects and to append the post-nominals RIBA after their name; Student Members are not permitted to do so. Formerly, fellowships of the institute were granted, although no longer; those who continue to hold this title instead add FRIBA. The Royal Institute of British Architects (RIBA) is a professional body for architects primarily in the United Kingdom, but also internationally, founded for the advancement of architecture under its charter granted in 1837 and Supplemental Charter granted in 1971. The RIBA Guide to its Archive and History (1986) has a section on the ""Statutory registration of architects"" with a bibliography extending from a draft bill of 1887 to one of 1969. The Guide's section on ""Education"" records the setting up in 1904 of the RIBA Board of Architectural Education, and the system by which any school which applied for recognition, whose syllabus was approved by the Board and whose examinations were conducted by an approved external examiner, and whose standard of attainment was guaranteed by periodical inspections by a ""Visiting Board"" from the BAE, could be placed on the list of ""recognized schools"" and its successful students could qualify for exemption from RIBA examinations. The original Charter of 1837 set out the purpose of the Royal Institute to be: '… the general advancement of Civil Architecture, and for promoting and facilitating the acquirement of the knowledge of the various arts and sciences connected therewith…' The overcrowded conditions of the library was one of the reasons why the RIBA moved from 9 Conduit Street to larger premises at 66 Portland Place in 1934. The library remained open throughout World War Two and was able to shelter the archives of Modernist architect Adolf Loos during the war. The British Architectural Library, sometimes referred to as the RIBA Library, was established in 1834 upon the founding of the institute with donations from members. Now, with over four million items, it is one of the three largest architectural libraries in the world and the largest in Europe. Some items from the collections are on permanent display at the Victoria and Albert Museum (V&A) in the V&A + RIBA Architecture Gallery and included in temporary exhibitions at the RIBA and across Europe and North America. Its collections include: The operational framework is provided by the Byelaws, which are more frequently updated than the Charter. Any revisions to the Charter or Byelaws require the Privy Council's approval.  RIBA is based at 66 Portland Place, London—a 1930s Grade II* listed building designed by architect George Grey Wornum with sculptures by Edward Bainbridge Copnall and James Woodford. Parts of the London building are open to the public, including the Library. It has a large architectural bookshop, a café, restaurant and lecture theatres. Rooms are hired out for events. In 2004, the two institutions created the Architecture Gallery (Room 128) at the V&A showing artefacts from the collections of both institutions, this was the first permanent gallery devoted to architecture in the UK. The adjacent Architecture Exhibition Space (Room 128a) is used for temporary displays related to architecture. Both spaces were designed by Gareth Hoskins Architects. At the same time the RIBA Library Drawing and Archives Collections moved from 21 Portman Place to new facilities in the Henry Cole Wing at the V&A. Under the Partnership new study rooms were opened where members of the public could view items from the RIBA and V&A architectural collections under the supervision of curatorial staff. These and the nearby education room were designed by Wright & Wright Architects. The RIBA has three parts to the education process: Part I which is generally a three-year first degree, a year-out of at least one year work experience in an architectural practice precedes the Part II which is generally a two-year post graduate diploma or masters. A further year out must be taken before the RIBA Part III professional exams can be taken. Overall it takes a minimum of seven years before an architecture student can seek chartered status. RIBA Enterprises is the commercial arm of RIBA, with a registered office in Newcastle upon Tyne, a base at 15 Bonhill Street in London, and an office in Newark. It employs over 250 staff, approximately 180 of whom are based in Newcastle. Originally named the Institute of British Architects in London, it was formed in 1834 by several prominent architects, including Philip Hardwick, Thomas Allom, William Donthorne, Thomas Leverton Donaldson, William Adams Nicholson, John Buonarotti Papworth, and Thomas de Grey, 2nd Earl de Grey. Architectural design competitions are used by an organisation that plans to build a new building or refurbish an existing building. They can be used for buildings, engineering work, structures, landscape design projects or public realm artworks. A competition typically asks for architects and/or designers to submit a design proposal in response to a given Brief. The winning design will then be selected by an independent jury panel of design professionals and client representatives. The independence of the jury is vital to the fair conduct of a competition."
Pope_John_XXIII,"On 25 May 1963, the pope suffered another haemorrhage and required several blood transfusions, but the cancer had perforated the stomach wall and peritonitis soon set in. The doctors conferred in a decision regarding this matter and John XXIII's aide Loris F. Capovilla broke the news to him saying that the cancer had done its work and nothing could be done for him. Around this time, his remaining siblings arrived to be with him. By 31 May, it had become clear that the cancer had overcome the resistance of John XXIII – it had left him confined to his bed. The Roman Catholic Church celebrates his feast day not on the date of his death, June 3, as is usual, nor even on the day of his papal inauguration (as is sometimes done with Popes who are Saints, such as with John Paul II) but on 11 October, the day of the first session of the Second Vatican Council. This is understandable, since he was the one who had had the idea for it and had convened it. On Thursday, 11 September 2014, Pope Francis added his optional memorial to the worldwide General Roman Calendar of saints' feast days, in response to global requests. He is commemorated on the date of his death, 3 June, by the Evangelical Lutheran Church in America and on the following day, 4 June, by the Anglican Church of Canada and the Episcopal Church (United States). Far from being a mere ""stopgap"" pope, to great excitement, John XXIII called for an ecumenical council fewer than ninety years after the First Vatican Council (Vatican I's predecessor, the Council of Trent, had been held in the 16th century). This decision was announced on 29 January 1959 at the Basilica of Saint Paul Outside the Walls. Cardinal Giovanni Battista Montini, who later became Pope Paul VI, remarked to Giulio Bevilacqua that ""this holy old boy doesn't realise what a hornet's nest he's stirring up"". From the Second Vatican Council came changes that reshaped the face of Catholicism: a comprehensively revised liturgy, a stronger emphasis on ecumenism, and a new approach to the world. Following the death of Pope Pius XII on 9 October 1958, Roncalli watched the live funeral on his last full day in Venice on 11 October. His journal was specifically concerned with the funeral and the abused state of the late pontiff's corpse. Roncalli left Venice for the conclave in Rome well aware that he was papabile,[b] and after eleven ballots, was elected to succeed the late Pius XII, so it came as no surprise to him, though he had arrived at the Vatican with a return train ticket to Venice.[citation needed] Pope John XXIII offered to mediate between US President John F. Kennedy and Nikita Khrushchev during the Cuban Missile Crisis in October 1962. Both men applauded the pope for his deep commitment to peace. Khrushchev would later send a message via Norman Cousins and the letter expressed his best wishes for the pontiff's ailing health. John XXIII personally typed and sent a message back to him, thanking him for his letter. Cousins, meanwhile, travelled to New York City and ensured that John would become Time magazine's 'Man of the Year'. John XXIII became the first Pope to receive the title, followed by John Paul II in 1994 and Francis in 2013. John XXIII died of peritonitis caused by a perforated stomach at 19:49 local time on 3 June 1963 at the age of 81, ending a historic pontificate of four years and seven months. He died just as a Mass for him finished in Saint Peter's Square below, celebrated by Luigi Traglia. After he died, his brow was ritually tapped to see if he was dead, and those with him in the room said prayers. Then the room was illuminated, thus informing the people of what had happened. He was buried on 6 June in the Vatican grottos. Two wreaths, placed on the two sides of his tomb, were donated by the prisoners of the Regina Coeli prison and the Mantova jail in Verona. On 22 June 1963, one day after his friend and successor Pope Paul VI was elected, the latter prayed at his tomb. The 50th anniversary of his death was celebrated on 3 June 2013 by Pope Francis, who visited his tomb and prayed there, then addressing the gathered crowd and spoke about the late pope. The people that gathered there at the tomb were from Bergamo, the province where the late pope came from. A month later, on 5 July 2013, Francis approved Pope John XXIII for canonization, along with Pope John Paul II without the traditional second miracle required. Instead, Francis based this decision on John XXIII's merits for the Second Vatican Council. On Sunday, 27 April 2014, John XXIII and Pope John Paul II were declared saints on Divine Mercy Sunday. Roncalli was elected pope on 28 October 1958 at age 76 after 11 ballots. His selection was unexpected, and Roncalli himself had come to Rome with a return train ticket to Venice. He was the first pope to take the pontifical name of ""John"" upon election in more than 500 years, and his choice settled the complicated question of official numbering attached to this papal name due to the antipope of this name. Pope John XXIII surprised those who expected him to be a caretaker pope by calling the historic Second Vatican Council (1962–65), the first session opening on 11 October 1962. His passionate views on equality were summed up in his famous statement, ""We were all made in God's image, and thus, we are all Godly alike."" John XXIII made many passionate speeches during his pontificate, one of which was on the day that he opened the Second Vatican Council in the middle of the night to the crowd gathered in St. Peter's Square: ""Dear children, returning home, you will find children: give your children a hug and say: This is a hug from the Pope!"" ""At 11 am Petrus Canisius Van Lierde as Papal Sacristan was at the bedside of the dying pope, ready to anoint him. The pope began to speak for the very last time: ""I had the great grace to be born into a Christian family, modest and poor, but with the fear of the Lord. My time on earth is drawing to a close. But Christ lives on and continues his work in the Church. Souls, souls, ut omnes unum sint.""[c] Van Lierde then anointed his eyes, ears, mouth, hands and feet. Overcome by emotion, Van Lierde forgot the right order of anointing. John XXIII gently helped him before bidding those present a last farewell. Pope John XXIII did not live to see the Vatican Council to completion. He died of stomach cancer on 3 June 1963, four and a half years after his election and two months after the completion of his final and famed encyclical, Pacem in terris. He was buried in the Vatican grottoes beneath Saint Peter's Basilica on 6 June 1963 and his cause for canonization was opened on 18 November 1965 by his successor, Pope Paul VI, who declared him a Servant of God. In addition to being named Venerable on 20 December 1999, he was beatified on 3 September 2000 by Pope John Paul II alongside Pope Pius IX and three others. Following his beatification, his body was moved on 3 June 2001 from its original place to the altar of Saint Jerome where it could be seen by the faithful. On 5 July 2013, Pope Francis – bypassing the traditionally required second miracle – declared John XXIII a saint, after unanimous agreement by a consistory, or meeting, of the College of Cardinals, based on the fact that he was considered to have lived a virtuous, model lifestyle, and because of the good for the Church which had come from his having opened the Second Vatican Council. He was canonised alongside Pope Saint John Paul II on 27 April 2014. John XXIII today is affectionately known as the ""Good Pope"" and in Italian, ""il Papa buono"". Pope Saint John XXIII (Latin: Ioannes XXIII; Italian: Giovanni XXIII) born Angelo Giuseppe Roncalli,[a] Italian pronunciation: [ˈandʒelo dʒuˈzɛppe roŋˈkalli]; 25 November 1881 – 3 June 1963) reigned as Pope from 28 October 1958 to his death in 1963 and was canonized on 27 April 2014. Angelo Giuseppe Roncalli was the fourth of fourteen children born to a family of sharecroppers who lived in a village in Lombardy. He was ordained to the priesthood on 10 August 1904 and served in a number of posts, including papal nuncio in France and a delegate to Bulgaria, Greece and Turkey. In a consistory on 12 January 1953 Pope Pius XII made Roncalli a cardinal as the Cardinal-Priest of Santa Prisca in addition to naming him as the Patriarch of Venice. Maintaining continuity with his predecessors, John XXIII continued the gradual reform of the Roman liturgy, and published changes that resulted in the 1962 Roman Missal, the last typical edition containing the Tridentine Mass established in 1570 by Pope Pius V at the request of the Council of Trent and whose continued use Pope Benedict XVI authorized in 2007, under the conditions indicated in his motu proprio Summorum Pontificum. In response to the directives of the Second Vatican Council, later editions of the Roman Missal present the 1970 form of the Roman Rite. In February 1925, the Cardinal Secretary of State Pietro Gasparri summoned him to the Vatican and informed him of Pope Pius XI's decision to appoint him as the Apostolic Visitor to Bulgaria (1925–35). On 3 March, Pius XI also named him for consecration as titular archbishop of Areopolis, Jordan. Roncalli was initially reluctant about a mission to Bulgaria, but he would soon relent. His nomination as apostolic visitor was made official on 19 March. Roncalli was consecrated by Giovanni Tacci Porcelli in the church of San Carlo alla Corso in Rome. After he was consecrated, he introduced his family to Pope Pius XI. He chose as his episcopal motto Obedientia et Pax (""Obedience and Peace""), which became his guiding motto. While he was in Bulgaria, an earthquake struck in a town not too far from where he was. Unaffected, he wrote to his sisters Ancilla and Maria and told them both that he was fine. The first session ended in a solemn ceremony on 8 December 1962 with the next session scheduled to occur in 1963 from 12 May to 29 June – this was announced on 12 November 1962. John XXIII's closing speech made subtle references to Pope Pius IX, and he had expressed the desire to see Pius IX beatified and eventually canonized. In his journal in 1959 during a spiritual retreat, John XXIII made this remark: ""I always think of Pius IX of holy and glorious memory, and by imitating him in his sacrifices, I would like to be worthy to celebrate his canonization"". On 3 December 1963, US President Lyndon B. Johnson posthumously awarded him the Presidential Medal of Freedom, the United States' highest civilian award, in recognition of the good relationship between Pope John XXIII and the United States of America. In his speech on 6 December 1963, Johnson said: ""I have also determined to confer the Presidential Medal of Freedom posthumously on another noble man whose death we mourned 6 months ago: His Holiness, Pope John XXIII. He was a man of simple origins, of simple faith, of simple charity. In this exalted office he was still the gentle pastor. He believed in discussion and persuasion. He profoundly respected the dignity of man. He gave the world immortal statements of the rights of man, of the obligations of men to each other, of their duty to strive for a world community in which all can live in peace and fraternal friendship. His goodness reached across temporal boundaries to warm the hearts of men of all nations and of all faiths"". On 12 January 1953, he was appointed Patriarch of Venice and, accordingly, raised to the rank of Cardinal-Priest of Santa Prisca by Pope Pius XII. Roncalli left France for Venice on 23 February 1953 stopping briefly in Milan and then to Rome. On 15 March 1953, he took possession of his new diocese in Venice. As a sign of his esteem, the President of France, Vincent Auriol, claimed the ancient privilege possessed by French monarchs and bestowed the red biretta on Roncalli at a ceremony in the Élysée Palace. It was around this time that he, with the aid of Monsignor Bruno Heim, formed his coat of arms with a lion of Saint Mark on a white ground. Auriol also awarded Roncalli three months later with the award of Commander of the Legion of Honour. On 10 May 1963, John XXIII received the Balzan Prize in private at the Vatican but deflected achievements of himself to the five popes of his lifetime, Pope Leo XIII to Pius XII. On 11 May, the Italian President Antonio Segni officially awarded Pope John XXIII with the Balzan Prize for his engagement for peace. While in the car en route to the official ceremony, he suffered great stomach pains but insisted on meeting with Segni to receive the award in the Quirinal Palace, refusing to do so within the Vatican. He stated that it would have been an insult to honour a pontiff on the remains of the crucified Saint Peter. It was the pope's last public appearance. Roncalli was summoned to the final ballot of the conclave at 4:00 pm. He was elected pope at 4:30 pm with a total of 38 votes. After the long pontificate of Pope Pius XII, the cardinals chose a man who – it was presumed because of his advanced age – would be a short-term or ""stop-gap"" pope. They wished to choose a candidate who would do little during the new pontificate. Upon his election, Cardinal Eugene Tisserant asked him the ritual questions of whether he would accept and if so, what name he would take for himself. Roncalli gave the first of his many surprises when he chose ""John"" as his regnal name. Roncalli's exact words were ""I will be called John"". This was the first time in over 500 years that this name had been chosen; previous popes had avoided its use since the time of the Antipope John XXIII during the Western Schism several centuries before. On 11 October 1962, the first session of the Second Vatican Council was held in the Vatican. He gave the Gaudet Mater Ecclesia speech, which served as the opening address for the council. The day was basically electing members for several council commissions that would work on the issues presented in the council. On that same night following the conclusion of the first session, the people in Saint Peter's Square chanted and yelled with the sole objective of getting John XXIII to appear at the window to address them. Many had considered Giovanni Battista Montini, the Archbishop of Milan, a possible candidate, but, although he was the archbishop of one of the most ancient and prominent sees in Italy, he had not yet been made a cardinal. Though his absence from the 1958 conclave did not make him ineligible – under Canon Law any Catholic male who is capable of receiving priestly ordination and episcopal consecration may be elected – the College of Cardinals usually chose the new pontiff from among the Cardinals who head archdioceses or departments of the Roman Curia that attend the papal conclave. At the time, as opposed to contemporary practice, the participating Cardinals did not have to be below age 80 to vote, there were few Eastern-rite Cardinals, and no Cardinals who were just priests at the time of their elevation. He was known affectionately as ""Good Pope John"". His cause for canonization was opened under Pope Paul VI during the final session of the Second Vatican Council on 18 November 1965, along with the cause of Pope Pius XII. On 3 September 2000, John XXIII was declared ""Blessed"" alongside Pope Pius IX by Pope John Paul II, the penultimate step on the road to sainthood after a miracle of curing an ill woman was discovered. He was the first pope since Pope Pius X to receive this honour. Following his beatification, his body was moved from its original burial place in the grottoes below the Vatican to the altar of St. Jerome and displayed for the veneration of the faithful.[citation needed] In February 1939, he received news from his sisters that his mother was dying. On 10 February 1939, Pope Pius XI died. Roncalli was unable to see his mother for the end as the death of a pontiff meant that he would have to stay at his post until the election of a new pontiff. Unfortunately, she died on 20 February 1939, during the nine days of mourning for the late Pius XI. He was sent a letter by Cardinal Eugenio Pacelli, and Roncalli later recalled that it was probably the last letter Pacelli sent until his election as Pope Pius XII on 2 March 1939. Roncalli expressed happiness that Pacelli was elected, and, on radio, listened to the coronation of the new pontiff.  John XXIII was an advocate for human rights which included the unborn and the elderly. He wrote about human rights in his Pacem in terris. He wrote, ""Man has the right to live. He has the right to bodily integrity and to the means necessary for the proper development of life, particularly food, clothing, shelter, medical care, rest, and, finally, the necessary social services. In consequence, he has the right to be looked after in the event of ill health; disability stemming from his work; widowhood; old age; enforced unemployment; or whenever through no fault of his own he is deprived of the means of livelihood."" His sister Ancilla would soon be diagnosed with stomach cancer in the early 1950s. Roncalli's last letter to her was dated on 8 November 1953 where he promised to visit her within the next week. He could not keep that promise, as Ancilla died on 11 November 1953 at the time when he was consecrating a new church in Venice. He attended her funeral back in his hometown. In his will around this time, he mentioned that he wished to be buried in the crypt of Saint Mark's in Venice with some of his predecessors rather than with the family in Sotto il Monte. On 30 November 1934, he was appointed Apostolic Delegate to Turkey and Greece and titular archbishop of Mesembria, Bulgaria. Thus, he is known as ""the Turcophile Pope,"" by the Turkish society which is predominantly Muslim. Roncalli took up this post in 1935 and used his office to help the Jewish underground in saving thousands of refugees in Europe, leading some to consider him to be a Righteous Gentile (see Pope John XXIII and Judaism). In October 1935, he led Bulgarian pilgrims to Rome and introduced them to Pope Pius XI on 14 October."
Southampton,"According to Hampshire Constabulary figures, Southampton is currently safer than it has ever been before, with dramatic reductions in violent crime year on year for the last three years. Data from the Southampton Safer City Partnership shows there has been a reduction in all crimes in recent years and an increase in crime detection rates. According to government figures Southampton has a higher crime rate than the national average. There is some controversy regarding comparative crime statisitics due to inconsistencies between different police forces recording methodologies. For example, in Hampshire all reported incidents are recorded and all records then retained. However, in neighbouring Dorset crimes reports withdrawn or shown to be false are not recorded, reducing apparent crime figures. In the violence against the person category, the national average is 16.7 per 1000 population while Southampton is 42.4 per 1000 population. In the theft from a vehicle category, the national average is 7.6 per 1000 compared to Southampton's 28.4 per 1000. Overall, for every 1,000 people in the city, 202 crimes are recorded. Hampshire Constabulary's figures for 2009/10 show fewer incidents of recorded crime in Southampton than the previous year. The geography of Southampton is influenced by the sea and rivers. The city lies at the northern tip of the Southampton Water, a deep water estuary, which is a ria formed at the end of the last Ice Age. Here, the rivers Test and Itchen converge. The Test—which has salt marsh that makes it ideal for salmon fishing—runs along the western edge of the city, while the Itchen splits Southampton in two—east and west. The city centre is located between the two rivers. The two local Sunday Leagues in the Southampton area are the City of Southampton Sunday Football League and the Southampton and District Sunday Football League. At certain times of the year, The Queen Mary 2, Queen Elizabeth and Queen Victoria may all visit Southampton at the same time, in an event commonly called 'Arrival of the Three Queens'. The city has undergone many changes to its governance over the centuries and once again became administratively independent from Hampshire County as it was made into a unitary authority in a local government reorganisation on 1 April 1997, a result of the 1992 Local Government Act. The district remains part of the Hampshire ceremonial county. Local train services operate in the central, southern and eastern sections of the city and are operated by South West Trains, with stations at Swaythling, St Denys, Millbrook, Redbridge, Bitterne, Sholing and Woolston. Plans were announced by Hampshire County Council in July 2009 for the introduction of tram-train running from Hythe (on what is now a freight-only line to Fawley) via Totton to Southampton Central Station and on to Fareham via St. Denys, and Swanwick. The proposal follows a failed plan to bring light rail to the Portsmouth and Gosport areas in 2005. The city is also well provided for in amateur men's and women's rugby with a number of teams in and around the city, the oldest of which is Trojans RFC who were promoted to London South West 2 division in 2008/9. A notable former player is Anthony Allen, who played with Leicester Tigers as a centre. Tottonians are also in London South West division 2 and Southampton RFC are in Hampshire division 1 in 2009/10, alongside Millbrook RFC and Eastleigh RFC. Many of the sides run mini and midi teams from under sevens up to under sixteens for both boys and girls. Southampton's strong economy is promoting redevelopment, and major projects are proposed, including the city's first skyscrapers on the waterfront. The three towers proposed will stand 23 storeys high and will be surrounded by smaller apartment blocks, office blocks and shops. There are also plans for a 15-storey hotel at the Ocean Village marina, and a 21-storey hotel on the north eastern corner of the city centre, as part of a £100m development. Many of the world's largest cruise ships can regularly be seen in Southampton water, including record-breaking vessels from Royal Caribbean and Carnival Corporation & plc. The latter has headquarters in Southampton, with its brands including Princess Cruises, P&O Cruises and Cunard Line. Southampton Solent University has 17,000 students and its strengths are in the training, design, consultancy, research and other services undertaken for business and industry. It is also host to the Warsash Maritime Academy, which provides training and certification for the international shipping and off-shore oil industries. The city provides for yachting and water sports, with a number of marinas. From 1977 to 2001 the Whitbread Around the World Yacht Race, which is now known as the Volvo Ocean Race was based in Southampton's Ocean Village marina. Pockets of Georgian architecture survived the war, but much of the city was levelled. There has been extensive redevelopment since World War II. Increasing traffic congestion in the 1920s led to partial demolition of medieval walls around the Bargate in 1932 and 1938. However a large portion of those walls remain. The River Test runs along the western border of the city, separating it from the New Forest. There are bridges over the Test from Southampton, including the road and rail bridges at Redbridge in the south and the M27 motorway to the north. The River Itchen runs through the middle of the city and is bridged in several places. The northernmost bridge, and the first to be built, is at Mansbridge, where the A27 road crosses the Itchen. The original bridge is closed to road traffic, but is still standing and open to pedestrians and cyclists. The river is bridged again at Swaythling, where Woodmill Bridge separates the tidal and non tidal sections of the river. Further south is Cobden Bridge which is notable as it was opened as a free bridge (it was originally named the Cobden Free Bridge), and was never a toll bridge. Downstream of the Cobden Bridge is the Northam Railway Bridge, then the Northam Road Bridge, which was the first major pre-stressed concrete bridge to be constructed in the United Kingdom. The southernmost, and newest, bridge on the Itchen is the Itchen Bridge, which is a toll bridge. The town was sacked in 1338 by French, Genoese and Monegasque ships (under Charles Grimaldi, who used the plunder to help found the principality of Monaco). On visiting Southampton in 1339, Edward III ordered that walls be built to 'close the town'. The extensive rebuilding—part of the walls dates from 1175—culminated in the completion of the western walls in 1380. Roughly half of the walls, 13 of the original towers, and six gates survive. The city hockey club, Southampton Hockey Club, founded in 1938, is now one of the largest and highly regarded clubs in Hampshire, fielding 7 senior men's and 5 senior ladies teams on a weekly basis along with boys’ and girls’ teams from 6 upwards. In addition to school sixth forms at St Anne's and King Edward's there are two sixth form colleges: Itchen College and Richard Taunton Sixth Form College. A number of Southampton pupils will travel outside the city, for example to Barton Peveril College. Southampton City College is a further education college serving the city. The college offers a range of vocational courses for school leavers, as well as ESOL programmes and Access courses for adult learners. Over 40 per cent of school pupils in the city that responded to a survey claimed to have been the victim of bullying. More than 2,000 took part and said that verbal bullying was the most common form, although physical bullying was a close second for boys. Just over a quarter of the jobs available in the city are in the health and education sector. A further 19 per cent are property and other business and the third largest sector is wholesale and retail, which accounts for 16.2 percent. Between 1995 and 2004, the number of jobs in Southampton has increased by 18.5 per cent. Significant employers in Southampton include The University of Southampton, Southampton Solent University, Southampton Airport, Ordnance Survey, BBC South, the NHS, ABP and Carnival UK. Southampton is noted for its association with the RMS Titanic, the Spitfire and more generally in the World War II narrative as one of the departure points for D-Day, and more recently as the home port of a number of the largest cruise ships in the world. Southampton has a large shopping centre and retail park called WestQuay. In October 2014, the City Council approved a follow-up from the WestQuay park, called WestQuay Watermark. Construction by Sir Robert McAlpine commenced in January 2015. Hammerson, the owners of the retail park, aim to have at least 1,550 people employed on its premises at year-end 2016. Surviving remains of 12th century merchants' houses such as King John's House and Canute's Palace are evidence of the wealth that existed in the town at this time. In 1348, the Black Death reached England via merchant vessels calling at Southampton. Southampton is home to Southampton Football Club—nicknamed ""The Saints""—who play in the Premier League at St Mary's Stadium, having relocated in 2001 from their 103-year-old former stadium, ""The Dell"". They reached the top flight of English football (First Division) for the first time in 1966, staying there for eight years. They lifted the FA Cup with a shock victory over Manchester United in 1976, returned to the top flight two years later, and stayed there for 27 years (becoming founder members of the Premier League in 1992) before they were relegated in 2005. The club was promoted back to the Premier League in 2012 following a brief spell in the third-tier and severe financial difficulties. In 2015, ""The Saints"" finished 7th in the Premier League, their highest league finish in 30 years, after a remarkable season under new manager Ronald Koeman. Their highest league position came in 1984 when they were runners-up in the old First Division. They were also runners-up in the 1979 Football League Cup final and 2003 FA Cup final. Notable former managers include Ted Bates, Lawrie McMenemy, Chris Nicholl, Ian Branfoot and Gordon Strachan. There is a strong rivalry between Portsmouth F.C. (""South Coast derby"") which is located only about 30 km (19 mi) away. University Hospital Southampton NHS Foundation Trust is one of the city's largest employers. It provides local hospital services to 500,000 people in the Southampton area and specialist regional services to more than 3 million people across the South of England. The Trust owns and manages Southampton General Hospital, the Princess Anne Hospital and a palliative care service at Countess Mountbatten House, part of the Moorgreen Hospital site in the village of West End, just outside the city. Southampton's largest retail centre, and 35th largest in the UK, is the WestQuay Shopping Centre, which opened in September 2000 and hosts major high street stores including John Lewis and Marks and Spencer. The centre was Phase Two of the West Quay development of the former Pirelli undersea cables factory; the first phase of this was the West Quay Retail Park, while the third phase (Watermark Westquay) was put on hold due to the recession. Work is has resumed in 2015, with plans for this third stage including shops, housing, an hotel and a public piazza alongside the Town Walls on Western Esplanade. Southampton has also been granted a licence for a large casino. A further part of the redevelopment of the West Quay site resulted in a new store, opened on 12 February 2009, for Swedish home products retailer IKEA. Marlands is a smaller shopping centre, built in the 1990s and located close to the northern side of WestQuay. Southampton currently has two disused shopping centres: the 1970s Eaststreet mall, and the 1980s Bargate centre. Neither of these were ever commercially successful; the former has been earmarked for redevelopment as a Morrison's supermarket, while the future of the latter is uncertain. There is also the East Street area which has been designated for speciality shopping, with the aim of promoting smaller retailers, alongside the chain store Debenhams. In 2007, Southampton was ranked 13th for shopping in the UK. The town experienced major expansion during the Victorian era. The Southampton Docks company had been formed in 1835. In October 1838 the foundation stone of the docks was laid and the first dock opened in 1842. The structural and economic development of docks continued for the next few decades. The railway link to London was fully opened in May 1840. Southampton subsequently became known as The Gateway to the Empire. Southampton became a spa town in 1740. It had also become a popular site for sea bathing by the 1760s, despite the lack of a good quality beach. Innovative buildings specifically for this purpose were built at West Quay, with baths that were filled and emptied by the flow of the tide. Southampton is also home to one of the most successful College American Football teams in the UK, the Southampton Stags, who play at the Wide Lane Sports Facility in Eastleigh. In his 1854 book ""The Cruise of the Steam Yacht North Star"" John Choules described Southampton thus: ""I hardly know a town that can show a more beautiful Main Street than Southampton, except it be Oxford. The High Street opens from the quay, and under various names it winds in a gently sweeping line for one mile and a half, and is of very handsome width. The variety of style and color of material in the buildings affords an exhibition of outline, light and color, that I think is seldom equalled. The shops are very elegant, and the streets are kept exceedingly clean."" After the establishment of Hampshire County Council, following the act in 1888, Southampton became a county borough within the county of Hampshire, which meant that it had many features of a county, but governance was now shared between the Corporation in Southampton and the new county council. There is a great source of confusion in the fact that the ancient shire county, along with its associated assizes, was known as the County of Southampton or Southamptonshire. This was officially changed to Hampshire in 1959 although the county had been commonly known as Hampshire or Hantscire for centuries. Southampton became a non-metropolitan district in 1974. Southampton is a major UK port which has good transport links with the rest of the country. The M27 motorway, linking places along the south coast of England, runs just to the north of the city. The M3 motorway links the city to London and also, via a link to the A34 (part of the European route E05) at Winchester, with the Midlands and North. The M271 motorway is a spur of the M27, linking it with the Western Docks and city centre. Southampton Airport is a regional airport located in the town of Eastleigh, just north of the city. It offers flights to UK and near European destinations, and is connected to the city by a frequent rail service from Southampton Airport (Parkway) railway station, and by bus services. Archaeological finds suggest that the area has been inhabited since the stone age. Following the Roman invasion of Britain in AD 43 and the conquering of the local Britons in 70 AD the fortress settlement of Clausentum was established. It was an important trading port and defensive outpost of Winchester, at the site of modern Bitterne Manor. Clausentum was defended by a wall and two ditches and is thought to have contained a bath house. Clausentum was not abandoned until around 410. Following the Norman Conquest in 1066, Southampton became the major port of transit between the then capital of England, Winchester, and Normandy. Southampton Castle was built in the 12th century and by the 13th century Southampton had become a leading port, particularly involved in the import of French wine in exchange for English cloth and wool. Other major employers in the city include Ordnance Survey, the UK's national mapping agency, whose headquarters is located in a new building on the outskirts of the city, opened in February 2011. The Lloyd's Register Group has announced plans to move its London marine operations to a specially developed site at the University of Southampton. The area of Swaythling is home to Ford's Southampton Assembly Plant, where the majority of their Transit models are manufactured. Closure of the plant in 2013 was announced in 2012, with the loss of hundreds of jobs. Southampton had an estimated 236,900 people living within the city boundary in 2011. There is a sizeable Polish population in the city, with estimates as high as 20,000. Southampton also has 2 community FM radio stations, the Queens Award winning Unity 101 Community Radio (www.unity101.org) broadcasting full-time on 101.1 FM since 2006 to the Asian and Ethnic communities, and Voice FM (http://www.voicefmradio.co.uk) located in St Mary's, which has been broadcasting full-time on 103.9 FM since September 2011, playing a wide range of music from Rock to Dance music and Top 40. A third station, Awaaz FM (www.awaazfm.co.uk), is an internet only radio stations also catering for Asian and Ethnic community. In the 2001 census Southampton and Portsmouth were recorded as being parts of separate urban areas, however by the time of the 2011 census they had merged to become the sixth largest built-up area in England with a population of 855,569. This built-up area is part of the metropolitan area known as South Hampshire, which is also known as Solent City, particularly in the media when discussing local governance organisational changes. With a population of over 1.5 million this makes the region one of the United Kingdom's most populous metropolitan areas. In 1642, during the English Civil War, a Parliamentary garrison moved into Southampton. The Royalists advanced as far as Redbridge, Southampton, in March 1644 but were prevented from taking the town. At the 2001 Census, 92.4 per cent of the city's populace was White—including one per cent White Irish—3.8 per cent were South Asian, 1.0 per cent Black, 1.3 per cent Chinese or other ethnic groups, and 1.5 per cent were of Mixed Race. Southampton City Council has developed twinning links with Le Havre in France (since 1973), Rems-Murr-Kreis in Germany (since 1991), Trieste in Italy (since 2002); Hampton, Virginia in USA, Qingdao in China (since 1998), and Busan in South Korea (since 1978). Southampton's police service is provided by Hampshire Constabulary. The main base of the Southampton operation is a new, eight storey purpose-built building which cost £30 million to construct. The building, located on Southern Road, opened in 2011 and is near to Southampton Central railway station. Previously, the central Southampton operation was located within the west wing of the Civic Centre, however the ageing facilities and the plans of constructing a new museum in the old police station and magistrates court necessitated the move. There are additional police stations at Portswood, Banister Park, Bitterne, and Shirley as well as a British Transport Police station at Southampton Central railway station. In March 2007 there were 120,305 jobs in Southampton, and 3,570 people claiming job seeker's allowance, approximately 2.4 per cent of the city's population. This compares with an average of 2.5 per cent for England as a whole. A Royal Charter in 1952 upgraded University College at Highfield to the University of Southampton. Southampton acquired city status, becoming the City of Southampton in 1964. Council estates are in the Weston, Thornhill and Townhill Park districts. The city is ranked 96th most deprived out of all 354 Local Authorities in England. During the latter half of the 20th century, a more diverse range of industry also came to the city, including aircraft and car manufacture, cables, electrical engineering products, and petrochemicals. These now exist alongside the city's older industries of the docks, grain milling, and tobacco processing. Between 1996 and 2004, the population of the city increased by 4.9 per cent—the tenth biggest increase in England. In 2005 the Government Statistics stated that Southampton was the third most densely populated city in the country after London and Portsmouth respectively. Hampshire County Council expects the city's population to grow by around a further two per cent between 2006 and 2013, adding around another 4,200 to the total number of residents. The highest increases are expected among the elderly. Southampton has two large live music venues, the Mayflower Theatre (formerly the Gaumont Theatre) and the Guildhall. The Guildhall has seen concerts from a wide range of popular artists including Pink Floyd, David Bowie, Delirious?, Manic Street Preachers, The Killers, The Kaiser Chiefs, Amy Winehouse, Lostprophets, The Midnight Beast, Modestep, and All Time Low. It also hosts classical concerts presented by the Bournemouth Symphony Orchestra, City of Southampton Orchestra, Southampton Concert Orchestra, Southampton Philharmonic Choir and Southampton Choral Society. The Anglo-Saxons formed a new, larger, settlement across the Itchen centred on what is now the St Mary's area of the city. The settlement was known as Hamwic, which evolved into Hamtun and then Hampton. Archaeological excavations of this site have uncovered one of the best collections of Saxon artefacts in Europe. It is from this town that the county of Hampshire gets its name. The city is home to the longest surviving stretch of medieval walls in England, as well as a number of museums such as Tudor House Museum, reopened on 30 July 2011 after undergoing extensive restoration and improvement; Southampton Maritime Museum; God's House Tower, an archaeology museum about the city's heritage and located in one of the tower walls; the Medieval Merchant's House; and Solent Sky, which focuses on aviation. The SeaCity Museum is located in the west wing of the civic centre, formerly occupied by Hampshire Constabulary and the Magistrates' Court, and focuses on Southampton's trading history and on the RMS Titanic. The museum received half a million pounds from the National Lottery in addition to interest from numerous private investors and is budgeted at £28 million. Southampton is also served by the rail network, which is used both by freight services to and from the docks and passenger services as part of the national rail system. The main station in the city is Southampton Central. Rail routes run east towards Portsmouth, north to Winchester, the Midlands and London, and westwards to Bournemouth, Poole, Dorchester, Weymouth, Salisbury, Bristol and Cardiff. The route to London was opened in 1840 by what was to become the London and South Western Railway Company. Both this and its successor the Southern Railway (UK) played a significant role in the creation of the modern port following their purchase and development of the town's docks. The Supermarine Spitfire was designed and developed in Southampton, evolving from the Schneider trophy-winning seaplanes of the 1920s and 1930s. Its designer, R J Mitchell, lived in the Portswood area of Southampton, and his house is today marked with a blue plaque. Heavy bombing of the factory in September 1940 destroyed it as well as homes in the vicinity, killing civilians and workers. World War II hit Southampton particularly hard because of its strategic importance as a major commercial port and industrial area. Prior to the Invasion of Europe, components for a Mulberry harbour were built here. After D-Day, Southampton docks handled military cargo to help keep the Allied forces supplied, making it a key target of Luftwaffe bombing raids until late 1944. Southampton docks was featured in the television show 24: Live Another Day in Day 9: 9:00 p.m. – 10:00 p.m. The largest theatre in the city is the 2,300 capacity Mayflower Theatre (formerly known as the Gaumont), which, as the largest theatre in Southern England outside London, has hosted West End shows such as Les Misérables, The Rocky Horror Show and Chitty Chitty Bang Bang, as well as regular visits from Welsh National Opera and English National Ballet. There is also the Nuffield Theatre based at the University of Southampton's Highfield campus, which is the city's primary producing theatre. It was awarded The Stage Award for Best Regional Theatre in 2015. It also hosts touring companies and local performing societies (such as Southampton Operatic Society, the Maskers and the University Players). Southampton (i/saʊθˈæmptən, -hæmptən/) is the largest city in the ceremonial county of Hampshire on the south coast of England, and is situated 75 miles (121 km) south-west of London and 19 miles (31 km) north-west of Portsmouth. Southampton is a major port and the closest city to the New Forest. It lies at the northernmost point of Southampton Water at the confluence of the River Test and River Itchen, with the River Hamble joining to the south of the urban area. The city, which is a unitary authority, has an estimated population of 253,651. The city's name is sometimes abbreviated in writing to ""So'ton"" or ""Soton"", and a resident of Southampton is called a Sotonian. Southampton Water has the benefit of a double high tide, with two high tide peaks, making the movement of large ships easier. This is not caused as popularly supposed by the presence of the Isle of Wight, but is a function of the shape and depth of the English Channel. In this area the general water flow is distorted by more local conditions reaching across to France. Southampton as a Port and city has had a long history of administrative independence of the surrounding County; as far back as the reign of King John the town and its port were removed from the writ of the King's Sheriff in Hampshire and the rights of custom and toll were granted by the King to the burgesses of Southampton over the port of Southampton and the Port of Portsmouth; this tax farm was granted for an annual fee of £200 in the charter dated at Orival on 29 June 1199. The definition of the port of Southampton was apparently broader than today and embraced all of the area between Lymington and Langstone. The corporation had resident representatives in Newport, Lymington and Portsmouth. By a charter of Henry VI, granted on 9 March 1446/7 (25+26 Hen. VI, m. 32), the mayor, bailiffs and burgesses of the towns and ports of Southampton and Portsmouth became a County incorporate and separate from Hampshire. The city has a strong higher education sector. The University of Southampton and Southampton Solent University together have a student population of over 40,000. The importance of Southampton to the cruise industry was indicated by P&O Cruises's 175th anniversary celebrations, which included all seven of the company's liners visiting Southampton in a single day. Adonia, Arcadia, Aurora, Azura, Oceana, Oriana and Ventura all left the city in a procession on 3 July 2012. Elsewhere, remnants of the medieval water supply system devised by the friars can still be seen today. Constructed in 1290, the system carried water from Conduit Head (remnants of which survive near Hill Lane, Shirley) some 1.7 kilometres to the site of the friary inside the town walls. The friars granted use of the water to the town in 1310 and passed on ownership of the water supply system itself in 1420. Further remains can be observed at Conduit House on Commercial Road. The city is home or birthplace to a number of contemporary musicians such as R'n'B singer Craig David, Coldplay drummer Will Champion, former Holloways singer Rob Skipper as well as 1980s popstar Howard Jones. Several rock bands were formed in Southampton, including Band of Skulls, The Delays, Bury Tomorrow, Heart in Hand, Thomas Tantrum (disbanded in 2011) and Kids Can't Fly (disbanded in 2014). James Zabiela, a highly regarded and recognised name in dance music, is also from Southampton. The annual Southampton Boat Show is held in September each year, with over 600 exhibitors present. It runs for just over a week at Mayflower Park on the city's waterfront, where it has been held since 1968. The Boat Show itself is the climax of Sea City, which runs from April to September each year to celebrate Southampton's links with the sea. On the other hand, many of the medieval buildings once situated within the town walls are now in ruins or have disappeared altogether. From successive incarnations of the motte and bailey castle, only a section of the bailey wall remains today, lying just off Castle Way. The last remains of the Franciscan friary in Southampton, founded circa 1233 and dissolved in 1538, were swept away in the 1940s. The site is now occupied by Friary House. The port was the point of departure for the Pilgrim Fathers aboard Mayflower in 1620. In 1912, the RMS Titanic sailed from Southampton. Four in five of the crew on board the vessel were Sotonians, with about a third of those who perished in the tragedy hailing from the city. Southampton was subsequently the home port for the transatlantic passenger services operated by Cunard with their Blue Riband liner RMS Queen Mary and her running mate RMS Queen Elizabeth. In 1938, Southampton docks also became home to the flying boats of Imperial Airways. Southampton Container Terminals first opened in 1968 and has continued to expand. Southampton was named ""fittest city in the UK"" in 2006 by Men's Fitness magazine. The results were based on the incidence of heart disease, the amount of junk food and alcohol consumed, and the level of gym membership. In 2007, it had slipped one place behind London, but was still ranked first when it came to the parks and green spaces available for exercise and the amount of television watched by Sotonians was the lowest in the country. Speedway racing took place at Banister Court Stadium in the pre-war era. It returned in the 1940s after WW2 and the Saints operated until the stadium closed down at the end of 1963. A training track operated in the 1950s in the Hamble area. There are 119,500 males within the city and 117,400 females. The 20–24 age range is the most populous, with an estimated 32,300 people falling in this age range. Next largest is the 25–29 range with 24,700 people and then 30–34 years with 17,800. By population, Southampton is the largest monocentric city in the South East England region and the second largest on the South Coast after Plymouth. During the Middle Ages, shipbuilding became an important industry for the town. Henry V's famous warship HMS Grace Dieu was built in Southampton. Walter Taylor's 18th century mechanisation of the block-making process was a significant step in the Industrial Revolution. From 1904 to 2004, the Thornycroft shipbuilding yard was a major employer in Southampton, building and repairing ships used in the two World Wars. While Southampton is no longer the base for any cross-channel ferries, it is the terminus for three internal ferry services, all of which operate from terminals at Town Quay. Two of these, a car ferry service and a fast catamaran passenger ferry service, provide links to East Cowes and Cowes respectively on the Isle of Wight and are operated by Red Funnel. The third ferry is the Hythe Ferry, providing a passenger service to Hythe on the other side of Southampton Water. Buses now provide the majority of local public transport. The main bus operators are First Southampton and Bluestar. Other operators include Brijan Tours, Stagecoach and Xelabus. The other large service provider is the Uni-link bus service (running from early in the morning to midnight), which was commissioned by the University of Southampton to provide transport from the university to the town. Previously run by Enterprise, it is now run by Bluestar. Free buses are provided by City-link'. The City-link runs from the Red Funnel ferry terminal at Town Quay to Central station via WestQuay and is operated by Bluestar. There is also a door-to-door minibus service called Southampton Dial a Ride, for residents who cannot access public transport. This is funded by the council and operated by SCA Support Services. As with the rest of the UK, Southampton experiences an oceanic climate (Köppen Cfb). Its southerly, low lying and sheltered location ensures it is among the warmer, sunnier cities in the UK. It has held the record for the highest temperature in the UK for June at 35.6 °C (96.1 °F) since 1976. Southampton has been used for military embarkation, including during 18th-century wars with the French, the Crimean war, and the Boer War. Southampton was designated No. 1 Military Embarkation port during the Great War and became a major centre for treating the returning wounded and POWs. It was also central to the preparations for the Invasion of Europe in 1944. The city also has several smaller music venues, including the Brook, The Talking Heads, The Soul Cellar, The Joiners and Turner Sims, as well as smaller ""club circuit"" venues like Hampton's and Lennon's, and a number of public houses including the Platform tavern, the Dolphin, the Blue Keys and many others. The Joiners has played host to such acts as Oasis, Radiohead, Green Day, Suede, PJ Harvey, the Manic Street Preachers, Coldplay, the Verve, the Libertines and Franz Ferdinand, while Hampton's and Lennon's have hosted early appearances by Kate Nash, Scouting for Girls and Band of Skulls. The nightclub, Junk, has been nominated for the UK's best small nightclub, and plays host to a range of dance music's top acts. Southampton has always been a port, and the docks have long been a major employer in the city. In particular, it is a port for cruise ships; its heyday was the first half of the 20th century, and in particular the inter-war years, when it handled almost half the passenger traffic of the UK. Today it remains home to luxury cruise ships, as well as being the largest freight port on the Channel coast and fourth largest UK port by tonnage, with several container terminals. Unlike some other ports, such as Liverpool, London, and Bristol, where industry and docks have largely moved out of the city centres leaving room for redevelopment, Southampton retains much of its inner-city industry. Despite the still active and expanding docklands to the west of the city centre, further enhanced with the opening of a fourth cruise terminal in 2009, parts of the eastern docks have been redeveloped; the Ocean Village development, which included a local marina and small entertainment complex, is a good example. Southampton is home to the headquarters of both the Maritime and Coastguard Agency and the Marine Accident Investigation Branch of the Department for Transport in addition to cruise operator Carnival UK. The status of the town was changed by a later charter of Charles I by at once the formal separation from Portsmouth and the recognition of Southampton as a county, In the charter dated 27 June 1640 the formal title of the town became 'The Town and County of the Town of Southampton'. These charters and Royal Grants, of which there were many, also set out the governance and regulation of the town and port which remained the 'constitution' of the town until the local government organisation of the later Victorian period which from about 1888 saw the setting up of County Councils across England and Wales and including Hampshire County Council who now took on some of the function of Government in Southampton Town. In this regime, The Town and County of the Town of Southampton also became a county borough with shared responsibility for aspects of local government. On 24 February 1964 the status changed again by a Charter of Elizabeth II, creating the City and County of the City of Southampton. The city also has the Southampton Sports Centre which is the focal point for the public's sporting and outdoor activities and includes an Alpine Centre, theme park and athletics centre which is used by professional athletes. With the addition of 11 other additional leisure venures which are currently operate by the Council leisure executives. However these have been sold the operating rights to ""Park Wood Leisure."" 630 people lost their lives as a result of the air raids on Southampton and nearly 2,000 more were injured, not to mention the thousands of buildings damaged or destroyed. In January 2007, the average annual salary in the city was £22,267. This was £1,700 lower than the national average and £3,800 less than the average for the South East. Hampshire County Cricket Club play close to the city, at the Rose Bowl in West End, after previously playing at the County Cricket Ground and the Antelope Ground, both near the city centre. There is also the Southampton Evening Cricket League. Prior to King Henry's departure for the Battle of Agincourt in 1415, the ringleaders of the ""Southampton Plot""—Richard, Earl of Cambridge, Henry Scrope, 3rd Baron Scrope of Masham, and Sir Thomas Grey of Heton—were accused of high treason and tried at what is now the Red Lion public house in the High Street. They were found guilty and summarily executed outside the Bargate. Local media include the Southern Daily Echo newspaper based in Redbridge and BBC South, which has its regional headquarters in the city centre opposite the civic centre. From there the BBC broadcasts South Today, the local television news bulletin and BBC Radio Solent. The local ITV franchise is Meridian, which has its headquarters in Whiteley, around nine miles (14 km) from the city. Until December 2004, the station's studios were located in the Northam area of the city on land reclaimed from the River Itchen. That's Solent is an local television channel that began broadcasting in November 2014, which will be based in and serve Southampton and Portsmouth. Southampton's fire cover is provided by Hampshire Fire and Rescue Service. There are three fire stations within the city boundaries at St Mary's, Hightown and Redbridge. There are two main termini for bus services. As the biggest operator, First uses stops around Pound Tree Road. This leaves the other terminal of West Quay available for other operators. Uni-link passes West Quay in both directions, and Wilts & Dorset drop passengers off and pick them up there, terminating at a series of bus stands along the road. Certain Bluestar services also do this, while others stop at Bargate and some loop round West Quay, stopping at Hanover Buildings. There was a tram system from 1879 to 1949. The town was the subject of an attempt by a separate company, the Didcot, Newbury and Southampton Railway, to open another rail route to the North in the 1880s and some building work, including a surviving embankment, was undertaken in the Hill Lane area. The city walls include God's House Tower, built in 1417, the first purpose-built artillery fortification in England. Over the years it has been used as home to the city's gunner, the Town Gaol and even as storage for the Southampton Harbour Board. Until September 2011, it housed the Museum of Archaeology. The walls were completed in the 15th century, but later development of several new fortifications along Southampton Water and the Solent by Henry VIII meant that Southampton was no longer dependent upon its fortifications. The city has a Mayor and is one of the 16 cities and towns in England and Wales to have a ceremonial sheriff who acts as a deputy for the Mayor. The current and 793rd Mayor of Southampton is Linda Norris. Catherine McEwing is the current and 578th sherriff. The town crier from 2004 until his death in 2014 was John Melody, who acted as master of ceremonies in the city and who possessed a cry of 104 decibels. There are many innovative art galleries in the city. The Southampton City Art Gallery at the Civic Centre is one of the best known and as well as a nationally important Designated Collection, houses several permanent and travelling exhibitions. The Millais Gallery at Southampton Solent University, the John Hansard Gallery at Southampton University as well as smaller galleries including the Art House in Above Bar Street provide a different view. The city's Bargate is also an art gallery run by the arts organisation ""a space"". A space also run the Art Vaults project, which creatively uses several of Southampton's medieval vaults, halls and cellars as venues for contemporary art installations. Viking raids from 840 onwards contributed to the decline of Hamwic in the 9th century, and by the 10th century a fortified settlement, which became medieval Southampton, had been established. There are three members of parliament for the city: Royston Smith (Conservative) for Southampton Itchen, the constituency covering the east of the city; Dr. Alan Whitehead (Labour) for Southampton Test, which covers the west of the city; and Caroline Nokes (Conservative) for Romsey and Southampton North, which includes a northern portion of the city. Southampton City Council consists of 48 councillors, 3 for each of the 16 wards. Council elections are held in early May for one third of the seats (one councillor for each ward), elected for a four-year term, so there are elections three years out of four. Since the 2015 council elections, the composition of the council is: The centre of Southampton is located above a large hot water aquifer that provides geothermal power to some of the city's buildings. This energy is processed at a plant in the West Quay region in Southampton city centre, the only geothermal power station in the UK. The plant provides private electricity for the Port of Southampton and hot water to the Southampton District Energy Scheme used by many buildings including the WestQuay shopping centre. In a 2006 survey of carbon emissions in major UK cities conducted by British Gas, Southampton was ranked as being one of the lowest carbon emitting cities in the United Kingdom. The city has a particular connection to Cunard Line and their fleet of ships. This was particularly evident on 11 November 2008 when the Cunard liner RMS Queen Elizabeth 2 departed the city for the final time amid a spectacular fireworks display after a full day of celebrations. Cunard ships are regularly launched in the city, for example Queen Victoria was named by HRH The Duchess of Cornwall in December 2007, and the Queen named Queen Elizabeth in the city during October 2011. The Duchess of Cambridge performed the naming ceremony of Royal Princess on 13 June 2013. Commercial radio stations broadcasting to the city include The Breeze, previously The Saint and currently broadcasting Hot adult contemporary music, Capital, previously Power FM and Galaxy and broadcasting popular music, Wave 105 and Heart Hampshire, the latter previously Ocean FM and both broadcasting adult contemporary music, and 106 Jack FM (www.jackradio.com), previously The Coast 106. In addition, Southampton University has a radio station called SURGE, broadcasting on AM band as well as through the web. According to 2004 figures, Southampton contributes around £4.2 bn to the regional economy annually. The vast majority of this is from the service sector, with the remainder coming from industry in the city. This figure has almost doubled since 1995. The University of Southampton, which was founded in 1862 and received its Royal Charter as a university in 1952, has over 22,000 students. The university is ranked in the top 100 research universities in the world in the Academic Ranking of World Universities 2010. In 2010, the THES - QS World University Rankings positioned the University of Southampton in the top 80 universities in the world. The university considers itself one of the top 5 research universities in the UK. The university has a global reputation for research into engineering sciences, oceanography, chemistry, cancer sciences, sound and vibration research, computer science and electronics, optoelectronics and textile conservation at the Textile Conservation Centre (which is due to close in October 2009.) It is also home to the National Oceanography Centre, Southampton (NOCS), the focus of Natural Environment Research Council-funded marine research. Southampton used to be home to a number of ferry services to the continent, with destinations such as San Sebastian, Lisbon, Tangier and Casablanca. A ferry port was built during the 1960s. However, a number of these relocated to Portsmouth and by 1996, there were no longer any car ferries operating from Southampton with the exception of services to the Isle of Wight. The land used for Southampton Ferry Port was sold off and a retail and housing development was built on the site. The Princess Alexandra Dock was converted into a marina. Reception areas for new cars now fill the Eastern Docks where passengers, dry docks and trains used to be. It has been revealed that Southampton has the worst behaved secondary schools within the UK. With suspension rates three times the national average, the suspension rate is approximately 1 in every 14 children, the highest in the country for physical or verbal assaults against staff. Southampton is divided into council wards, suburbs, constituencies, ecclesiastical parishes, and other less formal areas. It has a number of parks and green spaces, the largest being the 148 hectare Southampton Common, parts of which are used to host the annual summer festivals, circuses and fun fairs. The Common includes Hawthorns Urban Wildlife Centre on the former site of Southampton Zoo, a paddling pool and several lakes and ponds. Town Quay is the original public quay, and dates from the 13th century. Today's Eastern Docks were created in the 1830s by land reclamation of the mud flats between the Itchen & Test estuaries. The Western Docks date from the 1930s when the Southern Railway Company commissioned a major land reclamation and dredging programme. Most of the material used for reclamation came from dredging of Southampton Water, to ensure that the port can continue to handle large ships."
Neolithic,"In southeast Europe agrarian societies first appeared in the 7th millennium BC, attested by one of the earliest farming sites of Europe, discovered in Vashtëmi, southeastern Albania and dating back to 6,500 BC. Anthropomorphic figurines have been found in the Balkans from 6000 BC, and in Central Europe by c. 5800 BC (La Hoguette). Among the earliest cultural complexes of this area are the Sesklo culture in Thessaly, which later expanded in the Balkans giving rise to Starčevo-Körös (Cris), Linearbandkeramik, and Vinča. Through a combination of cultural diffusion and migration of peoples, the Neolithic traditions spread west and northwards to reach northwestern Europe by around 4500 BC. The Vinča culture may have created the earliest system of writing, the Vinča signs, though archaeologist Shan Winn believes they most likely represented pictograms and ideograms rather than a truly developed form of writing. The Cucuteni-Trypillian culture built enormous settlements in Romania, Moldova and Ukraine from 5300 to 2300 BC. The megalithic temple complexes of Ġgantija on the Mediterranean island of Gozo (in the Maltese archipelago) and of Mnajdra (Malta) are notable for their gigantic Neolithic structures, the oldest of which date back to c. 3600 BC. The Hypogeum of Ħal-Saflieni, Paola, Malta, is a subterranean structure excavated c. 2500 BC; originally a sanctuary, it became a necropolis, the only prehistoric underground temple in the world, and showing a degree of artistry in stone sculpture unique in prehistory to the Maltese islands. After 2500 BC, the Maltese Islands were depopulated for several decades until the arrival of a new influx of Bronze Age immigrants, a culture that cremated its dead and introduced smaller megalithic structures called dolmens to Malta. In most cases there are small chambers here, with the cover made of a large slab placed on upright stones. They are claimed to belong to a population certainly different from that which built the previous megalithic temples. It is presumed the population arrived from Sicily because of the similarity of Maltese dolmens to some small constructions found in the largest island of the Mediterranean sea. Families and households were still largely independent economically, and the household was probably the center of life. However, excavations in Central Europe have revealed that early Neolithic Linear Ceramic cultures (""Linearbandkeramik"") were building large arrangements of circular ditches between 4800 BC and 4600 BC. These structures (and their later counterparts such as causewayed enclosures, burial mounds, and henge) required considerable time and labour to construct, which suggests that some influential individuals were able to organise and direct human labour — though non-hierarchical and voluntary work remain possibilities. Neolithic peoples in the Levant, Anatolia, Syria, northern Mesopotamia and Central Asia were also accomplished builders, utilizing mud-brick to construct houses and villages. At Çatal höyük, houses were plastered and painted with elaborate scenes of humans and animals. In Europe, long houses built from wattle and daub were constructed. Elaborate tombs were built for the dead. These tombs are particularly numerous in Ireland, where there are many thousand still in existence. Neolithic people in the British Isles built long barrows and chamber tombs for their dead and causewayed camps, henges, flint mines and cursus monuments. It was also important to figure out ways of preserving food for future months, such as fashioning relatively airtight containers, and using substances like salt as preservatives. Most clothing appears to have been made of animal skins, as indicated by finds of large numbers of bone and antler pins which are ideal for fastening leather. Wool cloth and linen might have become available during the later Neolithic, as suggested by finds of perforated stones which (depending on size) may have served as spindle whorls or loom weights. The clothing worn in the Neolithic Age might be similar to that worn by Ötzi the Iceman, although he was not Neolithic (since he belonged to the later Copper age). The shelter of the early people changed dramatically from the paleolithic to the neolithic era. In the paleolithic, people did not normally live in permanent constructions. In the neolithic, mud brick houses started appearing that were coated with plaster. The growth of agriculture made permanent houses possible. Doorways were made on the roof, with ladders positioned both on the inside and outside of the houses. The roof was supported by beams from the inside. The rough ground was covered by platforms, mats, and skins on which residents slept. Stilt-houses settlements were common in the Alpine and Pianura Padana (Terramare) region. Remains have been found at the Ljubljana Marshes in Slovenia and at the Mondsee and Attersee lakes in Upper Austria, for example. The Neolithic 2 (PPNB) began around 8,800 BCE according to the ASPRO chronology in the Levant (Jericho, Israel). As with the PPNA dates, there are two versions from the same laboratories noted above. This system of terminology, however, is not convenient for southeast Anatolia and settlements of the middle Anatolia basin. This era was before the Mesolithic era.[citation needed] A settlement of 3,000 inhabitants was found in the outskirts of Amman, Jordan. Considered to be one of the largest prehistoric settlements in the Near East, called 'Ain Ghazal, it was continuously inhabited from approximately 7,250 – 5,000 B. In 1981 a team of researchers from the Maison de l'Orient et de la Méditerranée, including Jacques Cauvin and Oliver Aurenche divided Near East neolithic chronology into ten periods (0 to 9) based on social, economic and cultural characteristics. In 2002 Danielle Stordeur and Frédéric Abbès advanced this system with a division into five periods. Natufian (1) between 12,000 and 10,200 BC, Khiamian (2) between 10,200-8,800 BC, PPNA: Sultanian (Jericho), Mureybetian, early PPNB (PPNB ancien) (3) between 8,800-7,600 BC, middle PPNB (PPNB moyen) 7,600-6,900 BC, late PPNB (PPNB récent) (4) between 7,500 and 7,000 BC and a PPNB (sometimes called PPNC) transitional stage (PPNB final) (5) where Halaf and dark faced burnished ware begin to emerge between 6,900-6,400 BC. They also advanced the idea of a transitional stage between the PPNA and PPNB between 8,800 and 8,600 BC at sites like Jerf el Ahmar and Tell Aswad. Traditionally considered the last part of the Stone Age, the Neolithic followed the terminal Holocene Epipaleolithic period and commenced with the beginning of farming, which produced the ""Neolithic Revolution"". It ended when metal tools became widespread (in the Copper Age or Bronze Age; or, in some geographical regions, in the Iron Age). The Neolithic is a progression of behavioral and cultural characteristics and changes, including the use of wild and domestic crops and of domesticated animals. Domestication of sheep and goats reached Egypt from the Near East possibly as early as 6,000 BC. Graeme Barker states ""The first indisputable evidence for domestic plants and animals in the Nile valley is not until the early fifth millennium bc in northern Egypt and a thousand years later further south, in both cases as part of strategies that still relied heavily on fishing, hunting, and the gathering of wild plants"" and suggests that these subsistence changes were not due to farmers migrating from the Near East but was an indigenous development, with cereals either indigenous or obtained through exchange. Other scholars argue that the primary stimulus for agriculture and domesticated animals (as well as mud-brick architecture and other Neolithic cultural features) in Egypt was from the Middle East. Not all of these cultural elements characteristic of the Neolithic appeared everywhere in the same order: the earliest farming societies in the Near East did not use pottery. In other parts of the world, such as Africa, South Asia and Southeast Asia, independent domestication events led to their own regionally distinctive Neolithic cultures that arose completely independent of those in Europe and Southwest Asia. Early Japanese societies and other East Asian cultures used pottery before developing agriculture. In Mesoamerica, a similar set of events (i.e., crop domestication and sedentary lifestyles) occurred by around 4500 BC, but possibly as early as 11,000–10,000 BC. These cultures are usually not referred to as belonging to the Neolithic; in America different terms are used such as Formative stage instead of mid-late Neolithic, Archaic Era instead of Early Neolithic and Paleo-Indian for the preceding period. The Formative stage is equivalent to the Neolithic Revolution period in Europe, Asia, and Africa. In the Southwestern United States it occurred from 500 to 1200 C.E. when there was a dramatic increase in population and development of large villages supported by agriculture based on dryland farming of maize, and later, beans, squash, and domesticated turkeys. During this period the bow and arrow and ceramic pottery were also introduced. Neolithic people were skilled farmers, manufacturing a range of tools necessary for the tending, harvesting and processing of crops (such as sickle blades and grinding stones) and food production (e.g. pottery, bone implements). They were also skilled manufacturers of a range of other types of stone tools and ornaments, including projectile points, beads, and statuettes. But what allowed forest clearance on a large scale was the polished stone axe above all other tools. Together with the adze, fashioning wood for shelter, structures and canoes for example, this enabled them to exploit their newly won farmland. Control of labour and inter-group conflict is characteristic of corporate-level or 'tribal' groups, headed by a charismatic individual; whether a 'big man' or a proto-chief, functioning as a lineage-group head. Whether a non-hierarchical system of organization existed is debatable, and there is no evidence that explicitly suggests that Neolithic societies functioned under any dominating class or individual, as was the case in the chiefdoms of the European Early Bronze Age. Theories to explain the apparent implied egalitarianism of Neolithic (and Paleolithic) societies have arisen, notably the Marxist concept of primitive communism. In 2012, news was released about a new farming site discovered in Munam-ri, Goseong, Gangwon Province, South Korea, which may be the earliest farmland known to date in east Asia. ""No remains of an agricultural field from the Neolithic period have been found in any East Asian country before, the institute said, adding that the discovery reveals that the history of agricultural cultivation at least began during the period on the Korean Peninsula"". The farm was dated between 3600 and 3000 B.C. Pottery, stone projectile points, and possible houses were also found. ""In 2002, researchers discovered prehistoric earthenware, jade earrings, among other items in the area"". The research team will perform accelerator mass spectrometry (AMS) dating to retrieve a more precise date for the site. The domestication of large animals (c. 8000 BC) resulted in a dramatic increase in social inequality in most of the areas where it occurred; New Guinea being a notable exception. Possession of livestock allowed competition between households and resulted in inherited inequalities of wealth. Neolithic pastoralists who controlled large herds gradually acquired more livestock, and this made economic inequalities more pronounced. However, evidence of social inequality is still disputed, as settlements such as Catal Huyuk reveal a striking lack of difference in the size of homes and burial sites, suggesting a more egalitarian society with no evidence of the concept of capital, although some homes do appear slightly larger or more elaborately decorated than others. Another significant change undergone by many of these newly agrarian communities was one of diet. Pre-agrarian diets varied by region, season, available local plant and animal resources and degree of pastoralism and hunting. Post-agrarian diet was restricted to a limited package of successfully cultivated cereal grains, plants and to a variable extent domesticated animals and animal products. Supplementation of diet by hunting and gathering was to variable degrees precluded by the increase in population above the carrying capacity of the land and a high sedentary local population concentration. In some cultures, there would have been a significant shift toward increased starch and plant protein. The relative nutritional benefits and drawbacks of these dietary changes and their overall impact on early societal development is still debated. There is a large body of evidence for fortified settlements at Linearbandkeramik sites along the Rhine, as at least some villages were fortified for some time with a palisade and an outer ditch. Settlements with palisades and weapon-traumatized bones have been discovered, such as at the Talheim Death Pit demonstrates ""...systematic violence between groups"" and warfare was probably much more common during the Neolithic than in the preceding Paleolithic period. This supplanted an earlier view of the Linear Pottery Culture as living a ""peaceful, unfortified lifestyle"". Around 10,200 BC the first fully developed Neolithic cultures belonging to the phase Pre-Pottery Neolithic A (PPNA) appeared in the fertile crescent. Around 10,700 to 9,400 BC a settlement was established in Tell Qaramel, 10 miles north of Aleppo. The settlement included 2 temples dating back to 9,650. Around 9000 BC during the PPNA, one of the world's first towns, Jericho, appeared in the Levant. It was surrounded by a stone and marble wall and contained a population of 2000–3000 people and a massive stone tower. Around 6,400 BC the Halaf culture appeared in Lebanon, Israel and Palestine, Syria, Anatolia, and Northern Mesopotamia and subsisted on dryland agriculture. A significant and far-reaching shift in human subsistence and lifestyle was to be brought about in areas where crop farming and cultivation were first developed: the previous reliance on an essentially nomadic hunter-gatherer subsistence technique or pastoral transhumance was at first supplemented, and then increasingly replaced by, a reliance upon the foods produced from cultivated lands. These developments are also believed to have greatly encouraged the growth of settlements, since it may be supposed that the increased need to spend more time and labor in tending crop fields required more localized dwellings. This trend would continue into the Bronze Age, eventually giving rise to permanently settled farming towns, and later cities and states whose larger populations could be sustained by the increased productivity from cultivated lands. The beginning of the Neolithic culture is considered to be in the Levant (Jericho, modern-day West Bank) about 10,200 – 8,800 BC. It developed directly from the Epipaleolithic Natufian culture in the region, whose people pioneered the use of wild cereals, which then evolved into true farming. The Natufian period was between 12,000 and 10,200 BC, and the so-called ""proto-Neolithic"" is now included in the Pre-Pottery Neolithic (PPNA) between 10,200 and 8,800 BC. As the Natufians had become dependent on wild cereals in their diet, and a sedentary way of life had begun among them, the climatic changes associated with the Younger Dryas are thought to have forced people to develop farming. However, early farmers were also adversely affected in times of famine, such as may be caused by drought or pests. In instances where agriculture had become the predominant way of life, the sensitivity to these shortages could be particularly acute, affecting agrarian populations to an extent that otherwise may not have been routinely experienced by prior hunter-gatherer communities. Nevertheless, agrarian communities generally proved successful, and their growth and the expansion of territory under cultivation continued. During most of the Neolithic age of Eurasia, people lived in small tribes composed of multiple bands or lineages. There is little scientific evidence of developed social stratification in most Neolithic societies; social stratification is more associated with the later Bronze Age. Although some late Eurasian Neolithic societies formed complex stratified chiefdoms or even states, states evolved in Eurasia only with the rise of metallurgy, and most Neolithic societies on the whole were relatively simple and egalitarian. Beyond Eurasia, however, states were formed during the local Neolithic in three areas, namely in the Preceramic Andes with the Norte Chico Civilization, Formative Mesoamerica and Ancient Hawaiʻi. However, most Neolithic societies were noticeably more hierarchical than the Paleolithic cultures that preceded them and hunter-gatherer cultures in general. The Neolithic 1 (PPNA) period began roughly 10,000 years ago in the Levant. A temple area in southeastern Turkey at Göbekli Tepe dated around 9,500 BC may be regarded as the beginning of the period. This site was developed by nomadic hunter-gatherer tribes, evidenced by the lack of permanent housing in the vicinity and may be the oldest known human-made place of worship. At least seven stone circles, covering 25 acres (10 ha), contain limestone pillars carved with animals, insects, and birds. Stone tools were used by perhaps as many as hundreds of people to create the pillars, which might have supported roofs. Other early PPNA sites dating to around 9,500 to 9,000 BCE have been found in Jericho, Israel (notably Ain Mallaha, Nahal Oren, and Kfar HaHoresh), Gilgal in the Jordan Valley, and Byblos, Lebanon. The start of Neolithic 1 overlaps the Tahunian and Heavy Neolithic periods to some degree.[citation needed]"
Yale_University,"Yale's central campus in downtown New Haven covers 260 acres (1.1 km2) and comprises its main, historic campus and a medical campus adjacent to the Yale-New Haven Hospital. In western New Haven, the university holds 500 acres (2.0 km2) of athletic facilities, including the Yale Golf Course. In 2008, Yale purchased the 136-acre (0.55 km2) former Bayer Pharmaceutical campus in West Haven, Connecticut, the buildings of which are now used as laboratory and research space. Yale also owns seven forests in Connecticut, Vermont, and New Hampshire—the largest of which is the 7,840-acre (31.7 km2) Yale-Myers Forest in Connecticut's Quiet Corner—and nature preserves including Horse Island. Yale seniors at graduation smash clay pipes underfoot to symbolize passage from their ""bright college years,"" though in recent history the pipes have been replaced with ""bubble pipes"". (""Bright College Years,"" the University's alma mater, was penned in 1881 by Henry Durand, Class of 1881, to the tune of Die Wacht am Rhein.) Yale's student tour guides tell visitors that students consider it good luck to rub the toe of the statue of Theodore Dwight Woolsey on Old Campus. Actual students rarely do so. In the second half of the 20th century Bladderball, a campus-wide game played with a large inflatable ball, became a popular tradition but was banned by administration due to safety concerns. In spite of administration opposition, students revived the game in 2009, 2011, and 2014, but its future remains uncertain. The Revolutionary War soldier Nathan Hale (Yale 1773) was the prototype of the Yale ideal in the early 19th century: a manly yet aristocratic scholar, equally well-versed in knowledge and sports, and a patriot who ""regretted"" that he ""had but one life to lose"" for his country. Western painter Frederic Remington (Yale 1900) was an artist whose heroes gloried in combat and tests of strength in the Wild West. The fictional, turn-of-the-20th-century Yale man Frank Merriwell embodied the heroic ideal without racial prejudice, and his fictional successor Frank Stover in the novel Stover at Yale (1911) questioned the business mentality that had become prevalent at the school. Increasingly the students turned to athletic stars as their heroes, especially since winning the big game became the goal of the student body, and the alumni, as well as the team itself. Yale has numerous athletic facilities, including the Yale Bowl (the nation's first natural ""bowl"" stadium, and prototype for such stadiums as the Los Angeles Memorial Coliseum and the Rose Bowl), located at The Walter Camp Field athletic complex, and the Payne Whitney Gymnasium, the second-largest indoor athletic complex in the world. October 21, 2000, marked the dedication of Yale's fourth new boathouse in 157 years of collegiate rowing. The Richard Gilder Boathouse is named to honor former Olympic rower Virginia Gilder '79 and her father Richard Gilder '54, who gave $4 million towards the $7.5 million project. Yale also maintains the Gales Ferry site where the heavyweight men's team trains for the Yale-Harvard Boat Race. Several campus safety strategies have been pioneered at Yale. The first campus police force was founded at Yale in 1894, when the university contracted city police officers to exclusively cover the campus. Later hired by the university, the officers were originally brought in to quell unrest between students and city residents and curb destructive student behavior. In addition to the Yale Police Department, a variety of safety services are available including blue phones, a safety escort, and 24-hour shuttle service. Other examples of the Gothic (also called neo-Gothic and collegiate Gothic) style are on Old Campus by such architects as Henry Austin, Charles C. Haight and Russell Sturgis. Several are associated with members of the Vanderbilt family, including Vanderbilt Hall, Phelps Hall, St. Anthony Hall (a commission for member Frederick William Vanderbilt), the Mason, Sloane and Osborn laboratories, dormitories for the Sheffield Scientific School (the engineering and sciences school at Yale until 1956) and elements of Silliman College, the largest residential college. Yale's secret society buildings (some of which are called ""tombs"") were built both to be private yet unmistakable. A diversity of architectural styles is represented: Berzelius, Donn Barber in an austere cube with classical detailing (erected in 1908 or 1910); Book and Snake, Louis R. Metcalfe in a Greek Ionic style (erected in 1901); Elihu, architect unknown but built in a Colonial style (constructed on an early 17th-century foundation although the building is from the 18th century); Mace and Chain, in a late colonial, early Victorian style (built in 1823). Interior moulding is said to have belonged to Benedict Arnold; Manuscript Society, King Lui-Wu with Dan Kniley responsible for landscaping and Josef Albers for the brickwork intaglio mural. Building constructed in a mid-century modern style; Scroll and Key, Richard Morris Hunt in a Moorish- or Islamic-inspired Beaux-Arts style (erected 1869–70); Skull and Bones, possibly Alexander Jackson Davis or Henry Austin in an Egypto-Doric style utilizing Brownstone (in 1856 the first wing was completed, in 1903 the second wing, 1911 the Neo-Gothic towers in rear garden were completed); St. Elmo, (former tomb) Kenneth M. Murchison, 1912, designs inspired by Elizabethan manor. Current location, brick colonial; Shabtai, 1882, the Anderson Mansion built in the Second Empire architectural style; and Wolf's Head, Bertram Grosvenor Goodhue (erected 1923-4). Much of Yale University's staff, including most maintenance staff, dining hall employees, and administrative staff, are unionized. Clerical and technical employees are represented by Local 34 of UNITE HERE and service and maintenance workers by Local 35 of the same international. Together with the Graduate Employees and Students Organization (GESO), an unrecognized union of graduate employees, Locals 34 and 35 make up the Federation of Hospital and University Employees. Also included in FHUE are the dietary workers at Yale-New Haven Hospital, who are members of 1199 SEIU. In addition to these unions, officers of the Yale University Police Department are members of the Yale Police Benevolent Association, which affiliated in 2005 with the Connecticut Organization for Public Safety Employees. Finally, Yale security officers voted to join the International Union of Security, Police and Fire Professionals of America in fall 2010 after the National Labor Relations Board ruled they could not join AFSCME; the Yale administration contested the election. Yale's Office of Sustainability develops and implements sustainability practices at Yale. Yale is committed to reduce its greenhouse gas emissions 10% below 1990 levels by the year 2020. As part of this commitment, the university allocates renewable energy credits to offset some of the energy used by residential colleges. Eleven campus buildings are candidates for LEED design and certification. Yale Sustainable Food Project initiated the introduction of local, organic vegetables, fruits, and beef to all residential college dining halls. Yale was listed as a Campus Sustainability Leader on the Sustainable Endowments Institute’s College Sustainability Report Card 2008, and received a ""B+"" grade overall. Serious American students of theology and divinity, particularly in New England, regarded Hebrew as a classical language, along with Greek and Latin, and essential for study of the Old Testament in the original words. The Reverend Ezra Stiles, president of the College from 1778 to 1795, brought with him his interest in the Hebrew language as a vehicle for studying ancient Biblical texts in their original language (as was common in other schools), requiring all freshmen to study Hebrew (in contrast to Harvard, where only upperclassmen were required to study the language) and is responsible for the Hebrew phrase אורים ותמים (Urim and Thummim) on the Yale seal. Stiles' greatest challenge occurred in July 1779 when hostile British forces occupied New Haven and threatened to raze the College. However, Yale graduate Edmund Fanning, Secretary to the British General in command of the occupation, interceded and the College was saved. Fanning later was granted an honorary degree LL.D., at 1803, for his efforts. During the 1988 presidential election, George H. W. Bush (Yale '48) derided Michael Dukakis for having ""foreign-policy views born in Harvard Yard's boutique"". When challenged on the distinction between Dukakis's Harvard connection and his own Yale background, he said that, unlike Harvard, Yale's reputation was ""so diffuse, there isn't a symbol, I don't think, in the Yale situation, any symbolism in it"" and said Yale did not share Harvard's reputation for ""liberalism and elitism"". In 2004 Howard Dean stated, ""In some ways, I consider myself separate from the other three (Yale) candidates of 2004. Yale changed so much between the class of '68 and the class of '71. My class was the first class to have women in it; it was the first class to have a significant effort to recruit African Americans. It was an extraordinary time, and in that span of time is the change of an entire generation"". In the wake of the racially-motivated"" church shooting in Charleston, South Carolina, Yale was under criticism again in the summer of 2015 for Calhoun College, one of 12 residential colleges, which was named after John C. Calhoun, a slave-owner and strong slavery supporter in the nineteenth century. In July 2015 students signed a petition calling for the name change. They argued in the petition that—while Calhoun was respected in the 19th century as an ""extraordinary American statesman""—he was ""one of the most prolific defenders of slavery and white supremacy"" in the history of the United States. In August 2015 Yale President Peter Salovey addressed the Freshman Class of 2019 in which he responded to the racial tensions but explained why the college would not be renamed. He described Calhoun as a ""a notable political theorist, a vice president to two different U.S. presidents, a secretary of war and of state, and a congressman and senator representing South Carolina."" He acknowledged that Calhoun also ""believed that the highest forms of civilization depend on involuntary servitude. Not only that, but he also believed that the races he thought to be inferior, black people in particular, ought to be subjected to it for the sake of their own best interests."" Racial tensions increased in the fall of 2015 centering on comments by Nicholas A. Christakis and his wife Erika regarding freedom of speech. In April 2016 Salovey announced that ""despite decades of vigorous alumni and student protests,"" Calhoun's name will remain on the Yale residential college explaining that it is preferable for Yale students to live in Calhoun's ""shadow"" so they will be ""better prepared to rise to the challenges of the present and the future."" He claimed that if they removed Calhoun's name, it would ""obscure"" his ""legacy of slavery rather than addressing it."" ""Yale is part of that history"" and ""We cannot erase American history, but we can confront it, teach it and learn from it."" One change that will be issued is the title of “master” for faculty members who serve as residential college leaders will be renamed to “head of college” due to its connotation of slavery. Several explanations have been offered for Yale’s representation in national elections since the end of the Vietnam War. Various sources note the spirit of campus activism that has existed at Yale since the 1960s, and the intellectual influence of Reverend William Sloane Coffin on many of the future candidates. Yale President Richard Levin attributes the run to Yale’s focus on creating ""a laboratory for future leaders,"" an institutional priority that began during the tenure of Yale Presidents Alfred Whitney Griswold and Kingman Brewster. Richard H. Brodhead, former dean of Yale College and now president of Duke University, stated: ""We do give very significant attention to orientation to the community in our admissions, and there is a very strong tradition of volunteerism at Yale."" Yale historian Gaddis Smith notes ""an ethos of organized activity"" at Yale during the 20th century that led John Kerry to lead the Yale Political Union's Liberal Party, George Pataki the Conservative Party, and Joseph Lieberman to manage the Yale Daily News. Camille Paglia points to a history of networking and elitism: ""It has to do with a web of friendships and affiliations built up in school."" CNN suggests that George W. Bush benefited from preferential admissions policies for the ""son and grandson of alumni"", and for a ""member of a politically influential family."" New York Times correspondent Elisabeth Bumiller and The Atlantic Monthly correspondent James Fallows credit the culture of community and cooperation that exists between students, faculty, and administration, which downplays self-interest and reinforces commitment to others. Alumnus Eero Saarinen, Finnish-American architect of such notable structures as the Gateway Arch in St. Louis, Washington Dulles International Airport main terminal, Bell Labs Holmdel Complex and the CBS Building in Manhattan, designed Ingalls Rink at Yale and the newest residential colleges of Ezra Stiles and Morse. These latter were modelled after the medieval Italian hilltown of San Gimignano – a prototype chosen for the town's pedestrian-friendly milieu and fortress-like stone towers. These tower forms at Yale act in counterpoint to the college's many Gothic spires and Georgian cupolas. In 1909–10, football faced a crisis resulting from the failure of the previous reforms of 1905–06 to solve the problem of serious injuries. There was a mood of alarm and mistrust, and, while the crisis was developing, the presidents of Harvard, Yale, and Princeton developed a project to reform the sport and forestall possible radical changes forced by government upon the sport. President Arthur Hadley of Yale, A. Lawrence Lowell of Harvard, and Woodrow Wilson of Princeton worked to develop moderate changes to reduce injuries. Their attempts, however, were reduced by rebellion against the rules committee and formation of the Intercollegiate Athletic Association. The big three had tried to operate independently of the majority, but changes did reduce injuries. A decade into co-education, rampant student assault and harassment by faculty became the impetus for the trailblazing lawsuit Alexander v. Yale. While unsuccessful in the courts, the legal reasoning behind the case changed the landscape of sex discrimination law and resulted in the establishment of Yale's Grievance Board and the Yale Women's Center. In March 2011 a Title IX complaint was filed against Yale by students and recent graduates, including editors of Yale's feminist magazine Broad Recognition, alleging that the university had a hostile sexual climate. In response, the university formed a Title IX steering committee to address complaints of sexual misconduct. In 1718, at the behest of either Rector Samuel Andrew or the colony's Governor Gurdon Saltonstall, Cotton Mather contacted a successful businessman named Elihu Yale, who lived in Wales but had been born in Boston and whose father, David, had been one of the original settlers in New Haven, to ask him for financial help in constructing a new building for the college. Through the persuasion of Jeremiah Dummer, Yale, who had made a fortune through trade while living in Madras as a representative of the East India Company, donated nine bales of goods, which were sold for more than £560, a substantial sum at the time. Cotton Mather suggested that the school change its name to Yale College. Meanwhile, a Harvard graduate working in England convinced some 180 prominent intellectuals that they should donate books to Yale. The 1714 shipment of 500 books represented the best of modern English literature, science, philosophy and theology. It had a profound effect on intellectuals at Yale. Undergraduate Jonathan Edwards discovered John Locke's works and developed his original theology known as the ""new divinity."" In 1722 the Rector and six of his friends, who had a study group to discuss the new ideas, announced that they had given up Calvinism, become Arminians, and joined the Church of England. They were ordained in England and returned to the colonies as missionaries for the Anglican faith. Thomas Clapp became president in 1745, and struggled to return the college to Calvinist orthodoxy; but he did not close the library. Other students found Deist books in the library. Yale has a complicated relationship with its home city; for example, thousands of students volunteer every year in a myriad of community organizations, but city officials, who decry Yale's exemption from local property taxes, have long pressed the university to do more to help. Under President Levin, Yale has financially supported many of New Haven's efforts to reinvigorate the city. Evidence suggests that the town and gown relationships are mutually beneficial. Still, the economic power of the university increased dramatically with its financial success amid a decline in the local economy. Yale has produced alumni distinguished in their respective fields. Among the best-known are U.S. Presidents William Howard Taft, Gerald Ford, George H. W. Bush, Bill Clinton and George W. Bush; royals Crown Princess Victoria Bernadotte, Prince Rostislav Romanov and Prince Akiiki Hosea Nyabongo; heads of state, including Italian prime minister Mario Monti, Turkish prime minister Tansu Çiller, Mexican president Ernesto Zedillo, German president Karl Carstens, and Philippines president José Paciano Laurel; U.S. Supreme Court Justices Sonia Sotomayor, Samuel Alito and Clarence Thomas; U.S. Secretaries of State John Kerry, Hillary Clinton, Cyrus Vance, and Dean Acheson; authors Sinclair Lewis, Stephen Vincent Benét, and Tom Wolfe; lexicographer Noah Webster; inventors Samuel F. B. Morse and Eli Whitney; patriot and ""first spy"" Nathan Hale; theologian Jonathan Edwards; actors, directors and producers Paul Newman, Henry Winkler, Vincent Price, Meryl Streep, Sigourney Weaver, Jodie Foster, Angela Bassett, Patricia Clarkson, Courtney Vance, Frances McDormand, Elia Kazan, George Roy Hill, Edward Norton, Lupita Nyong'o, Allison Williams, Oliver Stone, Sam Waterston, and Michael Cimino; ""Father of American football"" Walter Camp, James Franco, ""The perfect oarsman"" Rusty Wailes; baseball players Ron Darling, Bill Hutchinson, and Craig Breslow; basketball player Chris Dudley; football players Gary Fencik, and Calvin Hill; hockey players Chris Higgins and Mike Richter; figure skater Sarah Hughes; swimmer Don Schollander; skier Ryan Max Riley; runner Frank Shorter; composers Charles Ives, Douglas Moore and Cole Porter; Peace Corps founder Sargent Shriver; child psychologist Benjamin Spock; architects Eero Saarinen and Norman Foster; sculptor Richard Serra; film critic Gene Siskel; television commentators Dick Cavett and Anderson Cooper; New York Times journalist David Gonzalez; pundits William F. Buckley, Jr., and Fareed Zakaria; economists Irving Fischer, Mahbub ul Haq, and Paul Krugman; cyclotron inventor and Nobel laureate in Physics, Ernest Lawrence; Human Genome Project director Francis S. Collins; mathematician and chemist Josiah Willard Gibbs; and businesspeople, including Time Magazine co-founder Henry Luce, Morgan Stanley founder Harold Stanley, Boeing CEO James McNerney, FedEx founder Frederick W. Smith, Time Warner president Jeffrey Bewkes, Electronic Arts co-founder Bing Gordon, and investor/philanthropist Sir John Templeton; pioneer in electrical applications Austin Cornelius Dunham. In 2009, former British Prime Minister Tony Blair picked Yale as one location – the others are Britain's Durham University and Universiti Teknologi Mara – for the Tony Blair Faith Foundation's United States Faith and Globalization Initiative. As of 2009, former Mexican President Ernesto Zedillo is the director of the Yale Center for the Study of Globalization and teaches an undergraduate seminar, ""Debating Globalization"". As of 2009, former presidential candidate and DNC chair Howard Dean teaches a residential college seminar, ""Understanding Politics and Politicians."" Also in 2009, an alliance was formed among Yale, University College London, and both schools’ affiliated hospital complexes to conduct research focused on the direct improvement of patient care—a growing field known as translational medicine. President Richard Levin noted that Yale has hundreds of other partnerships across the world, but ""no existing collaboration matches the scale of the new partnership with UCL"". Rare books are found in several Yale collections. The Beinecke Rare Book Library has a large collection of rare books and manuscripts. The Harvey Cushing/John Hay Whitney Medical Library includes important historical medical texts, including an impressive collection of rare books, as well as historical medical instruments. The Lewis Walpole Library contains the largest collection of 18th‑century British literary works. The Elizabethan Club, technically a private organization, makes its Elizabethan folios and first editions available to qualified researchers through Yale. Milton Winternitz led the Yale Medical School as its dean from 1920 to 1935. Dedicated to the new scientific medicine established in Germany, he was equally fervent about ""social medicine"" and the study of humans in their culture and environment. He established the ""Yale System"" of teaching, with few lectures and fewer exams, and strengthened the full-time faculty system; he also created the graduate-level Yale School of Nursing and the Psychiatry Department, and built numerous new buildings. Progress toward his plans for an Institute of Human Relations, envisioned as a refuge where social scientists would collaborate with biological scientists in a holistic study of humankind, unfortunately lasted for only a few years before the opposition of resentful anti-Semitic colleagues drove him to resign. Many of Yale's buildings were constructed in the Collegiate Gothic architecture style from 1917 to 1931, financed largely by Edward S. Harkness Stone sculpture built into the walls of the buildings portray contemporary college personalities such as a writer, an athlete, a tea-drinking socialite, and a student who has fallen asleep while reading. Similarly, the decorative friezes on the buildings depict contemporary scenes such as policemen chasing a robber and arresting a prostitute (on the wall of the Law School), or a student relaxing with a mug of beer and a cigarette. The architect, James Gamble Rogers, faux-aged these buildings by splashing the walls with acid, deliberately breaking their leaded glass windows and repairing them in the style of the Middle Ages, and creating niches for decorative statuary but leaving them empty to simulate loss or theft over the ages. In fact, the buildings merely simulate Middle Ages architecture, for though they appear to be constructed of solid stone blocks in the authentic manner, most actually have steel framing as was commonly used in 1930. One exception is Harkness Tower, 216 feet (66 m) tall, which was originally a free-standing stone structure. It was reinforced in 1964 to allow the installation of the Yale Memorial Carillon. Between 1892, when Harvard and Yale met in one of the first intercollegiate debates, and 1909, the year of the first Triangular Debate of Harvard, Yale, and Princeton, the rhetoric, symbolism, and metaphors used in athletics were used to frame these early debates. Debates were covered on front pages of college newspapers and emphasized in yearbooks, and team members even received the equivalent of athletic letters for their jackets. There even were rallies sending off the debating teams to matches. Yet, the debates never attained the broad appeal that athletics enjoyed. One reason may be that debates do not have a clear winner, as is the case in sports, and that scoring is subjective. In addition, with late 19th-century concerns about the impact of modern life on the human body, athletics offered hope that neither the individual nor the society was coming apart. Yale expanded gradually, establishing the Yale School of Medicine (1810), Yale Divinity School (1822), Yale Law School (1843), Yale Graduate School of Arts and Sciences (1847), the Sheffield Scientific School (1847), and the Yale School of Fine Arts (1869). In 1887, as the college continued to grow under the presidency of Timothy Dwight V, Yale College was renamed Yale University. The university would later add the Yale School of Music (1894), the Yale School of Forestry & Environmental Studies (founded by Gifford Pinchot in 1900), the Yale School of Public Health (1915), the Yale School of Nursing (1923), the Yale School of Drama (1955), the Yale Physician Associate Program (1973), and the Yale School of Management (1976). It would also reorganize its relationship with the Sheffield Scientific School. Expansion caused controversy about Yale's new roles. Noah Porter, moral philosopher, was president from 1871 to 1886. During an age of tremendous expansion in higher education, Porter resisted the rise of the new research university, claiming that an eager embrace of its ideals would corrupt undergraduate education. Many of Porter's contemporaries criticized his administration, and historians since have disparaged his leadership. Levesque argues Porter was not a simple-minded reactionary, uncritically committed to tradition, but a principled and selective conservative. He did not endorse everything old or reject everything new; rather, he sought to apply long-established ethical and pedagogical principles to a rapidly changing culture. He may have misunderstood some of the challenges of his time, but he correctly anticipated the enduring tensions that have accompanied the emergence and growth of the modern university. Yale University, one of the oldest universities in the United States, is a cultural referent as an institution that produces some of the most elite members of society and its grounds, alumni, and students have been prominently portrayed in fiction and U.S. popular culture. For example, Owen Johnson's novel, Stover at Yale, follows the college career of Dink Stover and Frank Merriwell, the model for all later juvenile sports fiction, plays football, baseball, crew, and track at Yale while solving mysteries and righting wrongs. Yale University also is featured in F. Scott Fitzgerald's novel ""The Great Gatsby"". The narrator, Nick Carraway, wrote a series of editorials for the Yale News, and Tom Buchanan was ""one of the most powerful ends that ever played football"" for Yale. Yale has a history of difficult and prolonged labor negotiations, often culminating in strikes. There have been at least eight strikes since 1968, and The New York Times wrote that Yale has a reputation as having the worst record of labor tension of any university in the U.S. Yale's unusually large endowment exacerbates the tension over wages. Moreover, Yale has been accused of failing to treat workers with respect. In a 2003 strike, however, the university claimed that more union employees were working than striking. Professor David Graeber was 'retired' after he came to the defense of a student who was involved in campus labor issues. Through its program of need-based financial aid, Yale commits to meet the full demonstrated financial need of all applicants. Most financial aid is in the form of grants and scholarships that do not need to be paid back to the university, and the average need-based aid grant for the Class of 2017 was $46,395. 15% of Yale College students are expected to have no parental contribution, and about 50% receive some form of financial aid. About 16% of the Class of 2013 had some form of student loan debt at graduation, with an average debt of $13,000 among borrowers. In 1966, Yale began discussions with its sister school Vassar College about merging to foster coeducation at the undergraduate level. Vassar, then all-female and part of the Seven Sisters—elite higher education schools that historically served as sister institutions to the Ivy League when the Ivy League still only admitted men—tentatively accepted, but then declined the invitation. Both schools introduced coeducation independently in 1969. Amy Solomon was the first woman to register as a Yale undergraduate; she was also the first woman at Yale to join an undergraduate society, St. Anthony Hall. The undergraduate class of 1973 was the first class to have women starting from freshman year; at the time, all undergraduate women were housed in Vanderbilt Hall at the south end of Old Campus.[citation needed] Slack (2003) compares three groups that conducted biological research at Yale during overlapping periods between 1910 and 1970. Yale proved important as a site for this research. The leaders of these groups were Ross Granville Harrison, Grace E. Pickford, and G. Evelyn Hutchinson, and their members included both graduate students and more experienced scientists. All produced innovative research, including the opening of new subfields in embryology, endocrinology, and ecology, respectively, over a long period of time. Harrison's group is shown to have been a classic research school; Pickford's and Hutchinson's were not. Pickford's group was successful in spite of her lack of departmental or institutional position or power. Hutchinson and his graduate and postgraduate students were extremely productive, but in diverse areas of ecology rather than one focused area of research or the use of one set of research tools. Hutchinson's example shows that new models for research groups are needed, especially for those that include extensive field research. Yale's residential college system was established in 1933 by Edward S. Harkness, who admired the social intimacy of Oxford and Cambridge and donated significant funds to found similar colleges at Yale and Harvard. Though Yale's colleges resemble their English precursors organizationally and architecturally, they are dependent entities of Yale College and have limited autonomy. The colleges are led by a master and an academic dean, who reside in the college, and university faculty and affiliates comprise each college's fellowship. Colleges offer their own seminars, social events, and speaking engagements known as ""Master's Teas,"" but do not contain programs of study or academic departments. Instead, all undergraduate courses are taught by the Faculty of Arts and Sciences and are open to members of any college. Yale traces its beginnings to ""An Act for Liberty to Erect a Collegiate School,"" passed by the General Court of the Colony of Connecticut on October 9, 1701, while meeting in New Haven. The Act was an effort to create an institution to train ministers and lay leadership for Connecticut. Soon thereafter, a group of ten Congregationalist ministers: Samuel Andrew, Thomas Buckingham, Israel Chauncy, Samuel Mather, Rev. James Noyes II (son of James Noyes), James Pierpont, Abraham Pierson, Noadiah Russell, Joseph Webb and Timothy Woodbridge, all alumni of Harvard, met in the study of Reverend Samuel Russell in Branford, Connecticut, to pool their books to form the school's library. The group, led by James Pierpont, is now known as ""The Founders"".[citation needed] While Harkness' original colleges were Georgian Revival or Collegiate Gothic in style, two colleges constructed in the 1960s, Morse and Ezra Stiles Colleges, have modernist designs. All twelve college quadrangles are organized around a courtyard, and each has a dining hall, courtyard, library, common room, and a range of student facilities. The twelve colleges are named for important alumni or significant places in university history. In 2017, the university expects to open two new colleges near Science Hill. The university hosts a variety of student journals, magazines, and newspapers. Established in 1872, The Yale Record is the world's oldest humor magazine. Newspapers include the Yale Daily News, which was first published in 1878, and the weekly Yale Herald, which was first published in 1986. Dwight Hall, an independent, non-profit community service organization, oversees more than 2,000 Yale undergraduates working on more than 70 community service initiatives in New Haven. The Yale College Council runs several agencies that oversee campus wide activities and student services. The Yale Dramatic Association and Bulldog Productions cater to the theater and film communities, respectively. In addition, the Yale Drama Coalition serves to coordinate between and provide resources for the various Sudler Fund sponsored theater productions which run each weekend. WYBC Yale Radio is the campus's radio station, owned and operated by students. While students used to broadcast on AM & FM frequencies, they now have an Internet-only stream. Yale's English and Comparative Literature departments were part of the New Criticism movement. Of the New Critics, Robert Penn Warren, W.K. Wimsatt, and Cleanth Brooks were all Yale faculty. Later, the Yale Comparative literature department became a center of American deconstruction. Jacques Derrida, the father of deconstruction, taught at the Department of Comparative Literature from the late seventies to mid-1980s. Several other Yale faculty members were also associated with deconstruction, forming the so-called ""Yale School"". These included Paul de Man who taught in the Departments of Comparative Literature and French, J. Hillis Miller, Geoffrey Hartman (both taught in the Departments of English and Comparative Literature), and Harold Bloom (English), whose theoretical position was always somewhat specific, and who ultimately took a very different path from the rest of this group. Yale's history department has also originated important intellectual trends. Historians C. Vann Woodward and David Brion Davis are credited with beginning in the 1960s and 1970s an important stream of southern historians; likewise, David Montgomery, a labor historian, advised many of the current generation of labor historians in the country. Yale's Music School and Department fostered the growth of Music Theory in the latter half of the 20th century. The Journal of Music Theory was founded there in 1957; Allen Forte and David Lewin were influential teachers and scholars. Yale University is an American private Ivy League research university in New Haven, Connecticut. Founded in 1701 in Saybrook Colony as the Collegiate School, the University is the third-oldest institution of higher education in the United States. The school was renamed Yale College in 1718 in recognition of a gift from Elihu Yale, who was governor of the British East India Company. Established to train Congregationalist ministers in theology and sacred languages, by 1777 the school's curriculum began to incorporate humanities and sciences. In the 19th century the school incorporated graduate and professional instruction, awarding the first Ph.D. in the United States in 1861 and organizing as a university in 1887. Yale's museum collections are also of international stature. The Yale University Art Gallery, the country's first university-affiliated art museum, contains more than 180,000 works, including Old Masters and important collections of modern art, in the Swartout and Kahn buildings. The latter, Louis Kahn's first large-scale American work (1953), was renovated and reopened in December 2006. The Yale Center for British Art, the largest collection of British art outside of the UK, grew from a gift of Paul Mellon and is housed in another Kahn-designed building. The Yale Provost's Office has launched several women into prominent university presidencies. In 1977 Hanna Holborn Gray was appointed acting President of Yale from this position, and went on to become President of the University of Chicago, the first woman to be full president of a major university. In 1994 Yale Provost Judith Rodin became the first female president of an Ivy League institution at the University of Pennsylvania. In 2002 Provost Alison Richard became the Vice Chancellor of the University of Cambridge. In 2004, Provost Susan Hockfield became the President of the Massachusetts Institute of Technology. In 2007 Deputy Provost Kim Bottomly was named President of Wellesley College. In 2003, the Dean of the Divinity School, Rebecca Chopp, was appointed president of Colgate University and now heads Swarthmore College. Between 1925 and 1940, philanthropic foundations, especially ones connected with the Rockefellers, contributed about $7 million to support the Yale Institute of Human Relations and the affiliated Yerkes Laboratories of Primate Biology. The money went toward behavioral science research, which was supported by foundation officers who aimed to ""improve mankind"" under an informal, loosely defined human engineering effort. The behavioral scientists at Yale, led by President James R. Angell and psychobiologist Robert M. Yerkes, tapped into foundation largesse by crafting research programs aimed to investigate, then suggest, ways to control, sexual and social behavior. For example, Yerkes analyzed chimpanzee sexual behavior in hopes of illuminating the evolutionary underpinnings of human development and providing information that could ameliorate dysfunction. Ultimately, the behavioral-science results disappointed foundation officers, who shifted their human-engineering funds toward biological sciences. Yale is organized into fourteen constituent schools: the original undergraduate college, the Yale Graduate School of Arts and Sciences, and twelve professional schools. While the university is governed by the Yale Corporation, each school's faculty oversees its curriculum and degree programs. In addition to a central campus in downtown New Haven, the University owns athletic facilities in western New Haven, including the Yale Bowl, a campus in West Haven, Connecticut, and forest and nature preserves throughout New England. The university's assets include an endowment valued at $25.6 billion as of September 2015, the second largest of any educational institution.The Yale University Library, serving all constituent schools, holds more than 15 million volumes and is the third-largest academic library in the United States. The Yale Report of 1828 was a dogmatic defense of the Latin and Greek curriculum against critics who wanted more courses in modern languages, mathematics, and science. Unlike higher education in Europe, there was no national curriculum for colleges and universities in the United States. In the competition for students and financial support, college leaders strove to keep current with demands for innovation. At the same time, they realized that a significant portion of their students and prospective students demanded a classical background. The Yale report meant the classics would not be abandoned. All institutions experimented with changes in the curriculum, often resulting in a dual track. In the decentralized environment of higher education in the United States, balancing change with tradition was a common challenge because no one could afford to be completely modern or completely classical. A group of professors at Yale and New Haven Congregationalist ministers articulated a conservative response to the changes brought about by the Victorian culture. They concentrated on developing a whole man possessed of religious values sufficiently strong to resist temptations from within, yet flexible enough to adjust to the 'isms' (professionalism, materialism, individualism, and consumerism) tempting him from without. William Graham Sumner, professor from 1872 to 1909, taught in the emerging disciplines of economics and sociology to overflowing classrooms. He bested President Noah Porter, who disliked social science and wanted Yale to lock into its traditions of classical education. Porter objected to Sumner's use of a textbook by Herbert Spencer that espoused agnostic materialism because it might harm students. Yale has had many financial supporters, but some stand out by the magnitude or timeliness of their contributions. Among those who have made large donations commemorated at the university are: Elihu Yale; Jeremiah Dummer; the Harkness family (Edward, Anna, and William); the Beinecke family (Edwin, Frederick, and Walter); John William Sterling; Payne Whitney; Joseph E. Sheffield, Paul Mellon, Charles B. G. Murphy and William K. Lanman. The Yale Class of 1954, led by Richard Gilder, donated $70 million in commemoration of their 50th reunion. Charles B. Johnson, a 1954 graduate of Yale College, pledged a $250 million gift in 2013 to support of the construction of two new residential colleges. The Boston Globe wrote that ""if there's one school that can lay claim to educating the nation's top national leaders over the past three decades, it's Yale."" Yale alumni were represented on the Democratic or Republican ticket in every U.S. Presidential election between 1972 and 2004. Yale-educated Presidents since the end of the Vietnam War include Gerald Ford, George H.W. Bush, Bill Clinton, and George W. Bush, and major-party nominees during this period include John Kerry (2004), Joseph Lieberman (Vice President, 2000), and Sargent Shriver (Vice President, 1972). Other Yale alumni who made serious bids for the Presidency during this period include Hillary Clinton (2008), Howard Dean (2004), Gary Hart (1984 and 1988), Paul Tsongas (1992), Pat Robertson (1988) and Jerry Brown (1976, 1980, 1992). Yale is noted for its largely Collegiate Gothic campus as well as for several iconic modern buildings commonly discussed in architectural history survey courses: Louis Kahn's Yale Art Gallery and Center for British Art, Eero Saarinen's Ingalls Rink and Ezra Stiles and Morse Colleges, and Paul Rudolph's Art & Architecture Building. Yale also owns and has restored many noteworthy 19th-century mansions along Hillhouse Avenue, which was considered the most beautiful street in America by Charles Dickens when he visited the United States in the 1840s. In 2011, Travel+Leisure listed the Yale campus as one of the most beautiful in the United States. The American studies program reflected the worldwide anti-Communist ideological struggle. Norman Holmes Pearson, who worked for the Office of Strategic Studies in London during World War II, returned to Yale and headed the new American studies program, in which scholarship quickly became an instrument of promoting liberty. Popular among undergraduates, the program sought to instruct them in the fundamentals of American civilization and thereby instill a sense of nationalism and national purpose. Also during the 1940s and 1950s, Wyoming millionaire William Robertson Coe made large contributions to the American studies programs at Yale University and at the University of Wyoming. Coe was concerned to celebrate the 'values' of the Western United States in order to meet the ""threat of communism."""
Videoconferencing,"Technological developments by videoconferencing developers in the 2010s have extended the capabilities of video conferencing systems beyond the boardroom for use with hand-held mobile devices that combine the use of video, audio and on-screen drawing capabilities broadcasting in real-time over secure networks, independent of location. Mobile collaboration systems now allow multiple people in previously unreachable locations, such as workers on an off-shore oil rig, the ability to view and discuss issues with colleagues thousands of miles away. Traditional videoconferencing system manufacturers have begun providing mobile applications as well, such as those that allow for live and still image streaming. Videoconferencing is a highly useful technology for real-time telemedicine and telenursing applications, such as diagnosis, consulting, transmission of medical images, etc... With videoconferencing, patients may contact nurses and physicians in emergency or routine situations; physicians and other paramedical professionals can discuss cases across large distances. Rural areas can use this technology for diagnostic purposes, thus saving lives and making more efficient use of health care money. For example, a rural medical center in Ohio, United States, used videoconferencing to successfully cut the number of transfers of sick infants to a hospital 70 miles (110 km) away. This had previously cost nearly $10,000 per transfer. The core technology used in a videoconferencing system is digital compression of audio and video streams in real time. The hardware or software that performs compression is called a codec (coder/decoder). Compression rates of up to 1:500 can be achieved. The resulting digital stream of 1s and 0s is subdivided into labeled packets, which are then transmitted through a digital network of some kind (usually ISDN or IP). The use of audio modems in the transmission line allow for the use of POTS, or the Plain Old Telephone System, in some low-speed applications, such as videotelephony, because they convert the digital pulses to/from analog waves in the audio spectrum range. While videoconferencing technology was initially used primarily within internal corporate communication networks, one of the first community service usages of the technology started in 1992 through a unique partnership with PictureTel and IBM Corporations which at the time were promoting a jointly developed desktop based videoconferencing product known as the PCS/1. Over the next 15 years, Project DIANE (Diversified Information and Assistance Network) grew to utilize a variety of videoconferencing platforms to create a multi-state cooperative public service and distance education network consisting of several hundred schools, neighborhood centers, libraries, science museums, zoos and parks, public assistance centers, and other community oriented organizations. Videoconferencing can enable individuals in distant locations to participate in meetings on short notice, with time and money savings. Technology such as VoIP can be used in conjunction with desktop videoconferencing to enable low-cost face-to-face business meetings without leaving the desk, especially for businesses with widespread offices. The technology is also used for telecommuting, in which employees work from home. One research report based on a sampling of 1,800 corporate employees showed that, as of June 2010, 54% of the respondents with access to video conferencing used it “all of the time” or “frequently”. In May 2005, the first high definition video conferencing systems, produced by LifeSize Communications, were displayed at the Interop trade show in Las Vegas, Nevada, able to provide video at 30 frames per second with a 1280 by 720 display resolution. Polycom introduced its first high definition video conferencing system to the market in 2006. As of the 2010s, high definition resolution for videoconferencing became a popular feature, with most major suppliers in the videoconferencing market offering it. The MC controls the conferencing while it is active on the signaling plane, which is simply where the system manages conferencing creation, endpoint signaling and in-conferencing controls. This component negotiates parameters with every endpoint in the network and controls conferencing resources. While the MC controls resources and signaling negotiations, the MP operates on the media plane and receives media from each endpoint. The MP generates output streams from each endpoint and redirects the information to other endpoints in the conference. Videophone calls (also: videocalls, video chat as well as Skype and Skyping in verb form), differ from videoconferencing in that they expect to serve individuals, not groups. However that distinction has become increasingly blurred with technology improvements such as increased bandwidth and sophisticated software clients that can allow for multiple parties on a call. In general everyday usage the term videoconferencing is now frequently used instead of videocall for point-to-point calls between two units. Both videophone calls and videoconferencing are also now commonly referred to as a video link. Videoconferencing provides students with the opportunity to learn by participating in two-way communication forums. Furthermore, teachers and lecturers worldwide can be brought to remote or otherwise isolated educational facilities. Students from diverse communities and backgrounds can come together to learn about one another, although language barriers will continue to persist. Such students are able to explore, communicate, analyze and share information and ideas with one another. Through videoconferencing, students can visit other parts of the world to speak with their peers, and visit museums and educational facilities. Such virtual field trips can provide enriched learning opportunities to students, especially those in geographically isolated locations, and to the economically disadvantaged. Small schools can use these technologies to pool resources and provide courses, such as in foreign languages, which could not otherwise be offered. The U.S. Social Security Administration (SSA), which oversees the world's largest administrative judicial system under its Office of Disability Adjudication and Review (ODAR), has made extensive use of videoconferencing to conduct hearings at remote locations. In Fiscal Year (FY) 2009, the U.S. Social Security Administration (SSA) conducted 86,320 videoconferenced hearings, a 55% increase over FY 2008. In August 2010, the SSA opened its fifth and largest videoconferencing-only National Hearing Center (NHC), in St. Louis, Missouri. This continues the SSA's effort to use video hearings as a means to clear its substantial hearing backlog. Since 2007, the SSA has also established NHCs in Albuquerque, New Mexico, Baltimore, Maryland, Falls Church, Virginia, and Chicago, Illinois. High speed Internet connectivity has become more widely available at a reasonable cost and the cost of video capture and display technology has decreased. Consequently, personal videoconferencing systems based on a webcam, personal computer system, software compression and broadband Internet connectivity have become affordable to the general public. Also, the hardware used for this technology has continued to improve in quality, and prices have dropped dramatically. The availability of freeware (often as part of chat programs) has made software based videoconferencing accessible to many. A videoconference system is generally higher cost than a videophone and deploys greater capabilities. A videoconference (also known as a videoteleconference) allows two or more locations to communicate via live, simultaneous two-way video and audio transmissions. This is often accomplished by the use of a multipoint control unit (a centralized distribution and call management system) or by a similar non-centralized multipoint capability embedded in each videoconferencing unit. Again, technology improvements have circumvented traditional definitions by allowing multiple party videoconferencing via web-based applications. Typical use of the various technologies described above include calling or conferencing on a one-on-one, one-to-many or many-to-many basis for personal, business, educational, deaf Video Relay Service and tele-medical, diagnostic and rehabilitative use or services. New services utilizing videocalling and videoconferencing, such as teachers and psychologists conducting online sessions, personal videocalls to inmates incarcerated in penitentiaries, and videoconferencing to resolve airline engineering issues at maintenance facilities, are being created or evolving on an ongoing basis. VRS services have become well developed nationally in Sweden since 1997 and also in the United States since the first decade of the 2000s. With the exception of Sweden, VRS has been provided in Europe for only a few years since the mid-2000s, and as of 2010 has not been made available in many European Union countries, with most European countries still lacking the legislation or the financing for large-scale VRS services, and to provide the necessary telecommunication equipment to deaf users. Germany and the Nordic countries are among the other leaders in Europe, while the United States is another world leader in the provisioning of VRS services. This technique was very expensive, though, and could not be used for applications such as telemedicine, distance education, and business meetings. Attempts at using normal telephony networks to transmit slow-scan video, such as the first systems developed by AT&T Corporation, first researched in the 1950s, failed mostly due to the poor picture quality and the lack of efficient video compression techniques. The greater 1 MHz bandwidth and 6 Mbit/s bit rate of the Picturephone in the 1970s also did not achieve commercial success, mostly due to its high cost, but also due to a lack of network effect —with only a few hundred Picturephones in the world, users had extremely few contacts they could actually call to, and interoperability with other videophone systems would not exist for decades. Some systems are capable of multipoint conferencing with no MCU, stand-alone, embedded or otherwise. These use a standards-based H.323 technique known as ""decentralized multipoint"", where each station in a multipoint call exchanges video and audio directly with the other stations with no central ""manager"" or other bottleneck. The advantages of this technique are that the video and audio will generally be of higher quality because they don't have to be relayed through a central point. Also, users can make ad-hoc multipoint calls without any concern for the availability or control of an MCU. This added convenience and quality comes at the expense of some increased network bandwidth, because every station must transmit to every other station directly. In the increasingly globalized film industry, videoconferencing has become useful as a method by which creative talent in many different locations can collaborate closely on the complex details of film production. For example, for the 2013 award-winning animated film Frozen, Burbank-based Walt Disney Animation Studios hired the New York City-based husband-and-wife songwriting team of Robert Lopez and Kristen Anderson-Lopez to write the songs, which required two-hour-long transcontinental videoconferences nearly every weekday for about 14 months. Simultaneous videoconferencing among three or more remote points is possible by means of a Multipoint Control Unit (MCU). This is a bridge that interconnects calls from several sources (in a similar way to the audio conference call). All parties call the MCU, or the MCU can also call the parties which are going to participate, in sequence. There are MCU bridges for IP and ISDN-based videoconferencing. There are MCUs which are pure software, and others which are a combination of hardware and software. An MCU is characterised according to the number of simultaneous calls it can handle, its ability to conduct transposing of data rates and protocols, and features such as Continuous Presence, in which multiple parties can be seen on-screen at once. MCUs can be stand-alone hardware devices, or they can be embedded into dedicated videoconferencing units. One of the first demonstrations of the ability for telecommunications to help sign language users communicate with each other occurred when AT&T's videophone (trademarked as the ""Picturephone"") was introduced to the public at the 1964 New York World's Fair –two deaf users were able to communicate freely with each other between the fair and another city. Various universities and other organizations, including British Telecom's Martlesham facility, have also conducted extensive research on signing via videotelephony. The use of sign language via videotelephony was hampered for many years due to the difficulty of its use over slow analogue copper phone lines, coupled with the high cost of better quality ISDN (data) phone lines. Those factors largely disappeared with the introduction of more efficient video codecs and the advent of lower cost high-speed ISDN data and IP (Internet) services in the 1990s. It was only in the 1980s that digital telephony transmission networks became possible, such as with ISDN networks, assuring a minimum bit rate (usually 128 kilobits/s) for compressed video and audio transmission. During this time, there was also research into other forms of digital video and audio communication. Many of these technologies, such as the Media space, are not as widely used today as videoconferencing but were still an important area of research. The first dedicated systems started to appear in the market as ISDN networks were expanding throughout the world. One of the first commercial videoconferencing systems sold to companies came from PictureTel Corp., which had an Initial Public Offering in November, 1984. Finally, in the 1990s, Internet Protocol-based videoconferencing became possible, and more efficient video compression technologies were developed, permitting desktop, or personal computer (PC)-based videoconferencing. In 1992 CU-SeeMe was developed at Cornell by Tim Dorcey et al. In 1995 the first public videoconference between North America and Africa took place, linking a technofair in San Francisco with a techno-rave and cyberdeli in Cape Town. At the Winter Olympics opening ceremony in Nagano, Japan, Seiji Ozawa conducted the Ode to Joy from Beethoven's Ninth Symphony simultaneously across five continents in near-real time."
Phonology,"In addition to the minimal units that can serve the purpose of differentiating meaning (the phonemes), phonology studies how sounds alternate, i.e. replace one another in different forms of the same morpheme (allomorphs), as well as, for example, syllable structure, stress, feature geometry, accent, and intonation. Natural phonology is a theory based on the publications of its proponent David Stampe in 1969 and (more explicitly) in 1979. In this view, phonology is based on a set of universal phonological processes that interact with one another; which ones are active and which are suppressed is language-specific. Rather than acting on segments, phonological processes act on distinctive features within prosodic groups. Prosodic groups can be as small as a part of a syllable or as large as an entire utterance. Phonological processes are unordered with respect to each other and apply simultaneously (though the output of one process may be the input to another). The second most prominent natural phonologist is Patricia Donegan (Stampe's wife); there are many natural phonologists in Europe, and a few in the U.S., such as Geoffrey Nathan. The principles of natural phonology were extended to morphology by Wolfgang U. Dressler, who founded natural morphology. Different linguists therefore take different approaches to the problem of assigning sounds to phonemes. For example, they differ in the extent to which they require allophones to be phonetically similar. There are also differing ideas as to whether this grouping of sounds is purely a tool for linguistic analysis, or reflects an actual process in the way the human brain processes a language. An important part of traditional, pre-generative schools of phonology is studying which sounds can be grouped into distinctive units within a language; these units are known as phonemes. For example, in English, the ""p"" sound in pot is aspirated (pronounced [pʰ]) while that in spot is not aspirated (pronounced [p]). However, English speakers intuitively treat both sounds as variations (allophones) of the same phonological category, that is of the phoneme /p/. (Traditionally, it would be argued that if an aspirated [pʰ] were interchanged with the unaspirated [p] in spot, native speakers of English would still hear the same words; that is, the two sounds are perceived as ""the same"" /p/.) In some other languages, however, these two sounds are perceived as different, and they are consequently assigned to different phonemes. For example, in Thai, Hindi, and Quechua, there are minimal pairs of words for which aspiration is the only contrasting feature (two words can have different meanings but with the only difference in pronunciation being that one has an aspirated sound where the other has an unaspirated one). The principles of phonological analysis can be applied independently of modality because they are designed to serve as general analytical tools, not language-specific ones. The same principles have been applied to the analysis of sign languages (see Phonemes in sign languages), even though the sub-lexical units are not instantiated as speech sounds. The word phonology (as in the phonology of English) can also refer to the phonological system (sound system) of a given language. This is one of the fundamental systems which a language is considered to comprise, like its syntax and its vocabulary. The Polish scholar Jan Baudouin de Courtenay (together with his former student Mikołaj Kruszewski) introduced the concept of the phoneme in 1876, and his work, though often unacknowledged, is considered to be the starting point of modern phonology. He also worked on the theory of phonetic alternations (what is now called allophony and morphophonology), and had a significant influence on the work of Ferdinand de Saussure. The word phonology comes from the Greek φωνή, phōnḗ, ""voice, sound,"" and the suffix -logy (which is from Greek λόγος, lógos, ""word, speech, subject of discussion""). Definitions of the term vary. Nikolai Trubetzkoy in Grundzüge der Phonologie (1939) defines phonology as ""the study of sound pertaining to the system of language,"" as opposed to phonetics, which is ""the study of sound pertaining to the act of speech"" (the distinction between language and speech being basically Saussure's distinction between langue and parole). More recently, Lass (1998) writes that phonology refers broadly to the subdiscipline of linguistics concerned with the sounds of language, while in more narrow terms, ""phonology proper is concerned with the function, behavior and organization of sounds as linguistic items."" According to Clark et al. (2007), it means the systematic use of sound to encode meaning in any spoken human language, or the field of linguistics studying this use. Part of the phonological study of a language therefore involves looking at data (phonetic transcriptions of the speech of native speakers) and trying to deduce what the underlying phonemes are and what the sound inventory of the language is. The presence or absence of minimal pairs, as mentioned above, is a frequently used criterion for deciding whether two sounds should be assigned to the same phoneme. However, other considerations often need to be taken into account as well. Phonology is often distinguished from phonetics. While phonetics concerns the physical production, acoustic transmission and perception of the sounds of speech, phonology describes the way sounds function within a given language or across languages to encode meaning. For many linguists, phonetics belongs to descriptive linguistics, and phonology to theoretical linguistics, although establishing the phonological system of a language is necessarily an application of theoretical principles to analysis of phonetic evidence. Note that this distinction was not always made, particularly before the development of the modern concept of the phoneme in the mid 20th century. Some subfields of modern phonology have a crossover with phonetics in descriptive disciplines such as psycholinguistics and speech perception, resulting in specific areas like articulatory phonology or laboratory phonology. The particular contrasts which are phonemic in a language can change over time. At one time, [f] and [v], two sounds that have the same place and manner of articulation and differ in voicing only, were allophones of the same phoneme in English, but later came to belong to separate phonemes. This is one of the main factors of historical change of languages as described in historical linguistics. Phonology is a branch of linguistics concerned with the systematic organization of sounds in languages. It has traditionally focused largely on the study of the systems of phonemes in particular languages (and therefore used to be also called phonemics, or phonematics), but it may also cover any linguistic analysis either at a level beneath the word (including syllable, onset and rime, articulatory gestures, articulatory features, mora, etc.) or at all levels of language where sound is considered to be structured for conveying linguistic meaning. Phonology also includes the study of equivalent organizational systems in sign languages. Since the early 1960s, theoretical linguists have moved away from the traditional concept of a phoneme, preferring to consider basic units at a more abstract level, as a component of morphemes; these units can be called morphophonemes, and analysis using this approach is called morphophonology. In 1976 John Goldsmith introduced autosegmental phonology. Phonological phenomena are no longer seen as operating on one linear sequence of segments, called phonemes or feature combinations, but rather as involving some parallel sequences of features which reside on multiple tiers. Autosegmental phonology later evolved into feature geometry, which became the standard theory of representation for theories of the organization of phonology as different as lexical phonology and optimality theory. An influential school of phonology in the interwar period was the Prague school. One of its leading members was Prince Nikolai Trubetzkoy, whose Grundzüge der Phonologie (Principles of Phonology), published posthumously in 1939, is among the most important works in the field from this period. Directly influenced by Baudouin de Courtenay, Trubetzkoy is considered the founder of morphophonology, although this concept had also been recognized by de Courtenay. Trubetzkoy also developed the concept of the archiphoneme. Another important figure in the Prague school was Roman Jakobson, who was one of the most prominent linguists of the 20th century. Government phonology, which originated in the early 1980s as an attempt to unify theoretical notions of syntactic and phonological structures, is based on the notion that all languages necessarily follow a small set of principles and vary according to their selection of certain binary parameters. That is, all languages' phonological structures are essentially the same, but there is restricted variation that accounts for differences in surface realizations. Principles are held to be inviolable, though parameters may sometimes come into conflict. Prominent figures in this field include Jonathan Kaye, Jean Lowenstamm, Jean-Roger Vergnaud, Monik Charette, and John Harris. The history of phonology may be traced back to the Ashtadhyayi, the Sanskrit grammar composed by Pāṇini in the 4th century BC. In particular the Shiva Sutras, an auxiliary text to the Ashtadhyayi, introduces what can be considered a list of the phonemes of the Sanskrit language, with a notational system for them that is used throughout the main text, which deals with matters of morphology, syntax and semantics. Phonology also includes topics such as phonotactics (the phonological constraints on what sounds can appear in what positions in a given language) and phonological alternation (how the pronunciation of a sound changes through the application of phonological rules, sometimes in a given order which can be feeding or bleeding,) as well as prosody, the study of suprasegmentals and topics such as stress and intonation. The findings and insights of speech perception and articulation research complicate the traditional and somewhat intuitive idea of interchangeable allophones being perceived as the same phoneme. First, interchanged allophones of the same phoneme can result in unrecognizable words. Second, actual speech, even at a word level, is highly co-articulated, so it is problematic to expect to be able to splice words into simple segments without affecting speech perception. In a course at the LSA summer institute in 1991, Alan Prince and Paul Smolensky developed optimality theory—an overall architecture for phonology according to which languages choose a pronunciation of a word that best satisfies a list of constraints ordered by importance; a lower-ranked constraint can be violated when the violation is necessary in order to obey a higher-ranked constraint. The approach was soon extended to morphology by John McCarthy and Alan Prince, and has become a dominant trend in phonology. The appeal to phonetic grounding of constraints and representational elements (e.g. features) in various approaches has been criticized by proponents of 'substance-free phonology', especially Mark Hale and Charles Reiss. An integrated approach to phonological theory that combines synchronic and diachronic accounts to sound patterns was initiated with Evolutionary Phonology in recent years. In 1968 Noam Chomsky and Morris Halle published The Sound Pattern of English (SPE), the basis for generative phonology. In this view, phonological representations are sequences of segments made up of distinctive features. These features were an expansion of earlier work by Roman Jakobson, Gunnar Fant, and Morris Halle. The features describe aspects of articulation and perception, are from a universally fixed set, and have the binary values + or −. There are at least two levels of representation: underlying representation and surface phonetic representation. Ordered phonological rules govern how underlying representation is transformed into the actual pronunciation (the so-called surface form). An important consequence of the influence SPE had on phonological theory was the downplaying of the syllable and the emphasis on segments. Furthermore, the generativists folded morphophonology into phonology, which both solved and created problems. Broadly speaking, government phonology (or its descendant, strict-CV phonology) has a greater following in the United Kingdom, whereas optimality theory is predominant in the United States.[citation needed]"
Swaziland,"The Sangoma is a traditional diviner chosen by the ancestors of that particular family. The training of the Sangoma is called ""kwetfwasa"". At the end of the training, a graduation ceremony takes place where all the local sangoma come together for feasting and dancing. The diviner is consulted for various reasons, such the cause of sickness or even death. His diagnosis is based on ""kubhula"", a process of communication, through trance, with the natural superpowers. The Inyanga (a medical and pharmaceutical specialist in western terms) possesses the bone throwing skill (""kushaya ematsambo"") used to determine the cause of the sickness. 83% of the total population adheres to Christianity, making it the most common religion in Swaziland. Anglican, Protestant and indigenous African churches, including African Zionist, constitute the majority of the Christians (40%), followed by Roman Catholicism at 20% of the population. On 18 July 2012, Ellinah Wamukoya, was elected Anglican Bishop of Swaziland, becoming the first woman to be a bishop in Africa. 15% of the population follows traditional religions; other non-Christian religions practised in the country include Islam (1%), the Bahá'í Faith (0.5%), and Hinduism (0.2%). There are 14 Jewish families. Swaziland's most well-known cultural event is the annual Umhlanga Reed Dance. In the eight-day ceremony, girls cut reeds and present them to the queen mother and then dance. (There is no formal competition.) It is done in late August or early September. Only childless, unmarried girls can take part. The aims of the ceremony are to preserve girls' chastity, provide tribute labour for the Queen mother, and to encourage solidarity by working together. The royal family appoints a commoner maiden to be ""induna"" (captain) of the girls and she announces over the radio the dates of the ceremony. She will be an expert dancer and knowledgeable on royal protocol. One of the King's daughters will be her counterpart. The Reed Dance today is not an ancient ceremony but a development of the old ""umchwasho"" custom. In ""umchwasho"", all young girls were placed in a female age-regiment. If any girl became pregnant outside of marriage, her family paid a fine of one cow to the local chief. After a number of years, when the girls had reached a marriageable age, they would perform labour service for the Queen Mother, ending with dancing and feasting. The country was under the chastity rite of ""umchwasho"" until 19 August 2005. The Swazi economy is very closely linked to the economy of South Africa, from which it receives over 90% of its imports and to which it sends about 70% of its exports. Swaziland's other key trading partners are the United States and the EU, from whom the country has received trade preferences for apparel exports (under the African Growth and Opportunity Act – AGOA – to the US) and for sugar (to the EU). Under these agreements, both apparel and sugar exports did well, with rapid growth and a strong inflow of foreign direct investment. Textile exports grew by over 200% between 2000 and 2005 and sugar exports increasing by more than 50% over the same period. The secondary and high school education system in Swaziland is a five-year programme divided into three years junior secondary and two years senior secondary. There is an external public examination (Junior Certificate) at the end of the junior secondary that learners have to pass to progress to the senior secondary level. The Examination Council of Swaziland (ECOS) administers this examination. At the end of the senior secondary level, learners sit for a public examination, the Swaziland General Certificate of Secondary Education (SGCSE) and International General Certificate of Secondary Education (IGCSE) which is accredited by the Cambridge International Examination (CIE). A few schools offer the Advanced Studies (AS) programme in their curriculum. As noted above, there are 55 tinkhundla in Swaziland and each elects one representative to the House of Assembly of Swaziland. Each inkhundla has a development committee (bucopho) elected from the various constituency chiefdoms in its area for a five-year term. Bucopho bring to the inkhundla all matters of interest and concern to their various chiefdoms, and take back to the chiefdoms the decisions of the inkhundla. The chairman of the bucopho is elected at the inkhundla and is called indvuna ye nkhundla. In 1903, after British victory in the Anglo-Boer war, Swaziland became a British protectorate. Much of its early administration (for example, postal services) being carried out from South Africa until 1906 when the Transvaal colony was granted self-government. Following this, Swaziland was partitioned into European and non-European (or native reserves) areas with the former being two-thirds of the total land. Sobhuza's official coronation was in December 1921 after the regency of Labotsibeni after which he led an unsuccessful deputation to the Privy council in London in 1922 regarding the issue of the land. The main centre for technical training in Swaziland is the Swaziland College of Technology which is slated to become a full university. It aims to provide and facilitating high quality training and learning in technology and business studies in collaboration with the Commercial, Industrial and Public Sectors. Other technical and vocational institutions are the Gwamile Vocational and Commercial Training Institute located in Matsapha and the Manzini Industrial and Training Centre (MITC) in Manzini. Other vocational institutions include Nhlangano Agricultural Skills Training Center and Siteki Industrial Training Centre. Following the elections of 1973, the constitution of Swaziland was suspended by King Sobhuza II who thereafter ruled the country by decree until his death in 1982. At this point Sobhuza II had ruled Swaziland for 61 years, making him the longest ruling monarch in history. A regency followed his death, with Queen Regent Dzeliwe Shongwe being head of state until 1984 when she was removed by Liqoqo and replaced by Queen Mother Ntfombi Tfwala. Mswati III, the son of Ntfombi, was crowned king on 25 April 1986 as King and Ingwenyama of Swaziland. The constitution for independent Swaziland was promulgated by Britain in November 1963 under the terms of which legislative and executive councils were established. This development was opposed by the Swazi National Council (liqoqo). Despite such opposition, elections took place and the first Legislative Council of Swaziland was constituted on 9 September 1964. Changes to the original constitution proposed by the Legislative Council were accepted by Britain and a new constitution providing for a House of Assembly and Senate was drawn up. Elections under this constitution were held in 1967. The University of Swaziland, Southern African Nazarene University, Swaziland Christian University are the institutions that offer university education in the country. A campus of Limkokwing University of Creative Technology can be found at Sidvwashini, a suburb of the capital Mbabane. There are some teaching and nursing assistant colleges around the country. Ngwane Teacher's College and William Pitcher College are the country's teaching colleges. The Good Shepherd Hospital in Siteki is home to the College for Nursing Assistants. In 2005, the constitution was put into effect. There is still much debate in the country about the constitutional reforms. From the early seventies, there was active resistance to the royal hegemony. Despite complaints from progressive formations, support for the monarchy and the current political system remains strong among the majority of the population.[citation needed] Submissions were made by citizens around the country to commissions, including the constitutional draft committee, indicating that they would prefer to maintain the current situation. About 75% of the population is employed in subsistence agriculture upon Swazi Nation Land (SNL). In contrast with the commercial farms, Swazi Nation Land suffers from low productivity and investment. This dual nature of the Swazi economy, with high productivity in textile manufacturing and in the industrialised agricultural TDLs on the one hand, and declining productivity subsistence agriculture (on SNL) on the other, may well explain the country's overall low growth, high inequality and unemployment. In addition to these institutions, Swaziland also has the Swaziland Institute of Management and Public Administration (SIMPA) and Institute of Development Management (IDM). SIMPA is a government owned management and development institute and IDM is a regional organisation in Botswana, Lesotho and Swaziland that provides training, consultancy, and research in management. The Mananga management centre was established as Mananga Agricultural Management Centre in 1972 as an International Management Development Centre catering for middle and senior managers, it is located at Ezulwini. In 2004, the Swaziland government acknowledged for the first time that it suffered an AIDS crisis, with 38.8% of tested pregnant women infected with HIV (see AIDS in Africa). The then Prime Minister Themba Dlamini declared a humanitarian crisis due to the combined effect of drought, land degradation, increased poverty, and HIV/AIDS. According to the 2011 UNAIDS Report, Swaziland is close to achieving universal access to HIV/AIDS treatment, defined as 80% coverage or greater. Estimates of treatment coverage range from 70% to 80% of those infected. Life expectancy had fallen from 61 years in 2000 to 32 years in 2009. Tuberculosis is also a significant problem, with an 18% mortality rate. Many patients have a multi-drug resistant strain, and 83% are co-infected with HIV. The Swazi bicameral Parliament or Libandla consists of the Senate (30 seats; 10 members appointed by the House of Assembly and 20 appointed by the monarch; to serve five-year terms) and the House of Assembly (65 seats; 10 members appointed by the monarch and 55 elected by popular vote; to serve five-year terms). The elections are held every five years after dissolution of parliament by the king. The last elections were held on 20 September 2013. The balloting is done on a non-party basis in all categories. All election procedures are overseen by the elections and boundaries commission. At no more than 200 kilometres (120 mi) north to south and 130 kilometres (81 mi) east to west, Swaziland is one of the smallest countries in Africa. Despite its size, however, its climate and topography is diverse, ranging from a cool and mountainous highveld to a hot and dry lowveld. The population is primarily ethnic Swazis whose language is siSwati. They established their kingdom in the mid-18th century under the leadership of Ngwane III; the present boundaries were drawn up in 1881. After the Anglo-Boer War, Swaziland was a British protectorate from 1903 until 1967. It regained its independence on 6 September 1968. Economic growth in Swaziland has lagged behind that of its neighbours. Real GDP growth since 2001 has averaged 2.8%, nearly 2 percentage points lower than growth in other Southern African Customs Union (SACU) member countries. Low agricultural productivity in the SNLs, repeated droughts, the devastating effect of HIV/AIDS and an overly large and inefficient government sector are likely contributing factors. Swaziland's public finances deteriorated in the late 1990s following sizeable surpluses a decade earlier. A combination of declining revenues and increased spending led to significant budget deficits. Nominations take place at the chiefdoms. On the day of nomination, the name of the nominee is raised by a show of hand and the nominee is given an opportunity to indicate whether he or she accepts the nomination. If he or she accepts it, he or she must be supported by at least ten members of that chiefdom. The nominations are for the position of Member of Parliament, Constituency Headman (Indvuna) and the Constituency Executive Committee (Bucopho). The minimum number of nominees is four and the maximum is ten. Swaziland's currency is pegged to the South African Rand, subsuming Swaziland's monetary policy to South Africa. Customs duties from the Southern African Customs Union, which may equal as much as 70% of government revenue this year, and worker remittances from South Africa substantially supplement domestically earned income. Swaziland is not poor enough to merit an IMF program; however, the country is struggling to reduce the size of the civil service and control costs at public enterprises. The government is trying to improve the atmosphere for foreign direct investment. Swaziland derives its name from a later king named Mswati II. KaNgwane, named for Ngwane III, is an alternative name for Swaziland the surname of whose royal house remains Nkhosi Dlamini. Nkhosi literally means ""king"". Mswati II was the greatest of the fighting kings of Swaziland, and he greatly extended the area of the country to twice its current size. The Emakhandzambili clans were initially incorporated into the kingdom with wide autonomy, often including grants of special ritual and political status. The extent of their autonomy however was drastically curtailed by Mswati, who attacked and subdued some of them in the 1850s. Education in Swaziland begins with pre-school education for infants, primary, secondary and high school education for general education and training (GET), and universities and colleges at tertiary level. Pre-school education is usually for children 5-year or younger after that the students can enroll in a primary school anywhere in the country. In Swaziland early childhood care and education (ECCE) centres are in the form of preschools or neighbourhood care points (NCPs). In the country 21.6% of preschool age children have access to early childhood education. The king appoints the prime minister from the legislature and also appoints a minority of legislators to both chambers of the Libandla (parliament) with help from an advisory council. The king is allowed by the constitution to appoint some members to parliament for special interests. These special interests are citizens who might have been left out by the electorate during the course of elections or did not enter as candidates. This is done to balance views in parliament. Special interests could be people of gender, race, disability, the business community, civic society, scholars, chiefs and so on. Swaziland is a developing country with a small economy. Its GDP per capita of $9,714 means it is classified as a country with a lower-middle income. As a member of the Southern African Customs Union (SACU) and Common Market for Eastern and Southern Africa (COMESA), its main local trading partner is South Africa. Swaziland's currency, the lilangeni, is pegged to the South African rand. Swaziland's major overseas trading partners are the United States and the European Union. The majority of the country's employment is provided by its agricultural and manufacturing sectors. Swaziland is a member of the Southern African Development Community (SADC), the African Union, the Commonwealth of Nations and the United Nations. The considerable spending did not lead to more growth and did not benefit the poor. Much of the increased spending has gone to current expenditures related to wages, transfers, and subsidies. The wage bill today constitutes over 15% of GDP and 55% of total public spending; these are some of the highest levels on the African continent. The recent rapid growth in SACU revenues has, however, reversed the fiscal situation, and a sizeable surplus was recorded since 2006. SACU revenues today account for over 60% of total government revenues. On the positive side, the external debt burden has declined markedly over the last 20 years, and domestic debt is almost negligible; external debt as a percent of GDP was less than 20% in 2006. A small, landlocked kingdom, Swaziland is bordered in the North, West and South by the Republic of South Africa and by Mozambique in the East. Swaziland has a land area of 17,364 km2. Swaziland has four separate geographical regions. These run from North to South and are determined by altitude. Swaziland is located at approximately 26°30'S, 31°30'E. Swaziland has a wide variety of landscapes, from the mountains along the Mozambican border to savannas in the east and rain forest in the northwest. Several rivers flow through the country, such as the Great Usutu River."
Alloy,"With the introduction of the blast furnace to Europe in the Middle Ages, pig iron was able to be produced in much higher volumes than wrought iron. Because pig iron could be melted, people began to develop processes of reducing the carbon in the liquid pig iron to create steel. Puddling was introduced during the 1700s, where molten pig iron was stirred while exposed to the air, to remove the carbon by oxidation. In 1858, Sir Henry Bessemer developed a process of steel-making by blowing hot air through liquid pig iron to reduce the carbon content. The Bessemer process was able to produce the first large scale manufacture of steel. Once the Bessemer process began to gain widespread use, other alloys of steel began to follow. Mangalloy, an alloy of steel and manganese exhibiting extreme hardness and toughness, was one of the first alloy steels, and was created by Robert Hadfield in 1882. Although an alloy is technically an impure metal, when referring to alloys, the term ""impurities"" usually denotes those elements which are not desired. These impurities are often found in the base metals or the solutes, but they may also be introduced during the alloying process. For instance, sulfur is a common impurity in steel. Sulfur combines readily with iron to form iron sulfide, which is very brittle, creating weak spots in the steel. Lithium, sodium and calcium are common impurities in aluminium alloys, which can have adverse effects on the structural integrity of castings. Conversely, otherwise pure-metals that simply contain unwanted impurities are often called ""impure metals"" and are not usually referred to as alloys. Oxygen, present in the air, readily combines with most metals to form metal oxides; especially at higher temperatures encountered during alloying. Great care is often taken during the alloying process to remove excess impurities, using fluxes, chemical additives, or other methods of extractive metallurgy. The term pewter covers a variety of alloys consisting primarily of tin. As a pure metal, tin was much too soft to be used for any practical purpose. However, in the Bronze age, tin was a rare metal and, in many parts of Europe and the Mediterranean, was often valued higher than gold. To make jewelry, forks and spoons, or other objects from tin, it was usually alloyed with other metals to increase its strength and hardness. These metals were typically lead, antimony, bismuth or copper. These solutes sometimes were added individually in varying amounts, or added together, making a wide variety of things, ranging from practical items, like dishes, surgical tools, candlesticks or funnels, to decorative items such as ear rings and hair clips. Conversely, most heat-treatable alloys are precipitation hardening alloys, which produce the opposite effects that steel does. When heated to form a solution and then cooled quickly, these alloys become much softer than normal, during the diffusionless transformation, and then harden as they age. The solutes in these alloys will precipitate over time, forming intermetallic phases, which are difficult to discern from the base metal. Unlike steel, in which the solid solution separates to form different crystal phases, precipitation hardening alloys separate to form different phases within the same crystal. These intermetallic alloys appear homogeneous in crystal structure, but tend to behave heterogeneous, becoming hard and somewhat brittle. At a certain temperature, (usually between 1,500 °F (820 °C) and 1,600 °F (870 °C), depending on carbon content), the base metal of steel undergoes a change in the arrangement of the atoms in its crystal matrix, called allotropy. This allows the small carbon atoms to enter the interstices of the iron crystal, diffusing into the iron matrix. When this happens, the carbon atoms are said to be in solution, or mixed with the iron, forming a single, homogeneous, crystalline phase called austenite. If the steel is cooled slowly, the iron will gradually change into its low temperature allotrope. When this happens the carbon atoms will no longer be soluble with the iron, and will be forced to precipitate out of solution, nucleating into the spaces between the crystals. The steel then becomes heterogeneous, being formed of two phases; the carbon (carbide) phase cementite, and ferrite. This type of heat treatment produces steel that is rather soft and bendable. However, if the steel is cooled quickly the carbon atoms will not have time to precipitate. When rapidly cooled, a diffusionless (martensite) transformation occurs, in which the carbon atoms become trapped in solution. This causes the iron crystals to deform intrinsically when the crystal structure tries to change to its low temperature state, making it very hard and brittle. Alloys are often made to alter the mechanical properties of the base metal, to induce hardness, toughness, ductility, or other desired properties. Most metals and alloys can be work hardened by creating defects in their crystal structure. These defects are created during plastic deformation, such as hammering or bending, and are permanent unless the metal is recrystallized. However, some alloys can also have their properties altered by heat treatment. Nearly all metals can be softened by annealing, which recrystallizes the alloy and repairs the defects, but not as many can be hardened by controlled heating and cooling. Many alloys of aluminium, copper, magnesium, titanium, and nickel can be strengthened to some degree by some method of heat treatment, but few respond to this to the same degree that steel does. The term ""alloy"" is sometimes used in everyday speech as a synonym for a particular alloy. For example, automobile wheels made of an aluminium alloy are commonly referred to as simply ""alloy wheels"", although in point of fact steels and most other metals in practical use are also alloys. Steel is such a common alloy that many items made from it, like wheels, barrels, or girders, are simply referred to by the name of the item, assuming it is made of steel. When made from other materials, they are typically specified as such, (i.e.: ""bronze wheel,"" ""plastic barrel,"" or ""wood girder""). Iron is usually found as iron ore on Earth, except for one deposit of native iron in Greenland, which was used by the Inuit people. Native copper, however, was found worldwide, along with silver, gold and platinum, which were also used to make tools, jewelry, and other objects since Neolithic times. Copper was the hardest of these metals, and the most widely distributed. It became one of the most important metals to the ancients. Eventually, humans learned to smelt metals such as copper and tin from ore, and, around 2500 BC, began alloying the two metals to form bronze, which is much harder than its ingredients. Tin was rare, however, being found mostly in Great Britain. In the Middle East, people began alloying copper with zinc to form brass. Ancient civilizations took into account the mixture and the various properties it produced, such as hardness, toughness and melting point, under various conditions of temperature and work hardening, developing much of the information contained in modern alloy phase diagrams. Arrowheads from the Chinese Qin dynasty (around 200 BC) were often constructed with a hard bronze-head, but a softer bronze-tang, combining the alloys to prevent both dulling and breaking during use. The use of alloys by humans started with the use of meteoric iron, a naturally occurring alloy of nickel and iron. It is the main constituent of iron meteorites which occasionally fall down on Earth from outer space. As no metallurgic processes were used to separate iron from nickel, the alloy was used as it was. Meteoric iron could be forged from a red heat to make objects such as tools, weapons, and nails. In many cultures it was shaped by cold hammering into knives and arrowheads. They were often used as anvils. Meteoric iron was very rare and valuable, and difficult for ancient people to work. An alloy is a mixture of either pure or fairly pure chemical elements, which forms an impure substance (admixture) that retains the characteristics of a metal. An alloy is distinct from an impure metal, such as wrought iron, in that, with an alloy, the added impurities are usually desirable and will typically have some useful benefit. Alloys are made by mixing two or more elements; at least one of which being a metal. This is usually called the primary metal or the base metal, and the name of this metal may also be the name of the alloy. The other constituents may or may not be metals but, when mixed with the molten base, they will be soluble, dissolving into the mixture. The term alloy is used to describe a mixture of atoms in which the primary constituent is a metal. The primary metal is called the base, the matrix, or the solvent. The secondary constituents are often called solutes. If there is a mixture of only two types of atoms, not counting impurities, such as a copper-nickel alloy, then it is called a binary alloy. If there are three types of atoms forming the mixture, such as iron, nickel and chromium, then it is called a ternary alloy. An alloy with four constituents is a quaternary alloy, while a five-part alloy is termed a quinary alloy. Because the percentage of each constituent can be varied, with any mixture the entire range of possible variations is called a system. In this respect, all of the various forms of an alloy containing only two constituents, like iron and carbon, is called a binary system, while all of the alloy combinations possible with a ternary alloy, such as alloys of iron, carbon and chromium, is called a ternary system. Mercury has been smelted from cinnabar for thousands of years. Mercury dissolves many metals, such as gold, silver, and tin, to form amalgams (an alloy in a soft paste, or liquid form at ambient temperature). Amalgams have been used since 200 BC in China for plating objects with precious metals, called gilding, such as armor and mirrors. The ancient Romans often used mercury-tin amalgams for gilding their armor. The amalgam was applied as a paste and then heated until the mercury vaporized, leaving the gold, silver, or tin behind. Mercury was often used in mining, to extract precious metals like gold and silver from their ores. While the use of iron started to become more widespread around 1200 BC, mainly because of interruptions in the trade routes for tin, the metal is much softer than bronze. However, very small amounts of steel, (an alloy of iron and around 1% carbon), was always a byproduct of the bloomery process. The ability to modify the hardness of steel by heat treatment had been known since 1100 BC, and the rare material was valued for the manufacture of tools and weapons. Because the ancients could not produce temperatures high enough to melt iron fully, the production of steel in decent quantities did not occur until the introduction of blister steel during the Middle Ages. This method introduced carbon by heating wrought iron in charcoal for long periods of time, but the penetration of carbon was not very deep, so the alloy was not homogeneous. In 1740, Benjamin Huntsman began melting blister steel in a crucible to even out the carbon content, creating the first process for the mass production of tool steel. Huntsman's process was used for manufacturing tool steel until the early 1900s. Many ancient civilizations alloyed metals for purely aesthetic purposes. In ancient Egypt and Mycenae, gold was often alloyed with copper to produce red-gold, or iron to produce a bright burgundy-gold. Gold was often found alloyed with silver or other metals to produce various types of colored gold. These metals were also used to strengthen each other, for more practical purposes. Copper was often added to silver to make sterling silver, increasing its strength for use in dishes, silverware, and other practical items. Quite often, precious metals were alloyed with less valuable substances as a means to deceive buyers. Around 250 BC, Archimedes was commissioned by the king to find a way to check the purity of the gold in a crown, leading to the famous bath-house shouting of ""Eureka!"" upon the discovery of Archimedes' principle. When the alloy cools and solidifies (crystallizes), its mechanical properties will often be quite different from those of its individual constituents. A metal that is normally very soft and malleable, such as aluminium, can be altered by alloying it with another soft metal, like copper. Although both metals are very soft and ductile, the resulting aluminium alloy will be much harder and stronger. Adding a small amount of non-metallic carbon to iron produces an alloy called steel. Due to its very-high strength and toughness (which is much higher than pure iron), and its ability to be greatly altered by heat treatment, steel is one of the most common alloys in modern use. By adding chromium to steel, its resistance to corrosion can be enhanced, creating stainless steel, while adding silicon will alter its electrical characteristics, producing silicon steel. Alloying a metal is done by combining it with one or more other metals or non-metals that often enhance its properties. For example, steel is stronger than iron, its primary element. The electrical and thermal conductivity of alloys is usually lower than that of the pure metals. The physical properties, such as density, reactivity, Young's modulus of an alloy may not differ greatly from those of its elements, but engineering properties such as tensile strength and shear strength may be substantially different from those of the constituent materials. This is sometimes a result of the sizes of the atoms in the alloy, because larger atoms exert a compressive force on neighboring atoms, and smaller atoms exert a tensile force on their neighbors, helping the alloy resist deformation. Sometimes alloys may exhibit marked differences in behavior even when small amounts of one element are present. For example, impurities in semiconducting ferromagnetic alloys lead to different properties, as first predicted by White, Hogan, Suhl, Tian Abrie and Nakamura. Some alloys are made by melting and mixing two or more metals. Bronze, an alloy of copper and tin, was the first alloy discovered, during the prehistoric period now known as the bronze age; it was harder than pure copper and originally used to make tools and weapons, but was later superseded by metals and alloys with better properties. In later times bronze has been used for ornaments, bells, statues, and bearings. Brass is an alloy made from copper and zinc. The first known smelting of iron began in Anatolia, around 1800 BC. Called the bloomery process, it produced very soft but ductile wrought iron. By 800 BC, iron-making technology had spread to Europe, arriving in Japan around 700 AD. Pig iron, a very hard but brittle alloy of iron and carbon, was being produced in China as early as 1200 BC, but did not arrive in Europe until the Middle Ages. Pig iron has a lower melting point than iron, and was used for making cast-iron. However, these metals found little practical use until the introduction of crucible steel around 300 BC. These steels were of poor quality, and the introduction of pattern welding, around the 1st century AD, sought to balance the extreme properties of the alloys by laminating them, to create a tougher metal. Around 700 AD, the Japanese began folding bloomery-steel and cast-iron in alternating layers to increase the strength of their swords, using clay fluxes to remove slag and impurities. This method of Japanese swordsmithing produced one of the purest steel-alloys of the early Middle Ages. Some alloys occur naturally, such as electrum, which is an alloy that is native to Earth, consisting of silver and gold. Meteorites are sometimes made of naturally occurring alloys of iron and nickel, but are not native to the Earth. One of the first alloys made by humans was bronze, which is made by mixing the metals tin and copper. Bronze was an extremely useful alloy to the ancients, because it is much stronger and harder than either of its components. Steel was another common alloy. However, in ancient times, it could only be created as an accidental byproduct from the heating of iron ore in fires (smelting) during the manufacture of iron. Other ancient alloys include pewter, brass and pig iron. In the modern age, steel can be created in many forms. Carbon steel can be made by varying only the carbon content, producing soft alloys like mild steel or hard alloys like spring steel. Alloy steels can be made by adding other elements, such as molybdenum, vanadium or nickel, resulting in alloys such as high-speed steel or tool steel. Small amounts of manganese are usually alloyed with most modern-steels because of its ability to remove unwanted impurities, like phosphorus, sulfur and oxygen, which can have detrimental effects on the alloy. However, most alloys were not created until the 1900s, such as various aluminium, titanium, nickel, and magnesium alloys. Some modern superalloys, such as incoloy, inconel, and hastelloy, may consist of a multitude of different components. In 1906, precipitation hardening alloys were discovered by Alfred Wilm. Precipitation hardening alloys, such as certain alloys of aluminium, titanium, and copper, are heat-treatable alloys that soften when quenched (cooled quickly), and then harden over time. After quenching a ternary alloy of aluminium, copper, and magnesium, Wilm discovered that the alloy increased in hardness when left to age at room temperature. Although an explanation for the phenomenon was not provided until 1919, duralumin was one of the first ""age hardening"" alloys to be used, and was soon followed by many others. Because they often exhibit a combination of high strength and low weight, these alloys became widely used in many forms of industry, including the construction of modern aircraft. Although the elements usually must be soluble in the liquid state, they may not always be soluble in the solid state. If the metals remain soluble when solid, the alloy forms a solid solution, becoming a homogeneous structure consisting of identical crystals, called a phase. If the mixture cools and the constituents become insoluble, they may separate to form two or more different types of crystals, creating a heterogeneous microstructure of different phases. However, in other alloys, the insoluble elements may not separate until after crystallization occurs. These alloys are called intermetallic alloys because, if cooled very quickly, they first crystallize as a homogeneous phase, but they are supersaturated with the secondary constituents. As time passes, the atoms of these supersaturated alloys separate within the crystals, forming intermetallic phases that serve to reinforce the crystals internally. When a molten metal is mixed with another substance, there are two mechanisms that can cause an alloy to form, called atom exchange and the interstitial mechanism. The relative size of each element in the mix plays a primary role in determining which mechanism will occur. When the atoms are relatively similar in size, the atom exchange method usually happens, where some of the atoms composing the metallic crystals are substituted with atoms of the other constituent. This is called a substitutional alloy. Examples of substitutional alloys include bronze and brass, in which some of the copper atoms are substituted with either tin or zinc atoms. With the interstitial mechanism, one atom is usually much smaller than the other, so cannot successfully replace an atom in the crystals of the base metal. The smaller atoms become trapped in the spaces between the atoms in the crystal matrix, called the interstices. This is referred to as an interstitial alloy. Steel is an example of an interstitial alloy, because the very small carbon atoms fit into interstices of the iron matrix. Stainless steel is an example of a combination of interstitial and substitutional alloys, because the carbon atoms fit into the interstices, but some of the iron atoms are replaced with nickel and chromium atoms. An alloy is a mixture of metals or a mixture of a metal and another element. Alloys are defined by metallic bonding character. An alloy may be a solid solution of metal elements (a single phase) or a mixture of metallic phases (two or more solutions). Intermetallic compounds are alloys with a defined stoichiometry and crystal structure. Zintl phases are also sometimes considered alloys depending on bond types (see also: Van Arkel-Ketelaar triangle for information on classifying bonding in binary compounds)."
Utrecht,"Utrecht Centraal is the main railway station of Utrecht. There are regular intercity services to all major Dutch cities; direct services to Schiphol Airport. Utrecht Centraal is a station on the night service, providing 7 days a week an all night service to (among others) Schiphol Airport, Amsterdam and Rotterdam. International InterCityExpress (ICE) services to Germany (and further) through Arnhem call at Utrecht Centraal. Regular local trains to all areas surrounding Utrecht also depart from Utrecht Centraal; and service several smaller stations: Utrecht Lunetten, Utrecht Vaartsche Rijn, Utrecht Overvecht, Utrecht Leidsche Rijn, Utrecht Terwijde, Utrecht Zuilen and Vleuten. A former station Utrecht Maliebaan closed in 1939 and has since been converted into the Dutch Railway Museum. The transition from independence to a relatively minor part of a larger union was not easily accepted. To quell uprisings Charles V was struggling to exert his power over the citizens of the city, who had struggled to gain a certain level of independence from the bishops and were not willing to cede this to their new lord. The heavily fortified castle Vredenburg was built to house a large garrison whose main task was to maintain control over the city. The castle would last less than 50 years before it was demolished in an uprising in the early stages of the Dutch Revolt. The fortified city temporarily fell to the French invasion in 1672 (the Disaster Year); where the French invasion was only stopped west of Utrecht at the Old Hollandic Waterline. In 1674, only two years after the French left, the centre of Utrecht was struck by a tornado. The halt to building before construction of flying buttresses in the 15th century now proved to be the undoing of the central section of the cathedral of St Martin church which collapsed; creating the current Dom square between the tower and choir. In 1713, Utrecht hosted one of the first international peace negotiations when the Treaty of Utrecht settled the War of the Spanish Succession. Since 1723 Utrecht became the centre of the non-Roman Old Catholic Churches in the world. Utrecht is the centre of a densely populated area, which makes concise definitions of its agglomeration difficult, and somewhat arbitrary. The smaller Utrecht agglomeration of continuously built up areas counts some 420,000 inhabitants and includes Nieuwegein, IJsselstein and Maarssen. It is sometimes argued that the close by municipalities De Bilt, Zeist, Houten, Vianen, Driebergen-Rijsenburg (Utrechtse Heuvelrug), and Bunnik should also be counted towards the Utrecht agglomeration, bringing the total to 640,000 inhabitants. The larger region, including slightly more remote towns such as Woerden and Amersfoort counts up to 820,000 inhabitants. Like most Dutch cities, Utrecht has an extensive network of cycle paths, making cycling safe and popular. 33% of journeys within the city are by bicycle, more than any other mode of transport. (Cars, for example, account for 30% of trips). Bicycles are used by young and old people, and by individuals and families. They are mostly traditional, upright, steel-framed bicycles, with few or no gears. There are also barrow bikes, for carrying shopping or small children. As thousands of bicycles are parked haphazardly in town, creating an eyesore but also impeding pedestrians, the City Council decided in 2014 to build the world's largest bicycle parking station, near the Central Railway Station. This 3-floor construction will cost an estimated 48 million Euro and will hold 12,500 bicycles. Completion is foreseen in 2018. Utrecht city has an active cultural life, and in the Netherlands is second only to Amsterdam. There are several theatres and theatre companies. The 1941 main city theatre was built by Dudok. Besides theatres there is a large number of cinemas including three arthouse cinemas. Utrecht is host to the international Early Music Festival (Festival Oude Muziek, for music before 1800) and the Netherlands Film Festival. The city has an important classical music hall Vredenburg (1979 by Herman Hertzberger). Its acoustics are considered among the best of the 20th-century original music halls.[citation needed] The original Vredenburg music hall has been redeveloped as part of the larger station area redevelopment plan and in 2014 has gained additional halls that allowed its merger with the rock club Tivoli and the SJU jazzpodium. There are several other venues for music throughout the city. Young musicians are educated in the conservatory, a department of the Utrecht School of the Arts. There is a specialised museum of automatically playing musical instruments. The main local and regional bus station of Utrecht is located adjacent to Utrecht Centraal railway station, at the East and West entrances. Due to large scale renovation and construction works at the railway station, the station's bus stops are changing frequently. As a general rule, westbound buses depart from the bus station on the west entrance, other buses from the east side station. Local buses in Utrecht are operated by Qbuzz – its services include a high-frequency service to the Uithof university district. The local bus fleet is one of Europe's cleanest, using only buses compliant with the Euro-VI standard as well as electric buses for inner city transport. Regional buses from the city are operated by Arriva and Connexxion. Utrecht is well-connected to the Dutch road network. Two of the most important major roads serve the city of Utrecht: the A12 and A2 motorways connect Amsterdam, Arnhem, The Hague and Maastricht, as well as Belgium and Germany. Other major motorways in the area are the Almere–Breda A27 and the Utrecht–Groningen A28. Due to the increasing traffic and the ancient city plan, traffic congestion is a common phenomenon in and around Utrecht, causing elevated levels of air pollutants. This has led to a passionate debate in the city about the best way to improve the city's air quality. Another landmark is the old centre and the canal structure in the inner city. The Oudegracht is a curved canal, partly following the ancient main branch of the Rhine. It is lined with the unique wharf-basement structures that create a two-level street along the canals. The inner city has largely retained its Medieval structure, and the moat ringing the old town is largely intact. Because of the role of Utrecht as a fortified city, construction outside the medieval centre and its city walls was restricted until the 19th century. Surrounding the medieval core there is a ring of late 19th- and early 20th-century neighbourhoods, with newer neighbourhoods positioned farther out. The eastern part of Utrecht remains fairly open. The Dutch Water Line, moved east of the city in the early 19th century required open lines of fire, thus prohibiting all permanent constructions until the middle of the 20th century on the east side of the city. The location on the banks of the river Rhine allowed Utrecht to become an important trade centre in the Northern Netherlands. The growing town Utrecht was granted city rights by Henry V in 1122. When the main flow of the Rhine moved south, the old bed, which still flowed through the heart of the town became evermore canalized; and the wharf system was built as an inner city harbour system. On the wharfs storage facilities (werfkelders) were built, on top of which the main street, including houses was constructed. The wharfs and the cellars are accessible from a platform at water level with stairs descending from the street level to form a unique structure.[nb 2] The relations between the bishop, who controlled many lands outside of the city, and the citizens of Utrecht was not always easy. The bishop, for example dammed the Kromme Rijn at Wijk bij Duurstede to protect his estates from flooding. This threatened shipping for the city and led the city of Utrecht to commission a canal to ensure access to the town for shipping trade: the Vaartse Rijn, connecting Utrecht to the Hollandse IJssel at IJsselstein. In 1579 the northern seven provinces signed the Union of Utrecht, in which they decided to join forces against Spanish rule. The Union of Utrecht is seen as the beginning of the Dutch Republic. In 1580 the new and predominantly Protestant state abolished the bishoprics, including the archbishopric of Utrecht. The stadtholders disapproved of the independent course of the Utrecht bourgeoisie and brought the city under much more direct control of the republic; which shifted the power towards its dominant province Holland. This was the start of a long period of stagnation of trade and development in Utrecht. Utrecht remained an atypical city in the new republic with about 40% Catholic in the mid-17th century, and even more among the elite groups, who included many rural nobility and gentry with town houses there. To promote culture Utrecht city organizes cultural Sundays. During a thematic Sunday several organisations create a program, which is open to everyone without, or with a very much reduced, admission fee. There are also initiatives for amateur artists. The city subsidises an organisation for amateur education in arts aimed at all inhabitants (Utrechts Centrum voor de Kunsten), as does the university for its staff and students. Additionally there are also several private initiatives. The city council provides coupons for discounts to inhabitants who receive welfare to be used with many of the initiatives. Production industry constitutes a small part of the economy of Utrecht. The economy of Utrecht depends for a large part on the several large institutions located in the city. It is the centre of the Dutch railroad network and the location of the head office of Nederlandse Spoorwegen. ProRail is headquartered in The De Inktpot (nl) (The Inkpot) – the largest brick building in the Netherlands (the ""UFO"" featured on its façade stems from an art program in 2000). Rabobank, a large bank, has its headquarters in Utrecht. Utrecht is home to the premier league (professional) football club FC Utrecht, which plays in Stadium Nieuw Galgenwaard. It is also the home of Kampong, the largest (amateur) sportsclub in the Netherlands (4,500 members), SV Kampong. Kampong features fieldhockey, soccer, cricket, tennis, squash and jeu de boules. Kampong's men and women top hockey squads play in the highest Dutch hockey league, the Rabohoofdklasse.Utrecht is also home to the baseball and Sofball club: UVV which plays in the highest Dutch baseball league: de Hoofdklasse. Utrecht's waterways are used by several rowing clubs. Viking is a large club open to the general public, and the student clubs Orca and Triton compete in the Varsity each year. The area surrounding Utrecht Centraal railway station and the station itself were developed following modernist ideas of the 1960s, in a brutalist style. This led to the construction of the shopping mall Hoog Catharijne (nl), music centre Vredenburg (Hertzberger, 1979), and conversion of part of the ancient canal structure into a highway (Catherijnebaan). Protest against further modernisation of the city centre followed even before the last buildings were finalised. In the early 21st century the whole area is being redeveloped. The music redeveloped music centre opened in 2014 where the original Vredenburg concert and rock and jazz halls are brought together in a single building. There are many art galleries in Utrecht. There are also several foundations to support art and artists. Training of artists is done at the Utrecht School of the Arts. The Centraal Museum has many exhibitions on the arts, including a permanent exhibition on the works of Utrecht resident illustrator Dick Bruna, who is best known for creating Miffy (""Nijntje"", in Dutch). Although street art is illegal in Utrecht, the Utrechtse Kabouter, a picture of a gnome with a red hat, became a common sight in 2004. Utrecht also houses one of the landmarks of modern architecture, the 1924 Rietveld Schröder House, which is listed on UNESCO's world heritage sites. Although there is some evidence of earlier inhabitation in the region of Utrecht, dating back to the Stone Age (app. 2200 BCE) and settling in the Bronze Age (app. 1800–800 BCE), the founding date of the city is usually related to the construction of a Roman fortification (castellum), probably built in around 50 CE. A series of such fortresses was built after the Roman emperor Claudius decided the empire should not expand north. To consolidate the border the limes Germanicus defense line was constructed  along the main branch of the river Rhine, which at that time flowed through a more northern bed compared to today (what is now the Kromme Rijn). These fortresses were designed to house a cohort of about 500 Roman soldiers. Near the fort settlements would grow housing artisans, traders and soldiers' wives and children. In the early 19th century, the role of Utrecht as a fortified town had become obsolete. The fortifications of the Nieuwe Hollandse Waterlinie were moved east of Utrecht. The town walls could now be demolished to allow for expansion. The moats remained intact and formed an important feature of the Zocher plantsoen, an English style landscape park that remains largely intact today. Growth of the city increased when, in 1843, a railway connecting Utrecht to Amsterdam was opened. After that, Utrecht gradually became the main hub of the Dutch railway network. With the industrial revolution finally gathering speed in the Netherlands and the ramparts taken down, Utrecht began to grow far beyond the medieval centre. In 1853, the Dutch government allowed the bishopric of Utrecht to be reinstated by Rome, and Utrecht became the centre of Dutch Catholicism once more. From the 1880s onward neighbourhoods such as Oudwijk, Wittevrouwen, Vogelenbuurt to the East, and Lombok to the West were developed. New middle class residential areas, such as Tuindorp and Oog in Al, were built in the 1920s and 1930s. During this period, several Jugendstil houses and office buildings were built, followed by Rietveld who built the Rietveld Schröder House (1924), and Dudok's construction of the city theater (1941). When the Frankish rulers established the system of feudalism, the Bishops of Utrecht came to exercise worldly power as prince-bishops. The territory of the bishopric not only included the modern province of Utrecht (Nedersticht, 'lower Sticht'), but also extended to the northeast. The feudal conflict of the Middle Ages heavily affected Utrecht. The prince-bishopric was involved in almost continuous conflicts with the Counts of Holland and the Dukes of Guelders. The Veluwe region was seized by Guelders, but large areas in the modern province of Overijssel remained as the Oversticht. Several churches and monasteries were built inside, or close to, the city of Utrecht. The most dominant of these was the Cathedral of Saint Martin, inside the old Roman fortress. The construction of the present Gothic building was begun in 1254 after an earlier romanesque construction had been badly damaged by fire. The choir and transept were finished from 1320 and were followed then by the ambitious Dom tower. The last part to be constructed was the central nave, from 1420. By that time, however, the age of the great cathedrals had come to an end and declining finances prevented the ambitious project from being finished, the construction of the central nave being suspended before the planned flying buttresses could be finished. Besides the cathedral there were four collegiate churches in Utrecht: St. Salvator's Church (demolished in the 16th century), on the Dom square, dating back to the early 8th century. Saint John (Janskerk), originating in 1040; Saint Peter, building started in 1039 and Saint Mary's church building started around 1090 (demolished in the early 19th century, cloister survives). Besides these churches the city housed St. Paul's Abbey, the 15th-century beguinage of St. Nicholas, and a 14th-century chapter house of the Teutonic Knights. By the mid-7th century, English and Irish missionaries set out to convert the Frisians. The pope appointed their leader, Willibrordus, bishop of the Frisians. The tenure of Willibrordus is generally considered to be the beginning of the Bishopric of Utrecht. In 723, the Frankish leader Charles Martel bestowed the fortress in Utrecht and the surrounding lands as the base of the bishops. From then on Utrecht became one of the most influential seats of power for the Roman Catholic Church in the Netherlands. The archbishops of Utrecht were based at the uneasy northern border of the Carolingian Empire. In addition, the city of Utrecht had competition from the nearby trading centre Dorestad. After the fall of Dorestad around 850, Utrecht became one of the most important cities in the Netherlands. The importance of Utrecht as a centre of Christianity is illustrated by the election of the Utrecht-born Adriaan Florenszoon Boeyens as pope in 1522 (the last non-Italian pope before John Paul II). Utrecht hosts several large institutions of higher education. The most prominent of these is Utrecht University (est. 1636), the largest university of the Netherlands with 30,449 students (as of 2012). The university is partially based in the inner city as well as in the Uithof campus area, to the east of the city. According to Shanghai Jiaotong University's university ranking in 2014 it is the 57th best university in the world. Utrecht also houses the much smaller University of Humanistic Studies, which houses about 400 students. A large indoor shopping centre Hoog Catharijne (nl) is located between Utrecht Centraal railway station and the city centre. The corridors are treated as public places like streets, and the route between the station and the city centre is open all night. In 20 years from 2004, parts of Hoog Catharijne will be redeveloped as part of the renovation of the larger station area. Parts of the city's network of canals, which were filled to create the shopping center and central station area, will be recreated. The Jaarbeurs, one of the largest convention centres in the Netherlands, is located at the west side of the central railway station. Utrecht's cityscape is dominated by the Dom Tower, the tallest belfry in the Netherlands and originally part of the Cathedral of Saint Martin. An ongoing debate is over whether any building in or near the centre of town should surpass the Dom Tower in height (112 m). Nevertheless, some tall buildings are now being constructed that will become part of the skyline of Utrecht. The second tallest building of the city, the Rabobank-tower, was completed in 2010 and stands 105 m (344.49 ft) tall. Two antennas will increase that height to 120 m (393.70 ft). Two other buildings were constructed around the Nieuw Galgenwaard stadium (2007). These buildings, the 'Kantoortoren Galghenwert' and 'Apollo Residence', stand 85.5 and 64.5 metres high respectively. From the middle of the 3rd century Germanic tribes regularly invaded the Roman territories. Around 275 the Romans could no longer maintain the northern border and Utrecht was abandoned. Little is known about the next period 270–650. Utrecht is first spoken of again several centuries after the Romans left. Under the influence of the growing realms of the Franks, during Dagobert I's reign in the 7th century, a church was built within the walls of the Roman fortress. In ongoing border conflicts with the Frisians this first church was destroyed. In 1528 the bishop lost secular power over both Neder- and Oversticht – which included the city of Utrecht – to Charles V, Holy Roman Emperor. Charles V combined the Seventeen Provinces (the current Benelux and the northern parts of France) as a personal union. This ended the prince-bishopric Utrecht, as the secular rule was now the lordship of Utrecht, with the religious power remaining with the bishop, although Charles V had gained the right to appoint new bishops. In 1559 the bishopric of Utrecht was raised to archbishopric to make it the religious center of the Northern ecclesiastical province in the Seventeen provinces. About 69% of the population is of Dutch ancestry. Approximately 10% of the population consists of immigrants from Western countries, while 21% of the population is of non-Western origin (9% Moroccan, 5% Turkish, 3% Surinamese and Dutch Caribbean and 5% of other countries). Some of the city's boroughs have a relatively high percentage of originally non-Dutch inhabitants – i.e. Kanaleneiland 83% and Overvecht 57%. Like Rotterdam, Amsterdam, The Hague and other large Dutch cities, Utrecht faces some socio-economic problems. About 38% percent of its population either earns a minimum income or is dependent on social welfare (17% of all households). Boroughs such as Kanaleneiland, Overvecht and Hoograven consist primarily of high-rise housing developments, and are known for relatively high poverty and crime rate."
Rhine,"The other third of the water flows through the Pannerdens Kanaal and redistributes in the IJssel and Nederrijn. The IJssel branch carries one ninth of the water flow of the Rhine north into the IJsselmeer (a former bay), while the Nederrijn carries approximately two ninths of the flow west along a route parallel to the Waal. However, at Wijk bij Duurstede, the Nederrijn changes its name and becomes the Lek. It flows farther west, to rejoin the Noord River into the Nieuwe Maas and to the North Sea. The name Rijn, from here on, is used only for smaller streams farther to the north, which together formed the main river Rhine in Roman times. Though they retained the name, these streams no longer carry water from the Rhine, but are used for draining the surrounding land and polders. From Wijk bij Duurstede, the old north branch of the Rhine is called Kromme Rijn (""Bent Rhine"") past Utrecht, first Leidse Rijn (""Rhine of Leiden"") and then, Oude Rijn (""Old Rhine""). The latter flows west into a sluice at Katwijk, where its waters can be discharged into the North Sea. This branch once formed the line along which the Limes Germanicus were built. During periods of lower sea levels within the various ice ages, the Rhine took a left turn, creating the Channel River, the course of which now lies below the English Channel. Until 1932 the generally accepted length of the Rhine was 1,230 kilometres (764 miles). In 1932 the German encyclopedia Knaurs Lexikon stated the length as 1,320 kilometres (820 miles), presumably a typographical error. After this number was placed into the authoritative Brockhaus Enzyklopädie, it became generally accepted and found its way into numerous textbooks and official publications. The error was discovered in 2010, and the Dutch Rijkswaterstaat confirms the length at 1,232 kilometres (766 miles).[note 1] The Rhine is the longest river in Germany. It is here that the Rhine encounters some more of its main tributaries, such as the Neckar, the Main and, later, the Moselle, which contributes an average discharge of more than 300 m3/s (11,000 cu ft/s). Northeastern France drains to the Rhine via the Moselle; smaller rivers drain the Vosges and Jura Mountains uplands. Most of Luxembourg and a very small part of Belgium also drain to the Rhine via the Moselle. As it approaches the Dutch border, the Rhine has an annual mean discharge of 2,290 m3/s (81,000 cu ft/s) and an average width of 400 m (1,300 ft). Since the Peace of Westphalia, the Upper Rhine formed a contentious border between France and Germany. Establishing ""natural borders"" on the Rhine was a long-term goal of French foreign policy, since the Middle Ages, though the language border was – and is – far more to the west. French leaders, such as Louis XIV and Napoleon Bonaparte, tried with varying degrees of success to annex lands west of the Rhine. The Confederation of the Rhine was established by Napoleon, as a French client state, in 1806 and lasted until 1814, during which time it served as a significant source of resources and military manpower for the First French Empire. In 1840, the Rhine crisis, prompted by French prime minister Adolphe Thiers's desire to reinstate the Rhine as a natural border, led to a diplomatic crisis and a wave of nationalism in Germany. At present, the branches Waal and Nederrijn-Lek discharge to the North Sea, through the former Meuse estuary, near Rotterdam. The river IJssel branch flows to the north and enters the IJsselmeer, formerly the Zuider Zee brackish lagoon; however, since 1932, a freshwater lake. The discharge of the Rhine is divided among three branches: the River Waal (6/9 of total discharge), the River Nederrijn – Lek (2/9 of total discharge) and the River IJssel (1/9 of total discharge). This discharge distribution has been maintained since 1709, by river engineering works, including the digging of the Pannerdens canal and since the 20th century, with the help of weirs in the Nederrijn river. From the Eocene onwards, the ongoing Alpine orogeny caused a N–S rift system to develop in this zone. The main elements of this rift are the Upper Rhine Graben, in southwest Germany and eastern France and the Lower Rhine Embayment, in northwest Germany and the southeastern Netherlands. By the time of the Miocene, a river system had developed in the Upper Rhine Graben, that continued northward and is considered the first Rhine river. At that time, it did not yet carry discharge from the Alps; instead, the watersheds of the Rhone and Danube drained the northern flanks of the Alps. Before the St. Elizabeth's flood (1421), the Meuse flowed just south of today's line Merwede-Oude Maas to the North Sea and formed an archipelago-like estuary with Waal and Lek. This system of numerous bays, estuary-like extended rivers, many islands and constant changes of the coastline, is hard to imagine today. From 1421 to 1904, the Meuse and Waal merged further upstream at Gorinchem to form Merwede. For flood protection reasons, the Meuse was separated from the Waal through a lock and diverted into a new outlet called ""Bergse Maas"", then Amer and then flows into the former bay Hollands Diep. Through stream capture, the Rhine extended its watershed southward. By the Pliocene period, the Rhine had captured streams down to the Vosges Mountains, including the Mosel, the Main and the Neckar. The northern Alps were then drained by the Rhone. By the early Pleistocene period, the Rhine had captured most of its current Alpine watershed from the Rhône, including the Aar. Since that time, the Rhine has added the watershed above Lake Constance (Vorderrhein, Hinterrhein, Alpenrhein; captured from the Rhône), the upper reaches of the Main, beyond Schweinfurt and the Vosges Mountains, captured from the Meuse, to its watershed. Germanic tribes crossed the Rhine in the Migration period, by the 5th century establishing the kingdoms of Francia on the Lower Rhine, Burgundy on the Upper Rhine and Alemannia on the High Rhine. This ""Germanic Heroic Age"" is reflected in medieval legend, such as the Nibelungenlied which tells of the hero Siegfried killing a dragon on the Drachenfels (Siebengebirge) (""dragons rock""), near Bonn at the Rhine and of the Burgundians and their court at Worms, at the Rhine and Kriemhild's golden treasure, which was thrown into the Rhine by Hagen. Near Tamins-Reichenau the Anterior Rhine and the Posterior Rhine join and form the Rhine. The river makes a distinctive turn to the north near Chur. This section is nearly 86 km long, and descends from a height of 599 m to 396 m. It flows through a wide glacial alpine valley known as the Rhine Valley (German: Rheintal). Near Sargans a natural dam, only a few metres high, prevents it from flowing into the open Seeztal valley and then through Lake Walen and Lake Zurich into the river Aare. The Alpine Rhine begins in the most western part of the Swiss canton of Graubünden, and later forms the border between Switzerland to the West and Liechtenstein and later Austria to the East. Most of the Rhine's current course was not under the ice during the last Ice Age; although, its source must still have been a glacier. A tundra, with Ice Age flora and fauna, stretched across middle Europe, from Asia to the Atlantic Ocean. Such was the case during the Last Glacial Maximum, ca. 22,000–14,000 yr BP, when ice-sheets covered Scandinavia, the Baltics, Scotland and the Alps, but left the space between as open tundra. The loess or wind-blown dust over that tundra, settled in and around the Rhine Valley, contributing to its current agricultural usefulness. In World War II, it was recognised that the Rhine would present a formidable natural obstacle to the invasion of Germany, by the Western Allies. The Rhine bridge at Arnhem, immortalized in the book, A Bridge Too Far and the film, was a central focus of the battle for Arnhem, during the failed Operation Market Garden of September 1944. The bridges at Nijmegen, over the Waal distributary of the Rhine, were also an objective of Operation Market Garden. In a separate operation, the Ludendorff Bridge, crossing the Rhine at Remagen, became famous, when U.S. forces were able to capture it intact – much to their own surprise – after the Germans failed to demolish it. This also became the subject of a film, The Bridge at Remagen. Seven Days to the River Rhine was a Warsaw Pact war plan for an invasion of Western Europe during the Cold War. The Upper Rhine region was changed significantly by a Rhine straightening program in the 19th Century. The rate of flow was increased and the ground water level fell significantly. Dead branches dried up and the amount of forests on the flood plains decreased sharply. On the French side, the Grand Canal d'Alsace was dug, which carries a significant part of the river water, and all of the traffic. In some places, there are large compensation pools, for example the huge Bassin de compensation de Plobsheim in Alsace. At the end of World War I, the Rhineland was subject to the Treaty of Versailles. This decreed that it would be occupied by the allies, until 1935 and after that, it would be a demilitarised zone, with the German army forbidden to enter. The Treaty of Versailles and this particular provision, in general, caused much resentment in Germany and is often cited as helping Adolf Hitler's rise to power. The allies left the Rhineland, in 1930 and the German army re-occupied it in 1936, which was enormously popular in Germany. Although the allies could probably have prevented the re-occupation, Britain and France were not inclined to do so, a feature of their policy of appeasement to Hitler. Since ~3000 yr BP (= years Before Present), human impact is seen in the delta. As a result of increasing land clearance (Bronze Age agriculture), in the upland areas (central Germany), the sediment load of the Rhine has strongly increased and delta growth has sped up. This caused increased flooding and sedimentation, ending peat formation in the delta. The shifting of river channels to new locations, on the floodplain (termed avulsion), was the main process distributing sediment across the subrecent delta. Over the past 6000 years, approximately 80 avulsions have occurred. Direct human impact in the delta started with peat mining, for salt and fuel, from Roman times onward. This was followed by embankment, of the major distributaries and damming of minor distributaries, which took place in the 11–13th century AD. Thereafter, canals were dug, bends were short cut and groynes were built, to prevent the river's channels from migrating or silting up. The mouth of the Rhine into Lake Constance forms an inland delta. The delta is delimited in the West by the Alter Rhein (""Old Rhine"") and in the East by a modern canalized section. Most of the delta is a nature reserve and bird sanctuary. It includes the Austrian towns of Gaißau, Höchst and Fußach. The natural Rhine originally branched into at least two arms and formed small islands by precipitating sediments. In the local Alemannic dialect, the singular is pronounced ""Isel"" and this is also the local pronunciation of Esel (""Donkey""). Many local fields have an official name containing this element. Between Bingen and Bonn, the Middle Rhine flows through the Rhine Gorge, a formation which was created by erosion. The rate of erosion equaled the uplift in the region, such that the river was left at about its original level while the surrounding lands raised. The gorge is quite deep and is the stretch of the river which is known for its many castles and vineyards. It is a UNESCO World Heritage Site (2002) and known as ""the Romantic Rhine"", with more than 40 castles and fortresses from the Middle Ages and many quaint and lovely country villages. The Rhine emerges from Lake Constance, flows generally westward, as the Hochrhein, passes the Rhine Falls, and is joined by its major tributary, the river Aare. The Aare more than doubles the Rhine's water discharge, to an average of nearly 1,000 m3/s (35,000 cu ft/s), and provides more than a fifth of the discharge at the Dutch border. The Aare also contains the waters from the 4,274 m (14,022 ft) summit of Finsteraarhorn, the highest point of the Rhine basin. The Rhine roughly forms the German-Swiss border from Lake Constance with the exceptions of the canton of Schaffhausen and parts of the cantons of Zürich and Basel-Stadt, until it turns north at the so-called Rhine knee at Basel, leaving Switzerland. Until the early 1980s, industry was a major source of water pollution. Although many plants and factories can be found along the Rhine up into Switzerland, it is along the Lower Rhine that the bulk of them are concentrated, as the river passes the major cities of Cologne, Düsseldorf and Duisburg. Duisburg is the home of Europe's largest inland port and functions as a hub to the sea ports of Rotterdam, Antwerp and Amsterdam. The Ruhr, which joins the Rhine in Duisburg, is nowadays a clean river, thanks to a combination of stricter environmental controls, a transition from heavy industry to light industry and cleanup measures, such as the reforestation of Slag and brownfields. The Ruhr currently provides the region with drinking water. It contributes 70 m3/s (2,500 cu ft/s) to the Rhine. Other rivers in the Ruhr Area, above all, the Emscher, still carry a considerable degree of pollution. The Rhine-Meuse Delta is a tidal delta, shaped not only by the sedimentation of the rivers, but also by tidal currents. This meant that high tide formed a serious risk because strong tidal currents could tear huge areas of land into the sea. Before the construction of the Delta Works, tidal influence was palpable up to Nijmegen, and even today, after the regulatory action of the Delta Works, the tide acts far inland. At the Waal, for example, the most landward tidal influence can be detected between Brakel and Zaltbommel. From here, the situation becomes more complicated, as the Dutch name Rijn no longer coincides with the main flow of water. Two thirds of the water flow volume of the Rhine flows farther west, through the Waal and then, via the Merwede and Nieuwe Merwede (De Biesbosch), merging with the Meuse, through the Hollands Diep and Haringvliet estuaries, into the North Sea. The Beneden Merwede branches off, near Hardinxveld-Giessendam and continues as the Noord, to join the Lek, near the village of Kinderdijk, to form the Nieuwe Maas; then flows past Rotterdam and continues via Het Scheur and the Nieuwe Waterweg, to the North Sea. The Oude Maas branches off, near Dordrecht, farther down rejoining the Nieuwe Maas to form Het Scheur. As northwest Europe slowly began to warm up from 22,000 years ago onward, frozen subsoil and expanded alpine glaciers began to thaw and fall-winter snow covers melted in spring. Much of the discharge was routed to the Rhine and its downstream extension. Rapid warming and changes of vegetation, to open forest, began about 13,000 BP. By 9000 BP, Europe was fully forested. With globally shrinking ice-cover, ocean water levels rose and the English Channel and North Sea re-inundated. Meltwater, adding to the ocean and land subsidence, drowned the former coasts of Europe transgressionally. The Rhine was not known to Herodotus and first enters the historical period in the 1st century BC in Roman-era geography. At that time, it formed the boundary between Gaul and Germania. The Upper Rhine had been part of the areal of the late Hallstatt culture since the 6th century BC, and by the 1st century BC, the areal of the La Tène culture covered almost its entire length, forming a contact zone with the Jastorf culture, i.e. the locus of early Celtic-Germanic cultural contact. In Roman geography, the Rhine formed the boundary between Gallia and Germania by definition; e.g. Maurus Servius Honoratus, Commentary on the Aeneid of Vergil (8.727) (Rhenus) fluvius Galliae, qui Germanos a Gallia dividit ""(The Rhine is a) river of Gaul, which divides the Germanic people from Gaul."" By the 6th century, the Rhine was within the borders of Francia. In the 9th, it formed part of the border between Middle and Western Francia, but in the 10th century, it was fully within the Holy Roman Empire, flowing through Swabia, Franconia and Lower Lorraine. The mouths of the Rhine, in the county of Holland, fell to the Burgundian Netherlands in the 15th century; Holland remained contentious territory throughout the European wars of religion and the eventual collapse of the Holy Roman Empire, when the length of the Rhine fell to the First French Empire and its client states. The Alsace on the left banks of the Upper Rhine was sold to Burgundy by Archduke Sigismund of Austria in 1469 and eventually fell to France in the Thirty Years' War. The numerous historic castles in Rhineland-Palatinate attest to the importance of the river as a commercial route. The length of the Rhine is conventionally measured in ""Rhine-kilometers"" (Rheinkilometer), a scale introduced in 1939 which runs from the Old Rhine Bridge at Constance (0 km) to Hoek van Holland (1036.20 km). The river length is significantly shortened from the river's natural course due to number of canalisation projects completed in the 19th and 20th century.[note 7] The ""total length of the Rhine"", to the inclusion of Lake Constance and the Alpine Rhine is more difficult to measure objectively; it was cited as 1,232 kilometres (766 miles) by the Dutch Rijkswaterstaat in 2010.[note 1] The shape of the Rhine delta is determined by two bifurcations: first, at Millingen aan de Rijn, the Rhine splits into Waal and Pannerdens Kanaal, which changes its name to Nederrijn at Angeren, and second near Arnhem, the IJssel branches off from the Nederrijn. This creates three main flows, two of which change names rather often. The largest and southern main branch begins as Waal and continues as Boven Merwede (""Upper Merwede""), Beneden Merwede (""Lower Merwede""), Noord River (""North River""), Nieuwe Maas (""New Meuse""), Het Scheur (""the Rip"") and Nieuwe Waterweg (""New Waterway""). The middle flow begins as Nederrijn, then changes into Lek, then joins the Noord, thereby forming Nieuwe Maas. The northern flow keeps the name IJssel until it flows into Lake IJsselmeer. Three more flows carry significant amounts of water: the Nieuwe Merwede (""New Merwede""), which branches off from the southern branch where it changes from Boven to Beneden Merwede; the Oude Maas (""Old Meuse""), which branches off from the southern branch where it changes from Beneden Merwede into Noord, and Dordtse Kil, which branches off from Oude Maas. The Rhine-Meuse Delta, the most important natural region of the Netherlands begins near Millingen aan de Rijn, close to the Dutch-German border with the division of the Rhine into Waal and Nederrijn. Since the Rhine contributes most of the water, the shorter term Rhine Delta is commonly used. However, this name is also used for the river delta where the Rhine flows into Lake Constance, so it is clearer to call the larger one Rhine-Meuse delta, or even Rhine–Meuse–Scheldt delta, as the Scheldt ends in the same delta. Since 7500 yr ago, a situation with tides and currents, very similar to present has existed. Rates of sea-level rise had dropped so far, that natural sedimentation by the Rhine and coastal processes together, could compensate the transgression by the sea; in the last 7000 years, the coast line was roughly at the same location. In the southern North Sea, due to ongoing tectonic subsidence, the sea level is still rising, at the rate of about 1–3 cm (0.39–1.18 in) per century (1 metre or 39 inches in last 3000 years). From the death of Augustus in AD 14 until after AD 70, Rome accepted as her Germanic frontier the water-boundary of the Rhine and upper Danube. Beyond these rivers she held only the fertile plain of Frankfurt, opposite the Roman border fortress of Moguntiacum (Mainz), the southernmost slopes of the Black Forest and a few scattered bridge-heads. The northern section of this frontier, where the Rhine is deep and broad, remained the Roman boundary until the empire fell. The southern part was different. The upper Rhine and upper Danube are easily crossed. The frontier which they form is inconveniently long, enclosing an acute-angled wedge of foreign territory between the modern Baden and Württemberg. The Germanic populations of these lands seem in Roman times to have been scanty, and Roman subjects from the modern Alsace-Lorraine had drifted across the river eastwards. A regulation of the Rhine was called for, with an upper canal near Diepoldsau and a lower canal at Fußach, in order to counteract the constant flooding and strong sedimentation in the western Rhine Delta. The Dornbirner Ach had to be diverted, too, and it now flows parallel to the canalized Rhine into the lake. Its water has a darker color than the Rhine; the latter's lighter suspended load comes from higher up the mountains. It is expected that the continuous input of sediment into the lake will silt up the lake. This has already happened to the former Lake Tuggenersee. The Lower Rhine flows through North Rhine-Westphalia. Its banks are usually heavily populated and industrialized, in particular the agglomerations Cologne, Düsseldorf and Ruhr area. Here the Rhine flows through the largest conurbation in Germany, the Rhine-Ruhr region. One of the most important cities in this region is Duisburg with the largest river port in Europe (Duisport). The region downstream of Duisburg is more agricultural. In Wesel, 30 km downstream of Duisburg, is located the western end of the second east-west shipping route, the Wesel-Datteln Canal, which runs parallel to the Lippe. Between Emmerich and Cleves the Emmerich Rhine Bridge, the longest suspension bridge in Germany, crosses the 400 m wide river. Near Krefeld, the river crosses the Uerdingen line, the line which separates the areas where Low German and High German are spoken. The variant forms of the name of the Rhine in modern languages are all derived from the Gaulish name Rēnos, which was adapted in Roman-era geography (1st century BC) as Greek Ῥῆνος (Rhēnos), Latin Rhenus.[note 3] The spelling with Rh- in English Rhine as well as in German Rhein and French Rhin is due to the influence of Greek orthography, while the vocalisation -i- is due to the Proto-Germanic adoption of the Gaulish name as *Rīnaz, via Old Frankish giving Old English Rín, Old High German Rīn, Dutch Rijn (formerly also spelled Rhijn)). The diphthong in modern German Rhein (also adopted in Romansh Rein, Rain) is a Central German development of the early modern period, the Alemannic name Rī(n) retaining the older vocalism,[note 4] as does Ripuarian Rhing, while Palatine has diphthongized Rhei, Rhoi. Spanish is with French in adopting the Germanic vocalism Rin-, while Italian, Occitan and Portuguese retain the Latin Ren-. Around 2.5 million years ago (ending 11,600 years ago) was the geological period of the Ice Ages. Since approximately 600,000 years ago, six major Ice Ages have occurred, in which sea level dropped 120 m (390 ft) and much of the continental margins became exposed. In the Early Pleistocene, the Rhine followed a course to the northwest, through the present North Sea. During the so-called Anglian glaciation (~450,000 yr BP, marine oxygen isotope stage 12), the northern part of the present North Sea was blocked by the ice and a large lake developed, that overflowed through the English Channel. This caused the Rhine's course to be diverted through the English Channel. Since then, during glacial times, the river mouth was located offshore of Brest, France and rivers, like the Thames and the Seine, became tributaries to the Rhine. During interglacials, when sea level rose to approximately the present level, the Rhine built deltas, in what is now the Netherlands. The Romans kept eight legions in five bases along the Rhine. The actual number of legions present at any base or in all, depended on whether a state or threat of war existed. Between about AD 14 and 180, the assignment of legions was as follows: for the army of Germania Inferior, two legions at Vetera (Xanten), I Germanica and XX Valeria (Pannonian troops); two legions at oppidum Ubiorum (""town of the Ubii""), which was renamed to Colonia Agrippina, descending to Cologne, V Alaudae, a Celtic legion recruited from Gallia Narbonensis and XXI, possibly a Galatian legion from the other side of the empire. In southern Europe, the stage was set in the Triassic Period of the Mesozoic Era, with the opening of the Tethys Ocean, between the Eurasian and African tectonic plates, between about 240 MBP and 220 MBP (million years before present). The present Mediterranean Sea descends from this somewhat larger Tethys sea. At about 180 MBP, in the Jurassic Period, the two plates reversed direction and began to compress the Tethys floor, causing it to be subducted under Eurasia and pushing up the edge of the latter plate in the Alpine Orogeny of the Oligocene and Miocene Periods. Several microplates were caught in the squeeze and rotated or were pushed laterally, generating the individual features of Mediterranean geography: Iberia pushed up the Pyrenees; Italy, the Alps, and Anatolia, moving west, the mountains of Greece and the islands. The compression and orogeny continue today, as shown by the ongoing raising of the mountains a small amount each year and the active volcanoes. The last glacial ran from ~74,000 (BP = Before Present), until the end of the Pleistocene (~11,600 BP). In northwest Europe, it saw two very cold phases, peaking around 70,000 BP and around 29,000–24,000 BP. The last phase slightly predates the global last ice age maximum (Last Glacial Maximum). During this time, the lower Rhine flowed roughly west through the Netherlands and extended to the southwest, through the English Channel and finally, to the Atlantic Ocean. The English Channel, the Irish Channel and most of the North Sea were dry land, mainly because sea level was approximately 120 m (390 ft) lower than today. Lake Constance consists of three bodies of water: the Obersee (""upper lake""), the Untersee (""lower lake""), and a connecting stretch of the Rhine, called the Seerhein (""Lake Rhine""). The lake is situated in Germany, Switzerland and Austria near the Alps. Specifically, its shorelines lie in the German states of Bavaria and Baden-Württemberg, the Austrian state of Vorarlberg, and the Swiss cantons of Thurgau and St. Gallen. The Rhine flows into it from the south following the Swiss-Austrian border. It is located at approximately 47°39′N 9°19′E﻿ / ﻿47.650°N 9.317°E﻿ / 47.650; 9.317. The dominant economic sectors in the Middle Rhine area are viniculture and tourism. The Rhine Gorge between Rüdesheim am Rhein and Koblenz is listed as a UNESCO World Heritage Site. Near Sankt Goarshausen, the Rhine flows around the famous rock Lorelei. With its outstanding architectural monuments, the slopes full of vines, settlements crowded on the narrow river banks and scores of castles lined up along the top of the steep slopes, the Middle Rhine Valley can be considered the epitome of the Rhine romanticism. At the begin of the Holocene (~11,700 years ago), the Rhine occupied its Late-Glacial valley. As a meandering river, it reworked its ice-age braidplain. As sea-level continued to rise in the Netherlands, the formation of the Holocene Rhine-Meuse delta began (~8,000 years ago). Coeval absolute sea-level rise and tectonic subsidence have strongly influenced delta evolution. Other factors of importance to the shape of the delta are the local tectonic activities of the Peel Boundary Fault, the substrate and geomorphology, as inherited from the Last Glacial and the coastal-marine dynamics, such as barrier and tidal inlet formations. The hydrography of the current delta is characterized by the delta's main arms, disconnected arms (Hollandse IJssel, Linge, Vecht, etc.) and smaller rivers and streams. Many rivers have been closed (""dammed"") and now serve as drainage channels for the numerous polders. The construction of Delta Works changed the Delta in the second half of the 20th Century fundamentally. Currently Rhine water runs into the sea, or into former marine bays now separated from the sea, in five places, namely at the mouths of the Nieuwe Merwede, Nieuwe Waterway (Nieuwe Maas), Dordtse Kil, Spui and IJssel. The flow of cold, gray mountain water continues for some distance into the lake. The cold water flows near the surface and at first doesn't mix with the warmer, green waters of Upper Lake. But then, at the so-called Rheinbrech, the Rhine water abruptly falls into the depths because of the greater density of cold water. The flow reappears on the surface at the northern (German) shore of the lake, off the island of Lindau. The water then follows the northern shore until Hagnau am Bodensee. A small fraction of the flow is diverted off the island of Mainau into Lake Überlingen. Most of the water flows via the Constance hopper into the Rheinrinne (""Rhine Gutter"") and Seerhein. Depending on the water level, this flow of the Rhine water is clearly visible along the entire length of the lake. The Rhine (Romansh: Rein, German: Rhein, French: le Rhin, Dutch: Rijn) is a European river that begins in the Swiss canton of Graubünden in the southeastern Swiss Alps, forms part of the Swiss-Austrian, Swiss-Liechtenstein border, Swiss-German and then the Franco-German border, then flows through the Rhineland and eventually empties into the North Sea in the Netherlands. The biggest city on the river Rhine is Cologne, Germany with a population of more than 1,050,000 people. It is the second-longest river in Central and Western Europe (after the Danube), at about 1,230 km (760 mi),[note 2][note 1] with an average discharge of about 2,900 m3/s (100,000 cu ft/s). In the centre of Basel, the first major city in the course of the stream, is located the ""Rhine knee""; this is a major bend, where the overall direction of the Rhine changes from West to North. Here the High Rhine ends. Legally, the Central Bridge is the boundary between High and Upper Rhine. The river now flows North as Upper Rhine through the Upper Rhine Plain, which is about 300 km long and up to 40 km wide. The most important tributaries in this area are the Ill below of Strasbourg, the Neckar in Mannheim and the Main across from Mainz. In Mainz, the Rhine leaves the Upper Rhine Valley and flows through the Mainz Basin."
American_Idol,"Both finalists found success after the show, but Aiken out-performed Studdard's coronation song ""Flying Without Wings"" with his single release from the show ""This Is the Night"", as well as in their subsequent album releases. The fourth-place finisher Josh Gracin also enjoyed some success as a country singer. There were 13 finalists this season, but two were eliminated in the first result show of the finals. A new feature introduced was the ""Judges' Save"", and Matt Giraud was saved from elimination at the top seven by the judges when he received the fewest votes. The next week, Lil Rounds and Anoop Desai were eliminated. As of 2013, the American Idol alumni in their post-Idol careers have amassed over 59 million albums and 120 million singles and digital track downloads in the United States alone. The finalists were Cook and Archuleta. David Cook was announced the winner on May 21, 2008, the first rocker to win the show. Both Cook and Archuleta had some success as recording artists with both selling over a million albums in the U.S. This season saw the first Idol Gives Back telethon-inspired event, which raised more than $76 million in corporate and viewer donations. No contestant was eliminated that week, but two (Phil Stacey and Chris Richardson) were eliminated the next. Melinda Doolittle was eliminated in the final three. From seasons four to seven and nine, the twenty-four semi-finalists were divided by gender in order to ensure an equal gender division in the top twelve. The men and women sang separately on consecutive nights, and the bottom two in each groups were eliminated each week until only six of each remained to form the top twelve. Phillips released ""Home"" as his coronation song, while Sanchez released ""Change Nothing"". Phillips' ""Home"" has since become the best selling of all coronation songs. The growth continued into the next season, starting with a season premiere of 26.5 million. The season attracted an average of 21.7 million viewers, and was placed second overall amongst the 18–49 age group. The finale night when Ruben Studdard won over Clay Aiken was also the highest-rated ever American Idol episode at 38.1 million for the final hour. By season three, the show had become the top show in the 18–49 demographic a position it has held for all subsequent years up to and including season ten, and its competition stages ranked first in the nationwide overall ratings. By season four, American Idol had become the most watched series amongst all viewers on American TV for the first time, with an average viewership of 26.8 million. The show reached its peak in season five with numbers averaging 30.6 million per episode, and season five remains the highest-rated season of the series. American Idol became the most expensive series on broadcast networks for advertisers starting season four, and by the next season, it had broken the record in advertising rate for a regularly scheduled prime-time network series, selling over $700,000 for a 30-seconds slot, and reaching up to $1.3 million for the finale. Its ad prices reached a peak in season seven at $737,000. Estimated revenue more than doubled from $404 million in season three to $870 million in season six. While that declined from season eight onwards, it still earned significantly more than its nearest competitor, with advertising revenue topping $800 million annually the next few seasons. However, the sharp drop in ratings in season eleven also resulted in a sharp drop in advertising rate for season twelve, and the show lost its leading position as the costliest show for advertisers. By 2014, ad revenue from had fallen to $427 million where a 30-second spot went for less than $300,000. On May 30, 2006, Taylor Hicks was named American Idol, with Katharine McPhee the runner-up. ""Do I Make You Proud"" was released as Hicks' first single and McPhee's was ""My Destiny"". Despite being eliminated earlier in the season, Chris Daughtry (as lead of the band Daughtry) became the most successful recording artist from this season. Other contestants, such as Hicks, McPhee, Bucky Covington, Mandisa, Kellie Pickler, and Elliott Yamin have had varying levels of success. Following the success of season one, the second season was moved up to air in January 2003. The number of episodes increased, as did the show's budget and the charge for commercial spots. Dunkleman left the show, leaving Seacrest as the lone host. Kristin Adams was a correspondent for this season. Season ten is the first to include online auditions where contestants could submit a 40-second video audition via Myspace. Karen Rodriguez was one such auditioner and reached the final rounds. Teenager Sanjaya Malakar was the season's most talked-about contestant for his unusual hairdo, and for managing to survive elimination for many weeks due in part to the weblog Vote for the Worst and satellite radio personality Howard Stern, who both encouraged fans to vote for him. However, on April 18, Sanjaya was voted off. From the semi-finals onwards, the fate of the contestants is decided by public vote. During the contestant's performance as well as the recap at the end, a toll-free telephone number for each contestant is displayed on the screen. For a two-hour period after the episode ends (up to four hours for the finale) in each US time zone, viewers may call or send a text message to their preferred contestant's telephone number, and each call or text message is registered as a vote for that contestant. Viewers are allowed to vote as many times as they can within the two-hour voting window. However, the show reserves the right to discard votes by power dialers. One or more of the least popular contestants may be eliminated in successive weeks until a winner emerges. Over 110 million votes were cast in the first season, and by season ten the seasonal total had increased to nearly 750 million. Voting via text messaging was made available in the second season when AT&T Wireless joined as a sponsor of the show, and 7.5 million text messages were sent to American Idol that season. The number of text messages rapidly increased, reaching 178 million texts by season eight. Online voting was offered for the first time in season ten. The votes are counted and verified by Telescope Inc. The enormous success of the show and the revenue it generated was transformative for Fox Broadcasting Company. American Idol and fellow competing shows Survivor and Who Wants to Be a Millionaire were altogether credited for expanding reality television programming in the United States in the 1990s and 2000s, and Idol became the most watched non-scripted primetime television series for almost a decade, from 2003 to 2012, breaking records on U.S. television (dominated by drama shows and sitcoms in the preceding decades). The American Idol Songwriter contest was also held this season. From ten of the most popular submissions, each of the final two contestants chose a song to perform, although neither of their selections was used as the ""coronation song"". The winning song, ""The Time of My Life"", was recorded by David Cook and released on May 22, 2008. Season ten of the series premiered on January 19, 2011. Many changes were introduced this season, from the format to the personnel of the show. Jennifer Lopez and Steven Tyler joined Randy Jackson as judges following the departures of Simon Cowell (who left to launch the U.S. version of The X Factor), Kara DioGuardi (whose contract was not renewed) and Ellen DeGeneres, while Nigel Lythgoe returned as executive producer. Jimmy Iovine, chairman of the Interscope Geffen A&M label group, the new partner of American Idol, acted as the in-house mentor in place of weekly guest mentors, although in later episodes special guest mentors such as Beyoncé, will.i.am and Lady Gaga were brought in. In season ten, the total viewer numbers for the first week of shows fell 12–13%, and by up to 23% in the 18–49 demo compared to season nine. Later episodes, however, retained viewers better, and the season ended on a high with a significant increase in viewership for the finale – up 12% for the adults 18–49 demo and a 21% increase in total viewers from the season nine finale. While the overall viewer number has increased this season, its viewer demographics have continued to age year on year – the median age this season was 47.2 compared to a median age of 32.1 in its first season. By the time of the 2010–11 television season, Fox was in its seventh consecutive season of victory overall in the 18–49 demographic ratings in the United States. Voting results have been a consistent source of controversy. The mechanism of voting had also aroused considerable criticisms, most notably in season two when Ruben Studdard beat Clay Aiken in a close vote, and in season eight, when the massive increase in text votes (100 million more text votes than season 7) fueled the texting controversy. Concerns about power voting have been expressed from the very first season. Since 2004, votes also have been affected to a limited degree by online communities such as DialIdol, Vote for the Worst (closed in 2013), and Vote for the Girls (started 2010). Crystal Bowersox, who has Type-I diabetes, fell ill due to diabetic ketoacidosis on the morning of the girls performance night for the top 20 week and was hospitalized. The schedule was rearranged so the boys performed first and she could perform the following night instead; she later revealed that Ken Warwick, the show producer, wanted to disqualify her but she begged to be allowed to stay on the show. The performance of ""Summertime"" by Barrino, later known simply as ""Fantasia"", at Top 8 was widely praised, and Simon Cowell considered it as his favorite Idol moment in the nine seasons he was on the show. Fantasia and Diana DeGarmo were the last two finalists, and Fantasia was crowned as the winner. Fantasia released as her coronation single ""I Believe"", a song co-written by season one finalist Tamyra Gray, and DeGarmo released ""Dreams"". Fantasia went on to gain some successes as a recording artist, while Hudson, who placed seventh, became the only Idol contestant so far to win both an Academy Award and a Grammy. David Archuleta's performance of John Lennon's ""Imagine"" was considered by many as one of the best of the season. Jennifer Lopez, who was brought in as a judge in season ten, called it a beautiful song-moment that she will never forget. Jason Castro's semi-final performance of ""Hallelujah"" also received considerable attention, and it propelled Jeff Buckley's version of the song to the top of the Billboard digital song chart. This was the first season in which contestants' recordings were released onto iTunes after their performances, and although sales information was not released so as not to prejudice the contest, leaked information indicated that contestants' songs frequently reached the top of iTunes sales charts. Contestants go through at least three sets of cuts. The first is a brief audition with a few other contestants in front of selectors which may include one of the show's producers. Although auditions can exceed 10,000 in each city, only a few hundred of these make it past the preliminary round of auditions. Successful contestants then sing in front of producers, where more may be cut. Only then can they proceed to audition in front of the judges, which is the only audition stage shown on television. Those selected by the judges are sent to Hollywood. Between 10–60 people in each city may make it to Hollywood[citation needed]. The two finalists in 2011 were Lauren Alaina and Scotty McCreery, both teenage country singers. McCreery won the competition on May 25, being the youngest male winner and the fourth male in a row to win American Idol. McCreery released his first single, ""I Love You This Big"", as his coronation song, and Alaina released ""Like My Mother Does"". McCreery's debut album, Clear as Day, became the first debut album by an Idol winner to reach No. 1 on the US Billboard 200 since Ruben Studdard's Soulful in 2003, and he became the youngest male artist to reach No. 1 on the Billboard 200. The dominance of American Idol in the ratings had made it the most profitable show in U.S. TV for many years. The show was estimated to generate $900 million for the year 2004 through sales of TV ads, albums, merchandise and concert tickets. By season seven, the show was estimated to earn around $900 million from its ad revenue alone, not including ancillary sponsorship deals and other income. One estimate puts the total TV revenue for the first eight seasons of American at $6.4 billion. Sponsors that bought fully integrated packages can expect a variety of promotions of their products on the show, such as product placement, adverts and product promotion integrated into the show, and various promotional opportunities. Other off-air promotional partners pay for the rights to feature ""Idol"" branding on their packaging, products and marketing programs. American Idol also partnered with Disney in its theme park attraction The American Idol Experience. The loss of viewers continued into season 12, which saw the show hitting a number of series low in the 18-49 demo. The finale had 7.2 million fewer viewers than the previous season, and saw a drop of 44% in the 18-49 demo. The season viewers averaged at 13.3 million, a drop of 24% from the previous season. The thirteenth season suffered a huge decline in the 18–49 demographic, a drop of 28% from the twelfth season, and American Idol lost its Top 10 position in the Nielsen ratings by the end of the 2013–14 television season for the first time since its entry to the rankings in 2003 as a result, although the entire series to date had not yet been dropped from the Nielsen Top 30 rankings since its inception in 2002. Much media attention on the season had been focused on the three black singers, Fantasia Barrino, LaToya London, and Jennifer Hudson, dubbed the Three Divas. All three unexpectedly landed on the bottom three on the top seven result show, with Hudson controversially eliminated. Elton John, who was one of the mentors that season, called the results of the votes ""incredibly racist"". The prolonged stays of John Stevens and Jasmine Trias in the finals, despite negative comments from the judges, had aroused resentment, so much so that John Stevens reportedly received a death threat, which he dismissed as a joke 'blown out of proportion'. The first season of American Idol debuted as a summer replacement show in June 2002 on the Fox network. It was co-hosted by Ryan Seacrest and Brian Dunkleman. Caleb Johnson was named the winner of the season, with Jena Irene as the runner-up. Johnson released ""As Long as You Love Me"" as his coronation single while Irene released ""We Are One"". American Idol has traditionally released studio recordings of contestants' performances as well as the winner's coronation single for sale. For the first five seasons, the recordings were released as a compilation album at the end of the season. All five of these albums reached the top ten in Billboard 200 which made then American Idol the most successful soundtrack franchise of any motion picture or television program. Starting late in season five, individual performances were released during the season as digital downloads, initially from the American Idol official website only. In season seven the live performances and studio recordings were made available during the season from iTunes when it joined as a sponsor. In Season ten the weekly studio recordings were also released as compilation digital album straight after performance night. Some in the entertainment industry were critical of the star-making aspect of the show. Usher, a mentor on the show, bemoaning the loss of the ""true art form of music"", thought that shows like American Idol made it seem ""so easy that everyone can do it, and that it can happen overnight"", and that ""television is a lie"". Musician Michael Feinstein, while acknowledging that the show had uncovered promising performers, said that American Idol ""isn't really about music. It's about all the bad aspects of the music business – the arrogance of commerce, this sense of 'I know what will make this person a star; artists themselves don't know.' "" That American Idol is seen to be a fast track to success for its contestants has been a cause of resentment for some in the industry. LeAnn Rimes, commenting on Carrie Underwood winning Best Female Artist in Country Music Awards over Faith Hill in 2006, said that ""Carrie has not paid her dues long enough to fully deserve that award"". It is a common theme that has been echoed by many others. Elton John, who had appeared as a mentor in the show but turned down an offer to be a judge on American Idol, commenting on talent shows in general, said that ""there have been some good acts but the only way to sustain a career is to pay your dues in small clubs"". American Idol was nominated for the Emmy's Outstanding Reality Competition Program for nine years but never won. Director Bruce Gower won a Primetime Emmy Award for Outstanding Directing For A Variety, Music Or Comedy Series in 2009, and the show won a Creative Arts Emmys each in 2007 and 2008, three in 2009, and two in 2011, as well as a Governor's Award in 2007 for its Idol Gives Back edition. It won the People's Choice Award, which honors the popular culture of the previous year as voted by the public, for favorite competition/reality show in 2005, 2006, 2007, 2010, 2011 and 2012. It won the first Critics' Choice Television Award in 2011 for Best Reality Competition. The finale is the two-hour last episode of the season, culminating in revealing the winner. For seasons one, three through six, and fourteen, it was broadcast from the Dolby Theatre, which has an audience capacity of approximately 3,400. The finale for season two took place at the Gibson Amphitheatre, which has an audience capacity of over 6,000. In seasons seven through thirteen, the venue was at the Nokia Theatre, which holds an audience of over 7,000. 19 Recordings, a recording label owned by 19 Entertainment, currently hold the rights to phonographic material recorded by all the contestants. 19 originally partnered with Bertelsmann Music Group (BMG) to promote and distribute the recordings through its labels RCA Records, Arista Records, J Records, Jive Records. In 2005-2007, BMG partnered with Sony Music Entertainment to form a joint venture known as Sony BMG Music Entertainment. From 2008-2010, Sony Music handled the distribution following their acquisition of BMG. Sony Music was partnered with American Idol and distribute its music, and In 2010, Sony was replaced by as the music label for American Idol by UMG's Interscope-Geffen-A&M Records. Coca-Cola's archrival PepsiCo declined to sponsor American Idol at the show's start. What the Los Angeles Times later called ""missing one of the biggest marketing opportunities in a generation"" contributed to Pepsi losing market share, by 2010 falling to third place from second in the United States. PepsiCo sponsored the American version of Cowell's The X Factor in hopes of not repeating its Idol mistake until its cancellation. Since the show's inception in 2002, ten of the fourteen Idol winners, including its first five, have come from the Southern United States. A large number of other notable finalists during the series' run have also hailed from the American South, including Clay Aiken, Kellie Pickler, and Chris Daughtry, who are all from North Carolina. In 2012, an analysis of the 131 contestants who have appeared in the finals of all seasons of the show up to that point found that 48% have some connection to the Southern United States. Chris Daughtry's performance of Fuel's ""Hemorrhage (In My Hands)"" on the show was widely praised and led to an invitation to join the band as Fuel's new lead singer, an invitation he declined. His performance of Live's version of ""I Walk the Line"" was well received by the judges but later criticized in some quarters for not crediting the arrangement to Live. He was eliminated at the top four in a shocking result. 23-year-old Candice Glover won the season with Kree Harrison taking the runner-up spot. Glover is the first female to win American Idol since Jordin Sparks. Glover released ""I Am Beautiful"" as a single while Harrison released ""All Cried Out"" immediately after the show. Glover sold poorly with her debut album, and this is also the first season that the runner-up was not signed by a music label. The final showdown was between Justin Guarini, one of the early favorites, and Kelly Clarkson. Clarkson was not initially thought of as a contender, but impressed the judges with some good performances in the final rounds, such as her performance of Aretha Franklin's ""Natural Woman"", and Betty Hutton's ""Stuff Like That There"", and eventually won the crown on September 4, 2002. Michael Lynche was the lowest vote getter at top nine and was given the Judges' Save. The next week Katie Stevens and Andrew Garcia were eliminated. That week, Adam Lambert was invited back to be a mentor, the first Idol alum to do so. Idol Gives Back returned this season on April 21, 2010, and raised $45 million. Season three premiered on January 19, 2004. One of the most talked-about contestants during the audition process was William Hung whose off-key rendition of Ricky Martin's ""She Bangs"" received widespread attention. His exposure on Idol landed him a record deal and surprisingly he became the third best-selling singer from that season. Early reviews were mixed in their assessment. Ken Tucker of Entertainment Weekly considered that ""As TV, American Idol is crazily entertaining; as music, it's dust-mote inconsequential"". Others, however, thought that ""the most striking aspect of the series was the genuine talent it revealed"". It was also described as a ""sadistic musical bake-off"", and ""a romp in humiliation"". Other aspects of the show have attracted criticisms. The product placement in the show in particular was noted, and some critics were harsh about what they perceived as its blatant commercial calculations – Karla Peterson of The San Diego Union-Tribune charged that American Idol is ""a conniving multimedia monster"" that has ""absorbed the sin of our debauched culture and spit them out in a lump of reconstituted evil"". The decision to send the season one winner to sing the national anthem at the Lincoln Memorial on the first anniversary of the September 11 attacks in 2002 was also poorly received by many. Lisa de Moraes of The Washington Post noted sarcastically that ""The terrorists have won"" and, with a sideswipe at the show's commercialism and voting process, that the decision as to who ""gets to turn this important site into just another cog in the 'Great American Idol Marketing Mandala' is in the hands of the millions of girls who have made American Idol a hit. Them and a handful of phone-redialer geeks who have been clocking up to 10,000 calls each week for their contestant of choice (but who, according to Fox, are in absolutely no way skewing the outcome)."" In season eight, Latin Grammy Award-nominated singer–songwriter and record producer Kara DioGuardi was added as a fourth judge. She stayed for two seasons and left the show before season ten. Paula Abdul left the show before season nine after failing to agree terms with the show producers. Emmy Award-winning talk show host Ellen DeGeneres replaced Paula Abdul for that season, but left after just one season. On January 11, 2010, Simon Cowell announced that he was leaving the show to pursue introducing the American version of his show The X Factor to the USA for 2011. Jennifer Lopez and Steven Tyler joined the judging panel in season ten, but both left after two seasons. They were replaced by three new judges, Mariah Carey, Nicki Minaj and Keith Urban, who joined Randy Jackson in season 12. However both Carey and Minaj left after one season, and Randy Jackson also announced that he would depart the show after twelve seasons as a judge but would return as a mentor. Urban is the only judge from season 12 to return in season 13. He was joined by previous judge Jennifer Lopez and former mentor Harry Connick, Jr.. Lopez, Urban and Connick, Jr. all returned as judges for the show's fourteenth and fifteenth seasons. Season six premiered with the series' highest-rated debut episode and a few of its succeeding episodes rank among the most watched episodes of American Idol. During this time, many television executives begun to regard the show as a programming force unlike any seen before, as its consistent dominance of up to two hours two or three nights a week exceeded the 30- or 60-minute reach of previous hits such as NBC's The Cosby Show. The show was dubbed ""the Death Star"", and competing networks often rearranged their schedules in order to minimize losses. However, season six also showed a steady decline in viewership over the course of the season. The season finale saw a drop in ratings of 16% from the previous year. Season six was the first season wherein the average results show rated higher than the competition stages (unlike in the previous seasons), and became the second highest-rated of the series after the preceding season. The winner receives a record deal with a major label, which may be for up to six albums, and secures a management contract with American Idol-affiliated 19 Management (which has the right of first refusal to sign all contestants), as well as various lucrative contracts. All winners prior to season nine reportedly earned at least $1 million in their first year as winner. All the runners-up of the first ten seasons, as well as some of other finalists, have also received record deals with major labels. However, starting in season 11, the runner-up may only be guaranteed a single-only deal. BMG/Sony (seasons 1–9) and UMG (season 10–) had the right of first refusal to sign contestants for three months after the season's finale. Starting in the fourteenth season, the winner was signed with Big Machine Records. Prominent music mogul Clive Davis also produced some of the selected contestants' albums, such as Kelly Clarkson, Clay Aiken, Fantasia Barrino and Diana DeGarmo. All top 10 (11 in seasons 10 and 12) finalists earn the privilege of going on a tour, where the participants may each earn a six-figure sum. The show's massive success in the mid-2000s and early 2010s spawned a number of imitating singing-competition shows, such as Rock Star, Nashville Star, The Voice, Rising Star, The Sing-Off, and The X Factor. Its format also served as a blueprint for non-singing TV shows such as Dancing with the Stars and So You Think You Can Dance, most of which contribute to the current highly competitive reality TV landscape on American television. On February 14, 2009, The Walt Disney Company debuted ""The American Idol Experience"" at its Disney's Hollywood Studios theme park at the Walt Disney World Resort in Florida. In this live production, co-produced by 19 Entertainment, park guests chose from a list of songs and auditioned privately for Disney cast members. Those selected then performed on a stage in a 1000-seat theater replicating the Idol set. Three judges, whose mannerisms and style mimicked those of the real Idol judges, critiqued the performances. Audience members then voted for their favorite performer. There were several preliminary-round shows during the day that culminated in a ""finals"" show in the evening where one of the winners of the previous rounds that day was selected as the overall winner. The winner of the finals show received a ""Dream Ticket"" that granted them front-of-the-line privileges at any future American Idol audition. The attraction closed on August 30, 2014. American Idol was based on the British show Pop Idol created by Simon Fuller, which was in turn inspired by the New Zealand television singing competition Popstars. Television producer Nigel Lythgoe saw it in Australia and helped bring it over to Britain. Fuller was inspired by the idea from Popstars of employing a panel of judges to select singers in audition. He then added other elements, such as telephone voting by the viewing public (which at the time was already in use in shows such as the Eurovision Song Contest), the drama of backstories and real-life soap opera unfolding in real time. The show debuted in 2001 in Britain with Lythgoe as showrunner‍—‌the executive producer and production leader‍—‌and Simon Cowell as one of the judges, and was a big success with the viewing public. The show pushed Fox to become the number one U.S. TV network amongst adults 18–49, the key demographic coveted by advertisers, for an unprecedented eight consecutive years by 2012. Its success also helped lift the ratings of other shows that were scheduled around it such as House and Bones, and Idol, for years, had become Fox's strongest platform primetime television program for promoting eventual hit shows of the 2010s (of the same network) such as Glee and New Girl. The show, its creator Simon Fuller claimed, ""saved Fox"". Once in Hollywood, the contestants perform individually or in groups in a series of rounds. Until season ten, there were usually three rounds of eliminations in Hollywood. In the first round the contestants emerged in groups but performed individually. For the next round, the contestants put themselves in small groups and perform a song together. In the final round, the contestants perform solo with a song of their choice a cappella or accompanied by a band‍—‌depending on the season. In seasons two and three, contestants were also asked to write original lyrics or melody in an additional round after the first round. In season seven, the group round was eliminated and contestants may, after a first solo performance and on judges approval, skip a second solo round and move directly to the final Hollywood round. In season twelve, the executive producers split up the females and males and chose the members to form the groups in the group round. In the first three seasons, the semi-finalists were split into different groups to perform individually in their respective night. In season one, there were three groups of ten, with the top three contestants from each group making the finals. In seasons two and three, there were four groups of eight, and the top two of each selected. These seasons also featured a wildcard round, where contestants who failed to qualify were given another chance. In season one, only one wildcard contestant was chosen by the judges, giving a total of ten finalists. In seasons two and three, each of the three judges championed one contestant with the public advancing a fourth into the finals, making 12 finalists in all. The ""Fan Save"" was introduced in the fourteenth season. During the finals, viewers are given a five-minute window to vote for the contestants in danger of elimination by using their Twitter account to decide which contestant will move on to the next show, starting with the Top 8. In the first major change to the judging panel, a fourth judge, Kara DioGuardi, was introduced. This was also the first season without executive producer Nigel Lythgoe who left to focus on the international versions of his show So You Think You Can Dance. The Hollywood round was moved to the Kodak Theatre for 2009 and was also extended to two weeks. Idol Gives Back was canceled for this season due to the global recession at the time. In the audition rounds, 121 contestants were selected from around 10,000 who attended the auditions. These were cut to 30 for the semifinal, with ten going on to the finals. One semifinalist, Delano Cagnolatti, was disqualified for lying to evade the show's age limit. One of the early favorites, Tamyra Gray, was eliminated at the top four, the first of several such shock eliminations that were to be repeated in later seasons. Christina Christian was hospitalized before the top six result show due to chest pains and palpitations, and she was eliminated while she was in the hospital. Jim Verraros was the first openly gay contestant on the show; his sexual orientation was revealed in his online journal, however it was removed during the competition after a request from the show producers over concerns that it might be unfairly influencing votes. The thirteenth season premiered on January 15, 2014, with Ryan Seacrest returning as host. Randy Jackson and Keith Urban returned, though Jackson moved from the judging panel to the role of in-mentor. Mariah Carey and Nicki Minaj left the panel after one season. Former judge Jennifer Lopez and former mentor Harry Connick, Jr. joined Urban on the panel. Also, Nigel Lythgoe and Ken Warwick were replaced as executive producers by Per Blankens, Jesse Ignjatovic and Evan Pragger. Bill DeRonde replaced Warwick as a director of the audition episodes, while Louis J. Horvitz replaced Gregg Gelfand as a director of the show. The continuing decline influenced further changes for season 14, including the loss of Coca-Cola as the show's major sponsor, and a decision to only broadcast one, two-hour show per week during the top 12 rounds (with results from the previous week integrated into the performance show, rather than having a separate results show). On May 11, 2015, prior to the fourteenth season finale, Fox announced that the fifteenth season of American Idol would be its last. Despite these changes, the show's ratings would decline more sharply. The fourteenth season finale was the lowest-rated finale ever, with an average of only 8.03 million viewers watching the finale. The top 10 contestants started with five males and five females, however, the males were eliminated consecutively in the first five weeks, with Lazaro Arbos the last male to be eliminated. For the first time in the show's history, the top 5 contestants were all female. It was also the first time that the judges' ""save"" was not used, the top four contestants were therefore given an extra week to perform again with their votes carried over with no elimination in the first week. Finalist Phillip Phillips suffered from kidney pain and was taken to the hospital before the Top 13 results show, and later received medical procedure to alleviate a blockage caused by kidney stones. He was reported to have eight surgeries during his Idol run, and had considered quitting the show due to the pain. He underwent surgery to remove the stones and reconstruct his kidney soon after the season had finished. The show had originally planned on having four judges following the Pop Idol format; however, only three judges had been found by the time of the audition round in the first season, namely Randy Jackson, Paula Abdul and Simon Cowell. A fourth judge, radio DJ Stryker, was originally chosen but he dropped out citing ""image concerns"". In the second season, New York radio personality Angie Martinez had been hired as a fourth judge but withdrew only after a few days of auditions due to not being comfortable with giving out criticism. The show decided to continue with the three judges format until season eight. All three original judges stayed on the judging panel for eight seasons. For five consecutive seasons, starting in season seven, the title was given to a white male who plays the guitar – a trend that Idol pundits call the ""White guy with guitar"" or ""WGWG"" factor. Just hours before the season eleven finale, where Phillip Phillips was named the winner, Richard Rushfield, author of the book American Idol: The Untold Story, said, ""You have this alliance between young girls and grandmas and they see it, not necessarily as a contest to create a pop star competing on the contemporary radio, but as .... who's the nicest guy in a popularity contest,"" he says, ""And that has led to this dynasty of four, and possibly now five, consecutive, affable, very nice, good-looking white boys."" The loss of viewers continued into season seven. The premiere was down 11% among total viewers, and the results show in which Kristy Lee Cook was eliminated delivered its lowest-rated Wednesday show among the 18–34 demo since the first season in 2002. However, the ratings rebounded for the season seven finale with the excitement over the battle of the Davids, and improved over season six as the series' third most watched finale. The strong finish of season seven also helped Fox become the most watched TV network in the country for the first time since its inception, a first ever in American television history for a non-Big Three major broadcast network. Overall ratings for the season were down 10% from season six, which is in line with the fall in viewership across all networks due in part to the 2007–2008 Writers Guild of America strike. Season 12 premiered on January 16, 2013. Judges Jennifer Lopez and Steven Tyler left the show after two seasons. This season's judging panel consisted of Randy Jackson, along with Mariah Carey, Keith Urban and Nicki Minaj. This was the first season since season nine to have four judges on the panel. The pre-season buzz and the early episodes of the show were dominated by the feud between the judges Minaj and Carey after a video of their dispute was leaked to TMZ. The top ten (eleven in season ten) toured at the end of every season. In the season twelve tour a semi-finalist who won a sing-off was also added to the tour. Kellogg's Pop-Tarts was the sponsor for the first seven seasons, and Guitar Hero was added for the season seven tour. M&M's Pretzel Chocolate Candies was a sponsor of the season nine tour. The season five tour was the most successful tour with gross of over $35 million. American Idol prominent display of its sponsors' logo and products had been noted since the early seasons. By season six, Idol showed 4,349 product placements according to Nielsen Media Research. The branded entertainment integration proved beneficial to its advertisers – promotion of AT&T text-messaging as a means to vote successfully introduced the technology into the wider culture, and Coca-Cola has seen its equity increased during the show. This season also saw the launch of the American Idol Songwriter contest which allows fans to vote for the ""coronation song"". Thousands of recordings of original songs were submitted by songwriters, and 20 entries selected for the public vote. The winning song, ""This Is My Now"", was performed by both finalists during the finale and released by Sparks on May 24, 2007. Season eight premiered on January 13, 2009. Mike Darnell, the president of alternative programming for Fox, stated that the season would focus more on the contestants' personal life. Much early attention on the show was therefore focused on the widowhood of Danny Gokey.[citation needed] As one of the most successful shows on U.S. television history, American Idol has a strong impact not just on television, but also in the wider world of entertainment. It helped create a number of highly successful recording artists, such as Kelly Clarkson, Daughtry and Carrie Underwood, as well as others of varying notability. Corey Clark was disqualified during the finals for having an undisclosed police record; however, he later alleged that he and Paula Abdul had an affair while on the show and that this contributed to his expulsion. Clark also claimed that Abdul gave him preferential treatment on the show due to their affair. The allegations were dismissed by Fox after an independent investigation. Two semi-finalists were also disqualified that year – Jaered Andrews for an arrest on an assault charge, and Frenchie Davis for having previously modelled for an adult website. In what was to become a tradition, Clarkson performed the coronation song during the finale, and released the song immediately after the season ended. The single, ""A Moment Like This"", went on to break a 38-year-old record held by The Beatles for the biggest leap to number one on the Billboard Hot 100. Guarini did not release any song immediately after the show and remains the only runner-up not to do so. Both Clarkson and Guarini made a musical film, From Justin to Kelly, which was released in 2003 but was widely panned. Clarkson has since become the most successful Idol contestant internationally, with worldwide album sales of more than 23 million. Season six began on Tuesday, January 16, 2007. The premiere drew a massive audience of 37.3 million viewers, peaking in the last half hour with more than 41 million viewers. Individual contestants have generated controversy in this competition for their past actions, or for being 'ringers' planted by the producers. A number of contestants had been disqualified for various reasons, such as for having an existing contract or undisclosed criminal records, although the show had been accused of double standard for disqualifying some but not others. This was the first season where the contestants were permitted to perform in the final rounds songs they wrote themselves. In the Top 8, Sam Woolf received the fewest votes, but he was saved from elimination by the judges. The 500th episode of the series was the Top 3 performance night. Season 11 premiered on January 18, 2012. On February 23, it was announced that one more finalist would join the Top 24 making it the Top 25, and that was Jermaine Jones. However, on March 14, Jones was disqualified in 12th place for concealing arrests and outstanding warrants. Jones denied the accusation that he concealed his arrests. Pia Toscano, one of the presumed favorites to advance far in the season, was unexpectedly eliminated on April 7, 2011, finishing in ninth place. Her elimination drew criticisms from some former Idol contestants, as well as actor Tom Hanks. The impact of American Idol is also strongly felt in musical theatre, where many of Idol alumni have forged successful careers. The striking effect of former American Idol contestants on Broadway has been noted and commented on. The casting of a popular Idol contestant can lead to significantly increased ticket sales. Other alumni have gone on to work in television and films, the most notable being Jennifer Hudson who, on the recommendation of the Idol vocal coach Debra Byrd, won a role in Dreamgirls and subsequently received an Academy Award for her performance. For the finals, American Idol debuted a new state-of-the-art set and stage on March 11, 2008, along with a new on-air look. David Cook's performance of ""Billie Jean"" on top-ten night was lauded by the judges, but provoked controversy when they apparently mistook the Chris Cornell arrangement to be David Cook's own even though the performance was introduced as Cornell's version. Cornell himself said he was 'flattered' and praised David Cook's performance. David Cook was taken to the hospital after the top-nine performance show due to heart palpitations and high blood pressure. In the May 23 season finale, Jordin Sparks was declared the winner with the runner-up being Blake Lewis. Sparks has had some success as a recording artist post-Idol. American Idol employs a panel of judges who critique the contestants' performances. The original judges were record producer and music manager Randy Jackson, pop singer and choreographer Paula Abdul and music executive and manager Simon Cowell. The judging panel for the most recent season consisted of country singer Keith Urban, singer and actress Jennifer Lopez, and jazz singer Harry Connick, Jr. The show was originally hosted by radio personality Ryan Seacrest and comedian Brian Dunkleman, with Seacrest continuing on for the rest of the seasons. The two finalists were Kris Allen and Adam Lambert, both of whom had previously landed in the bottom three at the top five. Allen won the contest in the most controversial voting result since season two. It was claimed, later retracted, that 38 million of the 100 million votes cast on the night came from Allen's home state of Arkansas alone, and that AT&T employees unfairly influenced the votes by giving lessons on power-texting at viewing parties in Arkansas. In seasons ten and eleven, a further round was added in Las Vegas, where the contestants perform in groups based on a theme, followed by one final solo round to determine the semi-finalists. At the end of this stage of the competition, 24 to 36 contestants are selected to move on to the semi-final stage. In season twelve the Las Vegas round became a Sudden Death round, where the judges had to choose five guys and five girls each night (four nights) to make the top twenty. In season thirteen, a new round called ""Hollywood or Home"" was added, where if the judges were uncertain about some contestants, those contestants were required to perform soon after landing in Los Angeles, and those who failed to impress were sent back home before they reached Hollywood. The most popular contestants are usually not revealed in the results show. Instead, typically the three contestants (two in later rounds) who received the lowest number of votes are called to the center of the stage. One of these three is usually sent to safety; however the two remaining are not necessarily the bottom two. The contestant with the fewest votes is then revealed and eliminated from the competition. A montage of the eliminated contestant's time on the show is played and they give their final performance. However, in season six, during the series' first ever Idol Gives Back episode, no contestant was eliminated, but on the following week, two were sent home. Moreover, starting in season eight, the judges may overturn viewers' decision with a ""Judges' Save"" if they unanimously agree to. ""The save"" can only be used once, and only up through the top five. In the eighth, ninth, tenth, and fourteenth seasons, a double elimination then took place in the week following the activation of the save, but in the eleventh and thirteenth seasons, a regular single elimination took place. The save was not activated in the twelfth season and consequently, a non-elimination took place in the week after its expiration with the votes then carrying over into the following week. The top 12 finalists originally included Mario Vazquez, but he dropped out citing 'personal reasons' and was replaced by Nikko Smith. Later, an employee of Freemantle Media, which produces the show, sued the company for wrongful termination, claiming that he was dismissed after complaining about lewd behavior by Vazquez toward him during the show. Idol Gives Back is a special charity event started in season six featuring performances by celebrities and various fund-raising initiatives. This event was also held in seasons seven and nine and has raised nearly $185 million in total. With the exception of seasons one and two, the contestants in the semifinals onwards perform in front of a studio audience. They perform with a full band in the finals. From season four to season nine, the American Idol band was led by Rickey Minor; from season ten onwards, Ray Chew. Assistance may also be given by vocal coaches and song arrangers, such as Michael Orland and Debra Byrd to contestants behind the scene. Starting with season seven, contestants may perform with a musical instrument from the Hollywood rounds onwards. In the first nine seasons, performances were usually aired live on Tuesday nights, followed by the results shows on Wednesdays in the United States and Canada, but moved to Wednesdays and Thursdays in season ten. In May 2005, Carrie Underwood was announced the winner, with Bice the runner-up. Both Underwood and Bice released the coronation song ""Inside Your Heaven"". Underwood has since sold 65 million records worldwide, and become the most successful Idol contestant in the U.S., selling over 14 million albums copies in the U.S. and has more  Underwood has won seven Grammy Awards, the most Grammys by an ""American Idol"" alumnus. Changes this season include only airing one episode a week during the final ten. Coca Cola ended their longtime sponsorship of the show and Ford Motor Company maintained a reduced role. The winner of the season also received a recording contract with Big Machine Records. Each season premieres with the audition round, taking place in different cities. The audition episodes typically feature a mix of potential finalists, interesting characters and woefully inadequate contestants. Each successful contestant receives a golden ticket to proceed on to the next round in Hollywood. Based on their performances during the Hollywood round (Las Vegas round for seasons 10 onwards), 24 to 36 contestants are selected by the judges to participate in the semifinals. From the semifinal onwards the contestants perform their songs live, with the judges making their critiques after each performance. The contestants are voted for by the viewing public, and the outcome of the public votes is then revealed in the results show typically on the following night. The results shows feature group performances by the contestants as well as guest performers. The Top-three results show also features the homecoming events for the Top 3 finalists. The season reaches its climax in a two-hour results finale show, where the winner of the season is revealed. During the top 11 week, due to a mix-up with the contestants' telephone number, voting was repeated on what was normally the result night, with the result reveal postponed until the following night. The final two contestants were Lee DeWyze and Bowersox. DeWyze was declared the winner during the May 26 finale. No new song was used as coronation song this year; instead, the two finalists each released a cover song – DeWyze chose U2's ""Beautiful Day"", and Bowersox chose Patty Griffin's ""Up to the Mountain"". This is the first season where neither finalist achieved significant album sales. The success of the show's alumni however has led to a more positive assessment of the show, and the show was described as having ""proven it has a valid way to pick talent and a proven way to sell records"". While the industry is divided on the show success, its impact is felt particularly strongly in the country music format. According to a CMT exec, reflecting on the success of Idol alumni in the country genre, ""if you want to try and get famous fast by going to a cattle call audition on TV, Idol reasonably remains the first choice for anyone,"" and that country music and Idol ""go together well"". In Latin America, the show is broadcast and subtitled by Sony Entertainment Television. In southeast Asia, it is broadcast by STAR World every Thursday and Friday nine or ten hours after. In Philippines, it is aired every Thursday and Friday nine or ten hours after its United States telecast; from 2002 to 2007 on ABC 5; 2008–11 on QTV, then GMA News TV; and since 2012 on ETC. On Philippine television history. In Australia, it is aired a few hours after the U.S. telecast. It was aired on Network Ten from 2002 to 2007 and then again in 2013, from 2008 to 2012 on Fox8, from season 13 onwards it airs on digital channel, Eleven, a sister channel to Network Ten. In the United Kingdom, episodes are aired one day after the U.S. broadcast on digital channel ITV2. As of season 12, the episodes air on 5*. It is also aired in Ireland on TV3 two days after the telecast. In Brazil and Israel, the show airs two days after its original broadcast. In the instances where the airing is delayed, the shows may sometimes be combined into one episode to summarize the results. In Italy, the twelfth season was broadcast by La3. The declining trend however continued into season eight, as total viewers numbers fell by 5–10% for early episodes compared to season seven, and by 9% for the finale. In season nine, Idol's six-year extended streak of perfection in the ratings was broken, when NBC's coverage of the 2010 Winter Olympics on February 17 beat Idol in the same time slot with 30.1 million viewers over Idol's 18.4 million. Nevertheless, American Idol overall finished its ninth season as the most watched TV series for the sixth year running, breaking the previous record of five consecutive seasons achieved by CBS' All in the Family and NBC's The Cosby Show. Season eleven, however, suffered a steep drop in ratings, a drop attributed by some to the arrival of new shows such as The Voice and The X-Factor. The ratings for the first two episodes of season eleven fell 16–21% in overall viewer numbers and 24–27% in the 18/49 demo, while the season finale fell 27% in total viewer number and 30% in the 18-49 demo. The average viewership for the season fell below 20 million viewers the first time since 2003, a drop of 23% in total viewers and 30% in the 18/49 demo. For the first time in eight years, American Idol lost the leading position in both the total viewers number and the 18/49 demo, coming in second to NBC Sunday Night Football, although the strengths of Idol in its second year in the Wednesday-Thursday primetime slots helped Fox achieve the longest period of 18-49 demographic victory in the Nielsen ratings, standing at 8 straight years from 2004 to 2012. The first season was co-hosted by Ryan Seacrest and Brian Dunkleman. Dunkleman quit thereafter, making Seacrest the sole emcee of the show starting with season two. American Idol premiered in June 2002 and became the surprise summer hit show of 2002. The first show drew 9.9 million viewers, giving Fox the best viewing figure for the 8.30 pm spot in over a year. The audience steadily grew, and by finale night, the audience had averaged 23 million, with more than 40 million watching some part of that show. That episode was placed third amongst all age groups, but more importantly it led in the 18–49 demographic, the age group most valued by advertisers. American Idol is broadcast to over 100 nations outside of the United States. In most nations these are not live broadcasts and may be tape delayed by several days or weeks. In Canada, the first thirteen seasons of American Idol were aired live by CTV and/or CTV Two, in simulcast with Fox. CTV dropped Idol after its thirteenth season and in August 2014, Yes TV announced that it had picked up Canadian rights to American Idol beginning in its 2015 season. A special tribute to Simon Cowell was presented in the finale for his final season with the show. Many figures from the show's past, including Paula Abdul, made an appearance. The show had been criticized in earlier seasons over the onerous contract contestants had to sign that gave excessive control to 19 Entertainment over their future career, and handed a large part of their future earnings to the management. Beginning in the tenth season[citation needed], permanent mentors were brought in during the live shows to help guide the contestants with their song choice and performance. Jimmy Iovine was the mentor in the tenth through twelfth seasons, former judge Randy Jackson was the mentor for the thirteenth season and Scott Borchetta was the mentor for the fourteenth and fifteenth season. The mentors regularly bring in guest mentors to aid them, including Akon, Alicia Keys, Lady Gaga, and current judge Harry Connick, Jr.. Season nine premiered on January 12, 2010. The upheaval at the judging panel continued. Ellen DeGeneres joined as a judge to replace Paula Abdul at the start of Hollywood Week. Various American Idol alumni had success on various record charts around the world; in the U.S. they had achieved 345 number ones on the Billboard charts in its first ten years. According to Fred Bronson, author of books on the Billboard charts, no other entity has ever created as many hit-making artists and best-selling albums and singles. In 2007, American Idol alums accounted for 2.1% of all music sales. Its alumni have a massive impact on radio; in 2007, American Idol had become ""a dominant force in radio"" according to the president of the research company Mediabase which monitors radio stations Rich Meyer. By 2010, four winners each had more than a million radio spins, with Kelly Clarkson leading the field with over four million spins. Towards the end of the season, Randy Jackson, the last remaining of the original judges, announced that he would no longer serve as a judge to pursue other business ventures. Both judges Mariah Carey and Nicki Minaj also decided to leave after one season to focus on their music careers. Guest judges may occasionally be introduced. In season two, guest judges such as Lionel Richie and Robin Gibb were used, and in season three Donna Summer, Quentin Tarantino and some of the mentors also joined as judges to critique the performances in the final rounds. Guest judges were used in the audition rounds for seasons four, six, nine, and fourteen such as Gene Simmons and LL Cool J in season four, Jewel and Olivia Newton-John in season six, Shania Twain in season eight, Neil Patrick Harris, Avril Lavigne and Katy Perry in season nine, and season eight runner-up, Adam Lambert, in season fourteen. Ford Motor Company and Coca-Cola were two of the first sponsors of American Idol in its first season. The sponsorship deal cost around $10 million in season one, rising to $35 million by season 7, and between $50 to $60 million in season 10. The third major sponsor AT&T Wireless joined in the second season but ended after season 12, and Coca-Cola officially ended its sponsorship after season 13 amidst the declining ratings of Idol in the mid-2010s. iTunes sponsored the show since season seven. The show itself is popular in the Southern United States, with households in the Southeastern United States 10% more likely to watch American Idol during the eighth season in 2009, and those in the East Central region, such as Kentucky, were 16 percent more likely to tune into the series. Data from Nielsen SoundScan, a music-sales tracking service, showed that of the 47 million CDs sold by Idol contestants through January 2010, 85 percent were by contestants with ties to the American South. In 2001, Fuller, Cowell, and TV producer Simon Jones attempted to sell the Pop Idol format to the United States, but the idea was met with poor response from United States television networks. However, Rupert Murdoch, head of Fox's parent company, was persuaded to buy the show by his daughter Elisabeth, who was a fan of the British show. The show was renamed American Idol: The Search for a Superstar and debuted in the summer of 2002. Cowell was initially offered the job as showrunner but refused; Lythgoe then took over that position. Much to Cowell's surprise, it became one of the hit shows for the summer that year. The show, with the personal engagement of the viewers with the contestants through voting, and the presence of the acid-tongued Cowell as a judge, grew into a phenomenon. By 2004, it had become the most-watched show in the U.S., a position it then held on for seven consecutive seasons. Season seven premiered on January 15, 2008, for a two-day, four-hour premiere. The media focused on the professional status of the season seven contestants, the so-called 'ringers', many of whom, including Kristy Lee Cook, Brooke White, Michael Johns, and in particular Carly Smithson, had prior recording contracts. Contestant David Hernandez also attracted some attention due to his past employment as a stripper. Theories given for the success of Southerners on Idol have been: more versatility with musical genres, as the Southern U.S. is home to several music genre scenes; not having as many opportunities to break into the pop music business; text-voting due to the South having the highest percentage of cell-phone only households; and the strong heritage of music and singing, which is notable in the Bible Belt, where it is in church that many people get their start in public singing. Others also suggest that the Southern character of these contestants appeal to the South, as well as local pride. According to season five winner Taylor Hicks, who is from the state of Alabama, ""People in the South have a lot of pride ... So, they're adamant about supporting the contestants who do well from their state or region."" Ruben Studdard emerged as the winner, beating Clay Aiken by a small margin. Out of a total of 24 million votes, Studdard finished just 134,000 votes ahead of Aiken. This slim margin of victory was controversial due to the large number of calls that failed to get through. In an interview prior to season five, executive producer Nigel Lythgoe indicated that Aiken had led the fan voting from the wildcard week onward until the finale. Season five began on January 17, 2006. It remains the highest-rated season in the show's run so far. Two of the more prominent contestants during the Hollywood round were the Brittenum twins who were later disqualified for identity theft. Nick Fradiani won the season, defeating Clark Beckham. By winning, Fradiani became the first winner from the Northeast region. Fradiani released ""Beautiful Life"" as his coronation single while Beckham released ""Champion"". Jax, the third place finalist, also released a single called ""Forcefield"". Both Allen and Lambert released the coronation song, ""No Boundaries"" which was co-written by DioGuardi. This is the first season in which the winner failed to achieve gold album status, and none from that season achieved platinum album status in the U.S.[citation needed] Seasonal rankings (based on average total viewers per episode) of American Idol. It holds the distinction of having the longest winning streak in the Nielsen annual television ratings; it became the highest-rated of all television programs in the United States overall for an unprecedented seven consecutive years, or eight consecutive (and total) years when either its performance or result show was ranked number one overall. Season four premiered on January 18, 2005; this was the first season of the series to be aired in high definition, although the finale of season three was also aired in high definition. The number of those attending the auditions by now had increased to over 100,000 from the 10,000 of the first season. The age limit was raised to 28 in this season, and among those who benefited from this new rule were Constantine Maroulis and Bo Bice, the two rockers of the show. The eligible age-range for contestants is currently fifteen to twenty-eight years old. The initial age limit was sixteen to twenty-four in the first three seasons, but the upper limit was raised to twenty-eight in season four, and the lower limit was reduced to fifteen in season ten. The contestants must be legal U.S. residents, cannot have advanced to particular stages of the competition in previous seasons (varies depending on the season, currently by the semi-final stage until season thirteen), and must not hold any current recording or talent representation contract by the semi-final stage (in previous years by the audition stage). Jessica Sanchez received the fewest number of votes during the Top 7 week, and the judges decided to use their ""save"" option on her, making her the first female recipient of the save. The following week, unlike previous seasons, Colton Dixon was the only contestant sent home. Sanchez later made the final two, the first season where a recipient of the save reached the finale. The success of American Idol has been described as ""unparalleled in broadcasting history"". The series was also said by a rival TV executive to be ""the most impactful show in the history of television"". It has become a recognized springboard for launching the career of many artists as bona fide stars. According to Billboard magazine, in its first ten years, ""Idol has spawned 345 Billboard chart-toppers and a platoon of pop idols, including Kelly Clarkson, Carrie Underwood, Chris Daughtry, Fantasia, Ruben Studdard, Jennifer Hudson, Clay Aiken, Adam Lambert and Jordin Sparks while remaining a TV ratings juggernaut."" The finals are broadcast in prime time from CBS Television City in Los Angeles, in front of a live studio audience. The finals lasted eight weeks in season one, eleven weeks in subsequent seasons until seasons ten and eleven which lasted twelve weeks except for season twelve, which lasted ten weeks, and season thirteen, which lasted for thirteen weeks. Each finalist performs songs based on a weekly theme which may be a musical genre such as Motown, disco, or big band, songs by artists such as Michael Jackson, Elvis Presley or The Beatles, or more general themes such as Billboard Number 1 hits or songs from the contestant's year of birth. Contestants usually work with a celebrity mentor related to the theme. In season ten, Jimmy Iovine was brought in as a mentor for the season. Initially the contestants sing one song each week, but this is increased to two songs from top four or five onwards, then three songs for the top two or three. Phillips became the winner, beating Sanchez. Prior to the announcement of the winner, season five finalist Ace Young proposed marriage to season three runner-up Diana DeGarmo on stage – which she accepted. Some of the later writers about the show were more positive, Michael Slezak, again of Entertainment Weekly, thought that ""for all its bloated, synthetic, product-shilling, money-making trappings, Idol provides a once-a-year chance for the average American to combat the evils of today's music business."" Singer Sheryl Crow, who was later to act as a mentor on the show, however took the view that the show ""undermines art in every way and promotes commercialism"". Pop music critic Ann Powers nevertheless suggested that Idol has ""reshaped the American songbook"", ""led us toward a new way of viewing ourselves in relationship to mainstream popular culture"", and connects ""the classic Hollywood dream to the multicentered popular culture of the future."" Others focused on the personalities in the show; Ramin Setoodeh of Newsweek accused judge Simon Cowell's cruel critiques in the show of helping to establish in the wider world a culture of meanness, that ""Simon Cowell has dragged the rest of us in the mud with him."" Some such as singer John Mayer disparaged the contestants, suggesting that those who appeared on Idol are not real artists with self-respect. One of the more prominent contestants this year was Chris Medina, whose story of caring for his brain-damaged fiancée received widespread coverage. Medina was cut in the Top 40 round. Casey Abrams, who suffers from ulcerative colitis, was hospitalized twice and missed the Top 13 result show. The judges used their one save on Abrams on the Top 11, and as a result this was the first season that 11 finalists went on tour instead of 10. In the following week, Naima Adedapo and Thia Megia were both eliminated the following week. The fourteenth season premiered on January 7, 2015. Ryan Seacrest returned to host, while Jennifer Lopez, Keith Urban and Harry Connick, Jr. returned for their respective fourth, third and second seasons as judges. Eighth season runner-up Adam Lambert filled in for Urban during the New York City auditions. Randy Jackson did not return as the in-house mentor for this season. For an unprecedented eight consecutive years, from the 2003–04 television season through the 2010–11 season, either its performance or result show had been ranked number one in U.S. television ratings. The popularity of American Idol however declined, and on May 11, 2015, Fox announced that the series would conclude its run in its fifteenth season. American Idol is an American singing competition series created by Simon Fuller and produced by 19 Entertainment, and is distributed by FremantleMedia North America. It began airing on Fox on June 11, 2002, as an addition to the Idols format based on the British series Pop Idol and has since become one of the most successful shows in the history of American television. The concept of the series is to find new solo recording artists, with the winner being determined by the viewers in America. Winners chosen by viewers through telephone, Internet, and SMS text voting were Kelly Clarkson, Ruben Studdard, Fantasia Barrino, Carrie Underwood, Taylor Hicks, Jordin Sparks, David Cook, Kris Allen, Lee DeWyze, Scotty McCreery, Phillip Phillips, Candice Glover, Caleb Johnson, and Nick Fradiani. Fox announced on May 11, 2015 that the fifteenth season would be the final season of American Idol; as such, the season is expected to have an additional focus on the program's alumni. Ryan Seacrest returns as host, with Harry Connick Jr., Keith Urban, and Jennifer Lopez all returning for their respective third, fourth, and fifth seasons as judges. The wildcard round returned in season eight, wherein there were three groups of twelve, with three contestants moving forward – the highest male, the highest female, and the next highest-placed singer - for each night, and four wildcards were chosen by the judges to produce a final 13. Starting season ten, the girls and boys perform on separate nights. In seasons ten and eleven, five of each gender were chosen, and three wildcards were chosen by the judges to form a final 13. In season twelve, the top twenty semifinalists were split into gender groups, with five of each gender advancing to form the final 10. In season thirteen, there were thirty semifinalists, but only twenty semifinalists (ten for each gender) were chosen by the judges to perform on the live shows, with five in each gender and three wildcards chosen by the judges composing the final 13."
Uranium,"The interactions of carbonate anions with uranium(VI) cause the Pourbaix diagram to change greatly when the medium is changed from water to a carbonate containing solution. While the vast majority of carbonates are insoluble in water (students are often taught that all carbonates other than those of alkali metals are insoluble in water), uranium carbonates are often soluble in water. This is because a U(VI) cation is able to bind two terminal oxides and three or more carbonates to form anionic complexes. The use of uranium in its natural oxide form dates back to at least the year 79 CE, when it was used to add a yellow color to ceramic glazes. Yellow glass with 1% uranium oxide was found in a Roman villa on Cape Posillipo in the Bay of Naples, Italy, by R. T. Gunther of the University of Oxford in 1912. Starting in the late Middle Ages, pitchblende was extracted from the Habsburg silver mines in Joachimsthal, Bohemia (now Jáchymov in the Czech Republic), and was used as a coloring agent in the local glassmaking industry. In the early 19th century, the world's only known sources of uranium ore were these mines. On 2 December 1942, as part of the Manhattan Project, another team led by Enrico Fermi was able to initiate the first artificial self-sustained nuclear chain reaction, Chicago Pile-1. Working in a lab below the stands of Stagg Field at the University of Chicago, the team created the conditions needed for such a reaction by piling together 400 short tons (360 metric tons) of graphite, 58 short tons (53 metric tons) of uranium oxide, and six short tons (5.5 metric tons) of uranium metal, a majority of which was supplied by Westinghouse Lamp Plant in a makeshift production process. Uranium ore is mined in several ways: by open pit, underground, in-situ leaching, and borehole mining (see uranium mining). Low-grade uranium ore mined typically contains 0.01 to 0.25% uranium oxides. Extensive measures must be employed to extract the metal from its ore. High-grade ores found in Athabasca Basin deposits in Saskatchewan, Canada can contain up to 23% uranium oxides on average. Uranium ore is crushed and rendered into a fine powder and then leached with either an acid or alkali. The leachate is subjected to one of several sequences of precipitation, solvent extraction, and ion exchange. The resulting mixture, called yellowcake, contains at least 75% uranium oxides U3O8. Yellowcake is then calcined to remove impurities from the milling process before refining and conversion. A team led by Enrico Fermi in 1934 observed that bombarding uranium with neutrons produces the emission of beta rays (electrons or positrons from the elements produced; see beta particle). The fission products were at first mistaken for new elements of atomic numbers 93 and 94, which the Dean of the Faculty of Rome, Orso Mario Corbino, christened ausonium and hesperium, respectively. The experiments leading to the discovery of uranium's ability to fission (break apart) into lighter elements and release binding energy were conducted by Otto Hahn and Fritz Strassmann in Hahn's laboratory in Berlin. Lise Meitner and her nephew, the physicist Otto Robert Frisch, published the physical explanation in February 1939 and named the process ""nuclear fission"". Soon after, Fermi hypothesized that the fission of uranium might release enough neutrons to sustain a fission reaction. Confirmation of this hypothesis came in 1939, and later work found that on average about 2.5 neutrons are released by each fission of the rare uranium isotope uranium-235. Further work found that the far more common uranium-238 isotope can be transmuted into plutonium, which, like uranium-235, is also fissile by thermal neutrons. These discoveries led numerous countries to begin working on the development of nuclear weapons and nuclear power. During the Cold War between the Soviet Union and the United States, huge stockpiles of uranium were amassed and tens of thousands of nuclear weapons were created using enriched uranium and plutonium made from uranium. Since the break-up of the Soviet Union in 1991, an estimated 600 short tons (540 metric tons) of highly enriched weapons grade uranium (enough to make 40,000 nuclear warheads) have been stored in often inadequately guarded facilities in the Russian Federation and several other former Soviet states. Police in Asia, Europe, and South America on at least 16 occasions from 1993 to 2005 have intercepted shipments of smuggled bomb-grade uranium or plutonium, most of which was from ex-Soviet sources. From 1993 to 2005 the Material Protection, Control, and Accounting Program, operated by the federal government of the United States, spent approximately US $550 million to help safeguard uranium and plutonium stockpiles in Russia. This money was used for improvements and security enhancements at research and storage facilities. Scientific American reported in February 2006 that in some of the facilities security consisted of chain link fences which were in severe states of disrepair. According to an interview from the article, one facility had been storing samples of enriched (weapons grade) uranium in a broom closet before the improvement project; another had been keeping track of its stock of nuclear warheads using index cards kept in a shoe box. To be considered 'enriched', the uranium-235 fraction should be between 3% and 5%. This process produces huge quantities of uranium that is depleted of uranium-235 and with a correspondingly increased fraction of uranium-238, called depleted uranium or 'DU'. To be considered 'depleted', the uranium-235 isotope concentration should be no more than 0.3%. The price of uranium has risen since 2001, so enrichment tailings containing more than 0.35% uranium-235 are being considered for re-enrichment, driving the price of depleted uranium hexafluoride above $130 per kilogram in July 2007 from $5 in 2001. Calcined uranium yellowcake, as produced in many large mills, contains a distribution of uranium oxidation species in various forms ranging from most oxidized to least oxidized. Particles with short residence times in a calciner will generally be less oxidized than those with long retention times or particles recovered in the stack scrubber. Uranium content is usually referenced to U
3O
8, which dates to the days of the Manhattan project when U
3O
8 was used as an analytical chemistry reporting standard. Many contemporary uses of uranium exploit its unique nuclear properties. Uranium-235 has the distinction of being the only naturally occurring fissile isotope. Uranium-238 is fissionable by fast neutrons, and is fertile, meaning it can be transmuted to fissile plutonium-239 in a nuclear reactor. Another fissile isotope, uranium-233, can be produced from natural thorium and is also important in nuclear technology. While uranium-238 has a small probability for spontaneous fission or even induced fission with fast neutrons, uranium-235 and to a lesser degree uranium-233 have a much higher fission cross-section for slow neutrons. In sufficient concentration, these isotopes maintain a sustained nuclear chain reaction. This generates the heat in nuclear power reactors, and produces the fissile material for nuclear weapons. Depleted uranium (238U) is used in kinetic energy penetrators and armor plating. Uranium-235 was the first isotope that was found to be fissile. Other naturally occurring isotopes are fissionable, but not fissile. On bombardment with slow neutrons, its uranium-235 isotope will most of the time divide into two smaller nuclei, releasing nuclear binding energy and more neutrons. If too many of these neutrons are absorbed by other uranium-235 nuclei, a nuclear chain reaction occurs that results in a burst of heat or (in special circumstances) an explosion. In a nuclear reactor, such a chain reaction is slowed and controlled by a neutron poison, absorbing some of the free neutrons. Such neutron absorbent materials are often part of reactor control rods (see nuclear reactor physics for a description of this process of reactor control). Uranium was also used in photographic chemicals (especially uranium nitrate as a toner), in lamp filaments for stage lighting bulbs, to improve the appearance of dentures, and in the leather and wood industries for stains and dyes. Uranium salts are mordants of silk or wool. Uranyl acetate and uranyl formate are used as electron-dense ""stains"" in transmission electron microscopy, to increase the contrast of biological specimens in ultrathin sections and in negative staining of viruses, isolated cell organelles and macromolecules. Natural uranium consists of three major isotopes: uranium-238 (99.28% natural abundance), uranium-235 (0.71%), and uranium-234 (0.0054%). All three are radioactive, emitting alpha particles, with the exception that all three of these isotopes have small probabilities of undergoing spontaneous fission, rather than alpha emission. There are also five other trace isotopes: uranium-239, which is formed when 238U undergoes spontaneous fission, releasing neutrons that are captured by another 238U atom; uranium-237, which is formed when 238U captures a neutron but emits two more, which then decays to neptunium-237; uranium-233, which is formed in the decay chain of that neptunium-237; and finally, uranium-236 and -240, which appear in the decay chain of primordial plutonium-244. It is also expected that thorium-232 should be able to undergo double beta decay, which would produce uranium-232, but this has not yet been observed experimentally. In 2005, seventeen countries produced concentrated uranium oxides, with Canada (27.9% of world production) and Australia (22.8%) being the largest producers and Kazakhstan (10.5%), Russia (8.0%), Namibia (7.5%), Niger (7.4%), Uzbekistan (5.5%), the United States (2.5%), Argentina (2.1%), Ukraine (1.9%) and China (1.7%) also producing significant amounts. Kazakhstan continues to increase production and may have become the world's largest producer of uranium by 2009 with an expected production of 12,826 tonnes, compared to Canada with 11,100 t and Australia with 9,430 t. In the late 1960s, UN geologists also discovered major uranium deposits and other rare mineral reserves in Somalia. The find was the largest of its kind, with industry experts estimating the deposits at over 25% of the world's then known uranium reserves of 800,000 tons. Uranium is a naturally occurring element that can be found in low levels within all rock, soil, and water. Uranium is the 51st element in order of abundance in the Earth's crust. Uranium is also the highest-numbered element to be found naturally in significant quantities on Earth and is almost always found combined with other elements. Along with all elements having atomic weights higher than that of iron, it is only naturally formed in supernovae. The decay of uranium, thorium, and potassium-40 in the Earth's mantle is thought to be the main source of heat that keeps the outer core liquid and drives mantle convection, which in turn drives plate tectonics. Uranium carbides and uranium nitrides are both relatively inert semimetallic compounds that are minimally soluble in acids, react with water, and can ignite in air to form U
3O
8. Carbides of uranium include uranium monocarbide (UC), uranium dicarbide (UC
2), and diuranium tricarbide (U
2C
3). Both UC and UC
2 are formed by adding carbon to molten uranium or by exposing the metal to carbon monoxide at high temperatures. Stable below 1800 °C, U
2C
3 is prepared by subjecting a heated mixture of UC and UC
2 to mechanical stress. Uranium nitrides obtained by direct exposure of the metal to nitrogen include uranium mononitride (UN), uranium dinitride (UN
2), and diuranium trinitride (U
2N
3). Uranium metal reacts with almost all non-metal elements (with an exception of the noble gases) and their compounds, with reactivity increasing with temperature. Hydrochloric and nitric acids dissolve uranium, but non-oxidizing acids other than hydrochloric acid attack the element very slowly. When finely divided, it can react with cold water; in air, uranium metal becomes coated with a dark layer of uranium oxide. Uranium in ores is extracted chemically and converted into uranium dioxide or other chemical forms usable in industry. Salts of many oxidation states of uranium are water-soluble and may be studied in aqueous solutions. The most common ionic forms are U3+ (brown-red), U4+ (green), UO+
2 (unstable), and UO2+
2 (yellow), for U(III), U(IV), U(V), and U(VI), respectively. A few solid and semi-metallic compounds such as UO and US exist for the formal oxidation state uranium(II), but no simple ions are known to exist in solution for that state. Ions of U3+ liberate hydrogen from water and are therefore considered to be highly unstable. The UO2+
2 ion represents the uranium(VI) state and is known to form compounds such as uranyl carbonate, uranyl chloride and uranyl sulfate. UO2+
2 also forms complexes with various organic chelating agents, the most commonly encountered of which is uranyl acetate. In 1972, the French physicist Francis Perrin discovered fifteen ancient and no longer active natural nuclear fission reactors in three separate ore deposits at the Oklo mine in Gabon, West Africa, collectively known as the Oklo Fossil Reactors. The ore deposit is 1.7 billion years old; then, uranium-235 constituted about 3% of the total uranium on Earth. This is high enough to permit a sustained nuclear fission chain reaction to occur, provided other supporting conditions exist. The capacity of the surrounding sediment to contain the nuclear waste products has been cited by the U.S. federal government as supporting evidence for the feasibility to store spent nuclear fuel at the Yucca Mountain nuclear waste repository. Some organisms, such as the lichen Trapelia involuta or microorganisms such as the bacterium Citrobacter, can absorb concentrations of uranium that are up to 300 times the level of their environment. Citrobacter species absorb uranyl ions when given glycerol phosphate (or other similar organic phosphates). After one day, one gram of bacteria can encrust themselves with nine grams of uranyl phosphate crystals; this creates the possibility that these organisms could be used in bioremediation to decontaminate uranium-polluted water. The proteobacterium Geobacter has also been shown to bioremediate uranium in ground water. The mycorrhizal fungus Glomus intraradices increases uranium content in the roots of its symbiotic plant. The most common forms of uranium oxide are triuranium octoxide (U
3O
8) and UO
2. Both oxide forms are solids that have low solubility in water and are relatively stable over a wide range of environmental conditions. Triuranium octoxide is (depending on conditions) the most stable compound of uranium and is the form most commonly found in nature. Uranium dioxide is the form in which uranium is most commonly used as a nuclear reactor fuel. At ambient temperatures, UO
2 will gradually convert to U
3O
8. Because of their stability, uranium oxides are generally considered the preferred chemical form for storage or disposal. Normal functioning of the kidney, brain, liver, heart, and other systems can be affected by uranium exposure, because, besides being weakly radioactive, uranium is a toxic metal. Uranium is also a reproductive toxicant. Radiological effects are generally local because alpha radiation, the primary form of 238U decay, has a very short range, and will not penetrate skin. Uranyl (UO2+
2) ions, such as from uranium trioxide or uranyl nitrate and other hexavalent uranium compounds, have been shown to cause birth defects and immune system damage in laboratory animals. While the CDC has published one study that no human cancer has been seen as a result of exposure to natural or depleted uranium, exposure to uranium and its decay products, especially radon, are widely known and significant health threats. Exposure to strontium-90, iodine-131, and other fission products is unrelated to uranium exposure, but may result from medical procedures or exposure to spent reactor fuel or fallout from nuclear weapons. Although accidental inhalation exposure to a high concentration of uranium hexafluoride has resulted in human fatalities, those deaths were associated with the generation of highly toxic hydrofluoric acid and uranyl fluoride rather than with uranium itself. Finely divided uranium metal presents a fire hazard because uranium is pyrophoric; small grains will ignite spontaneously in air at room temperature. Two major types of atomic bombs were developed by the United States during World War II: a uranium-based device (codenamed ""Little Boy"") whose fissile material was highly enriched uranium, and a plutonium-based device (see Trinity test and ""Fat Man"") whose plutonium was derived from uranium-238. The uranium-based Little Boy device became the first nuclear weapon used in war when it was detonated over the Japanese city of Hiroshima on 6 August 1945. Exploding with a yield equivalent to 12,500 tonnes of TNT, the blast and thermal wave of the bomb destroyed nearly 50,000 buildings and killed approximately 75,000 people (see Atomic bombings of Hiroshima and Nagasaki). Initially it was believed that uranium was relatively rare, and that nuclear proliferation could be avoided by simply buying up all known uranium stocks, but within a decade large deposits of it were discovered in many places around the world. Most ingested uranium is excreted during digestion. Only 0.5% is absorbed when insoluble forms of uranium, such as its oxide, are ingested, whereas absorption of the more soluble uranyl ion can be up to 5%. However, soluble uranium compounds tend to quickly pass through the body, whereas insoluble uranium compounds, especially when inhaled by way of dust into the lungs, pose a more serious exposure hazard. After entering the bloodstream, the absorbed uranium tends to bioaccumulate and stay for many years in bone tissue because of uranium's affinity for phosphates. Uranium is not absorbed through the skin, and alpha particles released by uranium cannot penetrate the skin. A person can be exposed to uranium (or its radioactive daughters, such as radon) by inhaling dust in air or by ingesting contaminated water and food. The amount of uranium in air is usually very small; however, people who work in factories that process phosphate fertilizers, live near government facilities that made or tested nuclear weapons, live or work near a modern battlefield where depleted uranium weapons have been used, or live or work near a coal-fired power plant, facilities that mine or process uranium ore, or enrich uranium for reactor fuel, may have increased exposure to uranium. Houses or structures that are over uranium deposits (either natural or man-made slag deposits) may have an increased incidence of exposure to radon gas. The Occupational Safety and Health Administration (OSHA) has set the permissible exposure limit for uranium exposure in the workplace as 0.25 mg/m3 over an 8-hour workday. The National Institute for Occupational Safety and Health (NIOSH) has set a recommended exposure limit (REL) of 0.2 mg/m3 over an 8-hour workday and a short-term limit of 0.6 mg/m3. At levels of 10 mg/m3, uranium is immediately dangerous to life and health. The discovery and isolation of radium in uranium ore (pitchblende) by Marie Curie sparked the development of uranium mining to extract the radium, which was used to make glow-in-the-dark paints for clock and aircraft dials. This left a prodigious quantity of uranium as a waste product, since it takes three tonnes of uranium to extract one gram of radium. This waste product was diverted to the glazing industry, making uranium glazes very inexpensive and abundant. Besides the pottery glazes, uranium tile glazes accounted for the bulk of the use, including common bathroom and kitchen tiles which can be produced in green, yellow, mauve, black, blue, red and other colors. Uranium is more plentiful than antimony, tin, cadmium, mercury, or silver, and it is about as abundant as arsenic or molybdenum. Uranium is found in hundreds of minerals, including uraninite (the most common uranium ore), carnotite, autunite, uranophane, torbernite, and coffinite. Significant concentrations of uranium occur in some substances such as phosphate rock deposits, and minerals such as lignite, and monazite sands in uranium-rich ores (it is recovered commercially from sources with as little as 0.1% uranium). The major application of uranium in the military sector is in high-density penetrators. This ammunition consists of depleted uranium (DU) alloyed with 1–2% other elements, such as titanium or molybdenum. At high impact speed, the density, hardness, and pyrophoricity of the projectile enable the destruction of heavily armored targets. Tank armor and other removable vehicle armor can also be hardened with depleted uranium plates. The use of depleted uranium became politically and environmentally contentious after the use of such munitions by the US, UK and other countries during wars in the Persian Gulf and the Balkans raised questions concerning uranium compounds left in the soil (see Gulf War Syndrome). Depleted uranium is also used as a shielding material in some containers used to store and transport radioactive materials. While the metal itself is radioactive, its high density makes it more effective than lead in halting radiation from strong sources such as radium. Other uses of depleted uranium include counterweights for aircraft control surfaces, as ballast for missile re-entry vehicles and as a shielding material. Due to its high density, this material is found in inertial guidance systems and in gyroscopic compasses. Depleted uranium is preferred over similarly dense metals due to its ability to be easily machined and cast as well as its relatively low cost. The main risk of exposure to depleted uranium is chemical poisoning by uranium oxide rather than radioactivity (uranium being only a weak alpha emitter). Uranium is a chemical element with symbol U and atomic number 92. It is a silvery-white metal in the actinide series of the periodic table. A uranium atom has 92 protons and 92 electrons, of which 6 are valence electrons. Uranium is weakly radioactive because all its isotopes are unstable (with half-lives of the six naturally known isotopes, uranium-233 to uranium-238, varying between 69 years and 4.5 billion years). The most common isotopes of uranium are uranium-238 (which has 146 neutrons and accounts for almost 99.3% of the uranium found in nature) and uranium-235 (which has 143 neutrons, accounting for 0.7% of the element found naturally). Uranium has the second highest atomic weight of the primordially occurring elements, lighter only than plutonium. Its density is about 70% higher than that of lead, but slightly lower than that of gold or tungsten. It occurs naturally in low concentrations of a few parts per million in soil, rock and water, and is commercially extracted from uranium-bearing minerals such as uraninite. The discovery of the element is credited to the German chemist Martin Heinrich Klaproth. While he was working in his experimental laboratory in Berlin in 1789, Klaproth was able to precipitate a yellow compound (likely sodium diuranate) by dissolving pitchblende in nitric acid and neutralizing the solution with sodium hydroxide. Klaproth assumed the yellow substance was the oxide of a yet-undiscovered element and heated it with charcoal to obtain a black powder, which he thought was the newly discovered metal itself (in fact, that powder was an oxide of uranium). He named the newly discovered element after the planet Uranus, (named after the primordial Greek god of the sky), which had been discovered eight years earlier by William Herschel. Uranium is used as a colorant in uranium glass producing orange-red to lemon yellow hues. It was also used for tinting and shading in early photography. The 1789 discovery of uranium in the mineral pitchblende is credited to Martin Heinrich Klaproth, who named the new element after the planet Uranus. Eugène-Melchior Péligot was the first person to isolate the metal and its radioactive properties were discovered in 1896 by Henri Becquerel. Research by Otto Hahn, Lise Meitner, Enrico Fermi and others, such as J. Robert Oppenheimer starting in 1934 led to its use as a fuel in the nuclear power industry and in Little Boy, the first nuclear weapon used in war. An ensuing arms race during the Cold War between the United States and the Soviet Union produced tens of thousands of nuclear weapons that used uranium metal and uranium-derived plutonium-239. The security of those weapons and their fissile material following the breakup of the Soviet Union in 1991 is an ongoing concern for public health and safety. See Nuclear proliferation. Uranium metal heated to 250 to 300 °C (482 to 572 °F) reacts with hydrogen to form uranium hydride. Even higher temperatures will reversibly remove the hydrogen. This property makes uranium hydrides convenient starting materials to create reactive uranium powder along with various uranium carbide, nitride, and halide compounds. Two crystal modifications of uranium hydride exist: an α form that is obtained at low temperatures and a β form that is created when the formation temperature is above 250 °C. An additional 4.6 billion tonnes of uranium are estimated to be in sea water (Japanese scientists in the 1980s showed that extraction of uranium from sea water using ion exchangers was technically feasible). There have been experiments to extract uranium from sea water, but the yield has been low due to the carbonate present in the water. In 2012, ORNL researchers announced the successful development of a new absorbent material dubbed HiCap which performs surface retention of solid or gas molecules, atoms or ions and also effectively removes toxic metals from water, according to results verified by researchers at Pacific Northwest National Laboratory. In nature, uranium is found as uranium-238 (99.2742%) and uranium-235 (0.7204%). Isotope separation concentrates (enriches) the fissionable uranium-235 for nuclear weapons and most nuclear power plants, except for gas cooled reactors and pressurised heavy water reactors. Most neutrons released by a fissioning atom of uranium-235 must impact other uranium-235 atoms to sustain the nuclear chain reaction. The concentration and amount of uranium-235 needed to achieve this is called a 'critical mass'. The gas centrifuge process, where gaseous uranium hexafluoride (UF
6) is separated by the difference in molecular weight between 235UF6 and 238UF6 using high-speed centrifuges, is the cheapest and leading enrichment process. The gaseous diffusion process had been the leading method for enrichment and was used in the Manhattan Project. In this process, uranium hexafluoride is repeatedly diffused through a silver-zinc membrane, and the different isotopes of uranium are separated by diffusion rate (since uranium 238 is heavier it diffuses slightly slower than uranium-235). The molecular laser isotope separation method employs a laser beam of precise energy to sever the bond between uranium-235 and fluorine. This leaves uranium-238 bonded to fluorine and allows uranium-235 metal to precipitate from the solution. An alternative laser method of enrichment is known as atomic vapor laser isotope separation (AVLIS) and employs visible tunable lasers such as dye lasers. Another method used is liquid thermal diffusion. It is estimated that 5.5 million tonnes of uranium exists in ore reserves that are economically viable at US$59 per lb of uranium, while 35 million tonnes are classed as mineral resources (reasonable prospects for eventual economic extraction). Prices went from about $10/lb in May 2003 to $138/lb in July 2007. This has caused a big increase in spending on exploration, with US$200 million being spent worldwide in 2005, a 54% increase on the previous year. This trend continued through 2006, when expenditure on exploration rocketed to over $774 million, an increase of over 250% compared to 2004. The OECD Nuclear Energy Agency said exploration figures for 2007 would likely match those for 2006. In nature, uranium(VI) forms highly soluble carbonate complexes at alkaline pH. This leads to an increase in mobility and availability of uranium to groundwater and soil from nuclear wastes which leads to health hazards. However, it is difficult to precipitate uranium as phosphate in the presence of excess carbonate at alkaline pH. A Sphingomonas sp. strain BSAR-1 has been found to express a high activity alkaline phosphatase (PhoK) that has been applied for bioprecipitation of uranium as uranyl phosphate species from alkaline solutions. The precipitation ability was enhanced by overexpressing PhoK protein in E. coli. Uranium-238 is the most stable isotope of uranium, with a half-life of about 4.468×109 years, roughly the age of the Earth. Uranium-235 has a half-life of about 7.13×108 years, and uranium-234 has a half-life of about 2.48×105 years. For natural uranium, about 49% of its alpha rays are emitted by each of 238U atom, and also 49% by 234U (since the latter is formed from the former) and about 2.0% of them by the 235U. When the Earth was young, probably about one-fifth of its uranium was uranium-235, but the percentage of 234U was probably much lower than this. During the later stages of World War II, the entire Cold War, and to a lesser extent afterwards, uranium-235 has been used as the fissile explosive material to produce nuclear weapons. Initially, two major types of fission bombs were built: a relatively simple device that uses uranium-235 and a more complicated mechanism that uses plutonium-239 derived from uranium-238. Later, a much more complicated and far more powerful type of fission/fusion bomb (thermonuclear weapon) was built, that uses a plutonium-based device to cause a mixture of tritium and deuterium to undergo nuclear fusion. Such bombs are jacketed in a non-fissile (unenriched) uranium case, and they derive more than half their power from the fission of this material by fast neutrons from the nuclear fusion process. The X-10 Graphite Reactor at Oak Ridge National Laboratory (ORNL) in Oak Ridge, Tennessee, formerly known as the Clinton Pile and X-10 Pile, was the world's second artificial nuclear reactor (after Enrico Fermi's Chicago Pile) and was the first reactor designed and built for continuous operation. Argonne National Laboratory's Experimental Breeder Reactor I, located at the Atomic Energy Commission's National Reactor Testing Station near Arco, Idaho, became the first nuclear reactor to create electricity on 20 December 1951. Initially, four 150-watt light bulbs were lit by the reactor, but improvements eventually enabled it to power the whole facility (later, the town of Arco became the first in the world to have all its electricity come from nuclear power generated by BORAX-III, another reactor designed and operated by Argonne National Laboratory). The world's first commercial scale nuclear power station, Obninsk in the Soviet Union, began generation with its reactor AM-1 on 27 June 1954. Other early nuclear power plants were Calder Hall in England, which began generation on 17 October 1956, and the Shippingport Atomic Power Station in Pennsylvania, which began on 26 May 1958. Nuclear power was used for the first time for propulsion by a submarine, the USS Nautilus, in 1954. Uranium's average concentration in the Earth's crust is (depending on the reference) 2 to 4 parts per million, or about 40 times as abundant as silver. The Earth's crust from the surface to 25 km (15 mi) down is calculated to contain 1017 kg (2×1017 lb) of uranium while the oceans may contain 1013 kg (2×1013 lb). The concentration of uranium in soil ranges from 0.7 to 11 parts per million (up to 15 parts per million in farmland soil due to use of phosphate fertilizers), and its concentration in sea water is 3 parts per billion."
Ottoman_Empire,"In the century after the death of Osman I, Ottoman rule began to extend over the Eastern Mediterranean and the Balkans. Osman's son, Orhan, captured the northwestern Anatolian city of Bursa in 1324, and made it the new capital of the Ottoman state. This Ottoman conquest meant the loss of Byzantine control over northwestern Anatolia. The important city of Thessaloniki was captured from the Venetians in 1387. The Ottoman victory at Kosovo in 1389 effectively marked the end of Serbian power in the region, paving the way for Ottoman expansion into Europe. The Battle of Nicopolis in 1396, widely regarded as the last large-scale crusade of the Middle Ages, failed to stop the advance of the victorious Ottoman Turks. The art of carpet weaving was particularly significant in the Ottoman Empire, carpets having an immense importance both as decorative furnishings, rich in religious and other symbolism, and as a practical consideration, as it was customary to remove one's shoes in living quarters. The weaving of such carpets originated in the nomadic cultures of central Asia (carpets being an easily transportable form of furnishing), and was eventually spread to the settled societies of Anatolia. Turks used carpets, rugs and kilims not just on the floors of a room, but also as a hanging on walls and doorways, where they provided additional insulation. They were also commonly donated to mosques, which often amassed large collections of them. The Ottoman Navy vastly contributed to the expansion of the Empire's territories on the European continent. It initiated the conquest of North Africa, with the addition of Algeria and Egypt to the Ottoman Empire in 1517. Starting with the loss of Greece in 1821 and Algeria in 1830, Ottoman naval power and control over the Empire's distant overseas territories began to decline. Sultan Abdülaziz (reigned 1861–1876) attempted to reestablish a strong Ottoman navy, building the largest fleet after those of Britain and France. The shipyard at Barrow, England, built its first submarine in 1886 for the Ottoman Empire. The Ottoman Empire (/ˈɒtəmən/; Ottoman Turkish: دَوْلَتِ عَلِيّهٔ عُثمَانِیّه‎ Devlet-i Aliyye-i Osmâniyye, Modern Turkish: Osmanlı İmparatorluğu or Osmanlı Devleti), also known as the Turkish Empire, Ottoman Turkey or Turkey, was an empire founded in 1299 by Oghuz Turks under Osman I in northwestern Anatolia. After conquests in the Balkans by Murad I between 1362 and 1389, the Ottoman sultanate was transformed into a transcontinental empire and claimant to the caliphate. The Ottomans ended the Byzantine Empire with the 1453 conquest of Constantinople by Mehmed the Conqueror. However, the collapsing Ottoman economy could not sustain the fleet's strength for too long. Sultan Abdülhamid II distrusted the admirals who sided with the reformist Midhat Pasha, and claimed that the large and expensive fleet was of no use against the Russians during the Russo-Turkish War. He locked most of the fleet inside the Golden Horn, where the ships decayed for the next 30 years. Following the Young Turk Revolution in 1908, the Committee of Union and Progress sought to develop a strong Ottoman naval force. The Ottoman Navy Foundation was established in 1910 to buy new ships through public donations. The history of the Ottoman Empire during World War I began with the Ottoman engagement in the Middle Eastern theatre. There were several important Ottoman victories in the early years of the war, such as the Battle of Gallipoli and the Siege of Kut. The Arab Revolt which began in 1916 turned the tide against the Ottomans on the Middle Eastern front, where they initially seemed to have the upper hand during the first two years of the war. The Armistice of Mudros was signed on 30 October 1918, and set the partition of the Ottoman Empire under the terms of the Treaty of Sèvres. This treaty, as designed in the conference of London, allowed the Sultan to retain his position and title. The occupation of Constantinople and İzmir led to the establishment of a Turkish national movement, which won the Turkish War of Independence (1919–22) under the leadership of Mustafa Kemal (later given the surname ""Atatürk""). The sultanate was abolished on 1 November 1922, and the last sultan, Mehmed VI (reigned 1918–22), left the country on 17 November 1922. The caliphate was abolished on 3 March 1924. Ottoman classical music was an important part of the education of the Ottoman elite, a number of the Ottoman sultans were accomplished musicians and composers themselves, such as Selim III, whose compositions are often still performed today. Ottoman classical music arose largely from a confluence of Byzantine music, Armenian music, Arabic music, and Persian music. Compositionally, it is organised around rhythmic units called usul, which are somewhat similar to meter in Western music, and melodic units called makam, which bear some resemblance to Western musical modes. Until the second half of the 15th century the empire had a Christian majority, under the rule of a Muslim minority. In the late 19th century, the non-Muslim population of the empire began to fall considerably, not only due to secession, but also because of migratory movements. The proportion of Muslims amounted to 60% in the 1820s, gradually increasing to 69% in the 1870s and then to 76% in the 1890s. By 1914, only 19.1% of the empire's population was non-Muslim, mostly made up of Christian Greeks, Assyrians, Armenians, and Jews. The instruments used are a mixture of Anatolian and Central Asian instruments (the saz, the bağlama, the kemence), other Middle Eastern instruments (the ud, the tanbur, the kanun, the ney), and—later in the tradition—Western instruments (the violin and the piano). Because of a geographic and cultural divide between the capital and other areas, two broadly distinct styles of music arose in the Ottoman Empire: Ottoman classical music, and folk music. In the provinces, several different kinds of folk music were created. The most dominant regions with their distinguished musical styles are: Balkan-Thracian Türküs, North-Eastern (Laz) Türküs, Aegean Türküs, Central Anatolian Türküs, Eastern Anatolian Türküs, and Caucasian Türküs. Some of the distinctive styles were: Janissary Music, Roma music, Belly dance, Turkish folk music. Ertuğrul, the father of Osman I (founder of the Ottoman Empire), arrived in Anatolia from Merv (Turkmenistan) with 400 horsemen to aid the Seljuks of Rum against the Byzantines. After the demise of the Turkish Seljuk Sultanate of Rum in the 14th century, Anatolia was divided into a patchwork of independent, mostly Turkish states, the so-called Ghazi emirates. One of the emirates was led by Osman I (1258–1326), from whom the name Ottoman is derived. Osman I extended the frontiers of Turkish settlement toward the edge of the Byzantine Empire. It is not well understood how the early Ottomans came to dominate their neighbours, as the history of medieval Anatolia is still little known. The son of Murad II, Mehmed the Conqueror, reorganized the state and the military, and conquered Constantinople on 29 May 1453. Mehmed allowed the Orthodox Church to maintain its autonomy and land in exchange for accepting Ottoman authority. Because of bad relations between the states of western Europe and the later Byzantine Empire, the majority of the Orthodox population accepted Ottoman rule as preferable to Venetian rule. Albanian resistance was a major obstacle to Ottoman expansion on the Italian peninsula. The rise of port cities saw the clustering of populations caused by the development of steamships and railroads. Urbanization increased from 1700 to 1922, with towns and cities growing. Improvements in health and sanitation made them more attractive to live and work in. Port cities like Salonica, in Greece, saw its population rise from 55,000 in 1800 to 160,000 in 1912 and İzmir which had a population of 150,000 in 1800 grew to 300,000 by 1914. Some regions conversely had population falls – Belgrade saw its population drop from 25,000 to 8,000 mainly due to political strife. Economic and political migrations made an impact across the empire. For example, the Russian and Austria-Habsburg annexation of the Crimean and Balkan regions respectively saw large influxes of Muslim refugees – 200,000 Crimean Tartars fleeing to Dobruja. Between 1783 and 1913, approximately 5–7 million refugees flooded into the Ottoman Empire, at least 3.8 million of whom were from Russia. Some migrations left indelible marks such as political tension between parts of the empire (e.g. Turkey and Bulgaria) whereas centrifugal effects were noticed in other territories, simpler demographics emerging from diverse populations. Economies were also impacted with the loss of artisans, merchants, manufacturers and agriculturists. Since the 19th century, a large proportion of Muslim peoples from the Balkans emigrated to present-day Turkey. These people are called Muhacir. By the time the Ottoman Empire came to an end in 1922, half of the urban population of Turkey was descended from Muslim refugees from Russia. By developing commercial centres and routes, encouraging people to extend the area of cultivated land in the country and international trade through its dominions, the state performed basic economic functions in the Empire. But in all this the financial and political interests of the state were dominant. Within the social and political system they were living in Ottoman administrators could not have comprehended or seen the desirability of the dynamics and principles of the capitalist and mercantile economies developing in Western Europe. The word Ottoman is a historical anglicisation of the name of Osman I, the founder of the Empire and of the ruling House of Osman (also known as the Ottoman dynasty). Osman's name in turn was derived from the Persian form of the name ʿUthmān عثمان of ultimately Arabic origin. In Ottoman Turkish, the empire was referred to as Devlet-i ʿAliyye-yi ʿOsmâniyye (دَوْلَتِ عَلِيّهٔ عُثمَانِیّه), (literally ""The Supreme State of the Ottomans"") or alternatively Osmanlı Devleti (عثمانلى دولتى).[dn 5] In Modern Turkish, it is known as Osmanlı İmparatorluğu (""Ottoman Empire"") or Osmanlı Devleti (""The Ottoman State""). As the Ottoman Empire gradually shrank in size, some 7–9 million Turkish-Muslims from its former territories in the Caucasus, Crimea, Balkans, and the Mediterranean islands migrated to Anatolia and Eastern Thrace. After the Empire lost the Balkan Wars (1912–13), it lost all its Balkan territories except East Thrace (European Turkey). This resulted in around 400,000 Muslims fleeing with the retreating Ottoman armies (with many dying from cholera brought by the soldiers), and with some 400,000 non-Muslims fleeing territory still under Ottoman rule. Justin McCarthy estimates that during the period 1821 to 1922 several million Muslims died in the Balkans, with the expulsion of a similar number. Under the millet system, non-Muslim people were considered subjects of the Empire, but were not subject to the Muslim faith or Muslim law. The Orthodox millet, for instance, was still officially legally subject to Justinian's Code, which had been in effect in the Byzantine Empire for 900 years. Also, as the largest group of non-Muslim subjects (or zimmi) of the Islamic Ottoman state, the Orthodox millet was granted a number of special privileges in the fields of politics and commerce, and had to pay higher taxes than Muslim subjects. The defeat and dissolution of the Ottoman Empire (1908–1922) began with the Second Constitutional Era, a moment of hope and promise established with the Young Turk Revolution. It restored the Ottoman constitution of 1876 and brought in multi-party politics with a two-stage electoral system (electoral law) under the Ottoman parliament. The constitution offered hope by freeing the empire’s citizens to modernize the state’s institutions, rejuvenate its strength, and enable it to hold its own against outside powers. Its guarantee of liberties promised to dissolve inter-communal tensions and transform the empire into a more harmonious place. Instead, this period became the story of the twilight struggle of the Empire. Young Turks movement members once underground (named committee, group, etc.) established (declared) their parties. Among them “Committee of Union and Progress,” and “Freedom and Accord Party” were major parties. On the other end of the spectrum were ethnic parties which included; Poale Zion, Al-Fatat, and Armenian national movement organized under Armenian Revolutionary Federation. Profiting from the civil strife, Austria-Hungary officially annexed Bosnia and Herzegovina in 1908. The last of Ottoman censuses was performed with the 1914 census. Ottoman military reforms resulted with the Ottoman Modern Army which engaged with Italo-Turkish War (1911), Balkan Wars (1912–1913), and continuous unrest (Counter coup followed by restoration and Saviors followed by Raid on Porte) in the Empire up to World War I. The stagnation and decline, Stephen Lee argues, was relentless after the death of Suleiman in 1566, interrupted by a few short revivals or reform and recovery. The decline gathered speed so that the Empire in 1699 was, ""a mere shadow of that which intimidated East and West alike in 1566."" Although there are dissenting scholars, most historians point to ""degenerate Sultans, incompetent Grand Viziers, debilitated and ill-equipped armies, corrupt officials, avaricious speculators, grasping enemies, and treacherous friends."" The main cause was a failure of leadership, as Lee argues the first 10 sultans from 1292 to 1566, with one exception, had performed admirably. The next 13 sultans from 1566 to 1703, with two exceptions, were lackadaisical or incompetent rulers, says Lee. In a highly centralized system, the failure at the center proved fatal. A direct result was the strengthening of provincial elites who increasingly ignored Constantinople. Secondly the military strength of European enemies grew stronger and stronger, while the Ottoman armies and arms scarcely improved. Finally the Ottoman economic system grew distorted and impoverished, as war caused inflation, world trade moved in other directions, and the deterioration of law and order made economic progress difficult. This period of renewed assertiveness came to a calamitous end in May 1683 when Grand Vizier Kara Mustafa Pasha led a huge army to attempt a second Ottoman siege of Vienna in the Great Turkish War of 1683–1687. The final assault being fatally delayed, the Ottoman forces were swept away by allied Habsburg, German and Polish forces spearheaded by the Polish king Jan III Sobieski at the Battle of Vienna. The alliance of the Holy League pressed home the advantage of the defeat at Vienna, culminating in the Treaty of Karlowitz (26 January 1699), which ended the Great Turkish War. The Ottomans surrendered control of significant territories, many permanently. Mustafa II (1695–1703) led the counterattack of 1695–96 against the Habsburgs in Hungary, but was undone at the disastrous defeat at Zenta (in modern Serbia), 11 September 1697. As the Ottoman state attempted to modernize its infrastructure and army in response to threats from the outside, it also opened itself up to a different kind of threat: that of creditors. Indeed, as the historian Eugene Rogan has written, ""the single greatest threat to the independence of the Middle East"" in the nineteenth century ""was not the armies of Europe but its banks."" The Ottoman state, which had begun taking on debt with the Crimean War, was forced to declare bankruptcy in 1875. By 1881, the Ottoman Empire agreed to have its debt controlled by an institution known as the Ottoman Public Debt Administration, a council of European men with presidency alternating between France and Britain. The body controlled swaths of the Ottoman economy, and used its position to insure that European capital continued to penetrate the empire, often to the detriment of local Ottoman interests. With the extension of Turkish dominion into the Balkans, the strategic conquest of Constantinople became a crucial objective. The empire had managed to control nearly all former Byzantine lands surrounding the city, but in 1402 the Byzantines were temporarily relieved when the Turco-Mongol leader Timur, founder of the Timurid Empire, invaded Anatolia from the east. In the Battle of Ankara in 1402, Timur defeated the Ottoman forces and took Sultan Bayezid I as a prisoner, throwing the empire into disorder. The ensuing civil war lasted from 1402 to 1413 as Bayezid's sons fought over succession. It ended when Mehmed I emerged as the sultan and restored Ottoman power, bringing an end to the Interregnum, also known as the Fetret Devri. Ottoman Turkish was the official language of the Empire. It was an Oghuz Turkic language highly influenced by Persian and Arabic. The Ottomans had several influential languages: Turkish, spoken by the majority of the people in Anatolia and by the majority of Muslims of the Balkans except in Albania and Bosnia; Persian, only spoken by the educated; Arabic, spoken mainly in Arabia, North Africa, Iraq, Kuwait, the Levant and parts of the Horn of Africa; and Somali throughout the Horn of Africa. In the last two centuries, usage of these became limited, though, and specific: Persian served mainly as a literary language for the educated, while Arabic was used for religious rites. By contrast, the Habsburg frontier had settled somewhat, a stalemate caused by a stiffening of the Habsburg defences. The Long War against Habsburg Austria (1593–1606) created the need for greater numbers of Ottoman infantry equipped with firearms, resulting in a relaxation of recruitment policy. This contributed to problems of indiscipline and outright rebelliousness within the corps, which were never fully solved. Irregular sharpshooters (Sekban) were also recruited, and on demobilization turned to brigandage in the Jelali revolts (1595–1610), which engendered widespread anarchy in Anatolia in the late 16th and early 17th centuries. With the Empire's population reaching 30 million people by 1600, the shortage of land placed further pressure on the government . In spite of these problems, the Ottoman state remained strong, and its army did not collapse or suffer crushing defeats. The only exceptions were campaigns against the Safavid dynasty of Persia, where many of the Ottoman eastern provinces were lost, some permanently. This 1603–1618 war eventually resulted in the Treaty of Nasuh Pasha, which ceded the entire Caucasus, except westernmost Georgia, back into Iranian Safavid possession. Campaigns during this era became increasingly inconclusive, even against weaker states with much smaller forces, such as Poland or Austria. The Christian population of the empire, owing to their higher educational levels, started to pull ahead of the Muslim majority, leading to much resentment on the part of the latter. In 1861, there were 571 primary and 94 secondary schools for Ottoman Christians with 140,000 pupils in total, a figure that vastly exceeded the number of Muslim children in school at the same time, who were further hindered by the amount of time spent learning Arabic and Islamic theology. In turn, the higher educational levels of the Christians allowed them to play a large role in the economy. In 1911, of the 654 wholesale companies in Istanbul, 528 were owned by ethnic Greeks. Of course, it would be a mistake to ignore the geopolitical dimensions of this dynamic. The preponderance of Christian merchants owed not to any innate business sense on their part, although plenty of European observers were keen on making this point. In fact, in many cases, Christians and also Jews were able to gain protection from European consuls and citizenship, meaning they were protected from Ottoman law and not subject to the same economic regulations as their Muslim comrades. The Ottoman Empire or, as a dynastic institution, the House of Osman was unprecedented and unequaled in the Islamic world for its size and duration. In Europe, only the House of Habsburg had a similarly unbroken line of sovereigns (kings/emperors) from the same family who ruled for so long, and during the same period, between the late 13th and early 20th centuries. The Ottoman dynasty was Turkish in origin. On eleven occasions, the sultan was deposed (replaced by another sultan of the Ottoman dynasty, who were either the former sultan's brother, son or nephew) because he was perceived by his enemies as a threat to the state. There were only two attempts in Ottoman history to unseat the ruling Ottoman dynasty, both failures, which suggests a political system that for an extended period was able to manage its revolutions without unnecessary instability. As such, the last Ottoman sultan Mehmed VI (r. 1918–1922) was a direct patrilineal (male-line) descendant of the first Ottoman sultan Osman I (r. 1299–1326), which was unparallelled in both Europe (e.g. the male line of the House of Habsburg became extinct in 1740) and in the Islamic world. The primary purpose of the Imperial Harem was to ensure the birth of male heirs to the Ottoman throne and secure the continuation of the direct patrilineal (male-line) descendance of the Ottoman sultans. Part of the Ottoman territories in the Balkans (such as Thessaloniki, Macedonia and Kosovo) were temporarily lost after 1402 but were later recovered by Murad II between the 1430s and 1450s. On 10 November 1444, Murad II defeated the Hungarian, Polish, and Wallachian armies under Władysław III of Poland (also King of Hungary) and John Hunyadi at the Battle of Varna, the final battle of the Crusade of Varna, although Albanians under Skanderbeg continued to resist. Four years later, John Hunyadi prepared another army (of Hungarian and Wallachian forces) to attack the Turks but was again defeated by Murad II at the Second Battle of Kosovo in 1448. The Ottoman bashi-bazouks brutally suppressed the Bulgarian uprising of 1876, massacring up to 100,000 people in the process. The Russo-Turkish War (1877–78) ended with a decisive victory for Russia. As a result, Ottoman holdings in Europe declined sharply; Bulgaria was established as an independent principality inside the Ottoman Empire, Romania achieved full independence. Serbia and Montenegro finally gained complete independence, but with smaller territories. In 1878, Austria-Hungary unilaterally occupied the Ottoman provinces of Bosnia-Herzegovina and Novi Pazar. The economic structure of the Empire was defined by its geopolitical structure. The Ottoman Empire stood between the West and the East, thus blocking the land route eastward and forcing Spanish and Portuguese navigators to set sail in search of a new route to the Orient. The Empire controlled the spice route that Marco Polo once used. When Vasco da Gama bypassed Ottoman controlled routes and established direct trade links with India in 1498, and Christopher Columbus first journeyed to the Bahamas in 1492, the Ottoman Empire was at its zenith. The Serbian revolution (1804–1815) marked the beginning of an era of national awakening in the Balkans during the Eastern Question. Suzerainty of Serbia as a hereditary monarchy under its own dynasty was acknowledged de jure in 1830. In 1821, the Greeks declared war on the Sultan. A rebellion that originated in Moldavia as a diversion was followed by the main revolution in the Peloponnese, which, along with the northern part of the Gulf of Corinth, became the first parts of the Ottoman Empire to achieve independence (in 1829). By the mid-19th century, the Ottoman Empire was called the ""sick man"" by Europeans. The suzerain states – the Principality of Serbia, Wallachia, Moldavia and Montenegro – moved towards de jure independence during the 1860s and 1870s. In southern Europe, a Catholic coalition led by Philip II of Spain won a victory over the Ottoman fleet at the Battle of Lepanto (1571). It was a startling, if mostly symbolic, blow to the image of Ottoman invincibility, an image which the victory of the Knights of Malta against the Ottoman invaders in the 1565 Siege of Malta had recently set about eroding. The battle was far more damaging to the Ottoman navy in sapping experienced manpower than the loss of ships, which were rapidly replaced. The Ottoman navy recovered quickly, persuading Venice to sign a peace treaty in 1573, allowing the Ottomans to expand and consolidate their position in North Africa. Muslim sects regarded as heretical, such as the Druze, Ismailis, Alevis, and Alawites, ranked below Jews and Christians. In 1514, Sultan Selim I, nicknamed ""the Grim"" because of his cruelty, ordered the massacre of 40,000 Anatolian Alevis (Qizilbash), whom he considered heretics, reportedly proclaiming that ""the killing of one Alevi had as much otherworldly reward as killing 70 Christians.""[page needed] Selim was also responsible for an unprecedented and rapid expansion of the Ottoman Empire into the Middle East, especially through his conquest of the entire Mamluk Sultanate of Egypt, which included much of the region. With these conquests, Selim further solidified the Ottoman claim for being an Islamic caliphate, although Ottoman sultans had been claiming the title of caliph since the 14th century starting with Murad I (reigned 1362 to 1389). The caliphate would remain held by Ottoman sultans for the rest of the office's duration, which ended with its abolition on 3 March 1924 by the Grand National Assembly of Turkey and the exile of the last caliph, Abdülmecid II, to France. The first military unit of the Ottoman State was an army that was organized by Osman I from the tribesmen inhabiting the hills of western Anatolia in the late 13th century. The military system became an intricate organization with the advance of the Empire. The Ottoman military was a complex system of recruiting and fief-holding. The main corps of the Ottoman Army included Janissary, Sipahi, Akıncı and Mehterân. The Ottoman army was once among the most advanced fighting forces in the world, being one of the first to use muskets and cannons. The Ottoman Turks began using falconets, which were short but wide cannons, during the Siege of Constantinople. The Ottoman cavalry depended on high speed and mobility rather than heavy armour, using bows and short swords on fast Turkoman and Arabian horses (progenitors of the Thoroughbred racing horse), and often applied tactics similar to those of the Mongol Empire, such as pretending to retreat while surrounding the enemy forces inside a crescent-shaped formation and then making the real attack. The decline in the army's performance became clear from the mid-17th century and after the Great Turkish War. The 18th century saw some limited success against Venice, but in the north the European-style Russian armies forced the Ottomans to concede land. During the 16th and 17th centuries, in particular at the height of its power under the reign of Suleiman the Magnificent, the Ottoman Empire was a multinational, multilingual empire controlling much of Southeast Europe, Western Asia, the Caucasus, North Africa, and the Horn of Africa. At the beginning of the 17th century the empire contained 32 provinces and numerous vassal states. Some of these were later absorbed into the Ottoman Empire, while others were granted various types of autonomy during the course of centuries.[dn 4] The discovery of new maritime trade routes by Western European states allowed them to avoid the Ottoman trade monopoly. The Portuguese discovery of the Cape of Good Hope in 1488 initiated a series of Ottoman-Portuguese naval wars in the Indian Ocean throughout the 16th century. The Somali Muslim Ajuran Empire, allied with the Ottomans, defied the Portuguese economic monopoly in the Indian Ocean by employing a new coinage which followed the Ottoman pattern, thus proclaiming an attitude of economic independence in regard to the Portuguese. The Ottoman Islamic legal system was set up differently from traditional European courts. Presiding over Islamic courts would be a Qadi, or judge. Since the closing of the ijtihad, or Gate of Interpretation, Qadis throughout the Ottoman Empire focused less on legal precedent, and more with local customs and traditions in the areas that they administered. However, the Ottoman court system lacked an appellate structure, leading to jurisdictional case strategies where plaintiffs could take their disputes from one court system to another until they achieved a ruling that was in their favor. Until the 19th century, Ottoman prose did not develop to the extent that contemporary Divan poetry did. A large part of the reason for this was that much prose was expected to adhere to the rules of sec (سجع, also transliterated as seci), or rhymed prose, a type of writing descended from the Arabic saj' and which prescribed that between each adjective and noun in a string of words, such as a sentence, there must be a rhyme. Nevertheless, there was a tradition of prose in the literature of the time, though exclusively non-fictional in nature. One apparent exception was Muhayyelât (""Fancies"") by Giritli Ali Aziz Efendi, a collection of stories of the fantastic written in 1796, though not published until 1867. The first novel published in the Ottoman Empire was by an Armenian named Vartan Pasha. Published in 1851, the novel was entitled The Story of Akabi (Turkish: Akabi Hikyayesi) and was written in Turkish but with Armenian script. Many of the writers in the Tanzimat period wrote in several different genres simultaneously: for instance, the poet Namik Kemal also wrote the important 1876 novel İntibâh (""Awakening""), while the journalist İbrahim Şinasi is noted for writing, in 1860, the first modern Turkish play, the one-act comedy ""Şair Evlenmesi"" (""The Poet's Marriage""). An earlier play, a farce entitled ""Vakâyi'-i 'Acibe ve Havâdis-i Garibe-yi Kefşger Ahmed"" (""The Strange Events and Bizarre Occurrences of the Cobbler Ahmed""), dates from the beginning of the 19th century, but there remains some doubt about its authenticity. In a similar vein, the novelist Ahmed Midhat Efendi wrote important novels in each of the major movements: Romanticism (Hasan Mellâh yâhud Sırr İçinde Esrâr, 1873; ""Hasan the Sailor, or The Mystery Within the Mystery""), Realism (Henüz On Yedi Yaşında, 1881; ""Just Seventeen Years Old""), and Naturalism (Müşâhedât, 1891; ""Observations""). This diversity was, in part, due to the Tanzimat writers' wish to disseminate as much of the new literature as possible, in the hopes that it would contribute to a revitalization of Ottoman social structures. Before the reforms of the 19th and 20th centuries, the state organisation of the Ottoman Empire was a simple system that had two main dimensions, which were the military administration and the civil administration. The Sultan was the highest position in the system. The civil system was based on local administrative units based on the region's characteristics. The Ottomans practiced a system in which the state (as in the Byzantine Empire) had control over the clergy. Certain pre-Islamic Turkish traditions that had survived the adoption of administrative and legal practices from Islamic Iran remained important in Ottoman administrative circles. According to Ottoman understanding, the state's primary responsibility was to defend and extend the land of the Muslims and to ensure security and harmony within its borders within the overarching context of orthodox Islamic practice and dynastic sovereignty. Ottoman illumination covers non-figurative painted or drawn decorative art in books or on sheets in muraqqa or albums, as opposed to the figurative images of the Ottoman miniature. It was a part of the Ottoman Book Arts together with the Ottoman miniature (taswir), calligraphy (hat), Islamic calligraphy, bookbinding (cilt) and paper marbling (ebru). In the Ottoman Empire, illuminated and illustrated manuscripts were commissioned by the Sultan or the administrators of the court. In Topkapi Palace, these manuscripts were created by the artists working in Nakkashane, the atelier of the miniature and illumination artists. Both religious and non-religious books could be illuminated. Also sheets for albums levha consisted of illuminated calligraphy (hat) of tughra, religious texts, verses from poems or proverbs, and purely decorative drawings. The effective military and bureaucratic structures of the previous century came under strain during a protracted period of misrule by weak Sultans. The Ottomans gradually fell behind the Europeans in military technology as the innovation that fed the Empire's forceful expansion became stifled by growing religious and intellectual conservatism. But in spite of these difficulties, the Empire remained a major expansionist power until the Battle of Vienna in 1683, which marked the end of Ottoman expansion into Europe. The establishment of Ottoman military aviation dates back to between June 1909 and July 1911. The Ottoman Empire started preparing its first pilots and planes, and with the founding of the Aviation School (Tayyare Mektebi) in Yeşilköy on 3 July 1912, the Empire began to tutor its own flight officers. The founding of the Aviation School quickened advancement in the military aviation program, increased the number of enlisted persons within it, and gave the new pilots an active role in the Ottoman Army and Navy. In May 1913 the world's first specialized Reconnaissance Training Program was started by the Aviation School and the first separate reconnaissance division was established.[citation needed] In June 1914 a new military academy, the Naval Aviation School (Bahriye Tayyare Mektebi) was founded. With the outbreak of World War I, the modernization process stopped abruptly. The Ottoman aviation squadrons fought on many fronts during World War I, from Galicia in the west to the Caucasus in the east and Yemen in the south. Suleiman the Magnificent (1520–1566) captured Belgrade in 1521, conquered the southern and central parts of the Kingdom of Hungary as part of the Ottoman–Hungarian Wars,[not in citation given] and, after his historical victory in the Battle of Mohács in 1526, he established Turkish rule in the territory of present-day Hungary (except the western part) and other Central European territories. He then laid siege to Vienna in 1529, but failed to take the city. In 1532, he made another attack on Vienna, but was repulsed in the Siege of Güns. Transylvania, Wallachia and, intermittently, Moldavia, became tributary principalities of the Ottoman Empire. In the east, the Ottoman Turks took Baghdad from the Persians in 1535, gaining control of Mesopotamia and naval access to the Persian Gulf. In 1555, the Caucasus became officially partitioned for the first time between the Safavids and the Ottomans, a status quo that would remain until the end of the Russo-Turkish War (1768–74). By this partitioning of the Caucasus as signed in the Peace of Amasya, Western Armenia, and Western Georgia fell into Ottoman hands, while Dagestan, Eastern Armenia, Eastern Georgia, and Azerbaijan remained Persian. In 1768 Russian-backed Ukrainian Haidamaks, pursuing Polish confederates, entered Balta, an Ottoman-controlled town on the border of Bessarabia in Ukraine, and massacred its citizens and burned the town to the ground. This action provoked the Ottoman Empire into the Russo-Turkish War of 1768–1774. The Treaty of Küçük Kaynarca of 1774 ended the war and provided freedom to worship for the Christian citizens of the Ottoman-controlled provinces of Wallachia and Moldavia. By the late 18th century, a number of defeats in several wars with Russia led some people in the Ottoman Empire to conclude that the reforms of Peter the Great had given the Russians an edge, and the Ottomans would have to keep up with Western technology in order to avoid further defeats. Examples of Ottoman architecture of the classical period, besides Istanbul and Edirne, can also be seen in Egypt, Eritrea, Tunisia, Algiers, the Balkans and Romania, where mosques, bridges, fountains and schools were built. The art of Ottoman decoration developed with a multitude of influences due to the wide ethnic range of the Ottoman Empire. The greatest of the court artists enriched the Ottoman Empire with many pluralistic artistic influences: such as mixing traditional Byzantine art with elements of Chinese art. The Ottoman legal system accepted the religious law over its subjects. At the same time the Qanun (or Kanun), a secular legal system, co-existed with religious law or Sharia. The Ottoman Empire was always organized around a system of local jurisprudence. Legal administration in the Ottoman Empire was part of a larger scheme of balancing central and local authority. Ottoman power revolved crucially around the administration of the rights to land, which gave a space for the local authority to develop the needs of the local millet. The jurisdictional complexity of the Ottoman Empire was aimed to permit the integration of culturally and religiously different groups. The Ottoman system had three court systems: one for Muslims, one for non-Muslims, involving appointed Jews and Christians ruling over their respective religious communities, and the ""trade court"". The entire system was regulated from above by means of the administrative Qanun, i.e. laws, a system based upon the Turkic Yassa and Töre, which were developed in the pre-Islamic era.[citation needed] Much of the cuisine of former Ottoman territories today is descended from a shared Ottoman cuisine, especially Turkish cuisine, and including Greek cuisine, Balkan cuisine, Armenian cuisine, and Middle Eastern cuisine. Many common dishes in the region, descendants of the once-common Ottoman cuisine, include yogurt, döner kebab/gyro/shawarma, cacık/tzatziki, ayran, pita bread, feta cheese, baklava, lahmacun, moussaka, yuvarlak, köfte/keftés/kofta, börek/boureki, rakı/rakia/tsipouro/tsikoudia, meze, dolma, sarma, rice pilaf, Turkish coffee, sujuk, kashk, keşkek, manti, lavash, kanafeh, and more. Due to historically close ties with France, French literature came to constitute the major Western influence on Ottoman literature throughout the latter half of the 19th century. As a result, many of the same movements prevalent in France during this period also had their Ottoman equivalents: in the developing Ottoman prose tradition, for instance, the influence of Romanticism can be seen during the Tanzimat period, and that of the Realist and Naturalist movements in subsequent periods; in the poetic tradition, on the other hand, it was the influence of the Symbolist and Parnassian movements that became paramount. These reforms were based heavily on French models, as indicated by the adoption of a three-tiered court system. Referred to as Nizamiye, this system was extended to the local magistrate level with the final promulgation of the Mecelle, a civil code that regulated marriage, divorce, alimony, will, and other matters of personal status. In an attempt to clarify the division of judicial competences, an administrative council laid down that religious matters were to be handled by religious courts, and statute matters were to be handled by the Nizamiye courts. The Ottoman economic mind was closely related to the basic concepts of state and society in the Middle East in which the ultimate goal of a state was consolidation and extension of the ruler's power, and the way to reach it was to get rich resources of revenues by making the productive classes prosperous. The ultimate aim was to increase the state revenues without damaging the prosperity of subjects to prevent the emergence of social disorder and to keep the traditional organization of the society intact. These court categories were not, however, wholly exclusive: for instance, the Islamic courts—which were the Empire's primary courts—could also be used to settle a trade conflict or disputes between litigants of differing religions, and Jews and Christians often went to them to obtain a more forceful ruling on an issue. The Ottoman state tended not to interfere with non-Muslim religious law systems, despite legally having a voice to do so through local governors. The Islamic Sharia law system had been developed from a combination of the Qur'an; the Hadīth, or words of the prophet Muhammad; ijmā', or consensus of the members of the Muslim community; qiyas, a system of analogical reasoning from earlier precedents; and local customs. Both systems were taught at the Empire's law schools, which were in Istanbul and Bursa. After the Austro-Turkish War of 1716–1718 the Treaty of Passarowitz confirmed the loss of the Banat, Serbia and ""Little Walachia"" (Oltenia) to Austria. The Treaty also revealed that the Ottoman Empire was on the defensive and unlikely to present any further aggression in Europe. The Austro-Russian–Turkish War, which was ended by the Treaty of Belgrade in 1739, resulted in the recovery of Serbia and Oltenia, but the Empire lost the port of Azov, north of the Crimean Peninsula, to the Russians. After this treaty the Ottoman Empire was able to enjoy a generation of peace, as Austria and Russia were forced to deal with the rise of Prussia. France and the Ottoman Empire, united by mutual opposition to Habsburg rule, became strong allies. The French conquests of Nice (1543) and Corsica (1553) occurred as a joint venture between the forces of the French king Francis I and Suleiman, and were commanded by the Ottoman admirals Barbarossa Hayreddin Pasha and Turgut Reis. A month prior to the siege of Nice, France supported the Ottomans with an artillery unit during the 1543 Ottoman conquest of Esztergom in northern Hungary. After further advances by the Turks, the Habsburg ruler Ferdinand officially recognized Ottoman ascendancy in Hungary in 1547. Educational and technological reforms came about, including the establishment of higher education institutions such as the Istanbul Technical University. In 1734 an artillery school was established to impart Western-style artillery methods, but the Islamic clergy successfully objected under the grounds of theodicy. In 1754 the artillery school was reopened on a semi-secret basis. In 1726, Ibrahim Muteferrika convinced the Grand Vizier Nevşehirli Damat İbrahim Pasha, the Grand Mufti, and the clergy on the efficiency of the printing press, and Muteferrika was later granted by Sultan Ahmed III permission to publish non-religious books (despite opposition from some calligraphers and religious leaders). Muteferrika's press published its first book in 1729 and, by 1743, issued 17 works in 23 volumes, each having between 500 and 1,000 copies. The highest position in Islam, caliphate, was claimed by the sultans starting since Murad I, which was established as Ottoman Caliphate. The Ottoman sultan, pâdişâh or ""lord of kings"", served as the Empire's sole regent and was considered to be the embodiment of its government, though he did not always exercise complete control. The Imperial Harem was one of the most important powers of the Ottoman court. It was ruled by the Valide Sultan. On occasion, the Valide Sultan would become involved in state politics. For a time, the women of the Harem effectively controlled the state in what was termed the ""Sultanate of Women"". New sultans were always chosen from the sons of the previous sultan. The strong educational system of the palace school was geared towards eliminating the unfit potential heirs, and establishing support among the ruling elite for a successor. The palace schools, which would also educate the future administrators of the state, were not a single track. First, the Madrasa (Ottoman Turkish: Medrese‎) was designated for the Muslims, and educated scholars and state officials according to Islamic tradition. The financial burden of the Medrese was supported by vakifs, allowing children of poor families to move to higher social levels and income. The second track was a free boarding school for the Christians, the Enderûn, which recruited 3,000 students annually from Christian boys between eight and twenty years old from one in forty families among the communities settled in Rumelia or the Balkans, a process known as Devshirme (Devşirme). Ottoman government deliberately pursued a policy for the development of Bursa, Edirne, and Istanbul, successive Ottoman capitals, into major commercial and industrial centres, considering that merchants and artisans were indispensable in creating a new metropolis. To this end, Mehmed and his successor Bayezid, also encouraged and welcomed migration of the Jews from different parts of Europe, who were settled in Istanbul and other port cities like Salonica. In many places in Europe, Jews were suffering persecution at the hands of their Christian counterparts, such as in Spain after the conclusion of Reconquista. The tolerance displayed by the Turks was welcomed by the immigrants. Modern Ottoman studies think that the change in relations between the Ottoman Turks and central Europe was caused by the opening of the new sea routes. It is possible to see the decline in the significance of the land routes to the East as Western Europe opened the ocean routes that bypassed the Middle East and Mediterranean as parallel to the decline of the Ottoman Empire itself. The Anglo-Ottoman Treaty, also known as the Treaty of Balta Liman that opened the Ottoman markets directly to English and French competitors, would be seen as one of the staging posts along this development. Over the course of Ottoman history, the Ottomans managed to build a large collection of libraries complete with translations of books from other cultures, as well as original manuscripts. A great part of this desire for local and foreign manuscripts arose in the 15th Century. Sultan Mehmet II ordered Georgios Amiroutzes, a Greek scholar from Trabzon, to translate and make available to Ottoman educational institutions the geography book of Ptolemy. Another example is Ali Qushji -an astronomer, mathematician and physicist originally from Samarkand- who became a professor in two madrasas, and influenced Ottoman circles as a result of his writings and the activities of his students, even though he only spent two or three years before his death in Istanbul. The main sports Ottomans were engaged in were Turkish Wrestling, hunting, Turkish archery, horseback riding, Equestrian javelin throw, arm wrestling, and swimming. European model sports clubs were formed with the spreading popularity of football matches in 19th century Constantinople. The leading clubs, according to timeline, were Beşiktaş Gymnastics Club (1903), Galatasaray Sports Club (1905) and Fenerbahçe Sports Club (1907) in Istanbul. Football clubs were formed in other provinces too, such as Karşıyaka Sports Club (1912), Altay Sports Club (1914) and Turkish Fatherland Football Club (later Ülküspor) (1914) of İzmir. The organization of the treasury and chancery were developed under the Ottoman Empire more than any other Islamic government and, until the 17th century, they were the leading organization among all their contemporaries. This organization developed a scribal bureaucracy (known as ""men of the pen"") as a distinct group, partly highly trained ulama, which developed into a professional body. The effectiveness of this professional financial body stands behind the success of many great Ottoman statesmen. Ottoman Divan poetry was a highly ritualized and symbolic art form. From the Persian poetry that largely inspired it, it inherited a wealth of symbols whose meanings and interrelationships—both of similitude (مراعات نظير mura'ât-i nazîr / تناسب tenâsüb) and opposition (تضاد tezâd) were more or less prescribed. Divan poetry was composed through the constant juxtaposition of many such images within a strict metrical framework, thus allowing numerous potential meanings to emerge. The vast majority of Divan poetry was lyric in nature: either gazels (which make up the greatest part of the repertoire of the tradition), or kasîdes. There were, however, other common genres, most particularly the mesnevî, a kind of verse romance and thus a variety of narrative poetry; the two most notable examples of this form are the Leyli and Majnun of Fuzûlî and the Hüsn ü Aşk of Şeyh Gâlib. Ottoman cuisine refers to the cuisine of the capital, Istanbul, and the regional capital cities, where the melting pot of cultures created a common cuisine that most of the population regardless of ethnicity shared. This diverse cuisine was honed in the Imperial Palace's kitchens by chefs brought from certain parts of the Empire to create and experiment with different ingredients. The creations of the Ottoman Palace's kitchens filtered to the population, for instance through Ramadan events, and through the cooking at the Yalıs of the Pashas, and from there on spread to the rest of the population. The Ottomans absorbed some of the traditions, art and institutions of cultures in the regions they conquered, and added new dimensions to them. Numerous traditions and cultural traits of previous empires (in fields such as architecture, cuisine, music, leisure and government) were adopted by the Ottoman Turks, who elaborated them into new forms, which resulted in a new and distinctively Ottoman cultural identity. Despite newer added amalgamations, the Ottoman dynasty, like their predecessors in the Sultanate of Rum and the Seljuk Empire, were thoroughly Persianised in their culture, language, habits and customs, and therefore, the empire has been described as a Persianate empire. Intercultural marriages also played their part in creating the characteristic Ottoman elite culture. When compared to the Turkish folk culture, the influence of these new cultures in creating the culture of the Ottoman elite was clear. During his brief majority reign, Murad IV (1612–1640) reasserted central authority and recaptured Iraq (1639) from the Safavids. The resulting Treaty of Zuhab of that same year decisively parted the Caucasus and adjacent regions between the two neighbouring empires as it had already been defined in the 1555 Peace of Amasya. The Sultanate of women (1648–1656) was a period in which the mothers of young sultans exercised power on behalf of their sons. The most prominent women of this period were Kösem Sultan and her daughter-in-law Turhan Hatice, whose political rivalry culminated in Kösem's murder in 1651. During the Köprülü Era (1656–1703), effective control of the Empire was exercised by a sequence of Grand Viziers from the Köprülü family. The Köprülü Vizierate saw renewed military success with authority restored in Transylvania, the conquest of Crete completed in 1669, and expansion into Polish southern Ukraine, with the strongholds of Khotyn and Kamianets-Podilskyi and the territory of Podolia ceding to Ottoman control in 1676. Because of a low literacy rate among the public (about 2–3% until the early 19th century and just about 15% at the end of 19th century),[citation needed] ordinary people had to hire scribes as ""special request-writers"" (arzuhâlcis) to be able to communicate with the government. The ethnic groups continued to speak within their families and neighborhoods (mahalles) with their own languages (e.g., Jews, Greeks, Armenians, etc.). In villages where two or more populations lived together, the inhabitants would often speak each other's language. In cosmopolitan cities, people often spoke their family languages; many of those who were not ethnic Turks spoke Turkish as a second language. The Crimean War (1853–1856) was part of a long-running contest between the major European powers for influence over territories of the declining Ottoman Empire. The financial burden of the war led the Ottoman state to issue foreign loans amounting to 5 million pounds sterling on 4 August 1854. The war caused an exodus of the Crimean Tatars, about 200,000 of whom moved to the Ottoman Empire in continuing waves of emigration. Toward the end of the Caucasian Wars, 90% of the Circassians were ethnically cleansed and exiled from their homelands in the Caucasus and fled to the Ottoman Empire, resulting in the settlement of 500,000 to 700,000 Circassians in Turkey.[page needed] Some Circassian organisations give much higher numbers, totaling 1–1.5 million deported or killed. In 1915, as the Russian Caucasus Army continued to advance into eastern Anatolia, the Ottoman government started the deportation of its ethnic Armenian population, resulting in the death of approximately 1.5 million Armenians in what became known as the Armenian Genocide. The genocide was carried out during and after World War I and implemented in two phases: the wholesale killing of the able-bodied male population through massacre and subjection of army conscripts to forced labour, followed by the deportation of women, children, the elderly and infirm on death marches leading to the Syrian desert. Driven forward by military escorts, the deportees were deprived of food and water and subjected to periodic robbery, rape, and systematic massacre. Large-scale massacres were also committed against the Empire's Greek and Assyrian minorities as part of the same campaign of ethnic cleansing. With Constantinople as its capital and control of lands around the Mediterranean basin, the Ottoman Empire was at the centre of interactions between the Eastern and Western worlds for six centuries. Following a long period of military setbacks against European powers, the Ottoman Empire gradually declined into the late nineteenth century. The empire allied with Germany in the early 20th century, with the imperial ambition of recovering its lost territories, joining in World War I to achieve this ambition on the side of Germany and the Central Powers. While the Empire was able to largely hold its own during the conflict, it was struggling with internal dissent, especially with the Arab Revolt in its Arabian holdings. Starting before the war, but growing increasingly common and violent during it, major atrocities were committed by the Ottoman government against the Armenians, Assyrians and Pontic Greeks. The Empire's defeat and the occupation of part of its territory by the Allied Powers in the aftermath of World War I resulted in the emergence of a new state, Turkey, in the Ottoman Anatolian heartland following the Turkish War of Independence, as well as the founding of modern Balkan and Middle Eastern states and the partitioning of the Ottoman Empire. Though the sultan was the supreme monarch, the sultan's political and executive authority was delegated. The politics of the state had a number of advisors and ministers gathered around a council known as Divan (after the 17th century it was renamed the ""Porte""). The Divan, in the years when the Ottoman state was still a Beylik, was composed of the elders of the tribe. Its composition was later modified to include military officers and local elites (such as religious and political advisors). Later still, beginning in 1320, a Grand Vizier was appointed to assume certain of the sultan's responsibilities. The Grand Vizier had considerable independence from the sultan with almost unlimited powers of appointment, dismissal and supervision. Beginning with the late 16th century, sultans withdrew from politics and the Grand Vizier became the de facto head of state."
Black_Death,"The historian Francis Aidan Gasquet wrote about the 'Great Pestilence' in 1893 and suggested that ""it would appear to be some form of the ordinary Eastern or bubonic plague"". He was able to adopt the epidemiology of the bubonic plague for the Black Death for the second edition in 1908, implicating rats and fleas in the process, and his interpretation was widely accepted for other ancient and medieval epidemics, such as the Justinian plague that was prevalent in the Eastern Roman Empire from 541 to 700 CE. Other forms of plague have been implicated by modern scientists. The modern bubonic plague has a mortality rate of 30–75% and symptoms including fever of 38–41 °C (100–106 °F), headaches, painful aching joints, nausea and vomiting, and a general feeling of malaise. Left untreated, of those that contract the bubonic plague, 80 percent die within eight days. Pneumonic plague has a mortality rate of 90 to 95 percent. Symptoms include fever, cough, and blood-tinged sputum. As the disease progresses, sputum becomes free flowing and bright red. Septicemic plague is the least common of the three forms, with a mortality rate near 100%. Symptoms are high fevers and purple skin patches (purpura due to disseminated intravascular coagulation). In cases of pneumonic and particularly septicemic plague, the progress of the disease is so rapid that there would often be no time for the development of the enlarged lymph nodes that were noted as buboes. In October 2010, the open-access scientific journal PLoS Pathogens published a paper by a multinational team who undertook a new investigation into the role of Yersinia pestis in the Black Death following the disputed identification by Drancourt and Raoult in 1998. They assessed the presence of DNA/RNA with Polymerase Chain Reaction (PCR) techniques for Y. pestis from the tooth sockets in human skeletons from mass graves in northern, central and southern Europe that were associated archaeologically with the Black Death and subsequent resurgences. The authors concluded that this new research, together with prior analyses from the south of France and Germany, "". . . ends the debate about the etiology of the Black Death, and unambiguously demonstrates that Y. pestis was the causative agent of the epidemic plague that devastated Europe during the Middle Ages"". The Black Death ravaged much of the Islamic world. Plague was present in at least one location in the Islamic world virtually every year between 1500 and 1850. Plague repeatedly struck the cities of North Africa. Algiers lost 30 to 50 thousand inhabitants to it in 1620–21, and again in 1654–57, 1665, 1691, and 1740–42. Plague remained a major event in Ottoman society until the second quarter of the 19th century. Between 1701 and 1750, thirty-seven larger and smaller epidemics were recorded in Constantinople, and an additional thirty-one between 1751 and 1800. Baghdad has suffered severely from visitations of the plague, and sometimes two-thirds of its population has been wiped out. The most widely accepted estimate for the Middle East, including Iraq, Iran and Syria, during this time, is for a death rate of about a third. The Black Death killed about 40% of Egypt's population. Half of Paris's population of 100,000 people died. In Italy, the population of Florence was reduced from 110–120 thousand inhabitants in 1338 down to 50 thousand in 1351. At least 60% of the population of Hamburg and Bremen perished, and a similar percentage of Londoners may have died from the disease as well. Interestingly while contemporary reports account of mass burial pits being created in response to the large numbers of dead, recent scientific investigations of a burial pit in Central London found well-preserved individuals to be buried in isolated, evenly spaced graves, suggesting at least some pre-planning and Christian burials at this time. Before 1350, there were about 170,000 settlements in Germany, and this was reduced by nearly 40,000 by 1450. In 1348, the plague spread so rapidly that before any physicians or government authorities had time to reflect upon its origins, about a third of the European population had already perished. In crowded cities, it was not uncommon for as much as 50% of the population to die. The disease bypassed some areas, and the most isolated areas were less vulnerable to contagion. Monks and priests were especially hard hit since they cared for victims of the Black Death. The dominant explanation for the Black Death is the plague theory, which attributes the outbreak to Yersinia pestis, also responsible for an epidemic that began in southern China in 1865, eventually spreading to India. The investigation of the pathogen that caused the 19th-century plague was begun by teams of scientists who visited Hong Kong in 1894, among whom was the French-Swiss bacteriologist Alexandre Yersin, after whom the pathogen was named Yersinia pestis. The mechanism by which Y. pestis was usually transmitted was established in 1898 by Paul-Louis Simond and was found to involve the bites of fleas whose midguts had become obstructed by replicating Y. pestis several days after feeding on an infected host. This blockage results in starvation and aggressive feeding behaviour by the fleas, which repeatedly attempt to clear their blockage by regurgitation, resulting in thousands of plague bacteria being flushed into the feeding site, infecting the host. The bubonic plague mechanism was also dependent on two populations of rodents: one resistant to the disease, which act as hosts, keeping the disease endemic, and a second that lack resistance. When the second population dies, the fleas move on to other hosts, including people, thus creating a human epidemic. The plague disease, caused by Yersinia pestis, is enzootic (commonly present) in populations of fleas carried by ground rodents, including marmots, in various areas including Central Asia, Kurdistan, Western Asia, Northern India and Uganda. Nestorian graves dating to 1338–39 near Lake Issyk Kul in Kyrgyzstan have inscriptions referring to plague and are thought by many epidemiologists to mark the outbreak of the epidemic, from which it could easily have spread to China and India. In October 2010, medical geneticists suggested that all three of the great waves of the plague originated in China. In China, the 13th century Mongol conquest caused a decline in farming and trading. However, economic recovery had been observed at the beginning of the 14th century. In the 1330s a large number of natural disasters and plagues led to widespread famine, starting in 1331, with a deadly plague arriving soon after. Epidemics that may have included plague killed an estimated 25 million Chinese and other Asians during the 15 years before it reached Constantinople in 1347. Medical knowledge had stagnated during the Middle Ages. The most authoritative account at the time came from the medical faculty in Paris in a report to the king of France that blamed the heavens, in the form of a conjunction of three planets in 1345 that caused a ""great pestilence in the air"". This report became the first and most widely circulated of a series of plague tracts that sought to give advice to sufferers. That the plague was caused by bad air became the most widely accepted theory. Today, this is known as the Miasma theory. The word 'plague' had no special significance at this time, and only the recurrence of outbreaks during the Middle Ages gave it the name that has become the medical term. The results of the Haensch study have since been confirmed and amended. Based on genetic evidence derived from Black Death victims in the East Smithfield burial site in England, Schuenemann et al. concluded in 2011 ""that the Black Death in medieval Europe was caused by a variant of Y. pestis that may no longer exist."" A study published in Nature in October 2011 sequenced the genome of Y. pestis from plague victims and indicated that the strain that caused the Black Death is ancestral to most modern strains of the disease. The study also found that there were two previously unknown but related clades (genetic branches) of the Y. pestis genome associated with medieval mass graves. These clades (which are thought to be extinct) were found to be ancestral to modern isolates of the modern Y. pestis strains Y. p. orientalis and Y. p. medievalis, suggesting the plague may have entered Europe in two waves. Surveys of plague pit remains in France and England indicate the first variant entered Europe through the port of Marseille around November 1347 and spread through France over the next two years, eventually reaching England in the spring of 1349, where it spread through the country in three epidemics. Surveys of plague pit remains from the Dutch town of Bergen op Zoom showed the Y. pestis genotype responsible for the pandemic that spread through the Low Countries from 1350 differed from that found in Britain and France, implying Bergen op Zoom (and possibly other parts of the southern Netherlands) was not directly infected from England or France in 1349 and suggesting a second wave of plague, different from those in Britain and France, may have been carried to the Low Countries from Norway, the Hanseatic cities or another site. Plague was reportedly first introduced to Europe via Genoese traders at the port city of Kaffa in the Crimea in 1347. After a protracted siege, during which the Mongol army under Jani Beg was suffering from the disease, the army catapulted the infected corpses over the city walls of Kaffa to infect the inhabitants. The Genoese traders fled, taking the plague by ship into Sicily and the south of Europe, whence it spread north. Whether or not this hypothesis is accurate, it is clear that several existing conditions such as war, famine, and weather contributed to the severity of the Black Death. A variety of alternatives to the Y. pestis have been put forward. Twigg suggested that the cause was a form of anthrax, and Norman Cantor (2001) thought it may have been a combination of anthrax and other pandemics. Scott and Duncan have argued that the pandemic was a form of infectious disease that characterise as hemorrhagic plague similar to Ebola. Archaeologist Barney Sloane has argued that there is insufficient evidence of the extinction of a large number of rats in the archaeological record of the medieval waterfront in London and that the plague spread too quickly to support the thesis that the Y. pestis was spread from fleas on rats; he argues that transmission must have been person to person. However, no single alternative solution has achieved widespread acceptance. Many scholars arguing for the Y. pestis as the major agent of the pandemic suggest that its extent and symptoms can be explained by a combination of bubonic plague with other diseases, including typhus, smallpox and respiratory infections. In addition to the bubonic infection, others point to additional septicemic (a type of ""blood poisoning"") and pneumonic (an airborne plague that attacks the lungs before the rest of the body) forms of the plague, which lengthen the duration of outbreaks throughout the seasons and help account for its high mortality rate and additional recorded symptoms. In 2014, scientists with Public Health England announced the results of an examination of 25 bodies exhumed from the Clerkenwell area of London, as well as of wills registered in London during the period, which supported the pneumonic hypothesis. The plague repeatedly returned to haunt Europe and the Mediterranean throughout the 14th to 17th centuries. According to Biraben, the plague was present somewhere in Europe in every year between 1346 and 1671. The Second Pandemic was particularly widespread in the following years: 1360–63; 1374; 1400; 1438–39; 1456–57; 1464–66; 1481–85; 1500–03; 1518–31; 1544–48; 1563–66; 1573–88; 1596–99; 1602–11; 1623–40; 1644–54; and 1664–67. Subsequent outbreaks, though severe, marked the retreat from most of Europe (18th century) and northern Africa (19th century). According to Geoffrey Parker, ""France alone lost almost a million people to the plague in the epidemic of 1628–31."" In the first half of the 17th century, a plague claimed some 1.7 million victims in Italy, or about 14% of the population. In 1656, the plague killed about half of Naples' 300,000 inhabitants. More than 1.25 million deaths resulted from the extreme incidence of plague in 17th-century Spain. The plague of 1649 probably reduced the population of Seville by half. In 1709–13, a plague epidemic that followed the Great Northern War (1700–21, Sweden v. Russia and allies) killed about 100,000 in Sweden, and 300,000 in Prussia. The plague killed two-thirds of the inhabitants of Helsinki, and claimed a third of Stockholm's population. Europe's last major epidemic occurred in 1720 in Marseille. In England, in the absence of census figures, historians propose a range of preincident population figures from as high as 7 million to as low as 4 million in 1300, and a postincident population figure as low as 2 million. By the end of 1350, the Black Death subsided, but it never really died out in England. Over the next few hundred years, further outbreaks occurred in 1361–62, 1369, 1379–83, 1389–93, and throughout the first half of the 15th century. An outbreak in 1471 took as much as 10–15% of the population, while the death rate of the plague of 1479–80 could have been as high as 20%. The most general outbreaks in Tudor and Stuart England seem to have begun in 1498, 1535, 1543, 1563, 1589, 1603, 1625, and 1636, and ended with the Great Plague of London in 1665. The plague theory was first significantly challenged by the work of British bacteriologist J. F. D. Shrewsbury in 1970, who noted that the reported rates of mortality in rural areas during the 14th-century pandemic were inconsistent with the modern bubonic plague, leading him to conclude that contemporary accounts were exaggerations. In 1984 zoologist Graham Twigg produced the first major work to challenge the bubonic plague theory directly, and his doubts about the identity of the Black Death have been taken up by a number of authors, including Samuel K. Cohn, Jr. (2002), David Herlihy (1997), and Susan Scott and Christopher Duncan (2001). In 1466, perhaps 40,000 people died of the plague in Paris. During the 16th and 17th centuries, the plague was present in Paris around 30 per cent of the time. The Black Death ravaged Europe for three years before it continued on into Russia, where the disease was present somewhere in the country 25 times between 1350 to 1490. Plague epidemics ravaged London in 1563, 1593, 1603, 1625, 1636, and 1665, reducing its population by 10 to 30% during those years. Over 10% of Amsterdam's population died in 1623–25, and again in 1635–36, 1655, and 1664. Plague occurred in Venice 22 times between 1361 and 1528. The plague of 1576–77 killed 50,000 in Venice, almost a third of the population. Late outbreaks in central Europe included the Italian Plague of 1629–1631, which is associated with troop movements during the Thirty Years' War, and the Great Plague of Vienna in 1679. Over 60% of Norway's population died in 1348–50. The last plague outbreak ravaged Oslo in 1654. The Black Death is thought to have originated in the arid plains of Central Asia, where it then travelled along the Silk Road, reaching Crimea by 1343. From there, it was most likely carried by Oriental rat fleas living on the black rats that were regular passengers on merchant ships. Spreading throughout the Mediterranean and Europe, the Black Death is estimated to have killed 30–60% of Europe's total population. In total, the plague reduced the world population from an estimated 450 million down to 350–375 million in the 14th century. The world population as a whole did not recover to pre-plague levels until the 17th century. The plague recurred occasionally in Europe until the 19th century. In addition to arguing that the rat population was insufficient to account for a bubonic plague pandemic, sceptics of the bubonic plague theory point out that the symptoms of the Black Death are not unique (and arguably in some accounts may differ from bubonic plague); that transference via fleas in goods was likely to be of marginal significance; and that the DNA results may be flawed and might not have been repeated elsewhere, despite extensive samples from other mass graves. Other arguments include the lack of accounts of the death of rats before outbreaks of plague between the 14th and 17th centuries; temperatures that are too cold in northern Europe for the survival of fleas; that, despite primitive transport systems, the spread of the Black Death was much faster than that of modern bubonic plague; that mortality rates of the Black Death appear to be very high; that, while modern bubonic plague is largely endemic as a rural disease, the Black Death indiscriminately struck urban and rural areas; and that the pattern of the Black Death, with major outbreaks in the same areas separated by 5 to 15 years, differs from modern bubonic plague—which often becomes endemic for decades with annual flare-ups. It is recognised that an epidemiological account of the plague is as important as an identification of symptoms, but researchers are hampered by the lack of reliable statistics from this period. Most work has been done on the spread of the plague in England, and even estimates of overall population at the start vary by over 100% as no census was undertaken between the time of publication of the Domesday Book and the year 1377. Estimates of plague victims are usually extrapolated from figures from the clergy. From Italy, the disease spread northwest across Europe, striking France, Spain, Portugal and England by June 1348, then turned and spread east through Germany and Scandinavia from 1348 to 1350. It was introduced in Norway in 1349 when a ship landed at Askøy, then spread to Bjørgvin (modern Bergen) and Iceland. Finally it spread to northwestern Russia in 1351. The plague was somewhat less common in parts of Europe that had smaller trade relations with their neighbours, including the Kingdom of Poland, the majority of the Basque Country, isolated parts of Belgium and the Netherlands, and isolated alpine villages throughout the continent. The plague struck various countries in the Middle East during the pandemic, leading to serious depopulation and permanent change in both economic and social structures. As it spread to western Europe, the disease entered the region from southern Russia also. By autumn 1347, the plague reached Alexandria in Egypt, probably through the port's trade with Constantinople, and ports on the Black Sea. During 1347, the disease travelled eastward to Gaza, and north along the eastern coast to cities in Lebanon, Syria and Palestine, including Ashkelon, Acre, Jerusalem, Sidon, Damascus, Homs, and Aleppo. In 1348–49, the disease reached Antioch. The city's residents fled to the north, most of them dying during the journey, but the infection had been spread to the people of Asia Minor.[citation needed] Gasquet (1908) claimed that the Latin name atra mors (Black Death) for the 14th-century epidemic first appeared in modern times in 1631 in a book on Danish history by J.I. Pontanus: ""Vulgo & ab effectu atram mortem vocatibant. (""Commonly and from its effects, they called it the black death""). The name spread through Scandinavia and then Germany, gradually becoming attached to the mid 14th-century epidemic as a proper name. In England, it was not until 1823 that the medieval epidemic was first called the Black Death."
Hunter-gatherer,"Hunter-gatherers would eventually flourish all over the Americas, primarily based in the Great Plains of the United States and Canada, with offshoots as far east as the Gaspé Peninsula on the Atlantic coast, and as far south as Chile, Monte Verde.[citation needed] American hunter-gatherers were spread over a wide geographical area, thus there were regional variations in lifestyles. However, all the individual groups shared a common style of stone tool production, making knapping styles and progress identifiable. This early Paleo-Indian period lithic reduction tool adaptations have been found across the Americas, utilized by highly mobile bands consisting of approximately 25 to 50 members of an extended family. According to the endurance running hypothesis, long-distance running as in persistence hunting, a method still practiced by some hunter-gatherer groups in modern times, was likely the driving evolutionary force leading to the evolution of certain human characteristics. This hypothesis does not necessarily contradict the scavenging hypothesis: both subsistence strategies could have been in use – sequentially, alternating or even simultaneously. Some of the theorists who advocate this ""revisionist"" critique imply that, because the ""pure hunter-gatherer"" disappeared not long after colonial (or even agricultural) contact began, nothing meaningful can be learned about prehistoric hunter-gatherers from studies of modern ones (Kelly, 24-29; see Wilmsen) In the 1950s, Lewis Binford suggested that early humans were obtaining meat via scavenging, not hunting. Early humans in the Lower Paleolithic lived in forests and woodlands, which allowed them to collect seafood, eggs, nuts, and fruits besides scavenging. Rather than killing large animals for meat, according to this view, they used carcasses of such animals that had either been killed by predators or that had died of natural causes. Archaeological and genetic data suggest that the source populations of Paleolithic hunter-gatherers survived in sparsely wooded areas and dispersed through areas of high primary productivity while avoiding dense forest cover. Hunting and gathering was presumably the subsistence strategy employed by human societies beginning some 1.8 million years ago, by Homo erectus, and from its appearance some 0.2 million years ago by Homo sapiens. It remained the only mode of subsistence until the end of the Mesolithic period some 10,000 years ago, and after this was replaced only gradually with the spread of the Neolithic Revolution. The transition from hunting and gathering to agriculture is not necessarily a one way process. It has been argued that hunting and gathering represents an adaptive strategy, which may still be exploited, if necessary, when environmental change causes extreme food stress for agriculturalists. In fact, it is sometimes difficult to draw a clear line between agricultural and hunter-gatherer societies, especially since the widespread adoption of agriculture and resulting cultural diffusion that has occurred in the last 10,000 years.[citation needed] This anthropological view has remained unchanged since the 1960s.[clarification needed][citation needed] One way to divide hunter-gatherer groups is by their return systems. James Woodburn uses the categories ""immediate return"" hunter-gatherers for egalitarian and ""delayed return"" for nonegalitarian. Immediate return foragers consume their food within a day or two after they procure it. Delayed return foragers store the surplus food (Kelly, 31). At the 1966 ""Man the Hunter"" conference, anthropologists Richard Borshay Lee and Irven DeVore suggested that egalitarianism was one of several central characteristics of nomadic hunting and gathering societies because mobility requires minimization of material possessions throughout a population. Therefore, no surplus of resources can be accumulated by any single member. Other characteristics Lee and DeVore proposed were flux in territorial boundaries as well as in demographic composition. As the number and size of agricultural societies increased, they expanded into lands traditionally used by hunter-gatherers. This process of agriculture-driven expansion led to the development of the first forms of government in agricultural centers, such as the Fertile Crescent, Ancient India, Ancient China, Olmec, Sub-Saharan Africa and Norte Chico. Forest gardening was also being used as a food production system in various parts of the world over this period. Forest gardens originated in prehistoric times along jungle-clad river banks and in the wet foothills of monsoon regions.[citation needed] In the gradual process of families improving their immediate environment, useful tree and vine species were identified, protected and improved, whilst undesirable species were eliminated. Eventually superior foreign species were selected and incorporated into the gardens. There are nevertheless a number of contemporary hunter-gatherer peoples who, after contact with other societies, continue their ways of life with very little external influence. One such group is the Pila Nguru (Spinifex people) of Western Australia, whose habitat in the Great Victoria Desert has proved unsuitable for European agriculture (and even pastoralism).[citation needed] Another are the Sentinelese of the Andaman Islands in the Indian Ocean, who live on North Sentinel Island and to date have maintained their independent existence, repelling attempts to engage with and contact them.[citation needed] The Archaic period in the Americas saw a changing environment featuring a warmer more arid climate and the disappearance of the last megafauna. The majority of population groups at this time were still highly mobile hunter-gatherers; but now individual groups started to focus on resources available to them locally, thus with the passage of time there is a pattern of increasing regional generalization like, the Southwest, Arctic, Poverty, Dalton and Plano traditions. This regional adaptations would become the norm, with reliance less on hunting and gathering, with a more mixed economy of small game, fish, seasonally wild vegetables and harvested plant foods. Some agriculturalists also regularly hunt and gather (e.g., farming during the frost-free season and hunting during the winter). Still others in developed countries go hunting, primarily for leisure. In the Brazilian rainforest, those groups that recently did, or even continue to, rely on hunting and gathering techniques seem to have adopted this lifestyle, abandoning most agriculture, as a way to escape colonial control and as a result of the introduction of European diseases reducing their populations to levels where agriculture became difficult.[citation needed][dubious – discuss] Many hunter-gatherers consciously manipulate the landscape through cutting or burning undesirable plants while encouraging desirable ones, some even going to the extent of slash-and-burn to create habitat for game animals. These activities are on an entirely different scale to those associated with agriculture, but they are nevertheless domestication on some level. Today, almost all hunter-gatherers depend to some extent upon domesticated food sources either produced part-time or traded for products acquired in the wild. To this day, most hunter-gatherers have a symbolically structured sexual division of labour. However, it is true that in a small minority of cases, women hunt the same kind of quarry as men, sometimes doing so alongside men. The best-known example are the Aeta people of the Philippines. According to one study, ""About 85% of Philippine Aeta women hunt, and they hunt the same quarry as men. Aeta women hunt in groups and with dogs, and have a 31% success rate as opposed to 17% for men. Their rates are even better when they combine forces with men: mixed hunting groups have a full 41% success rate among the Aeta."" Among the Ju'/hoansi people of Namibia, women help men track down quarry. Women in the Australian Martu also primarily hunt small animals like lizards to feed their children and maintain relations with other women. Some hunter-gatherer cultures, such as the indigenous peoples of the Pacific Northwest Coast, lived in particularly rich environments that allowed them to be sedentary or semi-sedentary. Anthropologists maintain that hunter/gatherers don't have permanent leaders; instead, the person taking the initiative at any one time depends on the task being performed. In addition to social and economic equality in hunter-gatherer societies, there is often, though not always, sexual parity as well. Hunter-gatherers are often grouped together based on kinship and band (or tribe) membership. Postmarital residence among hunter-gatherers tends to be matrilocal, at least initially. Young mothers can enjoy childcare support from their own mothers, who continue living nearby in the same camp. The systems of kinship and descent among human hunter-gatherers were relatively flexible, although there is evidence that early human kinship in general tended to be matrilineal. Starting at the transition between the Middle to Upper Paleolithic period, some 80,000 to 70,000 years ago, some hunter-gatherers bands began to specialize, concentrating on hunting a smaller selection of (often larger) game and gathering a smaller selection of food. This specialization of work also involved creating specialized tools, like fishing nets and hooks and bone harpoons. The transition into the subsequent Neolithic period is chiefly defined by the unprecedented development of nascent agricultural practices. Agriculture originated and spread in several different areas including the Middle East, Asia, Mesoamerica, and the Andes beginning as early as 12,000 years ago. In the early 1980s, a small but vocal segment of anthropologists and archaeologists attempted to demonstrate that contemporary groups usually identified as hunter-gatherers do not, in most cases, have a continuous history of hunting and gathering, and that in many cases their ancestors were agriculturalists and/or pastoralists[citation needed] who were pushed into marginal areas as a result of migrations, economic exploitation, and/or violent conflict (see, for example, the Kalahari Debate). The result of their effort has been the general acknowledgement that there has been complex interaction between hunter-gatherers and non-hunter-gatherers for millennia.[citation needed] As a result of the now near-universal human reliance upon agriculture, the few contemporary hunter-gatherer cultures usually live in areas unsuitable for agricultural use. Most hunter-gatherers are nomadic or semi-nomadic and live in temporary settlements. Mobile communities typically construct shelters using impermanent building materials, or they may use natural rock shelters, where they are available. Hunting-gathering was the common human mode of subsistence throughout the Paleolithic, but the observation of current-day hunters and gatherers does not necessarily reflect Paleolithic societies; the hunter-gatherer cultures examined today have had much contact with modern civilization and do not represent ""pristine"" conditions found in uncontacted peoples. Lee and Guenther have rejected most of the arguments put forward by Wilmsen. Doron Shultziner and others have argued that we can learn a lot about the life-styles of prehistoric hunter-gatherers from studies of contemporary hunter-gatherers—especially their impressive levels of egalitarianism. Evidence suggests big-game hunter gatherers crossed the Bering Strait from Asia (Eurasia) into North America over a land bridge (Beringia), that existed between 47,000–14,000 years ago. Around 18,500-15,500 years ago, these hunter-gatherers are believed to have followed herds of now-extinct Pleistocene megafauna along ice-free corridors that stretched between the Laurentide and Cordilleran ice sheets. Another route proposed is that, either on foot or using primitive boats, they migrated down the Pacific coast to South America. Hunter-gatherer societies manifest significant variability, depending on climate zone/life zone, available technology and societal structure. Archaeologists examine hunter-gatherer tool kits to measure variability across different groups. Collard et al. (2005) found temperature to be the only statistically significant factor to impact hunter-gatherer tool kits. Using temperature as a proxy for risk, Collard et al.'s results suggest that environments with extreme temperatures pose a threat to hunter-gatherer systems significant enough to warrant increased variability of tools. These results support Torrence's (1989) theory that risk of failure is indeed the most important factor in determining the structure of hunter-gatherer toolkits. Hunting and gathering was humanity's first and most successful adaptation, occupying at least 90 percent of human history. Following the invention of agriculture, hunter-gatherers have been displaced or conquered by farming or pastoralist groups in most parts of the world. It is easy for Western-educated scholars to fall into the trap of viewing hunter-gatherer social and sexual arrangements in the light of Western values.[editorializing] One common arrangement is the sexual division of labour, with women doing most of the gathering, while men concentrate on big game hunting. It might be imagined that this arrangement oppresses women, keeping them in the domestic sphere. However, according to some observers, hunter-gatherer women would not understand this interpretation. Since childcare is collective, with every baby having multiple mothers and male carers, the domestic sphere is not atomised or privatised but an empowering place to be.[citation needed] In all hunter-gatherer societies, women appreciate the meat brought back to camp by men. An illustrative account is Megan Biesele's study of the southern African Ju/'hoan, 'Women Like Meat'. Recent archaeological research suggests that the sexual division of labor was the fundamental organisational innovation that gave Homo sapiens the edge over the Neanderthals, allowing our ancestors to migrate from Africa and spread across the globe. Only a few contemporary societies are classified as hunter-gatherers, and many supplement their foraging activity with horticulture and/or keeping animals. The egalitarianism typical of human hunters and gatherers is never total, but is striking when viewed in an evolutionary context. One of humanity's two closest primate relatives, chimpanzees, are anything but egalitarian, forming themselves into hierarchies that are often dominated by an alpha male. So great is the contrast with human hunter-gatherers that it is widely argued by palaeoanthropologists that resistance to being dominated was a key factor driving the evolutionary emergence of human consciousness, language, kinship and social organization. At the same conference, Marshall Sahlins presented a paper entitled, ""Notes on the Original Affluent Society"", in which he challenged the popular view of hunter-gatherers lives as ""solitary, poor, nasty, brutish and short,"" as Thomas Hobbes had put it in 1651. According to Sahlins, ethnographic data indicated that hunter-gatherers worked far fewer hours and enjoyed more leisure than typical members of industrial society, and they still ate well. Their ""affluence"" came from the idea that they were satisfied with very little in the material sense. Later, in 1996, Ross Sackett performed two distinct meta-analyses to empirically test Sahlin's view. The first of these studies looked at 102 time-allocation studies, and the second one analyzed 207 energy-expenditure studies. Sackett found that adults in foraging and horticultural societies work, on average, about 6.5 hours a day, where as people in agricultural and industrial societies work on average 8.8 hours a day. A hunter-gatherer is a human living in a society in which most or all food is obtained by foraging (collecting wild plants and pursuing wild animals), in contrast to agricultural societies, which rely mainly on domesticated species. Mutual exchange and sharing of resources (i.e., meat gained from hunting) are important in the economic systems of hunter-gatherer societies. Therefore, these societies can be described as based on a ""gift economy."" Hunter-gatherers tend to have an egalitarian social ethos, although settled hunter-gatherers (for example, those inhabiting the Northwest Coast of North America) are an exception to this rule. Nearly all African hunter-gatherers are egalitarian, with women roughly as influential and powerful as men. Many groups continued their hunter-gatherer ways of life, although their numbers have continually declined, partly as a result of pressure from growing agricultural and pastoral communities. Many of them reside in the developing world, either in arid regions or tropical forests. Areas that were formerly available to hunter-gatherers were—and continue to be—encroached upon by the settlements of agriculturalists. In the resulting competition for land use, hunter-gatherer societies either adopted these practices or moved to other areas. In addition, Jared Diamond has blamed a decline in the availability of wild foods, particularly animal resources. In North and South America, for example, most large mammal species had gone extinct by the end of the Pleistocene—according to Diamond, because of overexploitation by humans, although the overkill hypothesis he advocates is strongly contested.[by whom?]"
Liberal_Party_of_Australia,"Fraser maintained some of the social reforms of the Whitlam era, while seeking increased fiscal restraint. His government included the first Aboriginal federal parliamentarian, Neville Bonner, and in 1976, Parliament passed the Aboriginal Land Rights Act 1976, which, while limited to the Northern Territory, affirmed ""inalienable"" freehold title to some traditional lands. Fraser established the multicultural broadcaster SBS, accepted Vietnamese refugees, opposed minority white rule in Apartheid South Africa and Rhodesia and opposed Soviet expansionism. A significant program of economic reform however was not pursued. By 1983, the Australian economy was suffering with the early 1980s recession and amidst the effects of a severe drought. Fraser had promoted ""states' rights"" and his government refused to use Commonwealth powers to stop the construction of the Franklin Dam in Tasmania in 1982. Liberal minister, Don Chipp split off from the party to form a new social liberal party, the Australian Democrats in 1977. Fraser won further substantial majorities at the 1977 and 1980 elections, before losing to the Bob Hawke led Australian Labor Party in the 1983 election. The formation of the party was formally announced at Sydney Town Hall on 31 August 1945. It took the name ""Liberal"" in honour of the old Commonwealth Liberal Party. The new party was dominated by the remains of the old UAP; with few exceptions, the UAP party room became the Liberal party room. The Australian Women's National League, a powerful conservative women's organisation, also merged with the new party. A conservative youth group Menzies had set up, the Young Nationalists, was also merged into the new party. It became the nucleus of the Liberal Party's youth division, the Young Liberals. By September 1945 there were more than 90,000 members, many of whom had not previously been members of any political party. The Liberal Party's organisation is dominated by the six state divisions, reflecting the party's original commitment to a federalised system of government (a commitment which was strongly maintained by all Liberal governments until 1983, but was to a large extent abandoned by the Howard Government, which showed strong centralising tendencies). Menzies deliberately created a weak national party machine and strong state divisions. Party policy is made almost entirely by the parliamentary parties, not by the party's rank-and-file members, although Liberal party members do have a degree of influence over party policy. Menzies continued the expanded immigration program established under Chifley, and took important steps towards dismantling the White Australia Policy. In the early 1950s, external affairs minister Percy Spender helped to establish the Colombo Plan for providing economic aid to underdeveloped nations in Australia's region. Under that scheme many future Asian leaders studied in Australia. In 1958 the government replaced the Immigration Act's arbitrarily applied European language dictation test with an entry permit system, that reflected economic and skills criteria. In 1962, Menzies' Commonwealth Electoral Act provided that all Indigenous Australians should have the right to enrol and vote at federal elections (prior to this, indigenous people in Queensland, Western Australia and some in the Northern Territory had been excluded from voting unless they were ex-servicemen). In 1949 the Liberals appointed Dame Enid Lyons as the first woman to serve in an Australian Cabinet. Menzies remained a staunch supporter of links to the monarchy and British Commonwealth but formalised an alliance with the United States and concluded the Agreement on Commerce between Australia and Japan which was signed in July 1957 and launched post-war trade with Japan, beginning a growth of Australian exports of coal, iron ore and mineral resources that would steadily climb until Japan became Australia's largest trading partner. Socially, while liberty and freedom of enterprise form the basis of its beliefs, elements of the party have wavered between what is termed ""small-l liberalism"" and social conservatism. Historically, Liberal Governments have been responsible for the carriage of a number of notable ""socially liberal"" reforms, including the opening of Australia to multiethnic immigration under Menzies and Harold Holt; Holt's 1967 Referendum on Aboriginal Rights; Sir John Gorton's support for cinema and the arts; selection of the first Aboriginal Senator, Neville Bonner, in 1971; and Malcolm Fraser's Aboriginal Land Rights Act 1976. A West Australian Liberal, Ken Wyatt, became the first Indigenous Australian elected to the House of Representatives in 2010. The Gorton Government increased funding for the arts, setting up the Australian Council for the Arts, the Australian Film Development Corporation and the National Film and Television Training School. The Gorton Government passed legislation establishing equal pay for men and women and increased pensions, allowances and education scholarships, as well as providing free health care to 250,000 of the nation's poor (but not universal health care). Gorton's government kept Australia in the Vietnam War but stopped replacing troops at the end of 1970. After an initial loss to Labor at the 1946 election, Menzies led the Liberals to victory at the 1949 election, and the party stayed in office for a record 23 years—still the longest unbroken run in government at the federal level. Australia experienced prolonged economic growth during the post-war boom period of the Menzies Government (1949–1966) and Menzies fulfilled his promises at the 1949 election to end rationing of butter, tea and petrol and provided a five-shilling endowment for first-born children, as well as for others. While himself an unashamed anglophile, Menzies' government concluded a number of major defence and trade treaties that set Australia on its post-war trajectory out of Britain's orbit; opened Australia to multi-ethnic immigration; and instigated important legal reforms regarding Aboriginal Australians. The UAP had been formed as a new conservative alliance in 1931, with Labor defector Joseph Lyons as its leader. The stance of Lyons and other Labor rebels against the more radical proposals of the Labor movement to deal the Great Depression had attracted the support of prominent Australian conservatives. With Australia still suffering the effects of the Great Depression, the newly formed party won a landslide victory at the 1931 Election, and the Lyons Government went on to win three consecutive elections. It largely avoided Keynesian pump-priming and pursued a more conservative fiscal policy of debt reduction and balanced budgets as a means of stewarding Australia out of the Depression. Lyons' death in 1939 saw Robert Menzies assume the Prime Ministership on the eve of war. Menzies served as Prime Minister from 1939 to 1941 but resigned as leader of the minority World War II government amidst an unworkable parliamentary majority. The UAP, led by Billy Hughes, disintegrated after suffering a heavy defeat in the 1943 election. Menzies called a conference of conservative parties and other groups opposed to the ruling Australian Labor Party, which met in Canberra on 13 October 1944 and again in Albury, New South Wales in December 1944. From 1942 onward Menzies had maintained his public profile with his series of ""The Forgotten People"" radio talks–similar to Franklin D. Roosevelt's ""fireside chats"" of the 1930s–in which he spoke of the middle class as the ""backbone of Australia"" but as nevertheless having been ""taken for granted"" by political parties. Throughout their history, the Liberals have been in electoral terms largely the party of the middle class (whom Menzies, in the era of the party's formation called ""The forgotten people""), though such class-based voting patterns are no longer as clear as they once were. In the 1970s a left-wing middle class emerged that no longer voted Liberal.[citation needed] One effect of this was the success of a breakaway party, the Australian Democrats, founded in 1977 by former Liberal minister Don Chipp and members of minor liberal parties; other members of the left-leaning section of the middle-class became Labor supporters.[citation needed] On the other hand, the Liberals have done increasingly well in recent years among socially conservative working-class voters.[citation needed]However the Liberal Party's key support base remains the upper-middle classes; 16 of the 20 richest federal electorates are held by the Liberals, most of which are safe seats. In country areas they either compete with or have a truce with the Nationals, depending on various factors. The Liberals' immediate predecessor was the United Australia Party (UAP). More broadly, the Liberal Party's ideological ancestry stretched back to the anti-Labor groupings in the first Commonwealth parliaments. The Commonwealth Liberal Party was a fusion of the Free Trade Party and the Protectionist Party in 1909 by the second prime minister, Alfred Deakin, in response to Labor's growing electoral prominence. The Commonwealth Liberal Party merged with several Labor dissidents (including Billy Hughes) to form the Nationalist Party of Australia in 1917. That party, in turn, merged with Labor dissidents to form the UAP in 1931. Following the 1974–75 Loans Affair, the Malcolm Fraser led Liberal-Country Party Coalition argued that the Whitlam Government was incompetent and delayed passage of the Government's money bills in the Senate, until the government would promise a new election. Whitlam refused, Fraser insisted leading to the divisive 1975 Australian constitutional crisis. The deadlock came to an end when the Whitlam government was dismissed by the Governor-General, Sir John Kerr on 11 November 1975 and Fraser was installed as caretaker Prime Minister, pending an election. Fraser won in a landslide at the resulting 1975 election. Holt increased Australian commitment to the growing War in Vietnam, which met with some public opposition. His government oversaw conversion to decimal currency. Holt faced Britain's withdrawal from Asia by visiting and hosting many Asian leaders and by expanding ties to the United States, hosting the first visit to Australia by an American president, his friend Lyndon B. Johnson. Holt's government introduced the Migration Act 1966, which effectively dismantled the White Australia Policy and increased access to non-European migrants, including refugees fleeing the Vietnam War. Holt also called the 1967 Referendum which removed the discriminatory clause in the Australian Constitution which excluded Aboriginal Australians from being counted in the census – the referendum was one of the few to be overwhelmingly endorsed by the Australian electorate (over 90% voted 'yes'). By the end of 1967, the Liberals' initially popular support for the war in Vietnam was causing increasing public protest. During McMahon's period in office, Neville Bonner joined the Senate and became the first Indigenous Australian in the Australian Parliament. Bonner was chosen by the Liberal Party to fill a Senate vacancy in 1971 and celebrated his maiden parliamentary speech with a boomerang throwing display on the lawns of Parliament. Bonner went on to win election at the 1972 election and served as a Liberal Senator for 12 years. He worked on Indigenous and social welfare issues and proved an independent minded Senator, often crossing the floor on Parliamentary votes. Following the 2007 Federal Election, Dr Brendan Nelson was elected leader by the Parliamentary Liberal Party. On 16 September 2008, in a second contest following a spill motion, Nelson lost the leadership to Malcolm Turnbull. On 1 December 2009, a subsequent leadership election saw Turnbull lose the leadership to Tony Abbott by 42 votes to 41 on the second ballot. Abbott led the party to the 2010 federal election, which saw an increase in the Liberal Party vote and resulted in the first hung parliament since the 1940 election. In South Australia, initially a Liberal and Country Party affiliated party, the Liberal and Country League (LCL), mostly led by Premier of South Australia Tom Playford, was in power from the 1933 election to the 1965 election, though with assistance from an electoral malapportionment, or gerrymander, known as the Playmander. The LCL's Steele Hall governed for one term from the 1968 election to the 1970 election and during this time began the process of dismantling the Playmander. David Tonkin, as leader of the South Australian Division of the Liberal Party of Australia, became Premier at the 1979 election for one term, losing office at the 1982 election. The Liberals returned to power at the 1993 election, led by Premiers Dean Brown, John Olsen and Rob Kerin through two terms, until their defeat at the 2002 election. They have since remained in opposition under a record five Opposition Leaders. Menzies came to power the year the Communist Party of Australia had led a coal strike to improve pit miners' working conditions. That same year Joseph Stalin's Soviet Union exploded its first atomic bomb, and Mao Zedong led the Communist Party of China to power in China; a year later came the invasion of South Korea by Communist North Korea. Anti-communism was a key political issue of the 1950s and 1960s. Menzies was firmly anti-Communist; he committed troops to the Korean War and attempted to ban the Communist Party of Australia in an unsuccessful referendum during the course of that war. The Labor Party split over concerns about the influence of the Communist Party over the Trade Union movement, leading to the foundation of the breakaway Democratic Labor Party whose preferences supported the Liberal and Country parties. Domestically, Menzies presided over a fairly regulated economy in which utilities were publicly owned, and commercial activity was highly regulated through centralised wage-fixing and high tariff protection. Liberal leaders from Menzies to Malcolm Fraser generally maintained Australia's high tariff levels. At that time the Liberals' coalition partner, the Country Party, the older of the two in the coalition (now known as the ""National Party""), had considerable influence over the government's economic policies. It was not until the late 1970s and through their period out of power federally in the 1980s that the party came to be influenced by what was known as the ""New Right"" – a conservative liberal group who advocated market deregulation, privatisation of public utilities, reductions in the size of government programs and tax cuts. Gorton maintained good relations with the United States and Britain, but pursued closer ties with Asia. The Gorton government experienced a decline in voter support at the 1969 election. State Liberal leaders saw his policies as too Centralist, while other Liberals didn't like his personal behaviour. In 1971, Defence Minister Malcolm Fraser, resigned and said Gorton was ""not fit to hold the great office of Prime Minister"". In a vote on the leadership the Liberal Party split 50/50, and although this was insufficient to remove him as the leader, Gorton decided this was also insufficient support for him, and he resigned. The contemporary Liberal Party generally advocates economic liberalism (see New Right). Historically, the party has supported a higher degree of economic protectionism and interventionism than it has in recent decades. However, from its foundation the party has identified itself as anti-socialist. Strong opposition to socialism and communism in Australia and abroad was one of its founding principles. The party's founder and longest-serving leader Robert Menzies envisaged that Australia's middle class would form its main constituency. In 1951, during the early stages of the Cold War, Menzies spoke of the possibility of a looming third world war. The Menzies Government entered Australia's first formal military alliance outside of the British Commonwealth with the signing of the ANZUS Treaty between Australia, New Zealand and the United States in San Francisco in 1951. External Affairs Minister Percy Spender had put forward the proposal to work along similar lines to the NATO Alliance. The Treaty declared that any attack on one of the three parties in the Pacific area would be viewed as a threat to each, and that the common danger would be met in accordance with each nation's constitutional processes. In 1954 the Menzies Government signed the South East Asia Collective Defence Treaty (SEATO) as a South East Asian counterpart to NATO. That same year, Soviet diplomat Vladimir Petrov and his wife defected from the Soviet embassy in Canberra, revealing evidence of Russian spying activities; Menzies called a Royal Commission to investigate. Menzies ran strongly against Labor's plans to nationalise the Australian banking system and, following victory in the 1949 election, secured a double dissolution election for April 1951, after the Labor-controlled Senate refused to pass his banking legislation. The Liberal-Country Coalition was returned with control of the Senate. The Government was returned again in the 1954 election; the formation of the anti-Communist Democratic Labor Party (DLP) and the consequent split in the Australian Labor Party early in 1955 helped the Liberals to another victory in December 1955. John McEwen replaced Arthur Fadden as leader of the Country Party in March 1958 and the Menzies-McEwen Coalition was returned again at elections in November 1958 – their third victory against Labor's H. V. Evatt. The Coalition was narrowly returned against Labor's Arthur Calwell in the December 1961 election, in the midst of a credit squeeze. Menzies stood for office for the last time in the November 1963 election, again defeating Calwell, with the Coalition winning back its losses in the House of Representatives. Menzies went on to resign from parliament on 26 January 1966. The party's leader is Malcolm Turnbull and its deputy leader is Julie Bishop. The pair were elected to their positions at the September 2015 Liberal leadership ballot, Bishop as the incumbent deputy leader and Turnbull as a replacement for Tony Abbott, whom he consequently succeeded as Prime Minister of Australia. Now the Turnbull Government, the party had been elected at the 2013 federal election as the Abbott Government which took office on 18 September 2013. At state and territory level, the Liberal Party is in office in three states: Colin Barnett has been Premier of Western Australia since 2008, Will Hodgman Premier of Tasmania since 2014 and Mike Baird Premier of New South Wales since 2014. Adam Giles is also the Chief Minister of the Northern Territory, having led a Country Liberal minority government since 2015. The party is in opposition in Victoria, Queensland, South Australia and the Australian Capital Territory. Howard differed from his Labor predecessor Paul Keating in that he supported traditional Australian institutions like the Monarchy in Australia, the commemoration of ANZAC Day and the design of the Australian flag, but like Keating he pursued privatisation of public utilities and the introduction of a broad based consumption tax (although Keating had dropped support for a GST by the time of his 1993 election victory). Howard's premiership coincided with Al Qaeda's 11 September attacks on the United States. The Howard Government invoked the ANZUS treaty in response to the attacks and supported America's campaigns in Afghanistan and Iraq. A period of division for the Liberals followed, with former Treasurer John Howard competing with former Foreign Minister Andrew Peacock for supremacy. The Australian economy was facing the early 1990s recession. Unemployment reached 11.4% in 1992. Under Dr John Hewson, in November 1991, the opposition launched the 650-page Fightback! policy document − a radical collection of ""dry"", economic liberal measures including the introduction of a Goods and Services Tax (GST), various changes to Medicare including the abolition of bulk billing for non-concession holders, the introduction of a nine-month limit on unemployment benefits, various changes to industrial relations including the abolition of awards, a $13 billion personal income tax cut directed at middle and upper income earners, $10 billion in government spending cuts, the abolition of state payroll taxes and the privatisation of a large number of government owned enterprises − representing the start of a very different future direction to the keynesian economic conservatism practiced by previous Liberal/National Coalition governments. The 15 percent GST was the centerpiece of the policy document. Through 1992, Labor Prime Minister Paul Keating mounted a campaign against the Fightback package, and particularly against the GST, which he described as an attack on the working class in that it shifted the tax burden from direct taxation of the wealthy to indirect taxation as a broad-based consumption tax. Pressure group activity and public opinion was relentless, which led Hewson to exempt food from the proposed GST − leading to questions surrounding the complexity of what food was and wasn't to be exempt from the GST. Hewson's difficulty in explaining this to the electorate was exemplified in the infamous birthday cake interview, considered by some as a turning point in the election campaign. Keating won a record fifth consecutive Labor term at the 1993 election. A number of the proposals were later adopted in to law in some form, to a small extent during the Keating Labor government, and to a larger extent during the Howard Liberal government (most famously the GST), while unemployment benefits and bulk billing were re-targeted for a time by the Abbott Liberal government. Through 2010, the party improved its vote in the Tasmanian and South Australian state elections and achieved state government in Victoria. In March 2011, the New South Wales Liberal-National Coalition led by Barry O'Farrell won government with the largest election victory in post-war Australian history at the State Election. In Queensland, the Liberal and National parties merged in 2008 to form the new Liberal National Party of Queensland (registered as the Queensland Division of the Liberal Party of Australia). In March 2012, the new party achieved Government in an historic landslide, led by former Brisbane Lord Mayor, Campbell Newman."
Orthodox_Judaism,"Orthodox Judaism maintains the historical understanding of Jewish identity. A Jew is someone who was born to a Jewish mother, or who converts to Judaism in accordance with Jewish law and tradition. Orthodoxy thus rejects patrilineal descent as a means of establishing Jewish identity. Similarly, Orthodoxy strongly condemns intermarriage. Intermarriage is seen as a deliberate rejection of Judaism, and an intermarried person is effectively cut off from most of the Orthodox community. However, some Orthodox Jewish organizations do reach out to intermarried Jews. According to Orthodox Judaism, Jewish law today is based on the commandments in the Torah, as viewed through the discussions and debates contained in classical rabbinic literature, especially the Mishnah and the Talmud. Orthodox Judaism thus holds that the halakha represents the ""will of God"", either directly, or as closely to directly as possible. The laws are from the word of God in the Torah, using a set of rules also revealed by God to Moses on Mount Sinai, and have been derived with the utmost accuracy and care, and thus the Oral Law is considered to be no less the word of God. If some of the details of Jewish law may have been lost over the millennia, they were reconstructed in accordance with internally consistent rules; see The 13 rules by which Jewish law was derived. Although sizable Orthodox Jewish communities are located throughout the United States, many American Orthodox Jews live in New York State, particularly in the New York City Metropolitan Area. Two of the main Orthodox communities in the United States are located in New York City and Rockland County. In New York City, the neighborhoods of Borough Park, Midwood, Williamsburg, and Crown Heights, located in the borough of Brooklyn, have particularly large Orthodox communities. The most rapidly growing community of American Orthodox Jews is located in Rockland County and the Hudson Valley of New York, including the communities of Monsey, Monroe, New Square, and Kiryas Joel. There are also sizable and rapidly growing Orthodox communities throughout New Jersey, particularly in Lakewood, Teaneck, Englewood, Passaic, and Fair Lawn. The roots of Orthodox Judaism can be traced to the late 18th or early 19th century, when elements within German Jewry sought to reform Jewish belief and practice in the early 19th century in response to the Age of Enlightenment, Jewish Emancipation, and Haskalah. They sought to modernize education in light of contemporary scholarship. They rejected claims of the absolute divine authorship of the Torah, declaring only biblical laws concerning ethics to be binding, and stated that the rest of halakha (Jewish law) need not be viewed as normative for Jews in wider society. (see Reform Judaism). Politically, Orthodox Jews, given their variety of movements and affiliations, tend not to conform easily to the standard left-right political spectrum, with one of the key differences between the movements stemming from the groups' attitudes to Zionism. Generally speaking, of the three key strands of Orthodox Judaism, Haredi Orthodox and Hasidic Orthodox Jews are at best ambivalent towards the ideology of Zionism and the creation of the State of Israel, and there are many groups and organisations who are outspokenly anti-Zionistic, seeing the ideology of Zionism as diametrically opposed to the teaching of the Torah, and the Zionist administration of the State of Israel, with its emphasis on militarism and nationalism, as destructive of the Judaic way of life. Orthodox Judaism holds that on Mount Sinai, the Written Law was transmitted along with an Oral Law. The words of the Torah (Pentateuch) were spoken to Moses by God; the laws contained in this Written Torah, the ""Mitzvot"", were given along with detailed explanations in the oral tradition as to how to apply and interpret them. Furthermore, the Oral law includes principles designed to create new rules. The Oral law is held to be transmitted with an extremely high degree of accuracy. Jewish theologians, who choose to emphasize the more evolutionary nature of the Halacha point to a famous story in the Talmud, where Moses is miraculously transported to the House of Study of Rabbi Akiva and is clearly unable to follow the ensuing discussion. According to the New Jersey Press Association, several media entities refrain from using the term ""ultra-Orthodox"", including the Religion Newswriters Association; JTA, the global Jewish news service; and the Star-Ledger, New Jersey’s largest daily newspaper. The Star-Ledger was the first mainstream newspaper to drop the term. Several local Jewish papers, including New York's Jewish Week and Philadelphia's Jewish Exponent have also dropped use of the term. According to Rabbi Shammai Engelmayer, spiritual leader of Temple Israel Community Center in Cliffside Park and former executive editor of Jewish Week, this leaves ""Orthodox"" as ""an umbrella term that designates a very widely disparate group of people very loosely tied together by some core beliefs."" Orthodox Judaism holds that the words of the Torah, including both the Written Law (Pentateuch) and those parts of the Oral Law which are ""halacha leMoshe m'Sinai"", were dictated by God to Moses essentially as they exist today. The laws contained in the Written Torah were given along with detailed explanations as how to apply and interpret them, the Oral Law. Although Orthodox Jews believe that many elements of current religious law were decreed or added as ""fences"" around the law by the rabbis, all Orthodox Jews believe that there is an underlying core of Sinaitic law and that this core of the religious laws Orthodox Jews know today is thus directly derived from Sinai and directly reflects the Divine will. As such, Orthodox Jews believe that one must be extremely careful in interpreting Jewish law. Orthodox Judaism holds that, given Jewish law's Divine origin, no underlying principle may be compromised in accounting for changing political, social or economic conditions; in this sense, ""creativity"" and development in Jewish law is limited. Hirsch held the opinion that Judaism demands an application of Torah thought to the entire realm of human experience, including the secular disciplines. His approach was termed the Torah im Derech Eretz approach, or ""neo-Orthodoxy"". While insisting on strict adherence to Jewish beliefs and practices, he held that Jews should attempt to engage and influence the modern world, and encouraged those secular studies compatible with Torah thought. This pattern of religious and secular involvement has been evident at many times in Jewish history. Scholars[who?] believe it was characteristic of the Jews in Babylon during the Amoraic and Geonic periods, and likewise in early medieval Spain, shown by their engagement with both Muslim and Christian society. It appeared as the traditional response to cultural and scientific innovation. Orthodox Judaism collectively considers itself the only true heir to the Jewish tradition. The Orthodox Jewish movements consider all non-Orthodox Jewish movements to be unacceptable deviations from authentic Judaism; both because of other denominations' doubt concerning the verbal revelation of Written and Oral Torah, and because of their rejection of Halakhic precedent as binding. As such, Orthodox Jewish groups characterize non-Orthodox forms of Judaism as heretical; see the article on Relationships between Jewish religious movements. Hasidic or Chasidic Judaism overlaps significantly with Haredi Judaism in its engagement with the secular and commercial world, and in regard to social issues. It precedes the later and differs in its genesis and emerged focus. The movement originated in Eastern Europe (what is now Belarus and Ukraine) in the 18th century. Founded by Israel ben Eliezer, known as the Baal Shem Tov (1698–1760), it emerged in an age of persecution of the Jewish people, when a schism existed between scholarly and common European Jews. In addition to bridging this class gap, Hasidic teachings sought to reintroduce joy in the performance of the commandments and in prayer through the popularisation of Jewish mysticism (this joy had been suppressed in the intense intellectual study of the Talmud). The Ba'al Shem Tov sought to combine rigorous scholarship with more emotional mitzvah observance. In a practical sense, what distinguishes Hasidic Judaism from other forms of Haredi Judaism is the close-knit organization of Hasidic communities centered on a Rebbe (sometimes translated as ""Grand Rabbi""), and various customs and modes of dress particular to each community. In some cases, there are religious ideological distinctions between Hasidic groups, as well. Another phenomenon that sets Hasidic Judaism apart from general Haredi Judaism is the strong emphasis placed on speaking Yiddish; in (many) Hasidic households and communities, Yiddish is spoken exclusively. In reaction to the emergence of Reform Judaism, a group of traditionalist German Jews emerged in support of some of the values of the Haskalah, but also wanted to defend the classic, traditional interpretation of Jewish law and tradition. This group was led by those who opposed the establishment of a new temple in Hamburg , as reflected in the booklet ""Ele Divrei HaBerit"". As a group of Reform Rabbis convened in Braunschweig, Rabbi Jacob Ettlinger of Altona published a manifesto entitled ""Shlomei Emunei Yisrael"" in German and Hebrew, having 177 Rabbis sign on. At this time the first Orthodox Jewish periodical, ""Der Treue Zions Waechter"", was launched with the Hebrew supplement ""Shomer Zion HaNe'eman"" [1845 - 1855]. In later years it was Rav Ettlinger's students Rabbi Samson Raphael Hirsch and Rabbi Azriel Hildesheimer of Berlin who deepened the awareness and strength of Orthodox Jewry. Rabbi Samson Raphael Hirsch commented in 1854: Modern Orthodoxy, as a stream of Orthodox Judaism represented by institutions such as the U.S. National Council for Young Israel, is pro-Zionist and thus places a high national, as well as religious, significance on the State of Israel, and its affiliates are, typically, Zionist in orientation. It also practices involvement with non-Orthodox Jews that extends beyond ""outreach (Kiruv)"" to continued institutional relations and cooperation; see further under Torah Umadda. Other ""core beliefs"" are a recognition of the value and importance of secular studies, a commitment to equality of education for both men and women, and a full acceptance of the importance of being able to financially support oneself and one's family. Modern Orthodoxy comprises a fairly broad spectrum of movements, each drawing on several distinct though related philosophies, which in some combination have provided the basis for all variations of the movement today. In general, Modern Orthodoxy holds that Jewish law is normative and binding, while simultaneously attaching a positive value to interaction with contemporary society. In this view, Orthodox Judaism can ""be enriched"" by its intersection with modernity; further, ""modern society creates opportunities to be productive citizens engaged in the Divine work of transforming the world to benefit humanity"". At the same time, in order to preserve the integrity of halakha, any area of ""powerful inconsistency and conflict"" between Torah and modern culture must be avoided. Modern Orthodoxy, additionally, assigns a central role to the ""People of Israel"". In contrast to the general American Jewish community, which is dwindling due to low fertility and high intermarriage and assimilation rates, the Orthodox Jewish community of the United States is growing rapidly. Among Orthodox Jews, the fertility rate stands at about 4.1 children per family, as compared to 1.9 children per family among non-Orthodox Jews, and intermarriage among Orthodox Jews is practically non-existent, standing at about 2%, in contrast to a 71% intermarriage rate among non-Orthodox Jews. In addition, Orthodox Judaism has a growing retention rate; while about half of those raised in Orthodox homes previously abandoned Orthodox Judaism, that number is declining. According to The New York Times, the high growth rate of Orthodox Jews will eventually render them the dominant demographic force in New York Jewry. However, the Orthodox claim to absolute fidelity to past tradition has been challenged by scholars who contend that the Judaism of the Middle Ages bore little resemblance to that practiced by today's Orthodox. Rather, the Orthodox community, as a counterreaction to the liberalism of the Haskalah movement, began to embrace far more stringent halachic practices than their predecessors, most notably in matters of Kashrut and Passover dietary laws, where the strictest possible interpretation becomes a religious requirement, even where the Talmud explicitly prefers a more lenient position, and even where a more lenient position was practiced by prior generations. Orthodox Judaism is the approach to religious Judaism which subscribes to a tradition of mass revelation and adheres to the interpretation and application of the laws and ethics of the Torah as legislated in the Talmudic texts by the Tanaim and Amoraim. These texts were subsequently developed and applied by later authorities, known as the Gaonim, Rishonim, and Acharonim. Orthodox Judaism generally includes Modern Orthodox Judaism (אורתודוקסיה מודרנית) and Ultra-Orthodox or Haredi Judaism (יהדות חרדית), but complete within is a wide range of philosophies. Although Orthodox Judaism would probably be considered the mainstream expression of Judaism prior to the 19th century, for some Orthodox Judaism is a modern self-identification that distinguishes it from traditional pre-modern Judaism. Jewish historians also note that certain customs of today's Orthodox are not continuations of past practice, but instead represent innovations that would have been unknown to prior generations. For example, the now-widespread haredi tradition of cutting a boy's hair for the first time on his third birthday (upshirin or upsheerin, Yiddish for ""haircut"") ""originated as an Arab custom that parents cut a newborn boy's hair and burned it in a fire as a sacrifice,"" and ""Jews in Palestine learned this custom from Arabs and adapted it to a special Jewish context."" The Ashkenazi prohibition against eating kitniyot (grains and legumes such as rice, corn, beans, and peanuts) during Passover was explicitly rejected in the Talmud, has no known precedent before the 12th century and represented a minority position for hundreds of years thereafter, but nonetheless has remained a mandatory prohibition among Ashkenazi Orthodox Jews due to their historic adherence to the ReMA's rulings in the Shulchan Aruch. For guidance in practical application of Jewish law, the majority of Orthodox Jews appeal to the Shulchan Aruch (""Code of Jewish Law"" composed in the 16th century by Rabbi Joseph Caro) together with its surrounding commentaries. Thus, at a general level, there is a large degree of uniformity amongst all Orthodox Jews. Concerning the details, however, there is often variance: decisions may be based on various of the standardized codes of Jewish Law that have been developed over the centuries, as well as on the various responsa. These codes and responsa may differ from each other as regards detail (and reflecting the above philosophical differences, as regards the weight assigned to these). By and large, however, the differences result from the historic dispersal of the Jews and the consequent development of differences among regions in their practices (see minhag). Some scholars believe that Modern Orthodoxy arose from the religious and social realities of Western European Jewry. While most Jews consider Modern Orthodoxy traditional today, some (the hareidi and hasidic groups) within the Orthodox community consider some elements to be of questionable validity. The neo-Orthodox movement holds that Hirsch's views are not accurately followed by Modern Orthodoxy. [See Torah im Derech Eretz and Torah Umadda ""Relationship with Torah im Derech Eretz"" for a more extensive listing.] Haredi Judaism advocates segregation from non-Jewish culture, although not from non-Jewish society entirely. It is characterised by its focus on community-wide Torah study. Haredi Orthodoxy's differences with Modern Orthodoxy usually lie in interpretation of the nature of traditional halakhic concepts and in acceptable application of these concepts. Thus, engaging in the commercial world is a legitimate means to achieving a livelihood, but individuals should participate in modern society as little as possible. The same outlook is applied with regard to obtaining degrees necessary to enter one's intended profession: where tolerated in the Haredi society, attending secular institutions of higher education is viewed as a necessary but inferior activity. Academic interest is instead to be directed toward the religious education found in the yeshiva. Both boys and girls attend school and may proceed to higher Torah study, starting anywhere between the ages of 13 and 18. A significant proportion of students, especially boys, remain in yeshiva until marriage (which is often arranged through facilitated dating – see shiduch), and many study in a kollel (Torah study institute for married men) for many years after marriage. Most Orthodox men (including many Modern Orthodox), even those not in Kollel, will study Torah daily. On the other hand, Orthodox Jews subscribing to Modern Orthodoxy in its American and UK incarnations, tend to be far more right-wing than both non-orthodox and other orthodox Jews. While the overwhelming majority of non-Orthodox American Jews are on average strongly liberal and supporters of the Democratic Party, the Modern Orthodox subgroup of Orthodox Judaism tends to be far more conservative, with roughly half describing themselves as political conservatives, and are mostly Republican Party supporters. Modern Orthodox Jews, compared to both the non-Orthodox American Jewry and the Haredi and Hasidic Jewry, also tend to have a stronger connection to Israel due to their attachment to Zionism. In practice, the emphasis on strictness has resulted in the rise of ""homogeneous enclaves"" with other haredi Jews that are less likely to be threatened by assimilation and intermarriage, or even to interact with other Jews who do not share their doctrines. Nevertheless, this strategy has proved successful and the number of adherents to Orthodox Judaism, especially Haredi and Chassidic communities, has grown rapidly. Some scholars estimate more Jewish men are studying in yeshivot (Talmudic schools) and Kollelim (post-graduate Talmudical colleges for married (male) students) than at any other time in history.[citation needed] However, there is significant disagreement within Orthodox Judaism, particularly between Haredi Judaism and Modern Orthodox Judaism, about the extent and circumstances under which the proper application of Halakha should be re-examined as a result of changing realities. As a general rule, Haredi Jews believe that when at all possible the law should be maintained as it was understood by their authorities at the haskalah, believing that it had never changed. Modern Orthodox authorities are more willing to assume that under scrupulous examination, identical principles may lead to different applications in the context of modern life. To the Orthodox Jew, halakha is a guide, God's Law, governing the structure of daily life from the moment he or she wakes up to the moment he goes to sleep. It includes codes of behaviour applicable to a broad range of circumstances (and many hypothetical ones). There are though a number of halakhic meta-principles that guide the halakhic process and in an instance of opposition between a specific halakha and a meta-principle, the meta-principle often wins out . Examples of Halakhic Meta-Principles are: ""Deracheha Darchei Noam"" - the ways of Torah are pleasant, ""Kavod Habriyot"" - basic respect for human beings, ""Pikuach Nefesh"" - the sanctity of human life. Externally, Orthodox Jews can be identified by their manner of dress and family lifestyle. Orthodox women dress modestly by keeping most of their skin covered. Additionally, married women cover their hair, most commonly in the form of a scarf, also in the form of hats, bandanas, berets, snoods or, sometimes, wigs. Orthodox men wear a skullcap known as a kipa and often fringes called ""tzitzit"". Haredi men often grow beards and always wear black hats and suits, indoors and outdoors. However, Modern Orthodox Jews are commonly indistinguishable in their dress from those around them."
Ctenophora,"The Beroida, also known as Nuda, have no feeding appendages, but their large pharynx, just inside the large mouth and filling most of the saclike body, bears ""macrocilia"" at the oral end. These fused bundles of several thousand large cilia are able to ""bite"" off pieces of prey that are too large to swallow whole – almost always other ctenophores. In front of the field of macrocilia, on the mouth ""lips"" in some species of Beroe, is a pair of narrow strips of adhesive epithelial cells on the stomach wall that ""zip"" the mouth shut when the animal is not feeding, by forming intercellular connections with the opposite adhesive strip. This tight closure streamlines the front of the animal when it is pursuing prey. Because of their soft, gelatinous bodies, ctenophores are extremely rare as fossils, and fossils that have been interpreted as ctenophores have been found only in lagerstätten, places where the environment was exceptionally suited to preservation of soft tissue. Until the mid-1990s only two specimens good enough for analysis were known, both members of the crown group, from the early Devonian (Emsian) period. Three additional putative species were then found in the Burgess Shale and other Canadian rocks of similar age, about 505 million years ago in the mid-Cambrian period. All three apparently lacked tentacles but had between 24 and 80 comb rows, far more than the 8 typical of living species. They also appear to have had internal organ-like structures unlike anything found in living ctenophores. One of the fossil species first reported in 1996 had a large mouth, apparently surrounded by a folded edge that may have been muscular. Evidence from China a year later suggests that such ctenophores were widespread in the Cambrian, but perhaps very different from modern species – for example one fossil's comb-rows were mounted on prominent vanes. The Ediacaran Eoandromeda could putatively represent a comb jelly. The early Cambrian sessile frond-like fossil Stromatoveris, from China's Chengjiang lagerstätte and dated to about 515 million years ago, is very similar to Vendobionta of the preceding Ediacaran period. De-Gan Shu, Simon Conway Morris et al. found on its branches what they considered rows of cilia, used for filter feeding. They suggested that Stromatoveris was an evolutionary ""aunt"" of ctenophores, and that ctenophores originated from sessile animals whose descendants became swimmers and changed the cilia from a feeding mechanism to a propulsion system. Ctenophores form an animal phylum that is more complex than sponges, about as complex as cnidarians (jellyfish, sea anemones, etc.), and less complex than bilaterians (which include almost all other animals). Unlike sponges, both ctenophores and cnidarians have: cells bound by inter-cell connections and carpet-like basement membranes; muscles; nervous systems; and some have sensory organs. Ctenophores are distinguished from all other animals by having colloblasts, which are sticky and adhere to prey, although a few ctenophore species lack them. The internal cavity forms: a mouth that can usually be closed by muscles; a pharynx (""throat""); a wider area in the center that acts as a stomach; and a system of internal canals. These branch through the mesoglea to the most active parts of the animal: the mouth and pharynx; the roots of the tentacles, if present; all along the underside of each comb row; and four branches round the sensory complex at the far end from the mouth – two of these four branches terminate in anal pores. The inner surface of the cavity is lined with an epithelium, the gastrodermis. The mouth and pharynx have both cilia and well-developed muscles. In other parts of the canal system, the gastrodermis is different on the sides nearest to and furthest from the organ that it supplies. The nearer side is composed of tall nutritive cells that store nutrients in vacuoles (internal compartments), germ cells that produce eggs or sperm, and photocytes that produce bioluminescence. The side furthest from the organ is covered with ciliated cells that circulate water through the canals, punctuated by ciliary rosettes, pores that are surrounded by double whorls of cilia and connect to the mesoglea. When some species, including Bathyctena chuni, Euplokamis stationis and Eurhamphaea vexilligera, are disturbed, they produce secretions (ink) that luminesce at much the same wavelengths as their bodies. Juveniles will luminesce more brightly in relation to their body size than adults, whose luminescence is diffused over their bodies. Detailed statistical investigation has not suggested the function of ctenophores' bioluminescence nor produced any correlation between its exact color and any aspect of the animals' environments, such as depth or whether they live in coastal or mid-ocean waters. The tentacles of cydippid ctenophores are typically fringed with tentilla (""little tentacles""), although a few genera have simple tentacles without these sidebranches. The tentacles and tentilla are densely covered with microscopic colloblasts that capture prey by sticking to it. Colloblasts are specialized mushroom-shaped cells in the outer layer of the epidermis, and have three main components: a domed head with vesicles (chambers) that contain adhesive; a stalk that anchors the cell in the lower layer of the epidermis or in the mesoglea; and a spiral thread that coils round the stalk and is attached to the head and to the root of the stalk. The function of the spiral thread is uncertain, but it may absorb stress when prey tries to escape, and thus prevent the collobast from being torn apart. In addition to colloblasts, members of the genus Haeckelia, which feed mainly on jellyfish, incorporate their victims' stinging nematocytes into their own tentacles – some cnidaria-eating nudibranchs similarly incorporate nematocytes into their bodies for defense. The tentilla of Euplokamis differ significantly from those of other cydippids: they contain striated muscle, a cell type otherwise unknown in the phylum Ctenophora; and they are coiled when relaxed, while the tentilla of all other known ctenophores elongate when relaxed. Euplokamis' tentilla have three types of movement that are used in capturing prey: they may flick out very quickly (in 40 to 60 milliseconds); they can wriggle, which may lure prey by behaving like small planktonic worms; and they coil round prey. The unique flicking is an uncoiling movement powered by contraction of the striated muscle. The wriggling motion is produced by smooth muscles, but of a highly specialized type. Coiling around prey is accomplished largely by the return of the tentilla to their inactive state, but the coils may be tightened by smooth muscle. Almost all species are hermaphrodites, in other words they function as both males and females at the same time – except that in two species of the genus Ocryopsis individuals remain of the same single sex all their lives. The gonads are located in the parts of the internal canal network under the comb rows, and eggs and sperm are released via pores in the epidermis. Fertilization is external in most species, but platyctenids use internal fertilization and keep the eggs in brood chambers until they hatch. Self-fertilization has occasionally been seen in species of the genus Mnemiopsis, and it is thought that most of the hermaphroditic species are self-fertile. There are eight rows of combs that run from near the mouth to the opposite end, and are spaced evenly round the body. The ""combs"" beat in a metachronal rhythm rather like that of a Mexican wave. From each balancer in the statocyst a ciliary groove runs out under the dome and then splits to connect with two adjacent comb rows, and in some species runs all the way along the comb rows. This forms a mechanical system for transmitting the beat rhythm from the combs to the balancers, via water disturbances created by the cilia. The relationship of ctenophores to the rest of Metazoa is very important to our understanding of the early evolution of animals and the origin of multicellularity. It has been the focus of debate for many years. Ctenophores have been purported to be the sister lineage to the Bilateria, sister to the Cnidaria, sister to Cnidaria, Placozoa and Bilateria, and sister to all other animal phyla. A series of studies that looked at the presence and absence of members of gene families and signalling pathways (e.g., homeoboxes, nuclear receptors, the Wnt signaling pathway, and sodium channels) showed evidence congruent with the latter two scenarios, that ctenophores are either sister to Cnidaria, Placozoa and Bilateria or sister to all other animal phyla. Several more recent studies comparing complete sequenced genomes of ctenophores with other sequenced animal genomes have also supported ctenophores as the sister lineage to all other animals. This position would suggest that neural and muscle cell types were either lost in major animal lineages (e.g., Porifera) or that they evolved independently in the ctenophore lineage. However, other researchers have argued that the placement of Ctenophora as sister to all other animals is a statistical anomaly caused by the high rate of evolution in ctenophore genomes, and that Porifera (sponges) is the earliest-diverging animal phylum instead. Ctenophores and sponges are also the only known animal phyla that lack any true hox genes. It is uncertain how ctenophores control their buoyancy, but experiments have shown that some species rely on osmotic pressure to adapt to water of different densities. Their body fluids are normally as concentrated as seawater. If they enter less dense brackish water, the ciliary rosettes in the body cavity may pump this into the mesoglea to increase its bulk and decrease its density, to avoid sinking. Conversely if they move from brackish to full-strength seawater, the rosettes may pump water out of the mesoglea to reduce its volume and increase its density. On the other hand, in the late 1980s the Western Atlantic ctenophore Mnemiopsis leidyi was accidentally introduced into the Black Sea and Sea of Azov via the ballast tanks of ships, and has been blamed for causing sharp drops in fish catches by eating both fish larvae and small crustaceans that would otherwise feed the adult fish. Mnemiopsis is well equipped to invade new territories (although this was not predicted until after it so successfully colonized the Black Sea), as it can breed very rapidly and tolerate a wide range of water temperatures and salinities. The impact was increased by chronic overfishing, and by eutrophication that gave the entire ecosystem a short-term boost, causing the Mnemiopsis population to increase even faster than normal – and above all by the absence of efficient predators on these introduced ctenophores. Mnemiopsis populations in those areas were eventually brought under control by the accidental introduction of the Mnemiopsis-eating North American ctenophore Beroe ovata, and by a cooling of the local climate from 1991 to 1993, which significantly slowed the animal's metabolism. However the abundance of plankton in the area seems unlikely to be restored to pre-Mnemiopsis levels. For a phylum with relatively few species, ctenophores have a wide range of body plans. Coastal species need to be tough enough to withstand waves and swirling sediment particles, while some oceanic species are so fragile that it is very difficult to capture them intact for study. In addition oceanic species do not preserve well, and are known mainly from photographs and from observers' notes. Hence most attention has until recently concentrated on three coastal genera – Pleurobrachia, Beroe and Mnemiopsis. At least two textbooks base their descriptions of ctenophores on the cydippid Pleurobrachia. Ctenophores used to be regarded as ""dead ends"" in marine food chains because it was thought their low ratio of organic matter to salt and water made them a poor diet for other animals. It is also often difficult to identify the remains of ctenophores in the guts of possible predators, although the combs sometimes remain intact long enough to provide a clue. Detailed investigation of chum salmon, Oncorhynchus keta, showed that these fish digest ctenophores 20 times as fast as an equal weight of shrimps, and that ctenophores can provide a good diet if there are enough of them around. Beroids prey mainly on other ctenophores. Some jellyfish and turtles eat large quantities of ctenophores, and jellyfish may temporarily wipe out ctenophore populations. Since ctenophores and jellyfish often have large seasonal variations in population, most fish that prey on them are generalists, and may have a greater effect on populations than the specialist jelly-eaters. This is underlined by an observation of herbivorous fishes deliberately feeding on gelatinous zooplankton during blooms in the Red Sea. The larvae of some sea anemones are parasites on ctenophores, as are the larvae of some flatworms that parasitize fish when they reach adulthood. The outer surface bears usually eight comb rows, called swimming-plates, which are used for swimming. The rows are oriented to run from near the mouth (the ""oral pole"") to the opposite end (the ""aboral pole""), and are spaced more or less evenly around the body, although spacing patterns vary by species and in most species the comb rows extend only part of the distance from the aboral pole towards the mouth. The ""combs"" (also called ""ctenes"" or ""comb plates"") run across each row, and each consists of thousands of unusually long cilia, up to 2 millimeters (0.079 in). Unlike conventional cilia and flagella, which has a filament structure arranged in a 9 + 2 pattern, these cilia are arranged in a 9 + 3 pattern, where the extra compact filament is suspected to have a supporting function. These normally beat so that the propulsion stroke is away from the mouth, although they can also reverse direction. Hence ctenophores usually swim in the direction in which the mouth is pointing, unlike jellyfish. When trying to escape predators, one species can accelerate to six times its normal speed; some other species reverse direction as part of their escape behavior, by reversing the power stroke of the comb plate cilia. Most species are hermaphrodites—a single animal can produce both eggs and sperm, meaning it can fertilize its own egg, not needing a mate. Some are simultaneous hermaphrodites, which can produce both eggs and sperm at the same time. Others are sequential hermaphrodites, in which the eggs and sperm mature at different times. Fertilization is generally external, although platyctenids' eggs are fertilized inside their parents' bodies and kept there until they hatch. The young are generally planktonic and in most species look like miniature cydippids, gradually changing into their adult shapes as they grow. The exceptions are the beroids, whose young are miniature beroids with large mouths and no tentacles, and the platyctenids, whose young live as cydippid-like plankton until they reach near-adult size, but then sink to the bottom and rapidly metamorphose into the adult form. In at least some species, juveniles are capable of reproduction before reaching the adult size and shape. The combination of hermaphroditism and early reproduction enables small populations to grow at an explosive rate. The Lobata have a pair of lobes, which are muscular, cuplike extensions of the body that project beyond the mouth. Their inconspicuous tentacles originate from the corners of the mouth, running in convoluted grooves and spreading out over the inner surface of the lobes (rather than trailing far behind, as in the Cydippida). Between the lobes on either side of the mouth, many species of lobates have four auricles, gelatinous projections edged with cilia that produce water currents that help direct microscopic prey toward the mouth. This combination of structures enables lobates to feed continuously on suspended planktonic prey. Ctenophores may be abundant during the summer months in some coastal locations, but in other places they are uncommon and difficult to find. In bays where they occur in very high numbers, predation by ctenophores may control the populations of small zooplanktonic organisms such as copepods, which might otherwise wipe out the phytoplankton (planktonic plants), which are a vital part of marine food chains. One ctenophore, Mnemiopsis, has accidentally been introduced into the Black Sea, where it is blamed for causing fish stocks to collapse by eating both fish larvae and organisms that would otherwise have fed the fish. The situation was aggravated by other factors, such as over-fishing and long-term environmental changes that promoted the growth of the Mnemiopsis population. The later accidental introduction of Beroe helped to mitigate the problem, as Beroe preys on other ctenophores. Cydippid ctenophores have bodies that are more or less rounded, sometimes nearly spherical and other times more cylindrical or egg-shaped; the common coastal ""sea gooseberry,"" Pleurobrachia, sometimes has an egg-shaped body with the mouth at the narrow end, although some individuals are more uniformly round. From opposite sides of the body extends a pair of long, slender tentacles, each housed in a sheath into which it can be withdrawn. Some species of cydippids have bodies that are flattened to various extents, so that they are wider in the plane of the tentacles. Almost all ctenophores are predators, taking prey ranging from microscopic larvae and rotifers to the adults of small crustaceans; the exceptions are juveniles of two species, which live as parasites on the salps on which adults of their species feed. In favorable circumstances, ctenophores can eat ten times their own weight in a day. Only 100–150 species have been validated, and possibly another 25 have not been fully described and named. The textbook examples are cydippids with egg-shaped bodies and a pair of retractable tentacles fringed with tentilla (""little tentacles"") that are covered with colloblasts, sticky cells that capture prey. The phylum has a wide range of body forms, including the flattened, deep-sea platyctenids, in which the adults of most species lack combs, and the coastal beroids, which lack tentacles and prey on other ctenophores by using huge mouths armed with groups of large, stiffened cilia that act as teeth. These variations enable different species to build huge populations in the same area, because they specialize in different types of prey, which they capture by as wide a range of methods as spiders use. Lobates have eight comb-rows, originating at the aboral pole and usually not extending beyond the body to the lobes; in species with (four) auricles, the cilia edging the auricles are extensions of cilia in four of the comb rows. Most lobates are quite passive when moving through the water, using the cilia on their comb rows for propulsion, although Leucothea has long and active auricles whose movements also contribute to propulsion. Members of the lobate genera Bathocyroe and Ocyropsis can escape from danger by clapping their lobes, so that the jet of expelled water drives them backwards very quickly. Unlike cydippids, the movements of lobates' combs are coordinated by nerves rather than by water disturbances created by the cilia, yet combs on the same row beat in the same Mexican wave style as the mechanically coordinated comb rows of cydippids and beroids. This may have enabled lobates to grow larger than cydippids and to have shapes that are less egg-like. Ranging from about 1 millimeter (0.039 in) to 1.5 meters (4.9 ft) in size, ctenophores are the largest non-colonial animals that use cilia (""hairs"") as their main method of locomotion. Most species have eight strips, called comb rows, that run the length of their bodies and bear comb-like bands of cilia, called ""ctenes,"" stacked along the comb rows so that when the cilia beat, those of each comb touch the comb below. The name ""ctenophora"" means ""comb-bearing"", from the Greek κτείς (stem-form κτεν-) meaning ""comb"" and the Greek suffix -φορος meaning ""carrying"". The Cestida (""belt animals"") are ribbon-shaped planktonic animals, with the mouth and aboral organ aligned in the middle of opposite edges of the ribbon. There is a pair of comb-rows along each aboral edge, and tentilla emerging from a groove all along the oral edge, which stream back across most of the wing-like body surface. Cestids can swim by undulating their bodies as well as by the beating of their comb-rows. There are two known species, with worldwide distribution in warm, and warm-temperate waters: Cestum veneris (""Venus' girdle"") is among the largest ctenophores – up to 1.5 meters (4.9 ft) long, and can undulate slowly or quite rapidly. Velamen parallelum, which is typically less than 20 centimeters (0.66 ft) long, can move much faster in what has been described as a ""darting motion"". Most Platyctenida have oval bodies that are flattened in the oral-aboral direction, with a pair of tentilla-bearing tentacles on the aboral surface. They cling to and creep on surfaces by everting the pharynx and using it as a muscular ""foot"". All but one of the known platyctenid species lack comb-rows. Platyctenids are usually cryptically colored, live on rocks, algae, or the body surfaces of other invertebrates, and are often revealed by their long tentacles with many sidebranches, seen streaming off the back of the ctenophore into the current. Despite their soft, gelatinous bodies, fossils thought to represent ctenophores, apparently with no tentacles but many more comb-rows than modern forms, have been found in lagerstätten as far back as the early Cambrian, about 515 million years ago. The position of the ctenophores in the evolutionary family tree of animals has long been debated, and the majority view at present, based on molecular phylogenetics, is that cnidarians and bilaterians are more closely related to each other than either is to ctenophores. A recent molecular phylogenetics analysis concluded that the common ancestor of all modern ctenophores was cydippid-like, and that all the modern groups appeared relatively recently, probably after the Cretaceous–Paleogene extinction event 66 million years ago. Evidence accumulating since the 1980s indicates that the ""cydippids"" are not monophyletic, in other words do not include all and only the descendants of a single common ancestor, because all the other traditional ctenophore groups are descendants of various cydippids. Almost all ctenophores are predators – there are no vegetarians and only one genus that is partly parasitic. If food is plentiful, they can eat 10 times their own weight per day. While Beroe preys mainly on other ctenophores, other surface-water species prey on zooplankton (planktonic animals) ranging in size from the microscopic, including mollusc and fish larvae, to small adult crustaceans such as copepods, amphipods, and even krill. Members of the genus Haeckelia prey on jellyfish and incorporate their prey's nematocysts (stinging cells) into their own tentacles instead of colloblasts. Ctenophores have been compared to spiders in their wide range of techniques from capturing prey – some hang motionless in the water using their tentacles as ""webs"", some are ambush predators like Salticid jumping spiders, and some dangle a sticky droplet at the end of a fine thread, as bolas spiders do. This variety explains the wide range of body forms in a phylum with rather few species. The two-tentacled ""cydippid"" Lampea feeds exclusively on salps, close relatives of sea-squirts that form large chain-like floating colonies, and juveniles of Lampea attach themselves like parasites to salps that are too large for them to swallow. Members of the cydippid genus Pleurobrachia and the lobate Bolinopsis often reach high population densities at the same place and time because they specialize in different types of prey: Pleurobrachia's long tentacles mainly capture relatively strong swimmers such as adult copepods, while Bolinopsis generally feeds on smaller, weaker swimmers such as rotifers and mollusc and crustacean larvae. The largest single sensory feature is the aboral organ (at the opposite end from the mouth). Its main component is a statocyst, a balance sensor consisting of a statolith, a solid particle supported on four bundles of cilia, called ""balancers"", that sense its orientation. The statocyst is protected by a transparent dome made of long, immobile cilia. A ctenophore does not automatically try to keep the statolith resting equally on all the balancers. Instead its response is determined by the animal's ""mood"", in other words the overall state of the nervous system. For example, if a ctenophore with trailing tentacles captures prey, it will often put some comb rows into reverse, spinning the mouth towards the prey. Like sponges and cnidarians, ctenophores have two main layers of cells that sandwich a middle layer of jelly-like material, which is called the mesoglea in cnidarians and ctenophores; more complex animals have three main cell layers and no intermediate jelly-like layer. Hence ctenophores and cnidarians have traditionally been labelled diploblastic, along with sponges. Both ctenophores and cnidarians have a type of muscle that, in more complex animals, arises from the middle cell layer, and as a result some recent text books classify ctenophores as triploblastic, while others still regard them as diploblastic. Ctenophora (/tᵻˈnɒfərə/; singular ctenophore, /ˈtɛnəfɔːr/ or /ˈtiːnəfɔːr/; from the Greek κτείς kteis 'comb' and φέρω pherō 'carry'; commonly known as comb jellies) is a phylum of animals that live in marine waters worldwide. Their most distinctive feature is the ‘combs’ – groups of cilia which they use for swimming – they are the largest animals that swim by means of cilia. Adults of various species range from a few millimeters to 1.5 m (4 ft 11 in) in size. Like cnidarians, their bodies consist of a mass of jelly, with one layer of cells on the outside and another lining the internal cavity. In ctenophores, these layers are two cells deep, while those in cnidarians are only one cell deep. Some authors combined ctenophores and cnidarians in one phylum, Coelenterata, as both groups rely on water flow through the body cavity for both digestion and respiration. Increasing awareness of the differences persuaded more recent authors to classify them as separate phyla. Development of the fertilized eggs is direct, in other words there is no distinctive larval form, and juveniles of all groups generally resemble miniature cydippid adults. In the genus Beroe the juveniles, like the adults, lack tentacles and tentacle sheaths. In most species the juveniles gradually develop the body forms of their parents. In some groups, such as the flat, bottom-dwelling platyctenids, the juveniles behave more like true larvae, as they live among the plankton and thus occupy a different ecological niche from their parents and attain the adult form by a more radical metamorphosis, after dropping to the sea-floor. Since all modern ctenophores except the beroids have cydippid-like larvae, it has widely been assumed that their last common ancestor also resembled cydippids, having an egg-shaped body and a pair of retractable tentacles. Richard Harbison's purely morphological analysis in 1985 concluded that the cydippids are not monophyletic, in other words do not contain all and only the descendants of a single common ancestor that was itself a cydippid. Instead he found that various cydippid families were more similar to members of other ctenophore orders than to other cydippids. He also suggested that the last common ancestor of modern ctenophores was either cydippid-like or beroid-like. A molecular phylogeny analysis in 2001, using 26 species, including 4 recently discovered ones, confirmed that the cydippids are not monophyletic and concluded that the last common ancestor of modern ctenophores was cydippid-like. It also found that the genetic differences between these species were very small – so small that the relationships between the Lobata, Cestida and Thalassocalycida remained uncertain. This suggests that the last common ancestor of modern ctenophores was relatively recent, and perhaps was lucky enough to survive the Cretaceous–Paleogene extinction event 65.5 million years ago while other lineages perished. When the analysis was broadened to include representatives of other phyla, it concluded that cnidarians are probably more closely related to bilaterians than either group is to ctenophores but that this diagnosis is uncertain."
Force,"Newton's Third Law is a result of applying symmetry to situations where forces can be attributed to the presence of different objects. The third law means that all forces are interactions between different bodies,[Note 3] and thus that there is no such thing as a unidirectional force or a force that acts on only one body. Whenever a first body exerts a force F on a second body, the second body exerts a force −F on the first body. F and −F are equal in magnitude and opposite in direction. This law is sometimes referred to as the action-reaction law, with F called the ""action"" and −F the ""reaction"". The action and the reaction are simultaneous: Forces act in a particular direction and have sizes dependent upon how strong the push or pull is. Because of these characteristics, forces are classified as ""vector quantities"". This means that forces follow a different set of mathematical rules than physical quantities that do not have direction (denoted scalar quantities). For example, when determining what happens when two forces act on the same object, it is necessary to know both the magnitude and the direction of both forces to calculate the result. If both of these pieces of information are not known for each force, the situation is ambiguous. For example, if you know that two people are pulling on the same rope with known magnitudes of force but you do not know which direction either person is pulling, it is impossible to determine what the acceleration of the rope will be. The two people could be pulling against each other as in tug of war or the two people could be pulling in the same direction. In this simple one-dimensional example, without knowing the direction of the forces it is impossible to decide whether the net force is the result of adding the two force magnitudes or subtracting one from the other. Associating forces with vectors avoids such problems. The shortcomings of Aristotelian physics would not be fully corrected until the 17th century work of Galileo Galilei, who was influenced by the late Medieval idea that objects in forced motion carried an innate force of impetus. Galileo constructed an experiment in which stones and cannonballs were both rolled down an incline to disprove the Aristotelian theory of motion early in the 17th century. He showed that the bodies were accelerated by gravity to an extent that was independent of their mass and argued that objects retain their velocity unless acted on by a force, for example friction. However, attempting to reconcile electromagnetic theory with two observations, the photoelectric effect, and the nonexistence of the ultraviolet catastrophe, proved troublesome. Through the work of leading theoretical physicists, a new theory of electromagnetism was developed using quantum mechanics. This final modification to electromagnetic theory ultimately led to quantum electrodynamics (or QED), which fully describes all electromagnetic phenomena as being mediated by wave–particles known as photons. In QED, photons are the fundamental exchange particle, which described all interactions relating to electromagnetism including the electromagnetic force.[Note 4] The development of fundamental theories for forces proceeded along the lines of unification of disparate ideas. For example, Isaac Newton unified the force responsible for objects falling at the surface of the Earth with the force responsible for the orbits of celestial mechanics in his universal theory of gravitation. Michael Faraday and James Clerk Maxwell demonstrated that electric and magnetic forces were unified through one consistent theory of electromagnetism. In the 20th century, the development of quantum mechanics led to a modern understanding that the first three fundamental forces (all except gravity) are manifestations of matter (fermions) interacting by exchanging virtual particles called gauge bosons. This standard model of particle physics posits a similarity between the forces and led scientists to predict the unification of the weak and electromagnetic forces in electroweak theory subsequently confirmed by observation. The complete formulation of the standard model predicts an as yet unobserved Higgs mechanism, but observations such as neutrino oscillations indicate that the standard model is incomplete. A Grand Unified Theory allowing for the combination of the electroweak interaction with the strong force is held out as a possibility with candidate theories such as supersymmetry proposed to accommodate some of the outstanding unsolved problems in physics. Physicists are still attempting to develop self-consistent unification models that would combine all four fundamental interactions into a theory of everything. Einstein tried and failed at this endeavor, but currently the most popular approach to answering this question is string theory.:212–219 The connection between macroscopic nonconservative forces and microscopic conservative forces is described by detailed treatment with statistical mechanics. In macroscopic closed systems, nonconservative forces act to change the internal energies of the system, and are often associated with the transfer of heat. According to the Second law of thermodynamics, nonconservative forces necessarily result in energy transformations within closed systems from ordered to more random conditions as entropy increases. where  is the mass of the object,  is the velocity of the object and  is the distance to the center of the circular path and  is the unit vector pointing in the radial direction outwards from the center. This means that the unbalanced centripetal force felt by any object is always directed toward the center of the curving path. Such forces act perpendicular to the velocity vector associated with the motion of an object, and therefore do not change the speed of the object (magnitude of the velocity), but only the direction of the velocity vector. The unbalanced force that accelerates an object can be resolved into a component that is perpendicular to the path, and one that is tangential to the path. This yields both the tangential force, which accelerates the object by either slowing it down or speeding it up, and the radial (centripetal) force, which changes its direction. Torque is the rotation equivalent of force in the same way that angle is the rotational equivalent for position, angular velocity for velocity, and angular momentum for momentum. As a consequence of Newton's First Law of Motion, there exists rotational inertia that ensures that all bodies maintain their angular momentum unless acted upon by an unbalanced torque. Likewise, Newton's Second Law of Motion can be used to derive an analogous equation for the instantaneous angular acceleration of the rigid body: Newton's Second Law asserts the direct proportionality of acceleration to force and the inverse proportionality of acceleration to mass. Accelerations can be defined through kinematic measurements. However, while kinematics are well-described through reference frame analysis in advanced physics, there are still deep questions that remain as to what is the proper definition of mass. General relativity offers an equivalence between space-time and mass, but lacking a coherent theory of quantum gravity, it is unclear as to how or whether this connection is relevant on microscales. With some justification, Newton's second law can be taken as a quantitative definition of mass by writing the law as an equality; the relative units of force and mass then are fixed. The concept of inertia can be further generalized to explain the tendency of objects to continue in many different forms of constant motion, even those that are not strictly constant velocity. The rotational inertia of planet Earth is what fixes the constancy of the length of a day and the length of a year. Albert Einstein extended the principle of inertia further when he explained that reference frames subject to constant acceleration, such as those free-falling toward a gravitating object, were physically equivalent to inertial reference frames. This is why, for example, astronauts experience weightlessness when in free-fall orbit around the Earth, and why Newton's Laws of Motion are more easily discernible in such environments. If an astronaut places an object with mass in mid-air next to himself, it will remain stationary with respect to the astronaut due to its inertia. This is the same thing that would occur if the astronaut and the object were in intergalactic space with no net force of gravity acting on their shared reference frame. This principle of equivalence was one of the foundational underpinnings for the development of the general theory of relativity. Pushing against an object on a frictional surface can result in a situation where the object does not move because the applied force is opposed by static friction, generated between the object and the table surface. For a situation with no movement, the static friction force exactly balances the applied force resulting in no acceleration. The static friction increases or decreases in response to the applied force up to an upper limit determined by the characteristics of the contact between the surface and the object. Through combining the definition of electric current as the time rate of change of electric charge, a rule of vector multiplication called Lorentz's Law describes the force on a charge moving in a magnetic field. The connection between electricity and magnetism allows for the description of a unified electromagnetic force that acts on a charge. This force can be written as a sum of the electrostatic force (due to the electric field) and the magnetic force (due to the magnetic field). Fully stated, this is the law: The weak force is due to the exchange of the heavy W and Z bosons. Its most familiar effect is beta decay (of neutrons in atomic nuclei) and the associated radioactivity. The word ""weak"" derives from the fact that the field strength is some 1013 times less than that of the strong force. Still, it is stronger than gravity over short distances. A consistent electroweak theory has also been developed, which shows that electromagnetic forces and the weak force are indistinguishable at a temperatures in excess of approximately 1015 kelvins. Such temperatures have been probed in modern particle accelerators and show the conditions of the universe in the early moments of the Big Bang. The pound-force has a metric counterpart, less commonly used than the newton: the kilogram-force (kgf) (sometimes kilopond), is the force exerted by standard gravity on one kilogram of mass. The kilogram-force leads to an alternate, but rarely used unit of mass: the metric slug (sometimes mug or hyl) is that mass that accelerates at 1 m·s−2 when subjected to a force of 1 kgf. The kilogram-force is not a part of the modern SI system, and is generally deprecated; however it still sees use for some purposes as expressing aircraft weight, jet thrust, bicycle spoke tension, torque wrench settings and engine output torque. Other arcane units of force include the sthène, which is equivalent to 1000 N, and the kip, which is equivalent to 1000 lbf. Historically, forces were first quantitatively investigated in conditions of static equilibrium where several forces canceled each other out. Such experiments demonstrate the crucial properties that forces are additive vector quantities: they have magnitude and direction. When two forces act on a point particle, the resulting force, the resultant (also called the net force), can be determined by following the parallelogram rule of vector addition: the addition of two vectors represented by sides of a parallelogram, gives an equivalent resultant vector that is equal in magnitude and direction to the transversal of the parallelogram. The magnitude of the resultant varies from the difference of the magnitudes of the two forces to their sum, depending on the angle between their lines of action. However, if the forces are acting on an extended body, their respective lines of application must also be specified in order to account for their effects on the motion of the body. What we now call gravity was not identified as a universal force until the work of Isaac Newton. Before Newton, the tendency for objects to fall towards the Earth was not understood to be related to the motions of celestial objects. Galileo was instrumental in describing the characteristics of falling objects by determining that the acceleration of every object in free-fall was constant and independent of the mass of the object. Today, this acceleration due to gravity towards the surface of the Earth is usually designated as  and has a magnitude of about 9.81 meters per second squared (this measurement is taken from sea level and may vary depending on location), and points toward the center of the Earth. This observation means that the force of gravity on an object at the Earth's surface is directly proportional to the object's mass. Thus an object that has a mass of  will experience a force: Dynamic equilibrium was first described by Galileo who noticed that certain assumptions of Aristotelian physics were contradicted by observations and logic. Galileo realized that simple velocity addition demands that the concept of an ""absolute rest frame"" did not exist. Galileo concluded that motion in a constant velocity was completely equivalent to rest. This was contrary to Aristotle's notion of a ""natural state"" of rest that objects with mass naturally approached. Simple experiments showed that Galileo's understanding of the equivalence of constant velocity and rest were correct. For example, if a mariner dropped a cannonball from the crow's nest of a ship moving at a constant velocity, Aristotelian physics would have the cannonball fall straight down while the ship moved beneath it. Thus, in an Aristotelian universe, the falling cannonball would land behind the foot of the mast of a moving ship. However, when this experiment is actually conducted, the cannonball always falls at the foot of the mast, as if the cannonball knows to travel with the ship despite being separated from it. Since there is no forward horizontal force being applied on the cannonball as it falls, the only conclusion left is that the cannonball continues to move with the same velocity as the boat as it falls. Thus, no force is required to keep the cannonball moving at the constant forward velocity. Philosophers in antiquity used the concept of force in the study of stationary and moving objects and simple machines, but thinkers such as Aristotle and Archimedes retained fundamental errors in understanding force. In part this was due to an incomplete understanding of the sometimes non-obvious force of friction, and a consequently inadequate view of the nature of natural motion. A fundamental error was the belief that a force is required to maintain motion, even at a constant velocity. Most of the previous misunderstandings about motion and force were eventually corrected by Galileo Galilei and Sir Isaac Newton. With his mathematical insight, Sir Isaac Newton formulated laws of motion that were not improved-on for nearly three hundred years. By the early 20th century, Einstein developed a theory of relativity that correctly predicted the action of forces on objects with increasing momenta near the speed of light, and also provided insight into the forces produced by gravitation and inertia. Newton came to realize that the effects of gravity might be observed in different ways at larger distances. In particular, Newton determined that the acceleration of the Moon around the Earth could be ascribed to the same force of gravity if the acceleration due to gravity decreased as an inverse square law. Further, Newton realized that the acceleration due to gravity is proportional to the mass of the attracting body. Combining these ideas gives a formula that relates the mass () and the radius () of the Earth to the gravitational acceleration: For certain physical scenarios, it is impossible to model forces as being due to gradient of potentials. This is often due to macrophysical considerations that yield forces as arising from a macroscopic statistical average of microstates. For example, friction is caused by the gradients of numerous electrostatic potentials between the atoms, but manifests as a force model that is independent of any macroscale position vector. Nonconservative forces other than friction include other contact forces, tension, compression, and drag. However, for any sufficiently detailed description, all these forces are the results of conservative ones since each of these macroscopic forces are the net results of the gradients of microscopic potentials. Newton's First Law of Motion states that objects continue to move in a state of constant velocity unless acted upon by an external net force or resultant force. This law is an extension of Galileo's insight that constant velocity was associated with a lack of net force (see a more detailed description of this below). Newton proposed that every object with mass has an innate inertia that functions as the fundamental equilibrium ""natural state"" in place of the Aristotelian idea of the ""natural state of rest"". That is, the first law contradicts the intuitive Aristotelian belief that a net force is required to keep an object moving with constant velocity. By making rest physically indistinguishable from non-zero constant velocity, Newton's First Law directly connects inertia with the concept of relative velocities. Specifically, in systems where objects are moving with different velocities, it is impossible to determine which object is ""in motion"" and which object is ""at rest"". In other words, to phrase matters more technically, the laws of physics are the same in every inertial frame of reference, that is, in all frames related by a Galilean transformation. Since forces are perceived as pushes or pulls, this can provide an intuitive understanding for describing forces. As with other physical concepts (e.g. temperature), the intuitive understanding of forces is quantified using precise operational definitions that are consistent with direct observations and compared to a standard measurement scale. Through experimentation, it is determined that laboratory measurements of forces are fully consistent with the conceptual definition of force offered by Newtonian mechanics. With modern insights into quantum mechanics and technology that can accelerate particles close to the speed of light, particle physics has devised a Standard Model to describe forces between particles smaller than atoms. The Standard Model predicts that exchanged particles called gauge bosons are the fundamental means by which forces are emitted and absorbed. Only four main interactions are known: in order of decreasing strength, they are: strong, electromagnetic, weak, and gravitational.:2–10:79 High-energy particle physics observations made during the 1970s and 1980s confirmed that the weak and electromagnetic forces are expressions of a more fundamental electroweak interaction. In modern particle physics, forces and the acceleration of particles are explained as a mathematical by-product of exchange of momentum-carrying gauge bosons. With the development of quantum field theory and general relativity, it was realized that force is a redundant concept arising from conservation of momentum (4-momentum in relativity and momentum of virtual particles in quantum electrodynamics). The conservation of momentum can be directly derived from the homogeneity or symmetry of space and so is usually considered more fundamental than the concept of a force. Thus the currently known fundamental forces are considered more accurately to be ""fundamental interactions"".:199–128 When particle A emits (creates) or absorbs (annihilates) virtual particle B, a momentum conservation results in recoil of particle A making impression of repulsion or attraction between particles A A' exchanging by B. This description applies to all forces arising from fundamental interactions. While sophisticated mathematical descriptions are needed to predict, in full detail, the accurate result of such interactions, there is a conceptually simple way to describe such interactions through the use of Feynman diagrams. In a Feynman diagram, each matter particle is represented as a straight line (see world line) traveling through time, which normally increases up or to the right in the diagram. Matter and anti-matter particles are identical except for their direction of propagation through the Feynman diagram. World lines of particles intersect at interaction vertices, and the Feynman diagram represents any force arising from an interaction as occurring at the vertex with an associated instantaneous change in the direction of the particle world lines. Gauge bosons are emitted away from the vertex as wavy lines and, in the case of virtual particle exchange, are absorbed at an adjacent vertex. where  is the relevant cross-sectional area for the volume for which the stress-tensor is being calculated. This formalism includes pressure terms associated with forces that act normal to the cross-sectional area (the matrix diagonals of the tensor) as well as shear terms associated with forces that act parallel to the cross-sectional area (the off-diagonal elements). The stress tensor accounts for forces that cause all strains (deformations) including also tensile stresses and compressions.:133–134:38-1–38-11 It is a common misconception to ascribe the stiffness and rigidity of solid matter to the repulsion of like charges under the influence of the electromagnetic force. However, these characteristics actually result from the Pauli exclusion principle.[citation needed] Since electrons are fermions, they cannot occupy the same quantum mechanical state as other electrons. When the electrons in a material are densely packed together, there are not enough lower energy quantum mechanical states for them all, so some of them must be in higher energy states. This means that it takes energy to pack them together. While this effect is manifested macroscopically as a structural force, it is technically only the result of the existence of a finite set of electron states. Aristotle provided a philosophical discussion of the concept of a force as an integral part of Aristotelian cosmology. In Aristotle's view, the terrestrial sphere contained four elements that come to rest at different ""natural places"" therein. Aristotle believed that motionless objects on Earth, those composed mostly of the elements earth and water, to be in their natural place on the ground and that they will stay that way if left alone. He distinguished between the innate tendency of objects to find their ""natural place"" (e.g., for heavy bodies to fall), which led to ""natural motion"", and unnatural or forced motion, which required continued application of a force. This theory, based on the everyday experience of how objects move, such as the constant application of a force needed to keep a cart moving, had conceptual trouble accounting for the behavior of projectiles, such as the flight of arrows. The place where the archer moves the projectile was at the start of the flight, and while the projectile sailed through the air, no discernible efficient cause acts on it. Aristotle was aware of this problem and proposed that the air displaced through the projectile's path carries the projectile to its target. This explanation demands a continuum like air for change of place in general. Newton's laws and Newtonian mechanics in general were first developed to describe how forces affect idealized point particles rather than three-dimensional objects. However, in real life, matter has extended structure and forces that act on one part of an object might affect other parts of an object. For situations where lattice holding together the atoms in an object is able to flow, contract, expand, or otherwise change shape, the theories of continuum mechanics describe the way forces affect the material. For example, in extended fluids, differences in pressure result in forces being directed along the pressure gradients as follows: The origin of electric and magnetic fields would not be fully explained until 1864 when James Clerk Maxwell unified a number of earlier theories into a set of 20 scalar equations, which were later reformulated into 4 vector equations by Oliver Heaviside and Josiah Willard Gibbs. These ""Maxwell Equations"" fully described the sources of the fields as being stationary and moving charges, and the interactions of the fields themselves. This led Maxwell to discover that electric and magnetic fields could be ""self-generating"" through a wave that traveled at a speed that he calculated to be the speed of light. This insight united the nascent fields of electromagnetic theory with optics and led directly to a complete description of the electromagnetic spectrum. For instance, while traveling in a moving vehicle at a constant velocity, the laws of physics do not change from being at rest. A person can throw a ball straight up in the air and catch it as it falls down without worrying about applying a force in the direction the vehicle is moving. This is true even though another person who is observing the moving vehicle pass by also observes the ball follow a curving parabolic path in the same direction as the motion of the vehicle. It is the inertia of the ball associated with its constant velocity in the direction of the vehicle's motion that ensures the ball continues to move forward even as it is thrown up and falls back down. From the perspective of the person in the car, the vehicle and everything inside of it is at rest: It is the outside world that is moving with a constant speed in the opposite direction. Since there is no experiment that can distinguish whether it is the vehicle that is at rest or the outside world that is at rest, the two situations are considered to be physically indistinguishable. Inertia therefore applies equally well to constant velocity motion as it does to rest. In this equation, a dimensional constant  is used to describe the relative strength of gravity. This constant has come to be known as Newton's Universal Gravitation Constant, though its value was unknown in Newton's lifetime. Not until 1798 was Henry Cavendish able to make the first measurement of  using a torsion balance; this was widely reported in the press as a measurement of the mass of the Earth since knowing  could allow one to solve for the Earth's mass given the above equation. Newton, however, realized that since all celestial bodies followed the same laws of motion, his law of gravity had to be universal. Succinctly stated, Newton's Law of Gravitation states that the force on a spherical object of mass  due to the gravitational pull of mass  is The notion ""force"" keeps its meaning in quantum mechanics, though one is now dealing with operators instead of classical variables and though the physics is now described by the Schrödinger equation instead of Newtonian equations. This has the consequence that the results of a measurement are now sometimes ""quantized"", i.e. they appear in discrete portions. This is, of course, difficult to imagine in the context of ""forces"". However, the potentials V(x,y,z) or fields, from which the forces generally can be derived, are treated similar to classical position variables, i.e., . The normal force is due to repulsive forces of interaction between atoms at close contact. When their electron clouds overlap, Pauli repulsion (due to fermionic nature of electrons) follows resulting in the force that acts in a direction normal to the surface interface between two objects.:93 The normal force, for example, is responsible for the structural integrity of tables and floors as well as being the force that responds whenever an external force pushes on a solid object. An example of the normal force in action is the impact force on an object crashing into an immobile surface. A static equilibrium between two forces is the most usual way of measuring forces, using simple devices such as weighing scales and spring balances. For example, an object suspended on a vertical spring scale experiences the force of gravity acting on the object balanced by a force applied by the ""spring reaction force"", which equals the object's weight. Using such tools, some quantitative force laws were discovered: that the force of gravity is proportional to volume for objects of constant density (widely exploited for millennia to define standard weights); Archimedes' principle for buoyancy; Archimedes' analysis of the lever; Boyle's law for gas pressure; and Hooke's law for springs. These were all formulated and experimentally verified before Isaac Newton expounded his Three Laws of Motion. Since then, and so far, general relativity has been acknowledged as the theory that best explains gravity. In GR, gravitation is not viewed as a force, but rather, objects moving freely in gravitational fields travel under their own inertia in straight lines through curved space-time – defined as the shortest space-time path between two space-time events. From the perspective of the object, all motion occurs as if there were no gravitation whatsoever. It is only when observing the motion in a global sense that the curvature of space-time can be observed and the force is inferred from the object's curved path. Thus, the straight line path in space-time is seen as a curved line in space, and it is called the ballistic trajectory of the object. For example, a basketball thrown from the ground moves in a parabola, as it is in a uniform gravitational field. Its space-time trajectory (when the extra ct dimension is added) is almost a straight line, slightly curved (with the radius of curvature of the order of few light-years). The time derivative of the changing momentum of the object is what we label as ""gravitational force"". A conservative force that acts on a closed system has an associated mechanical work that allows energy to convert only between kinetic or potential forms. This means that for a closed system, the net mechanical energy is conserved whenever a conservative force acts on the system. The force, therefore, is related directly to the difference in potential energy between two different locations in space, and can be considered to be an artifact of the potential field in the same way that the direction and amount of a flow of water can be considered to be an artifact of the contour map of the elevation of an area. As well as being added, forces can also be resolved into independent components at right angles to each other. A horizontal force pointing northeast can therefore be split into two forces, one pointing north, and one pointing east. Summing these component forces using vector addition yields the original force. Resolving force vectors into components of a set of basis vectors is often a more mathematically clean way to describe forces than using magnitudes and directions. This is because, for orthogonal components, the components of the vector sum are uniquely determined by the scalar addition of the components of the individual vectors. Orthogonal components are independent of each other because forces acting at ninety degrees to each other have no effect on the magnitude or direction of the other. Choosing a set of orthogonal basis vectors is often done by considering what set of basis vectors will make the mathematics most convenient. Choosing a basis vector that is in the same direction as one of the forces is desirable, since that force would then have only one non-zero component. Orthogonal force vectors can be three-dimensional with the third component being at right-angles to the other two. All of the forces in the universe are based on four fundamental interactions. The strong and weak forces are nuclear forces that act only at very short distances, and are responsible for the interactions between subatomic particles, including nucleons and compound nuclei. The electromagnetic force acts between electric charges, and the gravitational force acts between masses. All other forces in nature derive from these four fundamental interactions. For example, friction is a manifestation of the electromagnetic force acting between the atoms of two surfaces, and the Pauli exclusion principle, which does not permit atoms to pass through each other. Similarly, the forces in springs, modeled by Hooke's law, are the result of electromagnetic forces and the Exclusion Principle acting together to return an object to its equilibrium position. Centrifugal forces are acceleration forces that arise simply from the acceleration of rotating frames of reference.:12-11:359 This means that in a closed system of particles, there are no internal forces that are unbalanced. That is, the action-reaction force shared between any two objects in a closed system will not cause the center of mass of the system to accelerate. The constituent objects only accelerate with respect to each other, the system itself remains unaccelerated. Alternatively, if an external force acts on the system, then the center of mass will experience an acceleration proportional to the magnitude of the external force divided by the mass of the system.:19-1 The strong force only acts directly upon elementary particles. However, a residual of the force is observed between hadrons (the best known example being the force that acts between nucleons in atomic nuclei) as the nuclear force. Here the strong force acts indirectly, transmitted as gluons, which form part of the virtual pi and rho mesons, which classically transmit the nuclear force (see this topic for more). The failure of many searches for free quarks has shown that the elementary particles affected are not directly observable. This phenomenon is called color confinement. Tension forces can be modeled using ideal strings that are massless, frictionless, unbreakable, and unstretchable. They can be combined with ideal pulleys, which allow ideal strings to switch physical direction. Ideal strings transmit tension forces instantaneously in action-reaction pairs so that if two objects are connected by an ideal string, any force directed along the string by the first object is accompanied by a force directed along the string in the opposite direction by the second object. By connecting the same string multiple times to the same object through the use of a set-up that uses movable pulleys, the tension force on a load can be multiplied. For every string that acts on a load, another factor of the tension force in the string acts on the load. However, even though such machines allow for an increase in force, there is a corresponding increase in the length of string that must be displaced in order to move the load. These tandem effects result ultimately in the conservation of mechanical energy since the work done on the load is the same no matter how complicated the machine. A simple case of dynamic equilibrium occurs in constant velocity motion across a surface with kinetic friction. In such a situation, a force is applied in the direction of motion while the kinetic friction force exactly opposes the applied force. This results in zero net force, but since the object started with a non-zero velocity, it continues to move with a non-zero velocity. Aristotle misinterpreted this motion as being caused by the applied force. However, when kinetic friction is taken into consideration it is clear that there is no net force causing constant velocity motion. It was only the orbit of the planet Mercury that Newton's Law of Gravitation seemed not to fully explain. Some astrophysicists predicted the existence of another planet (Vulcan) that would explain the discrepancies; however, despite some early indications, no such planet could be found. When Albert Einstein formulated his theory of general relativity (GR) he turned his attention to the problem of Mercury's orbit and found that his theory added a correction, which could account for the discrepancy. This was the first time that Newton's Theory of Gravity had been shown to be less correct than an alternative. However, already in quantum mechanics there is one ""caveat"", namely the particles acting onto each other do not only possess the spatial variable, but also a discrete intrinsic angular momentum-like variable called the ""spin"", and there is the Pauli principle relating the space and the spin variables. Depending on the value of the spin, identical particles split into two different classes, fermions and bosons. If two identical fermions (e.g. electrons) have a symmetric spin function (e.g. parallel spins) the spatial variables must be antisymmetric (i.e. they exclude each other from their places much as if there was a repulsive force), and vice versa, i.e. for antiparallel spins the position variables must be symmetric (i.e. the apparent force must be attractive). Thus in the case of two fermions there is a strictly negative correlation between spatial and spin variables, whereas for two bosons (e.g. quanta of electromagnetic waves, photons) the correlation is strictly positive."
Southern_California,"Downtown San Diego is the central business district of San Diego, though the city is filled with business districts. These include Carmel Valley, Del Mar Heights, Mission Valley, Rancho Bernardo, Sorrento Mesa, and University City. Most of these districts are located in Northern San Diego and some within North County regions. The San Bernardino-Riverside area maintains the business districts of Downtown San Bernardino, Hospitality Business/Financial Centre, University Town which are in San Bernardino and Downtown Riverside. The motion picture, television, and music industry is centered on the Los Angeles in southern California. Hollywood, a district within Los Angeles, is also a name associated with the motion picture industry. Headquartered in southern California are The Walt Disney Company (which also owns ABC), Sony Pictures, Universal, MGM, Paramount Pictures, 20th Century Fox, and Warner Brothers. Universal, Warner Brothers, and Sony also run major record companies as well. Southern California includes the heavily built-up urban area stretching along the Pacific coast from Ventura, through the Greater Los Angeles Area and the Inland Empire, and down to Greater San Diego. Southern California's population encompasses seven metropolitan areas, or MSAs: the Los Angeles metropolitan area, consisting of Los Angeles and Orange counties; the Inland Empire, consisting of Riverside and San Bernardino counties; the San Diego metropolitan area; the Oxnard–Thousand Oaks–Ventura metropolitan area; the Santa Barbara metro area; the San Luis Obispo metropolitan area; and the El Centro area. Out of these, three are heavy populated areas: the Los Angeles area with over 12 million inhabitants, the Riverside-San Bernardino area with over four million inhabitants, and the San Diego area with over 3 million inhabitants. For CSA metropolitan purposes, the five counties of Los Angeles, Orange, Riverside, San Bernardino, and Ventura are all combined to make up the Greater Los Angeles Area with over 17.5 million people. With over 22 million people, southern California contains roughly 60 percent of California's population. Southern California, often abbreviated SoCal, is a geographic and cultural region that generally comprises California's southernmost 10 counties. The region is traditionally described as ""eight counties"", based on demographics and economic ties: Imperial, Los Angeles, Orange, Riverside, San Bernardino, San Diego, Santa Barbara, and Ventura. The more extensive 10-county definition, including Kern and San Luis Obispo counties, is also used based on historical political divisions. Southern California is a major economic center for the state of California and the United States. In 1900, the Los Angeles Times defined southern California as including ""the seven counties of Los Angeles, San Bernardino, Orange, Riverside, San Diego, Ventura and Santa Barbara."" In 1999, the Times added a newer county—Imperial—to that list. Southern California consists of one Combined Statistical Area, eight Metropolitan Statistical Areas, one international metropolitan area, and multiple metropolitan divisions. The region is home to two extended metropolitan areas that exceed five million in population. These are the Greater Los Angeles Area at 17,786,419, and San Diego–Tijuana at 5,105,768. Of these metropolitan areas, the Los Angeles-Long Beach-Santa Ana metropolitan area, Riverside-San Bernardino-Ontario metropolitan area, and Oxnard-Thousand Oaks-Ventura metropolitan area form Greater Los Angeles; while the El Centro metropolitan area and San Diego-Carlsbad-San Marcos metropolitan area form the Southern Border Region. North of Greater Los Angeles are the Santa Barbara, San Luis Obispo, and Bakersfield metropolitan areas. Within the Los Angeles Area are the major business districts of Downtown Burbank, Downtown Santa Monica, Downtown Glendale and Downtown Long Beach. Los Angeles itself has many business districts including the Downtown Los Angeles central business district as well as those lining the Wilshire Boulevard Miracle Mile including Century City, Westwood and Warner Center in the San Fernando Valley. The 8- and 10-county definitions are not used for the greater Southern California Megaregion, one of the 11 megaregions of the United States. The megaregion's area is more expansive, extending east into Las Vegas, Nevada, and south across the Mexican border into Tijuana. Within southern California are two major cities, Los Angeles and San Diego, as well as three of the country's largest metropolitan areas. With a population of 3,792,621, Los Angeles is the most populous city in California and the second most populous in the United States. To the south and with a population of 1,307,402 is San Diego, the second most populous city in the state and the eighth most populous in the nation. Each year, the southern California area has about 10,000 earthquakes. Nearly all of them are so small that they are not felt. Only several hundred are greater than magnitude 3.0, and only about 15–20 are greater than magnitude 4.0. The magnitude 6.7 1994 Northridge earthquake was particularly destructive, causing a substantial number of deaths, injuries, and structural collapses. It caused the most property damage of any earthquake in U.S. history, estimated at over $20 billion. Southern California is also home to a large home grown surf and skateboard culture. Companies such as Volcom, Quiksilver, No Fear, RVCA, and Body Glove are all headquartered here. Professional skateboarder Tony Hawk, professional surfers Rob Machado, Tim Curran, Bobby Martinez, Pat O'Connell, Dane Reynolds, and Chris Ward, and professional snowboarder Shaun White live in southern California. Some of the world's legendary surf spots are in southern California as well, including Trestles, Rincon, The Wedge, Huntington Beach, and Malibu, and it is second only to the island of Oahu in terms of famous surf breaks. Some of the world's biggest extreme sports events, including the X Games, Boost Mobile Pro, and the U.S. Open of Surfing are all in southern California. Southern California is also important to the world of yachting. The annual Transpacific Yacht Race, or Transpac, from Los Angeles to Hawaii, is one of yachting's premier events. The San Diego Yacht Club held the America's Cup, the most prestigious prize in yachting, from 1988 to 1995 and hosted three America's Cup races during that time. Since the 1920s, motion pictures, petroleum and aircraft manufacturing have been major industries. In one of the richest agricultural regions in the U.S., cattle and citrus were major industries until farmlands were turned into suburbs. Although military spending cutbacks have had an impact, aerospace continues to be a major factor. Southern California is home to many major business districts. Central business districts (CBD) include Downtown Los Angeles, Downtown San Diego, Downtown San Bernardino, Downtown Bakersfield, South Coast Metro and Downtown Riverside. Professional sports teams in Southern California include teams from the NFL (Los Angeles Rams, San Diego Chargers); NBA (Los Angeles Lakers, Los Angeles Clippers); MLB (Los Angeles Dodgers, Los Angeles Angels of Anaheim, San Diego Padres); NHL (Los Angeles Kings, Anaheim Ducks); and MLS (LA Galaxy). As of the 2010 United States Census, southern California has a population of 22,680,010. Despite a reputation for high growth rates, southern California's rate grew less than the state average of 10.0% in the 2000s as California's growth became concentrated in the northern part of the state due to a stronger, tech-oriented economy in the Bay Area and an emerging Greater Sacramento region. The Tech Coast is a moniker that has gained use as a descriptor for the region's diversified technology and industrial base as well as its multitude of prestigious and world-renowned research universities and other public and private institutions. Amongst these include 5 University of California campuses (Irvine, Los Angeles, Riverside, Santa Barbara, and San Diego); 12 California State University campuses (Bakersfield, Channel Islands, Dominguez Hills, Fullerton, Los Angeles, Long Beach, Northridge, Pomona, San Bernardino, San Diego, San Marcos, and San Luis Obispo); and private institutions such as the California Institute of Technology, Chapman University, the Claremont Colleges (Claremont McKenna College, Harvey Mudd College, Pitzer College, Pomona College, and Scripps College), Loma Linda University, Loyola Marymount University, Occidental College, Pepperdine University, University of Redlands, University of San Diego, and the University of Southern California. From 2005 to 2014, there were two Major League Soccer teams in Los Angeles — the LA Galaxy and Chivas USA — that both played at the StubHub Center and were local rivals. However, Chivas were suspended following the 2014 MLS season, with a second MLS team scheduled to return in 2018. Subsequently, Californios (dissatisfied with inequitable taxes and land laws) and pro-slavery southerners in the lightly populated ""Cow Counties"" of southern California attempted three times in the 1850s to achieve a separate statehood or territorial status separate from Northern California. The last attempt, the Pico Act of 1859, was passed by the California State Legislature and signed by the State governor John B. Weller. It was approved overwhelmingly by nearly 75% of voters in the proposed Territory of Colorado. This territory was to include all the counties up to the then much larger Tulare County (that included what is now Kings, most of Kern, and part of Inyo counties) and San Luis Obispo County. The proposal was sent to Washington, D.C. with a strong advocate in Senator Milton Latham. However, the secession crisis following the election of Abraham Lincoln in 1860 led to the proposal never coming to a vote. Southern California contains a Mediterranean climate, with infrequent rain and many sunny days. Summers are hot and dry, while winters are a bit warm or mild and wet. Serious rain can occur unusually. In the summers, temperature ranges are 90-60's while as winters are 70-50's, usually all of Southern California have Mediterranean climate. But snow is very rare in the Southwest of the state, it occurs on the Southeast of the state. Traveling south on Interstate 5, the main gap to continued urbanization is Camp Pendleton. The cities and communities along Interstate 15 and Interstate 215 are so inter-related that Temecula and Murrieta have as much connection with the San Diego metropolitan area as they do with the Inland Empire. To the east, the United States Census Bureau considers the San Bernardino and Riverside County areas, Riverside-San Bernardino area as a separate metropolitan area from Los Angeles County. While many commute to L.A. and Orange Counties, there are some differences in development, as most of San Bernardino and Riverside Counties (the non-desert portions) were developed in the 1980s and 1990s. Newly developed exurbs formed in the Antelope Valley north of Los Angeles, the Victor Valley and the Coachella Valley with the Imperial Valley. Also, population growth was high in the Bakersfield-Kern County, Santa Maria and San Luis Obispo areas. Los Angeles (at 3.7 million people) and San Diego (at 1.3 million people), both in southern California, are the two largest cities in all of California (and two of the eight largest cities in the United States). In southern California there are also twelve cities with more than 200,000 residents and 34 cities over 100,000 in population. Many of southern California's most developed cities lie along or in close proximity to the coast, with the exception of San Bernardino and Riverside. Southern California consists of a heavily developed urban environment, home to some of the largest urban areas in the state, along with vast areas that have been left undeveloped. It is the third most populated megalopolis in the United States, after the Great Lakes Megalopolis and the Northeastern megalopolis. Much of southern California is famous for its large, spread-out, suburban communities and use of automobiles and highways. The dominant areas are Los Angeles, Orange County, San Diego, and Riverside-San Bernardino, each of which is the center of its respective metropolitan area, composed of numerous smaller cities and communities. The urban area is also host to an international metropolitan region in the form of San Diego–Tijuana, created by the urban area spilling over into Baja California. To the east is the Colorado Desert and the Colorado River at the border with Arizona, and the Mojave Desert at the border with the state of Nevada. To the south is the Mexico–United States border. Rugby is also a growing sport in southern California, particularly at the high school level, with increasing numbers of schools adding rugby as an official school sport. Though there is no official definition for the northern boundary of southern California, such a division has existed from the time when Mexico ruled California, and political disputes raged between the Californios of Monterey in the upper part and Los Angeles in the lower part of Alta California. Following the acquisition of California by the United States, the division continued as part of the attempt by several pro-slavery politicians to arrange the division of Alta California at 36 degrees, 30 minutes, the line of the Missouri Compromise. Instead, the passing of the Compromise of 1850 enabled California to be admitted to the Union as a free state, preventing southern California from becoming its own separate slave state. Many locals and tourists frequent the southern California coast for its popular beaches, and the desert city of Palm Springs is popular for its resort feel and nearby open spaces. Its counties of Los Angeles, Orange, San Diego, San Bernardino, and Riverside are the five most populous in the state and all are in the top 15 most populous counties in the United States. ""Southern California"" is not a formal geographic designation, and definitions of what constitutes southern California vary. Geographically, California's north-south midway point lies at exactly 37° 9' 58.23"" latitude, around 11 miles (18 km) south of San Jose; however, this does not coincide with popular use of the term. When the state is divided into two areas (northern and southern California), the term ""southern California"" usually refers to the ten southern-most counties of the state. This definition coincides neatly with the county lines at 35° 47′ 28″ north latitude, which form the northern borders of San Luis Obispo, Kern, and San Bernardino counties. Another definition for southern California uses Point Conception and the Tehachapi Mountains as the northern boundary. Many faults are able to produce a magnitude 6.7+ earthquake, such as the San Andreas Fault, which can produce a magnitude 8.0 event. Other faults include the San Jacinto Fault, the Puente Hills Fault, and the Elsinore Fault Zone. The USGS has released a California Earthquake forecast which models Earthquake occurrence in California. Southern California is divided culturally, politically, and economically into distinctive regions, each containing its own culture and atmosphere, anchored usually by a city with both national and sometimes global recognition, which are often the hub of economic activity for its respective region and being home to many tourist destinations. Each region is further divided into many culturally distinct areas but as a whole combine to create the southern California atmosphere. Orange County is a rapidly developing business center that includes Downtown Santa Ana, the South Coast Metro and Newport Center districts; as well as the Irvine business centers of The Irvine Spectrum, West Irvine, and international corporations headquartered at the University of California, Irvine. West Irvine includes the Irvine Tech Center and Jamboree Business Parks. The state is most commonly divided and promoted by its regional tourism groups as consisting of northern, central, and southern California regions. The two AAA Auto Clubs of the state, the California State Automobile Association and the Automobile Club of Southern California, choose to simplify matters by dividing the state along the lines where their jurisdictions for membership apply, as either northern or southern California, in contrast to the three-region point of view. Another influence is the geographical phrase South of the Tehachapis, which would split the southern region off at the crest of that transverse range, but in that definition, the desert portions of north Los Angeles County and eastern Kern and San Bernardino Counties would be included in the southern California region due to their remoteness from the central valley and interior desert landscape. College sports are also popular in southern California. The UCLA Bruins and the USC Trojans both field teams in NCAA Division I in the Pac-12 Conference, and there is a longtime rivalry between the schools. Southern California is also home to the Port of Los Angeles, the United States' busiest commercial port; the adjacent Port of Long Beach, the United States' second busiest container port; and the Port of San Diego. Southern California consists of one of the more varied collections of geologic, topographic, and natural ecosystem landscapes in a diversity outnumbering other major regions in the state and country. The region spans from Pacific Ocean islands, shorelines, beaches, and coastal plains, through the Transverse and Peninsular Ranges with their peaks, into the large and small interior valleys, to the vast deserts of California. Southern California is home to Los Angeles International Airport, the second-busiest airport in the United States by passenger volume (see World's busiest airports by passenger traffic) and the third by international passenger volume (see Busiest airports in the United States by international passenger traffic); San Diego International Airport the busiest single runway airport in the world; Van Nuys Airport, the world's busiest general aviation airport; major commercial airports at Orange County, Bakersfield, Ontario, Burbank and Long Beach; and numerous smaller commercial and general aviation airports. Southern California's economy is diverse and one of the largest in the United States. It is dominated and heavily dependent upon abundance of petroleum, as opposed to other regions where automobiles not nearly as dominant, the vast majority of transport runs on this fuel. Southern California is famous for tourism and Hollywood (film, television, and music). Other industries include software, automotive, ports, finance, tourism, biomedical, and regional logistics. The region was a leader in the housing bubble 2001–2007, and has been heavily impacted by the housing crash. Six of the seven lines of the commuter rail system, Metrolink, run out of Downtown Los Angeles, connecting Los Angeles, Ventura, San Bernardino, Riverside, Orange, and San Diego counties with the other line connecting San Bernardino, Riverside, and Orange counties directly."
Anthropology,"Waitz was influential among the British ethnologists. In 1863 the explorer Richard Francis Burton and the speech therapist James Hunt broke away from the Ethnological Society of London to form the Anthropological Society of London, which henceforward would follow the path of the new anthropology rather than just ethnology. It was the 2nd society dedicated to general anthropology in existence. Representatives from the French Société were present, though not Broca. In his keynote address, printed in the first volume of its new publication, The Anthropological Review, Hunt stressed the work of Waitz, adopting his definitions as a standard.[n 5] Among the first associates were the young Edward Burnett Tylor, inventor of cultural anthropology, and his brother Alfred Tylor, a geologist. Previously Edward had referred to himself as an ethnologist; subsequently, an anthropologist. Political anthropology concerns the structure of political systems, looked at from the basis of the structure of societies. Political anthropology developed as a discipline concerned primarily with politics in stateless societies, a new development started from the 1960s, and is still unfolding: anthropologists started increasingly to study more ""complex"" social settings in which the presence of states, bureaucracies and markets entered both ethnographic accounts and analysis of local phenomena. The turn towards complex societies meant that political themes were taken up at two main levels. First of all, anthropologists continued to study political organization and political phenomena that lay outside the state-regulated sphere (as in patron-client relations or tribal political organization). Second of all, anthropologists slowly started to develop a disciplinary concern with states and their institutions (and of course on the relationship between formal and informal political institutions). An anthropology of the state developed, and it is a most thriving field today. Geertz' comparative work on ""Negara"", the Balinese state is an early, famous example. Anthropologists, along with other social scientists, are working with the US military as part of the US Army's strategy in Afghanistan. The Christian Science Monitor reports that ""Counterinsurgency efforts focus on better grasping and meeting local needs"" in Afghanistan, under the Human Terrain System (HTS) program; in addition, HTS teams are working with the US military in Iraq. In 2009, the American Anthropological Association's Commission on the Engagement of Anthropology with the US Security and Intelligence Communities released its final report concluding, in part, that, ""When ethnographic investigation is determined by military missions, not subject to external review, where data collection occurs in the context of war, integrated into the goals of counterinsurgency, and in a potentially coercive environment – all characteristic factors of the HTS concept and its application – it can no longer be considered a legitimate professional exercise of anthropology. In summary, while we stress that constructive engagement between anthropology and the military is possible, CEAUSSIC suggests that the AAA emphasize the incompatibility of HTS with disciplinary ethics and practice for job seekers and that it further recognize the problem of allowing HTS to define the meaning of ""anthropology"" within DoD."" Anthropology is the study of humans and their societies in the past and present. Its main subdivisions are social anthropology and cultural anthropology, which describes the workings of societies around the world, linguistic anthropology, which investigates the influence of language in social life, and biological or physical anthropology, which concerns long-term development of the human organism. Archaeology, which studies past human cultures through investigation of physical evidence, is thought of as a branch of anthropology in the United States, while in Europe, it is viewed as a discipline in its own right, or grouped under other related disciplines such as history. Cognitive anthropology seeks to explain patterns of shared knowledge, cultural innovation, and transmission over time and space using the methods and theories of the cognitive sciences (especially experimental psychology and evolutionary biology) often through close collaboration with historians, ethnographers, archaeologists, linguists, musicologists and other specialists engaged in the description and interpretation of cultural forms. Cognitive anthropology is concerned with what people from different groups know and how that implicit knowledge changes the way people perceive and relate to the world around them. Environmental anthropology is a sub-specialty within the field of anthropology that takes an active role in examining the relationships between humans and their environment across space and time. The contemporary perspective of environmental anthropology, and arguably at least the backdrop, if not the focus of most of the ethnographies and cultural fieldworks of today, is political ecology. Many characterize this new perspective as more informed with culture, politics and power, globalization, localized issues, and more. The focus and data interpretation is often used for arguments for/against or creation of policy, and to prevent corporate exploitation and damage of land. Often, the observer has become an active part of the struggle either directly (organizing, participation) or indirectly (articles, documentaries, books, ethnographies). Such is the case with environmental justice advocate Melissa Checker and her relationship with the people of Hyde Park. Urban anthropology is concerned with issues of urbanization, poverty, and neoliberalism. Ulf Hannerz quotes a 1960s remark that traditional anthropologists were ""a notoriously agoraphobic lot, anti-urban by definition"". Various social processes in the Western World as well as in the ""Third World"" (the latter being the habitual focus of attention of anthropologists) brought the attention of ""specialists in 'other cultures'"" closer to their homes. There are two principle approaches in urban anthropology: by examining the types of cities or examining the social issues within the cities. These two methods are overlapping and dependent of each other. By defining different types of cities, one would use social factors as well as economic and political factors to categorize the cities. By directly looking at the different social issues, one would also be studying how they affect the dynamic of the city. Anthropology and many other current fields are the intellectual results of the comparative methods developed in the earlier 19th century. Theorists in such diverse fields as anatomy, linguistics, and Ethnology, making feature-by-feature comparisons of their subject matters, were beginning to suspect that similarities between animals, languages, and folkways were the result of processes or laws unknown to them then. For them, the publication of Charles Darwin's On the Origin of Species was the epiphany of everything they had begun to suspect. Darwin himself arrived at his conclusions through comparison of species he had seen in agronomy and in the wild. But by the 1940s, many of Boas' anthropologist contemporaries were active in the allied war effort against the ""Axis"" (Nazi Germany, Fascist Italy, and Imperial Japan). Many served in the armed forces, while others worked in intelligence (for example, Office of Strategic Services and the Office of War Information). At the same time, David H. Price's work on American anthropology during the Cold War provides detailed accounts of the pursuit and dismissal of several anthropologists from their jobs for communist sympathies. Broca, being what today would be called a neurosurgeon, had taken an interest in the pathology of speech. He wanted to localize the difference between man and the other animals, which appeared to reside in speech. He discovered the speech center of the human brain, today called Broca's area after him. His interest was mainly in Biological anthropology, but a German philosopher specializing in psychology, Theodor Waitz, took up the theme of general and social anthropology in his six-volume work, entitled Die Anthropologie der Naturvölker, 1859–1864. The title was soon translated as ""The Anthropology of Primitive Peoples"". The last two volumes were published posthumously. Kinship can refer both to the study of the patterns of social relationships in one or more human cultures, or it can refer to the patterns of social relationships themselves. Over its history, anthropology has developed a number of related concepts and terms, such as ""descent"", ""descent groups"", ""lineages"", ""affines"", ""cognates"", and even ""fictive kinship"". Broadly, kinship patterns may be considered to include people related both by descent (one's social relations during development), and also relatives by marriage. During the last three decades of the 19th century a proliferation of anthropological societies and associations occurred, most independent, most publishing their own journals, and all international in membership and association. The major theorists belonged to these organizations. They supported the gradual osmosis of anthropology curricula into the major institutions of higher learning. By 1898 the American Association for the Advancement of Science was able to report that 48 educational institutions in 13 countries had some curriculum in anthropology. None of the 75 faculty members were under a department named anthropology. Linguistic anthropology (also called anthropological linguistics) seeks to understand the processes of human communications, verbal and non-verbal, variation in language across time and space, the social uses of language, and the relationship between language and culture. It is the branch of anthropology that brings linguistic methods to bear on anthropological problems, linking the analysis of linguistic forms and processes to the interpretation of sociocultural processes. Linguistic anthropologists often draw on related fields including sociolinguistics, pragmatics, cognitive linguistics, semiotics, discourse analysis, and narrative analysis. Political economy in anthropology is the application of the theories and methods of Historical Materialism to the traditional concerns of anthropology, including, but not limited to, non-capitalist societies. Political Economy introduced questions of history and colonialism to ahistorical anthropological theories of social structure and culture. Three main areas of interest rapidly developed. The first of these areas was concerned with the ""pre-capitalist"" societies that were subject to evolutionary ""tribal"" stereotypes. Sahlins work on Hunter-gatherers as the 'original affluent society' did much to dissipate that image. The second area was concerned with the vast majority of the world's population at the time, the peasantry, many of whom were involved in complex revolutionary wars such as in Vietnam. The third area was on colonialism, imperialism, and the creation of the capitalist world-system. More recently, these Political Economists have more directly addressed issues of industrial (and post-industrial) capitalism around the world. Darwin and Wallace unveiled evolution in the late 1850s. There was an immediate rush to bring it into the social sciences. Paul Broca in Paris was in the process of breaking away from the Société de biologie to form the first of the explicitly anthropological societies, the Société d'Anthropologie de Paris, meeting for the first time in Paris in 1859.[n 4] When he read Darwin he became an immediate convert to Transformisme, as the French called evolutionism. His definition now became ""the study of the human group, considered as a whole, in its details, and in relation to the rest of nature"". One of the central problems in the anthropology of art concerns the universality of 'art' as a cultural phenomenon. Several anthropologists have noted that the Western categories of 'painting', 'sculpture', or 'literature', conceived as independent artistic activities, do not exist, or exist in a significantly different form, in most non-Western contexts. To surmount this difficulty, anthropologists of art have focused on formal features in objects which, without exclusively being 'artistic', have certain evident 'aesthetic' qualities. Boas' Primitive Art, Claude Lévi-Strauss' The Way of the Masks (1982) or Geertz's 'Art as Cultural System' (1983) are some examples in this trend to transform the anthropology of 'art' into an anthropology of culturally specific 'aesthetics'. Media anthropology (also known as anthropology of media or mass media) emphasizes ethnographic studies as a means of understanding producers, audiences, and other cultural and social aspects of mass media. The types of ethnographic contexts explored range from contexts of media production (e.g., ethnographies of newsrooms in newspapers, journalists in the field, film production) to contexts of media reception, following audiences in their everyday responses to media. Other types include cyber anthropology, a relatively new area of internet research, as well as ethnographies of other areas of research which happen to involve media, such as development work, social movements, or health education. This is in addition to many classic ethnographic contexts, where media such as radio, the press, new media and television have started to make their presences felt since the early 1990s. Since the 1980s it has become common for social and cultural anthropologists to set ethnographic research in the North Atlantic region, frequently examining the connections between locations rather than limiting research to a single locale. There has also been a related shift toward broadening the focus beyond the daily life of ordinary people; increasingly, research is set in settings such as scientific laboratories, social movements, governmental and nongovernmental organizations and businesses. Professional anthropological bodies often object to the use of anthropology for the benefit of the state. Their codes of ethics or statements may proscribe anthropologists from giving secret briefings. The Association of Social Anthropologists of the UK and Commonwealth (ASA) has called certain scholarship ethically dangerous. The AAA's current 'Statement of Professional Responsibility' clearly states that ""in relation with their own government and with host governments ... no secret research, no secret reports or debriefings of any kind should be agreed to or given."" Since the work of Franz Boas and Bronisław Malinowski in the late 19th and early 20th centuries, social anthropology in Great Britain and cultural anthropology in the US have been distinguished from other social sciences by its emphasis on cross-cultural comparisons, long-term in-depth examination of context, and the importance it places on participant-observation or experiential immersion in the area of research. Cultural anthropology in particular has emphasized cultural relativism, holism, and the use of findings to frame cultural critiques. This has been particularly prominent in the United States, from Boas' arguments against 19th-century racial ideology, through Margaret Mead's advocacy for gender equality and sexual liberation, to current criticisms of post-colonial oppression and promotion of multiculturalism. Ethnography is one of its primary research designs as well as the text that is generated from anthropological fieldwork. Feminist anthropology is a four field approach to anthropology (archeological, biological, cultural, linguistic) that seeks to reduce male bias in research findings, anthropological hiring practices, and the scholarly production of knowledge. Anthropology engages often with feminists from non-Western traditions, whose perspectives and experiences can differ from those of white European and American feminists. Historically, such 'peripheral' perspectives have sometimes been marginalized and regarded as less valid or important than knowledge from the western world. Feminist anthropologists have claimed that their research helps to correct this systematic bias in mainstream feminist theory. Feminist anthropologists are centrally concerned with the construction of gender across societies. Feminist anthropology is inclusive of birth anthropology as a specialization. This meagre statistic expanded in the 20th century to comprise anthropology departments in the majority of the world's higher educational institutions, many thousands in number. Anthropology has diversified from a few major subdivisions to dozens more. Practical anthropology, the use of anthropological knowledge and technique to solve specific problems, has arrived; for example, the presence of buried victims might stimulate the use of a forensic archaeologist to recreate the final scene. Organization has reached global level. For example, the World Council of Anthropological Associations (WCAA), ""a network of national, regional and international associations that aims to promote worldwide communication and cooperation in anthropology"", currently contains members from about three dozen nations. Archaeology is the study of the human past through its material remains. Artifacts, faunal remains, and human altered landscapes are evidence of the cultural and material lives of past societies. Archaeologists examine these material remains in order to deduce patterns of past human behavior and cultural practices. Ethnoarchaeology is a type of archaeology that studies the practices and material remains of living human groups in order to gain a better understanding of the evidence left behind by past human groups, who are presumed to have lived in similar ways. Visual anthropology is concerned, in part, with the study and production of ethnographic photography, film and, since the mid-1990s, new media. While the term is sometimes used interchangeably with ethnographic film, visual anthropology also encompasses the anthropological study of visual representation, including areas such as performance, museums, art, and the production and reception of mass media. Visual representations from all cultures, such as sandpaintings, tattoos, sculptures and reliefs, cave paintings, scrimshaw, jewelry, hieroglyphics, paintings and photographs are included in the focus of visual anthropology. Some authors argue that anthropology originated and developed as the study of ""other cultures"", both in terms of time (past societies) and space (non-European/non-Western societies). For example, the classic of urban anthropology, Ulf Hannerz in the introduction to his seminal Exploring the City: Inquiries Toward an Urban Anthropology mentions that the ""Third World"" had habitually received most of attention; anthropologists who traditionally specialized in ""other cultures"" looked for them far away and started to look ""across the tracks"" only in late 1960s. Similar organizations in other countries followed: The American Anthropological Association in 1902, the Anthropological Society of Madrid (1865), the Anthropological Society of Vienna (1870), the Italian Society of Anthropology and Ethnology (1871), and many others subsequently. The majority of these were evolutionist. One notable exception was the Berlin Society of Anthropology (1869) founded by Rudolph Virchow, known for his vituperative attacks on the evolutionists. Not religious himself, he insisted that Darwin's conclusions lacked empirical foundation. Cyborg anthropology originated as a sub-focus group within the American Anthropological Association's annual meeting in 1993. The sub-group was very closely related to STS and the Society for the Social Studies of Science. Donna Haraway's 1985 Cyborg Manifesto could be considered the founding document of cyborg anthropology by first exploring the philosophical and sociological ramifications of the term. Cyborg anthropology studies humankind and its relations with the technological systems it has built, specifically modern technological systems that have reflexively shaped notions of what it means to be human beings. Sociocultural anthropology has been heavily influenced by structuralist and postmodern theories, as well as a shift toward the analysis of modern societies. During the 1970s and 1990s, there was an epistemological shift away from the positivist traditions that had largely informed the discipline.[page needed] During this shift, enduring questions about the nature and production of knowledge came to occupy a central place in cultural and social anthropology. In contrast, archaeology and biological anthropology remained largely positivist. Due to this difference in epistemology, the four sub-fields of anthropology have lacked cohesion over the last several decades. Evolutionary anthropology is the interdisciplinary study of the evolution of human physiology and human behaviour and the relation between hominins and non-hominin primates. Evolutionary anthropology is based in natural science and social science, combining the human development with socioeconomic factors. Evolutionary anthropology is concerned with both biological and cultural evolution of humans, past and present. It is based on a scientific approach, and brings together fields such as archaeology, behavioral ecology, psychology, primatology, and genetics. It is a dynamic and interdisciplinary field, drawing on many lines of evidence to understand the human experience, past and present. Biological anthropologists are interested in both human variation and in the possibility of human universals (behaviors, ideas or concepts shared by virtually all human cultures). They use many different methods of study, but modern population genetics, participant observation and other techniques often take anthropologists ""into the field,"" which means traveling to a community in its own setting, to do something called ""fieldwork."" On the biological or physical side, human measurements, genetic samples, nutritional data may be gathered and published as articles or monographs. Applied Anthropology refers to the application of the method and theory of anthropology to the analysis and solution of practical problems. It is a, ""complex of related, research-based, instrumental methods which produce change or stability in specific cultural systems through the provision of data, initiation of direct action, and/or the formulation of policy"". More simply, applied anthropology is the practical side of anthropological research; it includes researcher involvement and activism within the participating community. It is closely related to Development anthropology (distinct from the more critical Anthropology of development). Ethnohistory is the study of ethnographic cultures and indigenous customs by examining historical records. It is also the study of the history of various ethnic groups that may or may not exist today. Ethnohistory uses both historical and ethnographic data as its foundation. Its historical methods and materials go beyond the standard use of documents and manuscripts. Practitioners recognize the utility of such source material as maps, music, paintings, photography, folklore, oral tradition, site exploration, archaeological materials, museum collections, enduring customs, language, and place names. Ethical commitments in anthropology include noticing and documenting genocide, infanticide, racism, mutilation (including circumcision and subincision), and torture. Topics like racism, slavery, and human sacrifice attract anthropological attention and theories ranging from nutritional deficiencies to genes to acculturation have been proposed, not to mention theories of colonialism and many others as root causes of Man's inhumanity to man. To illustrate the depth of an anthropological approach, one can take just one of these topics, such as ""racism"" and find thousands of anthropological references, stretching across all the major and minor sub-fields. Anthropology of development tends to view development from a critical perspective. The kind of issues addressed and implications for the approach simply involve pondering why, if a key development goal is to alleviate poverty, is poverty increasing? Why is there such a gap between plans and outcomes? Why are those working in development so willing to disregard history and the lessons it might offer? Why is development so externally driven rather than having an internal basis? In short why does so much planned development fail? Nutritional anthropology is a synthetic concept that deals with the interplay between economic systems, nutritional status and food security, and how changes in the former affect the latter. If economic and environmental changes in a community affect access to food, food security, and dietary health, then this interplay between culture and biology is in turn connected to broader historical and economic trends associated with globalization. Nutritional status affects overall health status, work performance potential, and the overall potential for economic development (either in terms of human development or traditional western models) for any given group of people. Sporadic use of the term for some of the subject matter occurred subsequently, such as the use by Étienne Serres in 1838 to describe the natural history, or paleontology, of man, based on comparative anatomy, and the creation of a chair in anthropology and ethnography in 1850 at the National Museum of Natural History (France) by Jean Louis Armand de Quatrefages de Bréau. Various short-lived organizations of anthropologists had already been formed. The Société Ethnologique de Paris, the first to use Ethnology, was formed in 1839. Its members were primarily anti-slavery activists. When slavery was abolished in France in 1848 the Société was abandoned. Inquiry in sociocultural anthropology is guided in part by cultural relativism, the attempt to understand other societies in terms of their own cultural symbols and values. Accepting other cultures in their own terms moderates reductionism in cross-cultural comparison. This project is often accommodated in the field of ethnography. Ethnography can refer to both a methodology and the product of ethnographic research, i.e. an ethnographic monograph. As methodology, ethnography is based upon long-term fieldwork within a community or other research site. Participant observation is one of the foundational methods of social and cultural anthropology. Ethnology involves the systematic comparison of different cultures. The process of participant-observation can be especially helpful to understanding a culture from an emic (conceptual, vs. etic, or technical) point of view. Waitz defined anthropology as ""the science of the nature of man"". By nature he meant matter animated by ""the Divine breath""; i.e., he was an animist. Following Broca's lead, Waitz points out that anthropology is a new field, which would gather material from other fields, but would differ from them in the use of comparative anatomy, physiology, and psychology to differentiate man from ""the animals nearest to him"". He stresses that the data of comparison must be empirical, gathered by experimentation. The history of civilization as well as ethnology are to be brought into the comparison. It is to be presumed fundamentally that the species, man, is a unity, and that ""the same laws of thought are applicable to all men"". Sociocultural anthropology draws together the principle axes of cultural anthropology and social anthropology. Cultural anthropology is the comparative study of the manifold ways in which people make sense of the world around them, while social anthropology is the study of the relationships among persons and groups. Cultural anthropology is more related to philosophy, literature and the arts (how one's culture affects experience for self and group, contributing to more complete understanding of the people's knowledge, customs, and institutions), while social anthropology is more related to sociology and history. in that it helps develop understanding of social structures, typically of others and other populations (such as minorities, subgroups, dissidents, etc.). There is no hard-and-fast distinction between them, and these categories overlap to a considerable degree. Anthropology is a global discipline where humanities, social, and natural sciences are forced to confront one another. Anthropology builds upon knowledge from natural sciences, including the discoveries about the origin and evolution of Homo sapiens, human physical traits, human behavior, the variations among different groups of humans, how the evolutionary past of Homo sapiens has influenced its social organization and culture, and from social sciences, including the organization of human social and cultural relations, institutions, social conflicts, etc. Early anthropology originated in Classical Greece and Persia and studied and tried to understand observable cultural diversity. As such, anthropology has been central in the development of several new (late 20th century) interdisciplinary fields such as cognitive science, global studies, and various ethnic studies. The study of kinship and social organization is a central focus of sociocultural anthropology, as kinship is a human universal. Sociocultural anthropology also covers economic and political organization, law and conflict resolution, patterns of consumption and exchange, material culture, technology, infrastructure, gender relations, ethnicity, childrearing and socialization, religion, myth, symbols, values, etiquette, worldview, sports, music, nutrition, recreation, games, food, festivals, and language (which is also the object of study in linguistic anthropology). Psychological anthropology is an interdisciplinary subfield of anthropology that studies the interaction of cultural and mental processes. This subfield tends to focus on ways in which humans' development and enculturation within a particular cultural group—with its own history, language, practices, and conceptual categories—shape processes of human cognition, emotion, perception, motivation, and mental health. It also examines how the understanding of cognition, emotion, motivation, and similar psychological processes inform or constrain our models of cultural and social processes. Along with dividing up their project by theoretical emphasis, anthropologists typically divide the world up into relevant time periods and geographic regions. Human time on Earth is divided up into relevant cultural traditions based on material, such as the Paleolithic and the Neolithic, of particular use in archaeology.[citation needed] Further cultural subdivisions according to tool types, such as Olduwan or Mousterian or Levalloisian help archaeologists and other anthropologists in understanding major trends in the human past.[citation needed] Anthropologists and geographers share approaches to Culture regions as well, since mapping cultures is central to both sciences. By making comparisons across cultural traditions (time-based) and cultural regions (space-based), anthropologists have developed various kinds of comparative method, a central part of their science. Anthrozoology (also known as ""human–animal studies"") is the study of interaction between living things. It is a burgeoning interdisciplinary field that overlaps with a number of other disciplines, including anthropology, ethology, medicine, psychology, veterinary medicine and zoology. A major focus of anthrozoologic research is the quantifying of the positive effects of human-animal relationships on either party and the study of their interactions. It includes scholars from a diverse range of fields, including anthropology, sociology, biology, and philosophy.[n 7] Economic anthropology attempts to explain human economic behavior in its widest historic, geographic and cultural scope. It has a complex relationship with the discipline of economics, of which it is highly critical. Its origins as a sub-field of anthropology begin with the Polish-British founder of Anthropology, Bronislaw Malinowski, and his French compatriot, Marcel Mauss, on the nature of gift-giving exchange (or reciprocity) as an alternative to market exchange. Economic Anthropology remains, for the most part, focused upon exchange. The school of thought derived from Marx and known as Political Economy focuses on production, in contrast. Economic Anthropologists have abandoned the primitivist niche they were relegated to by economists, and have now turned to examine corporations, banks, and the global financial system from an anthropological perspective."
Samurai,"Following the Battle of Hakusukinoe against Tang China and Silla in 663 AD that led to a Japanese retreat from Korean affairs, Japan underwent widespread reform. One of the most important was that of the Taika Reform, issued by Prince Naka no Ōe (Emperor Tenji) in 646 AD. This edict allowed the Japanese aristocracy to adopt the Tang dynasty political structure, bureaucracy, culture, religion, and philosophy. As part of the Taihō Code, of 702 AD, and the later Yōrō Code, the population was required to report regularly for census, a precursor for national conscription. With an understanding of how the population was distributed, Emperor Mommu introduced a law whereby 1 in 3–4 adult males was drafted into the national military. These soldiers were required to supply their own weapons, and in return were exempted from duties and taxes. This was one of the first attempts by the Imperial government to form an organized army modeled after the Chinese system. It was called ""Gundan-Sei"" (軍団制) by later historians and is believed to have been short-lived.[citation needed] Torii Mototada (1539–1600) was a feudal lord in the service of Tokugawa Ieyasu. On the eve of the battle of Sekigahara, he volunteered to remain behind in the doomed Fushimi Castle while his lord advanced to the east. Torii and Tokugawa both agreed that the castle was indefensible. In an act of loyalty to his lord, Torii chose to remain behind, pledging that he and his men would fight to the finish. As was custom, Torii vowed that he would not be taken alive. In a dramatic last stand, the garrison of 2,000 men held out against overwhelming odds for ten days against the massive army of Ishida Mitsunari's 40,000 warriors. In a moving last statement to his son Tadamasa, he wrote: In his book ""Ideals of the Samurai"" translator William Scott Wilson states: ""The warriors in the Heike Monogatari served as models for the educated warriors of later generations, and the ideals depicted by them were not assumed to be beyond reach. Rather, these ideals were vigorously pursued in the upper echelons of warrior society and recommended as the proper form of the Japanese man of arms. With the Heike Monogatari, the image of the Japanese warrior in literature came to its full maturity."" Wilson then translates the writings of several warriors who mention the Heike Monogatari as an example for their men to follow. The English sailor and adventurer William Adams (1564–1620) was the first Westerner to receive the dignity of samurai. The Shogun Tokugawa Ieyasu presented him with two swords representing the authority of a samurai, and decreed that William Adams the sailor was dead and that Anjin Miura (三浦按針), a samurai, was born. Adams also received the title of hatamoto (bannerman), a high-prestige position as a direct retainer in the Shogun's court. He was provided with generous revenues: ""For the services that I have done and do daily, being employed in the Emperor's service, the Emperor has given me a living"" (Letters). He was granted a fief in Hemi (逸見) within the boundaries of present-day Yokosuka City, ""with eighty or ninety husbandmen, that be my slaves or servants"" (Letters). His estate was valued at 250 koku. He finally wrote ""God hath provided for me after my great misery"", (Letters) by which he meant the disaster-ridden voyage that initially brought him to Japan. Created by Takashi Okazaki, Afro Samurai was initially a doujinshi, or manga series, which was then made into an animated series by Studio Gonzo. In 2007 the animated series debuted on American cable television on the Spike TV channel (Denison, 2010). The series was produced for American viewers which “embodies the trend... comparing hip-hop artists to samurai warriors, an image some rappers claim for themselves (Solomon, 2009). The storyline keeps in tone with the perception of a samurais finding vengeance against someone who has wronged him. Starring the voice of well known American actor Samuel L. Jackson, “Afro is the second-strongest fighter in a futuristic, yet, still feudal Japan and seeks revenge upon the gunman who killed his father” (King 2008). Due to its popularity, Afro Samurai was adopted into a full feature animated film and also became titles on gaming consoles such as the PlayStation 3 and Xbox. Not only has the samurai culture been adopted into animation and video games, it can also be seen in comic books. The Taira and the Minamoto clashed again in 1180, beginning the Gempei War, which ended in 1185. Samurai fought at the naval battle of Dan-no-ura, at the Shimonoseki Strait which separates Honshu and Kyushu in 1185. The victorious Minamoto no Yoritomo established the superiority of the samurai over the aristocracy. In 1190 he visited Kyoto and in 1192 became Sei'i-taishōgun, establishing the Kamakura Shogunate, or Kamakura Bakufu. Instead of ruling from Kyoto, he set up the Shogunate in Kamakura, near his base of power. ""Bakufu"" means ""tent government"", taken from the encampments the soldiers would live in, in accordance with the Bakufu's status as a military government. The philosophies of Buddhism and Zen, and to a lesser extent Confucianism and Shinto, influenced the samurai culture. Zen meditation became an important teaching due to it offering a process to calm one's mind. The Buddhist concept of reincarnation and rebirth led samurai to abandon torture and needless killing, while some samurai even gave up violence altogether and became Buddhist monks after realizing how fruitless their killings were. Some were killed as they came to terms with these realizations in the battlefield. The most defining role that Confucianism played in samurai philosophy was to stress the importance of the lord-retainer relationship—the loyalty that a samurai was required to show his lord. As the Tokugawa period progressed more value became placed on education, and the education of females beginning at a young age became important to families and society as a whole. Marriage criteria began to weigh intelligence and education as desirable attributes in a wife, right along with physical attractiveness. Though many of the texts written for women during the Tokugawa period only pertained to how a woman could become a successful wife and household manager, there were those that undertook the challenge of learning to read, and also tackled philosophical and literary classics. Nearly all women of the samurai class were literate by the end of the Tokugawa period. Emperor Meiji abolished the samurai's right to be the only armed force in favor of a more modern, western-style, conscripted army in 1873. Samurai became Shizoku (士族) who retained some of their salaries, but the right to wear a katana in public was eventually abolished along with the right to execute commoners who paid them disrespect. The samurai finally came to an end after hundreds of years of enjoyment of their status, their powers, and their ability to shape the government of Japan. However, the rule of the state by the military class was not yet over. In defining how a modern Japan should be, members of the Meiji government decided to follow the footsteps of the United Kingdom and Germany, basing the country on the concept of noblesse oblige. Samurai were not a political force under the new order. With the Meiji reforms in the late 19th century, the samurai class was abolished, and a western-style national army was established. The Imperial Japanese Armies were conscripted, but many samurai volunteered as soldiers, and many advanced to be trained as officers. Much of the Imperial Army officer class was of samurai origin, and were highly motivated, disciplined, and exceptionally trained. Jidaigeki (literally historical drama) has always been a staple program on Japanese movies and television. The programs typically feature a samurai. Samurai films and westerns share a number of similarities and the two have influenced each other over the years. One of Japan’s most renowned directors, Akira Kurosawa, greatly influenced the samurai aspect in western film-making.[citation needed] George Lucas’ Star Wars series incorporated many aspects from the Seven Samurai film. One example is that in the Japanese film, seven samurai warriors are hired by local farmers to protect their land from being overrun by bandits; In George Lucas’ Star Wars: A New Hope, a similar situation arises. Kurosawa was inspired by the works of director John Ford and in turn Kurosawa's works have been remade into westerns such as The Seven Samurai into The Magnificent Seven and Yojimbo into A Fistful of Dollars. There is also a 26 episode anime adaptation (Samurai 7) of The Seven Samurai. Along with film, literature containing samurai influences are seen as well. Samurai were many of the early exchange students, not directly because they were samurai, but because many samurai were literate and well-educated scholars. Some of these exchange students started private schools for higher educations, while many samurai took pens instead of guns and became reporters and writers, setting up newspaper companies, and others entered governmental service. Some samurai became businessmen. For example, Iwasaki Yatarō, who was the great-grandson of a samurai, established Mitsubishi. By the end of the 12th century, samurai became almost entirely synonymous with bushi, and the word was closely associated with the middle and upper echelons of the warrior class. The samurai were usually associated with a clan and their lord, were trained as officers in military tactics and grand strategy, and they followed a set of rules that later came to be known as the bushidō. While the samurai numbered less than 10% of then Japan's population, their teachings can still be found today in both everyday life and in modern Japanese martial arts. In December 1547, Francis was in Malacca (Malaysia) waiting to return to Goa (India) when he met a low-ranked samurai named Anjiro (possibly spelled ""Yajiro""). Anjiro was not an intellectual, but he impressed Xavier because he took careful notes of everything he said in church. Xavier made the decision to go to Japan in part because this low-ranking samurai convinced him in Portuguese that the Japanese people were highly educated and eager to learn. They were hard workers and respectful of authority. In their laws and customs they were led by reason, and, should the Christian faith convince them of its truth, they would accept it en masse. Bushi was the name given to the ancient Japanese soldiers from traditional warrior families. The bushi class was developed mainly in the north of Japan. They formed powerful clans, which in the 12th century were against the noble families who were grouping themselves to support the imperial family who lived in Kyoto. Samurai was a word used by the Kuge aristocratic class with warriors themselves preferring the word bushi. The term Bushidō, the ""way of the warrior"", is derived from this term and the mansion of a warrior was called bukeyashiki. Jan Joosten van Lodensteijn (1556?–1623?), a Dutch colleague of Adams' on their ill-fated voyage to Japan in the ship De Liefde, was also given similar privileges by Tokugawa Ieyasu. It appears Joosten became a samurai[citation needed] and was given a residence within Ieyasu's castle at Edo. Today, this area at the east exit of Tokyo Station is known as Yaesu (八重洲). Yaesu is a corruption of the Dutchman's Japanese name, Yayousu (耶楊子). Also in common with Adam's, Joostens was given a Red Seal Ship (朱印船) allowing him to trade between Japan and Indo-China. On a return journey from Batavia Joosten drowned after his ship ran aground. Most samurai were bound by a code of honor and were expected to set an example for those below them. A notable part of their code is seppuku (切腹, seppuku?) or hara kiri, which allowed a disgraced samurai to regain his honor by passing into death, where samurai were still beholden to social rules. Whilst there are many romanticized characterizations of samurai behavior such as the writing of Bushido (武士道, Bushidō?) in 1905, studies of Kobudo and traditional Budō indicate that the samurai were as practical on the battlefield as were any other warrior. After the Genpei war of the late 12th century, a clan leader Minamoto no Yoritomo obtained the right to appoint shugo and jito, and was allowed to organize soldiers and police, and to collect a certain amount of tax. Initially, their responsibility was restricted to arresting rebels and collecting needed army provisions, and they were forbidden from interfering with Kokushi Governors, but their responsibility gradually expanded and thus the samurai-class appeared as the political ruling power in Japan. Minamoto no Yoritomo opened the Kamakura Bakufu Shogunate in 1192. In the early Heian period, the late 8th and early 9th centuries, Emperor Kammu sought to consolidate and expand his rule in northern Honshū, but the armies he sent to conquer the rebellious Emishi people lacked motivation and discipline, and failed in their task.[citation needed] Emperor Kammu introduced the title of sei'i-taishōgun (征夷大将軍) or Shogun, and began to rely on the powerful regional clans to conquer the Emishi. Skilled in mounted combat and archery (kyūdō), these clan warriors became the Emperor's preferred tool for putting down rebellions.[citation needed] Though this is the first known use of the ""Shogun"" title, it was a temporary title, and was not imbued with political power until the 13th century. At this time (the 7th to 9th century) the Imperial Court officials considered them merely a military section under the control of the Imperial Court. In 1592, and again in 1597, Toyotomi Hideyoshi, aiming to invade China (唐入り) through Korea, mobilized an army of 160,000 peasants and samurai and deployed them to Korea. (See Hideyoshi's invasions of Korea, Chōsen-seibatsu (朝鮮征伐?). Taking advantage of arquebus mastery and extensive wartime experience from the Sengoku period, Japanese samurai armies made major gains in most of Korea. Kato Kiyomasa advanced to Orangkai territory (present-day Manchuria) bordering Korea to the northeast and crossed the border into Manchuria, but withdrew after retaliatory attacks from the Jurchens there, as it was clear he had outpaced the rest of the Japanese invasion force. A few of the more famous samurai generals of this war were Katō Kiyomasa, Konishi Yukinaga, and Shimazu Yoshihiro. Shimazu Yoshihiro led some 7,000 samurai and, despite being heavily outnumbered, defeated a host of allied Ming and Korean forces at the Battle of Sacheon in 1598, near the conclusion of the campaigns. Yoshihiro was feared as Oni-Shimazu (""Shimazu ogre"") and his nickname spread across not only Korea but to Ming Dynasty China. In spite of the superiority of Japanese land forces, ultimately the two expeditions failed (though they did devastate the Korean landmass) from factors such as Korean naval superiority (which, led by Admiral Yi Sun-shin, harassed Japanese supply lines continuously throughout the wars, resulting in supply shortages on land), the commitment of sizeable Ming forces to Korea, Korean guerrilla actions, the underestimation of resistance by Japanese commanders (in the first campaign of 1592, Korean defenses on land were caught unprepared, under-trained, and under-armed; they were rapidly overrun, with only a limited number of successfully resistant engagements against the more-experienced and battle-hardened Japanese forces - in the second campaign of 1597, Korean and Ming forces proved to be a far more difficult challenge and, with the support of continued Korean naval superiority, limited Japanese gains to parts southeastern Korea), and wavering Japanese commitment to the campaigns as the wars dragged on. The final death blow to the Japanese campaigns in Korea came with Hideyoshi's death in late 1598 and the recall of all Japanese forces in Korea by the Council of Five Elders (established by Hideyoshi to oversee the transition from his regency to that of his son Hideyori). Originally the Emperor and non-warrior nobility employed these warrior nobles. In time, they amassed enough manpower, resources and political backing in the form of alliances with one another, to establish the first samurai-dominated government. As the power of these regional clans grew, their chief was typically a distant relative of the Emperor and a lesser member of either the Fujiwara, Minamoto, or Taira clans. Though originally sent to provincial areas for a fixed four-year term as a magistrate, the toryo declined to return to the capital when their terms ended, and their sons inherited their positions and continued to lead the clans in putting down rebellions throughout Japan during the middle- and later-Heian period. Because of their rising military and economic power, the warriors ultimately became a new force in the politics of the court. Their involvement in the Hōgen in the late Heian period consolidated their power, and finally pitted the rival Minamoto and Taira clans against each other in the Heiji Rebellion of 1160. A samurai could take concubines but their backgrounds were checked by higher-ranked samurai. In many cases, taking a concubine was akin to a marriage. Kidnapping a concubine, although common in fiction, would have been shameful, if not criminal. If the concubine was a commoner, a messenger was sent with betrothal money or a note for exemption of tax to ask for her parents' acceptance. Even though the woman would not be a legal wife, a situation normally considered a demotion, many wealthy merchants believed that being the concubine of a samurai was superior to being the legal wife of a commoner. When a merchant's daughter married a samurai, her family's money erased the samurai's debts, and the samurai's social status improved the standing of the merchant family. If a samurai's commoner concubine gave birth to a son, the son could inherit his father's social status. The relative peace of the Tokugawa era was shattered with the arrival of Commodore Matthew Perry's massive U.S. Navy steamships in 1853. Perry used his superior firepower to force Japan to open its borders to trade. Prior to that only a few harbor towns, under strict control from the Shogunate, were allowed to participate in Western trade, and even then, it was based largely on the idea of playing the Franciscans and Dominicans off against one another (in exchange for the crucial arquebus technology, which in turn was a major contributor to the downfall of the classical samurai). Just in the last two decades,[when?] samurai have become more popular in America. “Hyperbolizing the samurai in such a way that they appear as a whole to be a loyal body of master warriors provides international interest in certain characters due to admirable traits” (Moscardi, N.D.). Through various medium, producers and writers have been capitalizing on the notion that Americans admire the samurai lifestyle. The animated series, Afro Samurai, became well-liked in American popular culture due to its blend of hack-and-slash animation and gritty urban music. The term samurai originally meant ""those who serve in close attendance to nobility"", and was written with a Chinese character (or kanji) that had the same meaning. In Japanese, it was originally recorded in the Nara Period as a verb *samorapu (""to watch, to keep watch, to observe, to be on the lookout for something; to serve, to attend""), which is believed to be derived from the frequentative form (*morapu 守らふ) of the verb moru (守る, ""to watch, to guard, to be on the lookout; to keep, to protect, to take care of, to be in charge of, to have as one's ward""). By the Heian period, this word had developed into the verb saburahu (さぶらふ, ""to serve, to attend""), from which a deverbal noun saburahi (さぶらひ, ""servant, attendant"") was later derived, and this noun then yielded samurahi (さむらひ) in the Edo period. In Japanese literature, there is an early reference to samurai in the Kokinshū (古今集, early 10th century): As far back as the seventh century Japanese warriors wore a form of lamellar armor, this armor eventually evolved into the armor worn by the samurai. The first types of Japanese armors identified as samurai armor were known as yoroi. These early samurai armors were made from small individual scales known as kozane. The kozane were made from either iron or leather and were bound together into small strips, the strips were coated with lacquer to protect the kozane from water. A series of strips of kozane were then laced together with silk or leather lace and formed into a complete chest armor (dou or dō). Oda Nobunaga made innovations in the fields of organization and war tactics, heavily used arquebuses, developed commerce and industry and treasured innovation. Consecutive victories enabled him to realize the termination of the Ashikaga Bakufu and the disarmament of the military powers of the Buddhist monks, which had inflamed futile struggles among the populace for centuries. Attacking from the ""sanctuary"" of Buddhist temples, they were constant headaches to any warlord and even the Emperor who tried to control their actions. He died in 1582 when one of his generals, Akechi Mitsuhide, turned upon him with his army. Katō Kiyomasa was one of the most powerful and well-known lords of the Sengoku Era. He commanded most of Japan's major clans during the invasion of Korea (1592–1598). In a handbook he addressed to ""all samurai, regardless of rank"" he told his followers that a warrior's only duty in life was to ""...grasp the long and the short swords and to die"". He also ordered his followers to put forth great effort in studying the military classics, especially those related to loyalty and filial piety. He is best known for his quote: ""If a man does not investigate into the matter of Bushido daily, it will be difficult for him to die a brave and manly death. Thus it is essential to engrave this business of the warrior into one's mind well."" From 1854, the samurai army and the navy were modernized. A Naval training school was established in Nagasaki in 1855. Naval students were sent to study in Western naval schools for several years, starting a tradition of foreign-educated future leaders, such as Admiral Enomoto. French naval engineers were hired to build naval arsenals, such as Yokosuka and Nagasaki. By the end of the Tokugawa shogunate in 1867, the Japanese navy of the shogun already possessed eight western-style steam warships around the flagship Kaiyō Maru, which were used against pro-imperial forces during the Boshin war, under the command of Admiral Enomoto. A French Military Mission to Japan (1867) was established to help modernize the armies of the Bakufu. ""First of all, a samurai who dislikes battle and has not put his heart in the right place even though he has been born in the house of the warrior, should not be reckoned among one's retainers....It is forbidden to forget the great debt of kindness one owes to his master and ancestors and thereby make light of the virtues of loyalty and filial piety....It is forbidden that one should...attach little importance to his duties to his master...There is a primary need to distinguish loyalty from disloyalty and to establish rewards and punishments."" In 1274, the Mongol-founded Yuan dynasty in China sent a force of some 40,000 men and 900 ships to invade Japan in northern Kyūshū. Japan mustered a mere 10,000 samurai to meet this threat. The invading army was harassed by major thunderstorms throughout the invasion, which aided the defenders by inflicting heavy casualties. The Yuan army was eventually recalled and the invasion was called off. The Mongol invaders used small bombs, which was likely the first appearance of bombs and gunpowder in Japan. As aristocrats for centuries, samurai developed their own cultures that influenced Japanese culture as a whole. The culture associated with the samurai such as the tea ceremony, monochrome ink painting, rock gardens and poetry were adopted by warrior patrons throughout the centuries 1200–1600. These practices were adapted from the Chinese arts. Zen monks introduced them to Japan and they were allowed to flourish due to the interest of powerful warrior elites. Musō Soseki (1275–1351) was a Zen monk who was advisor to both Emperor Go-Daigo and General Ashikaga Takauji (1304–58). Musō, as well as other monks, acted as political and cultural diplomat between Japan and China. Musō was particularly well known for his garden design. Another Ashikaga patron of the arts was Yoshimasa. His cultural advisor, the Zen monk Zeami, introduced tea ceremony to him. Previously, tea had been used primarily for Buddhist monks to stay awake during meditation. For example, the full name of Oda Nobunaga would be ""Oda Kazusanosuke Saburo Nobunaga"" (織田上総介三郎信長), in which ""Oda"" is a clan or family name, ""Kazusanosuke"" is a title of vice-governor of Kazusa province, ""Saburo"" is a formal nickname (yobina), and ""Nobunaga"" is an adult name (nanori) given at genpuku, the coming of age ceremony. A man was addressed by his family name and his title, or by his yobina if he did not have a title. However, the nanori was a private name that could be used by only a very few, including the Emperor. In the same war, the Prussian Edward Schnell served the Aizu domain as a military instructor and procurer of weapons. He was granted the Japanese name Hiramatsu Buhei (平松武兵衛), which inverted the characters of the daimyo's name Matsudaira. Hiramatsu (Schnell) was given the right to wear swords, as well as a residence in the castle town of Wakamatsu, a Japanese wife, and retainers. In many contemporary references, he is portrayed wearing a Japanese kimono, overcoat, and swords, with Western riding trousers and boots. Most common are historical works where the protagonist is either a samurai or former samurai (or another rank or position) who possesses considerable martial skill. Eiji Yoshikawa is one of the most famous Japanese historical novelists. His retellings of popular works, including Taiko, Musashi and Heike Tale, are popular among readers for their epic narratives and rich realism in depicting samurai and warrior culture.[citation needed] The samurai have also appeared frequently in Japanese comics (manga) and animation (anime). Samurai-like characters are not just restricted to historical settings and a number of works set in the modern age, and even the future, include characters who live, train and fight like samurai. Examples are Samurai Champloo, Requiem from the Darkness, Muramasa: The Demon Blade, and Afro Samurai. Some of these works have made their way to the west, where it has been increasing in popularity with America. This does not mean that samurai women were always powerless. Powerful women both wisely and unwisely wielded power at various occasions. After Ashikaga Yoshimasa, 8th shogun of the Muromachi shogunate, lost interest in politics, his wife Hino Tomiko largely ruled in his place. Nene, wife of Toyotomi Hideyoshi, was known to overrule her husband's decisions at times and Yodo-dono, his concubine, became the de facto master of Osaka castle and the Toyotomi clan after Hideyoshi's death. Tachibana Ginchiyo was chosen to lead the Tachibana clan after her father's death. Chiyo, wife of Yamauchi Kazutoyo, has long been considered the ideal samurai wife. According to legend, she made her kimono out of a quilted patchwork of bits of old cloth and saved pennies to buy her husband a magnificent horse, on which he rode to many victories. The fact that Chiyo (though she is better known as ""Wife of Yamauchi Kazutoyo"") is held in such high esteem for her economic sense is illuminating in the light of the fact that she never produced an heir and the Yamauchi clan was succeeded by Kazutoyo's younger brother. The source of power for women may have been that samurai left their finances to their wives. The Japanese defenders recognized the possibility of a renewed invasion, and began construction of a great stone barrier around Hakata Bay in 1276. Completed in 1277, this wall stretched for 20 kilometers around the border of the bay. This would later serve as a strong defensive point against the Mongols. The Mongols attempted to settle matters in a diplomatic way from 1275 to 1279, but every envoy sent to Japan was executed. This set the stage for one of the most famous engagements in Japanese history. During the Tokugawa shogunate, samurai increasingly became courtiers, bureaucrats, and administrators rather than warriors. With no warfare since the early 17th century, samurai gradually lost their military function during the Tokugawa era (also called the Edo period). By the end of the Tokugawa era, samurai were aristocratic bureaucrats for the daimyo, with their daisho, the paired long and short swords of the samurai (cf. katana and wakizashi) becoming more of a symbolic emblem of power rather than a weapon used in daily life. They still had the legal right to cut down any commoner who did not show proper respect kiri-sute gomen (斬り捨て御免?), but to what extent this right was used is unknown. When the central government forced daimyos to cut the size of their armies, unemployed rōnin became a social problem. In Japanese, they are usually referred to as bushi (武士?, [bu.ɕi]) or buke (武家?). According to translator William Scott Wilson: ""In Chinese, the character 侍 was originally a verb meaning ""to wait upon"" or ""accompany persons"" in the upper ranks of society, and this is also true of the original term in Japanese, saburau. In both countries the terms were nominalized to mean ""those who serve in close attendance to the nobility"", the pronunciation in Japanese changing to saburai. According to Wilson, an early reference to the word ""samurai"" appears in the Kokin Wakashū (905–914), the first imperial anthology of poems, completed in the first part of the 10th century. In the 1500s a new type of armor started to become popular due to the advent of firearms, new fighting tactics and the need for additional protection. The kozane dou made from individual scales was replaced by plate armor. This new armor, which used iron plated dou (dō), was referred to as Tosei-gusoku, or modern armor. Various other components of armor protected the samurai's body. The helmet kabuto was an important part of the samurai's armor. Samurai armor changed and developed as the methods of samurai warfare changed over the centuries. The known last use of samurai armor occurring in 1877 during the satsuma rebellion. As the last samurai rebellion was crushed, Japan modernized its defenses and turned to a national conscription army that used uniforms. American comic books have adopted the character type for stories of their own like the mutant-villain Silver Samurai of Marvel Comics. The design of this character preserves the samurai appearance; the villain is “Clad in traditional gleaming samurai armor and wielding an energy charged katana” (Buxton, 2013). Not only does the Silver Samurai make over 350 comic book appearances, the character is playable in several video games, such as Marvel Vs. Capcom 1 and 2. In 2013, the samurai villain was depicted in James Mangold’s film The Wolverine. Ten years before the Wolverine debuted, another film helped pave the way to ensure the samurai were made known to American cinema: A film released in 2003 titled The Last Samurai, starring Tom Cruise, is inspired by the samurai way of life. In the film, Cruise’s character finds himself deeply immersed in samurai culture. The character in the film, “Nathan Algren, is a fictional contrivance to make nineteenth-century Japanese history less foreign to American viewers”.(Ravina, 2010) After being captured by a group of samurai rebels, he becomes empathetic towards the cause they fight for. Taking place during the Meiji Period, Tom Cruise plays the role of US Army Captain Nathan Algren, who travels to Japan to train a rookie army in fighting off samurai rebel groups. Becoming a product of his environment, Algren joins the samurai clan in an attempt to rescue a captured samurai leader. “By the end of the film, he has clearly taken on many of the samurai traits, such as zen-like mastery of the sword, and a budding understanding of spirituality”. (Manion, 2006) It should be noted that many samurai forces that were active throughout this period were not deployed to Korea; most importantly, the daimyo Tokugawa Ieyasu carefully kept forces under his command out of the Korean campaigns, and other samurai commanders who were opposed to Hideyoshi's domination of Japan either mulled Hideyoshi's call to invade Korea or contributed a small token force. Most commanders who did opposed or otherwise resisted/resented Hideyoshi ended up as part of the so-called Eastern Army, while commanders loyal to Hideyoshi and his son (a notable exception to this trend was Katō Kiyomasa, who deployed with Tokugawa and the Eastern Army) were largely committed to the Western Army; the two opposing sides (so named for the relative geographical locations of their respective commanders' domains) would later clash, most notably at the Battle of Sekigahara, which was won by Tokugawa Ieyasu and the Eastern Forces, paving the way for the establishment of the Tokugawa Shogunate. Historian H. Paul Varley notes the description of Japan given by Jesuit leader St. Francis Xavier (1506–1552): ""There is no nation in the world which fears death less."" Xavier further describes the honour and manners of the people: ""I fancy that there are no people in the world more punctilious about their honour than the Japanese, for they will not put up with a single insult or even a word spoken in anger."" Xavier spent the years 1549–1551 converting Japanese to Christianity. He also observed: ""The Japanese are much braver and more warlike than the people of China, Korea, Ternate and all of the other nations around the Philippines."" Maintaining the household was the main duty of samurai women. This was especially crucial during early feudal Japan, when warrior husbands were often traveling abroad or engaged in clan battles. The wife, or okugatasama (meaning: one who remains in the home), was left to manage all household affairs, care for the children, and perhaps even defend the home forcibly. For this reason, many women of the samurai class were trained in wielding a polearm called a naginata or a special knife called the kaiken in an art called tantojutsu (lit. the skill of the knife), which they could use to protect their household, family, and honor if the need arose. In the 13th century, Hōjō Shigetoki (1198–1261 AD) wrote: ""When one is serving officially or in the master's court, he should not think of a hundred or a thousand people, but should consider only the importance of the master."" Carl Steenstrup noted that 13th and 14th century warrior writings (gunki) ""portrayed the bushi in their natural element, war, eulogizing such virtues as reckless bravery, fierce family pride, and selfless, at times senseless devotion of master and man"". Feudal lords such as Shiba Yoshimasa (1350–1410 AD) stated that a warrior looked forward to a glorious death in the service of a military leader or the Emperor: ""It is a matter of regret to let the moment when one should die pass by....First, a man whose profession is the use of arms should think and then act upon not only his own fame, but also that of his descendants. He should not scandalize his name forever by holding his one and only life too dear....One's main purpose in throwing away his life is to do so either for the sake of the Emperor or in some great undertaking of a military general. It is that exactly that will be the great fame of one's descendants."" The winner, Taira no Kiyomori, became an imperial advisor, and was the first warrior to attain such a position. He eventually seized control of the central government, establishing the first samurai-dominated government and relegating the Emperor to figurehead status. However, the Taira clan was still very conservative when compared to its eventual successor, the Minamoto, and instead of expanding or strengthening its military might, the clan had its women marry Emperors and exercise control through the Emperor. Despite the rampant romanticism of the 20th century, samurai could be disloyal and treacherous (e.g., Akechi Mitsuhide), cowardly, brave, or overly loyal (e.g., Kusunoki Masashige). Samurai were usually loyal to their immediate superiors, who in turn allied themselves with higher lords. These loyalties to the higher lords often shifted; for example, the high lords allied under Toyotomi Hideyoshi (豊臣秀吉) were served by loyal samurai, but the feudal lords under them could shift their support to Tokugawa, taking their samurai with them. There were, however, also notable instances where samurai would be disloyal to their lord or daimyo, when loyalty to the Emperor was seen to have supremacy. Theoretical obligations between a samurai and his lord (usually a daimyo) increased from the Genpei era to the Edo era. They were strongly emphasized by the teachings of Confucius and Mencius (ca 550 BC), which were required reading for the educated samurai class. Bushido was formalized by several influential leaders and families before the Edo Period. Bushido was an ideal, and it remained fairly uniform from the 13th century to the 19th century — the ideals of Bushido transcended social class, time and geographic location of the warrior class. A samurai could divorce his wife for a variety of reasons with approval from a superior, but divorce was, while not entirely nonexistent, a rare event. A wife's failure to produce a son was cause for divorce, but adoption of a male heir was considered an acceptable alternative to divorce. A samurai could divorce for personal reasons, even if he simply did not like his wife, but this was generally avoided as it would embarrass the person who had arranged the marriage. A woman could also arrange a divorce, although it would generally take the form of the samurai divorcing her. After a divorce samurai had to return the betrothal money, which often prevented divorces. The rival of Takeda Shingen (1521–1573) was Uesugi Kenshin (1530–1578), a legendary Sengoku warlord well-versed in the Chinese military classics and who advocated the ""way of the warrior as death"". Japanese historian Daisetz Teitaro Suzuki describes Uesugi's beliefs as: ""Those who are reluctant to give up their lives and embrace death are not true warriors.... Go to the battlefield firmly confident of victory, and you will come home with no wounds whatever. Engage in combat fully determined to die and you will be alive; wish to survive in the battle and you will surely meet death. When you leave the house determined not to see it again you will come home safely; when you have any thought of returning you will not return. You may not be in the wrong to think that the world is always subject to change, but the warrior must not entertain this way of thinking, for his fate is always determined."" Traits valued in women of the samurai class were humility, obedience, self-control, strength, and loyalty. Ideally, a samurai wife would be skilled at managing property, keeping records, dealing with financial matters, educating the children (and perhaps servants, too), and caring for elderly parents or in-laws that may be living under her roof. Confucian law, which helped define personal relationships and the code of ethics of the warrior class required that a woman show subservience to her husband, filial piety to her parents, and care to the children. Too much love and affection was also said to indulge and spoil the youngsters. Thus, a woman was also to exercise discipline."
Paper,"Paper made from mechanical pulp contains significant amounts of lignin, a major component in wood. In the presence of light and oxygen, lignin reacts to give yellow materials, which is why newsprint and other mechanical paper yellows with age. Paper made from bleached kraft or sulfite pulps does not contain significant amounts of lignin and is therefore better suited for books, documents and other applications where whiteness of the paper is essential. Besides the fibres, pulps may contain fillers such as chalk or china clay, which improve its characteristics for printing or writing. Additives for sizing purposes may be mixed with it and/or applied to the paper web later in the manufacturing process; the purpose of such sizing is to establish the correct level of surface absorbency to suit ink or paint. Worldwide consumption of paper has risen by 400% in the past 40 years leading to increase in deforestation, with 35% of harvested trees being used for paper manufacture. Most paper companies also plant trees to help regrow forests. Logging of old growth forests accounts for less than 10% of wood pulp, but is one of the most controversial issues. To make pulp from wood, a chemical pulping process separates lignin from cellulose fibres. This is accomplished by dissolving lignin in a cooking liquor, so that it may be washed from the cellulose; this preserves the length of the cellulose fibres. Paper made from chemical pulps are also known as wood-free papers–not to be confused with tree-free paper; this is because they do not contain lignin, which deteriorates over time. The pulp can also be bleached to produce white paper, but this consumes 5% of the fibres; chemical pulping processes are not used to make paper made from cotton, which is already 90% cellulose. Wove paper does not exhibit ""laidlines"", which are small regular lines left behind on paper when it was handmade in a mould made from rows of metal wires or bamboo. Laidlines are very close together. They run perpendicular to the ""chainlines"", which are further apart. Handmade paper similarly exhibits ""deckle edges"", or rough and feathery borders. Its knowledge and uses spread from China through the Middle East to medieval Europe in the 13th century, where the first water powered paper mills were built. Because of paper's introduction to the West through the city of Baghdad, it was first called bagdatikos. In the 19th century, industrial manufacture greatly lowered its cost, enabling mass exchange of information and contributing to significant cultural shifts. In 1844, the Canadian inventor Charles Fenerty and the German F. G. Keller independently developed processes for pulping wood fibres. All paper produced by paper machines as the Fourdrinier Machine are wove paper, i.e. the wire mesh that transports the web leaves a pattern that has the same density along the paper grain and across the grain. Textured finishes, watermarks and wire patterns imitating hand-made laid paper can be created by the use of appropriate rollers in the later stages of the machine. The thickness of paper is often measured by caliper, which is typically given in thousandths of an inch in the United States and in thousandths of a mm in the rest of the world. Paper may be between 0.07 and 0.18 millimetres (0.0028 and 0.0071 in) thick. Recycled papers can be made from 100% recycled materials or blended with virgin pulp, although they are (generally) not as strong nor as bright as papers made from the latter. The paper is then fed onto reels if it is to be used on web printing presses, or cut into sheets for other printing processes or other purposes. The fibres in the paper basically run in the machine direction. Sheets are usually cut ""long-grain"", i.e. with the grain parallel to the longer dimension of the sheet. Mechanical pulping yields almost a tonne of pulp per tonne of dry wood used, which is why mechanical pulps are sometimes referred to as ""high yield"" pulps. With almost twice the yield as chemical pulping, mechanical pulps is often cheaper. Mass-market paperback books and newspapers tend to use mechanical papers. Book publishers tend to use acid-free paper, made from fully bleached chemical pulps for hardback and trade paperback books. The pulp papermaking process is said to have been developed in China during the early 2nd century AD, possibly as early as the year 105 A.D., by the Han court eunuch Cai Lun, although the earliest archaeological fragments of paper derive from the 2nd century BC in China. The modern pulp and paper industry is global, with China leading its production and the United States right behind it. Paper waste accounts for up to 40% of total waste produced in the United States each year, which adds up to 71.6 million tons of paper waste per year in the United States alone. The average office worker in the US prints 31 pages every day. Americans also use on the order of 16 billion paper cups per year. Conventional bleaching of wood pulp using elemental chlorine produces and releases into the environment large amounts of chlorinated organic compounds, including chlorinated dioxins. Dioxins are recognized as a persistent environmental pollutant, regulated internationally by the Stockholm Convention on Persistent Organic Pollutants. Dioxins are highly toxic, and health effects on humans include reproductive, developmental, immune and hormonal problems. They are known to be carcinogenic. Over 90% of human exposure is through food, primarily meat, dairy, fish and shellfish, as dioxins accumulate in the food chain in the fatty tissue of animals. The ISO 216 system used in most other countries is based on the surface area of a sheet of paper, not on a sheet's width and length. It was first adopted in Germany in 1922 and generally spread as nations adopted the metric system. The largest standard size paper is A0 (A zero), measuring one square meter (approx. 1189 × 841 mm). Two sheets of A1, placed upright side by side fit exactly into one sheet of A0 laid on its side. Similarly, two sheets of A2 fit into one sheet of A1 and so forth. Common sizes used in the office and the home are A4 and A3 (A3 is the size of two A4 sheets). Paper at this point is uncoated. Coated paper has a thin layer of material such as calcium carbonate or china clay applied to one or both sides in order to create a surface more suitable for high-resolution halftone screens. (Uncoated papers are rarely suitable for screens above 150 lpi.) Coated or uncoated papers may have their surfaces polished by calendering. Coated papers are divided into matte, semi-matte or silk, and gloss. Gloss papers give the highest optical density in the printed image. The density of paper ranges from 250 kg/m3 (16 lb/cu ft) for tissue paper to 1,500 kg/m3 (94 lb/cu ft) for some speciality paper. Printing paper is about 800 kg/m3 (50 lb/cu ft). Some manufacturers have started using a new, significantly more environmentally friendly alternative to expanded plastic packaging. Made out of paper, and known commercially as paperfoam, the new packaging has very similar mechanical properties to some expanded plastic packaging, but is biodegradable and can also be recycled with ordinary paper. Paper recycling processes can use either chemically or mechanically produced pulp; by mixing it with water and applying mechanical action the hydrogen bonds in the paper can be broken and fibres separated again. Most recycled paper contains a proportion of virgin fibre for the sake of quality; generally speaking, de-inked pulp is of the same quality or lower than the collected paper it was made from. There are three main chemical pulping processes: the sulfite process dates back to the 1840s and it was the dominant method extent before the second world war. The kraft process, invented in the 1870s and first used in the 1890s, is now the most commonly practiced strategy, one of its advantages is the chemical reaction with lignin, that produces heat, which can be used to run a generator. Most pulping operations using the kraft process are net contributors to the electricity grid or use the electricity to run an adjacent paper mill. Another advantage is that this process recovers and reuses all inorganic chemical reagents. Soda pulping is another specialty process used to pulp straws, bagasse and hardwoods with high silicate content. In Europe, and other regions using the ISO 216 paper sizing system, the weight is expressed in grammes per square metre (g/m2 or usually just g) of the paper. Printing paper is generally between 60 g and 120 g. Anything heavier than 160 g is considered card. The weight of a ream therefore depends on the dimensions of the paper and its thickness. The oldest known archaeological fragments of the immediate precursor to modern paper, date to the 2nd century BC in China. The pulp papermaking process is ascribed to Cai Lun, a 2nd-century AD Han court eunuch. With paper as an effective substitute for silk in many applications, China could export silk in greater quantity, contributing to a Golden Age. Paper made from wood pulp is not necessarily less durable than a rag paper. The ageing behavior of a paper is determined by its manufacture, not the original source of the fibres. Furthermore, tests sponsored by the Library of Congress prove that all paper is at risk of acid decay, because cellulose itself produces formic, acetic, lactic and oxalic acids. Paper is a thin material produced by pressing together moist fibres of cellulose pulp derived from wood, rags or grasses, and drying them into flexible sheets. It is a versatile material with many uses, including writing, printing, packaging, cleaning, and a number of industrial and construction processes. Pressing the sheet removes the water by force; once the water is forced from the sheet, a special kind of felt, which is not to be confused with the traditional one, is used to collect the water; whereas when making paper by hand, a blotter sheet is used instead. Drying involves using air and/or heat to remove water from the paper sheets; in the earliest days of paper making this was done by hanging the sheets like laundry; in more modern times various forms of heated drying mechanisms are used. On the paper machine the most common is the steam heated can dryer. These can reach temperatures above 200 °F (93 °C) and are used in long sequences of more than 40 cans; where the heat produced by these can easily dry the paper to less than 6% moisture. The word ""paper"" is etymologically derived from Latin papyrus, which comes from the Greek πάπυρος (papuros), the word for the Cyperus papyrus plant. Papyrus is a thick, paper-like material produced from the pith of the Cyperus papyrus plant, which was used in ancient Egypt and other Mediterranean cultures for writing before the introduction of paper into the Middle East and Europe. Although the word paper is etymologically derived from papyrus, the two are produced very differently and the development of the first is distinct from the development of the second. Papyrus is a lamination of natural plant fibres, while paper is manufactured from fibres whose properties have been changed by maceration. There are two major mechanical pulps, the thermomechanical one (TMP) and groundwood pulp (GW). In the TMP process, wood is chipped and then fed into large steam heated refiners, where the chips are squeezed and converted to fibres between two steel discs. In the groundwood process, debarked logs are fed into grinders where they are pressed against rotating stones to be made into fibres. Mechanical pulping does not remove the lignin, so the yield is very high, >95%, however it causes the paper thus produced to turn yellow and become brittle over time. Mechanical pulps have rather short fibres, thus producing weak paper. Although large amounts of electrical energy are required to produce mechanical pulp, it costs less than the chemical kind. With increasing environmental concerns about synthetic coatings (such as PFOA) and the higher prices of hydrocarbon based petrochemicals, there is a focus on zein (corn protein) as a coating for paper in high grease applications such as popcorn bags. Most commercial paper sold in North America is cut to standard paper sizes based on customary units and is defined by the length and width of a sheet of paper. Paper is often characterized by weight. In the United States, the weight assigned to a paper is the weight of a ream, 500 sheets, of varying ""basic sizes"", before the paper is cut into the size it is sold to end customers. For example, a ream of 20 lb, 8.5 in × 11 in (216 mm × 279 mm) paper weighs 5 pounds, because it has been cut from a larger sheet into four pieces. In the United States, printing paper is generally 20 lb, 24 lb, or 32 lb at most. Cover stock is generally 68 lb, and 110 lb or more is considered card stock. Much of the early paper made from wood pulp contained significant amounts of alum, a variety of aluminium sulfate salts that is significantly acidic. Alum was added to paper to assist in sizing, making it somewhat water resistant so that inks did not ""run"" or spread uncontrollably. Early papermakers did not realize that the alum they added liberally to cure almost every problem encountered in making their product would eventually be detrimental. The cellulose fibres that make up paper are hydrolyzed by acid, and the presence of alum would eventually degrade the fibres until the paper disintegrated in a process that has come to be known as ""slow fire"". Documents written on rag paper were significantly more stable. The use of non-acidic additives to make paper is becoming more prevalent, and the stability of these papers is less of an issue."
Eritrea,"The culture of Eritrea has been largely shaped by the country's location on the Red Sea coast. One of the most recognizable parts of Eritrean culture is the coffee ceremony. Coffee (Ge'ez ቡን būn) is offered when visiting friends, during festivities, or as a daily staple of life. During the coffee ceremony, there are traditions that are upheld. The coffee is served in three rounds: the first brew or round is called awel in Tigrinya meaning first, the second round is called kalaay meaning second, and the third round is called bereka meaning ""to be blessed"". If coffee is politely declined, then most likely tea (""shai"" ሻሂ shahee) will instead be served. In 2010, a genetic study was conducted on the mummified remains of baboons that were brought back as gifts from Punt by the ancient Egyptians. Led by a research team from the Egyptian Museum and the University of California, the scientists used oxygen isotope analysis to examine hairs from two baboon mummies that had been preserved in the British Museum. One of the baboons had distorted isotopic data, so the other's oxygen isotope values were compared to those of present-day baboon specimens from regions of interest. The researchers found that the mummies most closely matched modern baboon specimens in Eritrea and Ethiopia, which they suggested implied that Punt was likely a narrow region that included eastern Ethiopia and all of Eritrea. The Scottish traveler James Bruce reported in 1770 that Medri Bahri was a distinct political entity from Abyssinia, noting that the two territories were frequently in conflict. The Bahre-Nagassi (""Kings of the Sea"") alternately fought with or against the Abyssinians and the neighbouring Muslim Adal Sultanate depending on the geopolitical circumstances. Medri Bahri was thus part of the Christian resistance against Imam Ahmad ibn Ibrahim al-Ghazi of Adal's forces, but later joined the Adalite states and the Ottoman Empire front against Abyssinia in 1572. That 16th century also marked the arrival of the Ottomans, who began making inroads in the Red Sea area. After the decline of Aksum, the Eritrean highlands were under the domain of Bahr Negash ruled by the Bahr Negus. The area was then known as Ma'ikele Bahr (""between the seas/rivers,"" i.e. the land between the Red Sea and the Mereb river). It was later renamed under Emperor Zara Yaqob as the domain of the Bahr Negash, the Medri Bahri (""Sea land"" in Tingrinya, although it included some areas like Shire on the other side of the Mereb, today in Ethiopia). With its capital at Debarwa, the state's main provinces were Hamasien, Serae and Akele Guzai. Eritrea (/ˌɛrᵻˈtreɪ.ə/ or /ˌɛrᵻˈtriːə/;, officially the State of Eritrea, is a country in East Africa. With its capital at Asmara, it is bordered by Sudan in the west, Ethiopia in the south, and Djibouti in the southeast. The northeastern and eastern parts of Eritrea have an extensive coastline along the Red Sea. The nation has a total area of approximately 117,600 km2 (45,406 sq mi), and includes the Dahlak Archipelago and several of the Hanish Islands. Its name Eritrea is based on the Greek name for the Red Sea (Ἐρυθρὰ Θάλασσα Erythra Thalassa), which was first adopted for Italian Eritrea in 1890. In the 1950s, the Ethiopian feudal administration under Emperor Haile Selassie sought to annex Eritrea and Italian Somaliland. He laid claim to both territories in a letter to Franklin D. Roosevelt at the Paris Peace Conference and at the First Session of the United Nations. In the United Nations, the debate over the fate of the former Italian colonies continued. The British and Americans preferred to cede all of Eritrea except the Western province to the Ethiopians as a reward for their support during World War II. The Independence Bloc of Eritrean parties consistently requested from the UN General Assembly that a referendum be held immediately to settle the Eritrean question of sovereignty. The kingdom is mentioned in the Periplus of the Erythraean Sea as an important market place for ivory, which was exported throughout the ancient world. Aksum was at the time ruled by Zoskales, who also governed the port of Adulis. The Aksumite rulers facilitated trade by minting their own Aksumite currency. The state also established its hegemony over the declining Kingdom of Kush and regularly entered the politics of the kingdoms on the Arabian peninsula, eventually extending its rule over the region with the conquest of the Himyarite Kingdom. In 1888, the Italian administration launched its first development projects in the new colony. The Eritrean Railway was completed to Saati in 1888, and reached Asmara in the highlands in 1911. The Asmara–Massawa Cableway was the longest line in the world during its time, but was later dismantled by the British in World War II. Besides major infrastructural projects, the colonial authorities invested significantly in the agricultural sector. It also oversaw the provision of urban amenities in Asmara and Massawa, and employed many Eritreans in public service, particularly in the police and public works departments. Thousands of Eritreans were concurrently enlisted in the army, serving during the Italo-Turkish War in Libya as well as the First and second Italo-Abyssinian Wars. In 1922, Benito Mussolini's rise to power in Italy brought profound changes to the colonial government in Italian Eritrea. After il Duce declared the birth of the Italian Empire in May 1936, Italian Eritrea (enlarged with northern Ethiopia's regions) and Italian Somaliland were merged with the just conquered Ethiopia in the new Italian East Africa (Africa Orientale Italiana) administrative territory. This Fascist period was characterized by imperial expansion in the name of a ""new Roman Empire"". Eritrea was chosen by the Italian government to be the industrial center of Italian East Africa. Eritrea is a member of the United Nations, the African Union, and is an observing member of the Arab League. The nation holds a seat on the United Nations' Advisory Committee on Administrative and Budgetary Questions (ACABQ). Eritrea also holds memberships in the International Bank for Reconstruction and Development, International Finance Corporation, International Criminal Police Organization (INTERPOL), Non-Aligned Movement, Organization for the Prohibition of Chemical Weapons, Permanent Court of Arbitration, and the World Customs Organization. When Emperor Haile Selassie unilaterally dissolved the Eritrean parliament and annexed the country in 1962, the Eritrean Liberation Front (ELF) waged an armed struggle for independence. The ensuing Eritrean War for Independence went on for 30 years against successive Ethiopian governments until 1991, when the Eritrean People's Liberation Front (EPLF), a successor of the ELF, defeated the Ethiopian forces in Eritrea and helped a coalition of Ethiopian rebel forces take control of the Ethiopian Capital Addis Ababa. However, Eritrea still faces many challenges. Despite number of physicians increasing from only 0.2 in 1993 to 0.5 in 2004 per 1000 population, this is still very low. Malaria and tuberculosis are common in Eritrea. HIV prevalence among the 15–49 group exceeds 2%. The fertility rate is at about 5 births per woman. Maternal mortality dropped by more than half from 1995 to 2002, although the figure is still high. Similarly, between 1995 and 2002, the number of births attended by skilled health personnel has doubled but still is only 28.3%. A major cause of death in neonates is by severe infection. Per capita expenditure on health is low in Eritrea. At Buya in Eritrea, one of the oldest hominids representing a possible link between Homo erectus and an archaic Homo sapiens was found by Italian scientists. Dated to over 1 million years old, it is the oldest skeletal find of its kind and provides a link between hominids and the earliest anatomically modern humans. It is believed that the section of the Danakil Depression in Eritrea was also a major player in terms of human evolution, and may contain other traces of evolution from Homo erectus hominids to anatomically modern humans. Eritrea has achieved significant improvements in health care and is one of the few countries to be on target to meet its Millennium Development Goal (MDG) targets in health, in particular child health. Life expectancy at birth has increased from 39.1 in 1960 to 59.5 years in 2008, maternal and child mortality rates have dropped dramatically and the health infrastructure has been expanded. Due to Eritrea's relative isolation, information and resources are extremely limited and according to the World Health Organisation (WHO) found in 2008 average life expectancy to be slightly less than 63 years. Immunisation and child nutrition has been tackled by working closely with schools in a multi-sectoral approach; the number of children vaccinated against measles almost doubled in seven years, from 40.7% to 78.5% and the underweight prevalence among children decreased by 12% in 1995–2002 (severe underweight prevalence by 28%). The National Malaria Protection Unit of the Ministry of Health has registered tremendous improvements in reducing malarial mortality by as much as 85% and the number of cases by 92% between 1998 and 2006. The Eritrean government has banned female genital mutilation (FGM), saying the practice was painful and put women at risk of life-threatening health problems. Excavations in and near Agordat in central Eritrea yielded the remains of an ancient pre-Aksumite civilization known as the Gash Group. Ceramics were discovered that were related to those of the C-Group (Temehu) pastoral culture, which inhabited the Nile Valley between 2500–1500 BC. Some sources dating back to 3500 BC. Shards akin to those of the Kerma culture, another community that flourished in the Nile Valley around the same period, were also found at other local archaeological sites in the Barka valley belonging to the Gash Group. According to Peter Behrens (1981) and Marianne Bechaus-Gerst (2000), linguistic evidence indicates that the C-Group and Kerma peoples spoke Afroasiatic languages of the Berber and Cushitic branches, respectively. Football and cycling are the most popular sports in Eritrea. In recent years, Eritrean athletes have also seen increasing success in the international arena. Zersenay Tadese, an Eritrean athlete, currently holds the world record in half marathon distance running. The Tour of Eritrea, a multi-stage international cycling event, is held annually throughout the country. The Eritrea national cycling team has experienced a lot of success, winning the continental cycling championship several years in a row. Six Eritrean riders have been signed to international cycling teams, including Natnael Berhane and Daniel Teklehaimanot. Berhane was named African Sportsman of the Year in 2013, ahead of footballers Yaya Touré and Didier Drogba, while Teklehaimanot became the first Eritrean to ride the Vuelta a España in 2012. In 2015 Teklehaimanot won the King of the Mountains classification in the Critérium du Dauphine. Teklehaimanot and fellow Eritrean Merhawi Kudus became the first black African riders to compete in the Tour de France when they were selected by the MTN–Qhubeka team for the 2015 edition of the race, where, on 9 July, Teklehaimanot became the first African rider to wear the polkadot jersey. During the Middle Ages, the Eritrea region was known as Medri Bahri (""sea-land""). The name Eritrea is derived from the ancient Greek name for Red Sea (Ἐρυθρὰ Θάλασσα Erythra Thalassa, based on the adjective ἐρυθρός erythros ""red""). It was first formally adopted in 1890, with the formation of Italian Eritrea (Colonia Eritrea). The territory became the Eritrea Governorate within Italian East Africa in 1936. Eritrea was annexed by Ethiopia in 1953 (nominally within a federation until 1962) and an Eritrean Liberation Front formed in 1960. Eritrea gained independence following the 1993 referendum, and the name of the new state was defined as State of Eritrea in the 1997 constitution.[citation needed] At the end of the 16th century, the Aussa Sultanate was established in the Denkel lowlands of Eritrea. The polity had come into existence in 1577, when Muhammed Jasa moved his capital from Harar to Aussa (Asaita) with the split of the Adal Sultanate into Aussa and the Sultanate of Harar. At some point after 1672, Aussa declined in conjunction with Imam Umar Din bin Adam's recorded ascension to the throne. In 1734, the Afar leader Kedafu, head of the Mudaito clan, seized power and established the Mudaito Dynasty. This marked the start of a new and more sophisticated polity that would last into the colonial period. Eritrea is a multilingual country. The nation has no official language, as the Constitution establishes the ""equality of all Eritrean languages"". However, Tigrinya serves as the de facto language of national identity. With 2,540,000 total speakers of a population of 5,254,000 in 2006, Tigrinya is the most widely spoken language, particularly in the southern and central parts of Eritrea. Modern Standard Arabic and English serve as de facto working languages, with the latter used in university education and many technical fields. Italian, the former colonial language, is widely used in commerce and is taught as a second language in schools, with a few elderly monolinguals. Even during the war, Eritrea developed its transportation infrastructure by asphalting new roads, improving its ports, and repairing war-damaged roads and bridges as a part of the Warsay Yika'alo Program. The most significant of these projects was the construction of a coastal highway of more than 500 km connecting Massawa with Asseb, as well as the rehabilitation of the Eritrean Railway. The rail line has been restored between the port of Massawa and the capital Asmara, although services are sporadic. Steam locomotives are sometimes used for groups of enthusiasts. Eritrea's ethnic groups each have their own styles of music and accompanying dances. Amongst the Tigrinya, the best known traditional musical genre is the guaila. Traditional instruments of Eritrean folk music include the stringed krar, kebero, begena, masenqo and the wata (a distant/rudimentary cousin of the violin). The most popular Eritrean artist is the Tigrinya singer Helen Meles, who is noted for her powerful voice and wide singing range. Other prominent local musicians include the Kunama singer Dehab Faytinga, Ruth Abraha, Bereket Mengisteab, Yemane Baria, and the late Abraham Afewerki. Traditional Eritrean attire is quite varied among the ethnic groups of Eritrea. In the larger cities, most people dress in Western casual dress such as jeans and shirts. In offices, both men and women often dress in suits. Traditional clothing for Christian Tigrinya-speaking highlanders consists of bright white gowns called zurias for the women, and long white shirts accompanied by white pants for the men. In Muslim communities in the Eritrean lowland, the women traditionally dress in brightly colored clothes. Only Rashaida women maintain a tradition of covering half of their faces, though they do not cover their hair. The Aksumites erected a number of large stelae, which served a religious purpose in pre-Christian times. One of these granite columns, the obelisk of Aksum, is the largest such structure in the world, standing at 90 feet. Under Ezana (fl. 320–360), Aksum later adopted Christianity. In the 7th century, early Muslims from Mecca also sought refuge from Quraysh persecution by travelling to the kingdom, a journey known in Islamic history as the First Hijra. It is also the alleged resting place of the Ark of the Covenant and the purported home of the Queen of Sheba. Eritrea can be split into three ecoregions. To the east of the highlands are the hot, arid coastal plains stretching down to the southeast of the country. The cooler, more fertile highlands, reaching up to 3000m has a different habitat. Habitats here vary from the sub-tropical rainforest at Filfil Solomona to the precipitous cliffs and canyons of the southern highlands. The Afar Triangle or Danakil Depression of Eritrea is the probable location of a triple junction where three tectonic plates are pulling away from one another.The highest point of the country, Emba Soira, is located in the center of Eritrea, at 3,018 meters (9,902 ft) above sea level. The Eritrean highway system is named according to the road classification. The three levels of classification are: primary (P), secondary (S), and tertiary (T). The lowest level road is tertiary and serves local interests. Typically they are improved earth roads which are occasionally paved. During the wet seasons these roads typically become impassable. The next higher level road is a secondary road and typically is a single-layered asphalt road that connects district capitals together and those to the regional capitals. Roads that are considered primary roads are those that are fully asphalted (throughout their entire length) and in general they carry traffic between all the major towns in Eritrea. In its 2014 Press Freedom Index, Reporters Without Borders ranked the media environment in Eritrea at the very bottom of a list of 178 countries, just below totalitarian North Korea. According to the BBC, ""Eritrea is the only African country to have no privately owned news media"", and Reporters Without Borders said of the public media, ""[they] do nothing but relay the regime's belligerent and ultra-nationalist discourse. ... Not a single [foreign correspondent] now lives in Asmara."" The state-owned news agency censors news about external events. Independent media have been banned since 2001. In 2015, The Guardian published an opinion piece claiming, Eritrea is a one-party state in which national legislative elections have been repeatedly postponed. According to Human Rights Watch, the government's human rights record is considered among the worst in the world. Most Western countries have accused the Eritrean authorities of arbitrary arrest and detentions, and of detaining an unknown number of people without charge for their political activism. However, the Eritrean government has continually dismissed the accusations as politically motivated. In June 2015, a 500-page United Nations Human Rights Council report accused Eritrea's government of extrajudicial executions, torture, indefinitely prolonged national service and forced labour, and indicated that sexual harassment, rape and sexual servitude by state officials are also widespread. Additionally, the Italian Eritrea administration opened a number of new factories, which produced buttons, cooking oil, pasta, construction materials, packing meat, tobacco, hide and other household commodities. In 1939, there were around 2,198 factories and most of the employees were Eritrean citizens. The establishment of industries also made an increase in the number of both Italians and Eritreans residing in the cities. The number of Italians residing in the territory increased from 4,600 to 75,000 in five years; and with the involvement of Eritreans in the industries, trade and fruit plantation was expanded across the nation, while some of the plantations were owned by Eritreans. Lions are said to inhabit the mountains of the Gash-Barka Region. There is also a small population of elephants that roam in some parts of the country. Dik-diks can also be found in many areas. The endangered African wild ass can be seen in Denakalia Region. Other local wildlife include bushbucks, duikers, greater kudus, klipspringers, African leopards, oryxs and crocodiles., The spotted hyena is widespread and fairly common. Between 1955 and 2001 there were no reported sightings of elephant herds, and they are thought to have fallen victim to the war of independence. In December 2001 a herd of about 30, including 10 juveniles, was observed in the vicinity of the Gash River. The elephants seemed to have formed a symbiotic relationship with olive baboons, with the baboons using the water holes dug by the elephants, while the elephants use the tree-top baboons as an early warning system. Additionally, owing to its colonial history, cuisine in Eritrea features more Italian influences than are present in Ethiopian cooking, including more pasta and greater use of curry powders and cumin.The Italian Eritrean cuisine started to be practiced during the colonial times of the Kingdom of Italy, when a large number of Italians moved to Eritrea. They brought the use of ""pasta"" to Italian Eritrea, and it is one of the main food eaten in present-day Asmara. An Italian Eritrean cuisine emerged, and dishes common dishes are 'Pasta al Sugo e Berbere', which means ""Pasta with tomato sauce and berbere"" (spice), but there are many more like ""lasagna"" and ""cotoletta alla milanese"" (milano cutlet). Alongside sowa, people in Eritrea also tend to drink coffee. Mies is another popular local alcoholic beverage, made out of honey. Following the adoption of UN Resolution 390A(V) in December 1950, Eritrea was federated with Ethiopia under the prompting of the United States. The resolution called for Eritrea and Ethiopia to be linked through a loose federal structure under the sovereignty of the Emperor. Eritrea was to have its own administrative and judicial structure, its own flag, and control over its domestic affairs, including police, local administration, and taxation. The federal government, which for all intents and purposes was the existing imperial government, was to control foreign affairs (including commerce), defense, finance, and transportation. The resolution ignored the wishes of Eritreans for independence, but guaranteed the population democratic rights and a measure of autonomy. During the last interglacial period, the Red Sea coast of Eritrea was occupied by early anatomically modern humans. It is believed that the area was on the route out of Africa that some scholars suggest was used by early humans to colonize the rest of the Old World. In 1999, the Eritrean Research Project Team composed of Eritrean, Canadian, American, Dutch and French scientists discovered a Paleolithic site with stone and obsidian tools dated to over 125,000 years old near the Bay of Zula south of Massawa, along the Red Sea littoral. The tools are believed to have been used by early humans to harvest marine resources like clams and oysters. A typical traditional Eritrean dish consists of injera accompanied by a spicy stew, which frequently includes beef, kid, lamb or fish. Overall, Eritrean cuisine strongly resembles those of neighboring Ethiopia, Eritrean cooking tend to feature more seafood than Ethiopian cuisine on account of their coastal location. Eritrean dishes are also frequently ""lighter"" in texture than Ethiopian meals. They likewise tend to employ less seasoned butter and spices and more tomatoes, as in the tsebhi dorho delicacy. It is estimated that there are around 100 elephants left in Eritrea, the most northerly of East Africa's elephants. The endangered African wild dog (Lycaon pictus) was previously found in Eritrea, but is now deemed extirpated from the entire country. In Gash Barka, deadly snakes like saw-scaled viper are common. Puff adder and red spitting cobra are widespread and can be found even in the highlands.In the coastal areas marine species that are common include dolphin, dugong, whale shark, turtles, marlin/swordfish, and manta ray. Education in Eritrea is officially compulsory between seven and 13 years of age. However, the education infrastructure is inadequate to meet current needs. Statistics vary at the elementary level, suggesting that between 65 and 70% of school-aged children attend primary school; Approximately 61% attend secondary school. Student-teacher ratios are high: 45 to 1 at the elementary level and 54 to 1 at the secondary level. There are an average 63 students per classroom at the elementary level and 97 per classroom at the secondary level. Learning hours at school are often less than six hours per day. Skill shortages are present at all levels of the education system, and funding for and access to education vary significantly by gender and location. Illiteracy estimates for Eritrea range from around 40% to as high as 70%. According to recent estimates, 50% of the population adheres to Christianity, Islam 48%, while 2% of the population follows other religions including traditional African religion and animism. According to a study made by Pew Research Center, 63% adheres to Christianity and 36% adheres to Islam. Since May 2002, the government of Eritrea has officially recognized the Eritrean Orthodox Tewahedo Church (Oriental Orthodox), Sunni Islam, the Eritrean Catholic Church (a Metropolitanate sui juris) and the Evangelical Lutheran church. All other faiths and denominations are required to undergo a registration process. Among other things, the government's registration system requires religious groups to submit personal information on their membership to be allowed to worship. The creation of modern-day Eritrea is a result of the incorporation of independent, distinct kingdoms and sultanates (for example, Medri Bahri and the Sultanate of Aussa) eventually resulting in the formation of Italian Eritrea. In 1947 Eritrea became part of a federation with Ethiopia, the Federation of Ethiopia and Eritrea. Subsequent annexation into Ethiopia led to the Eritrean War of Independence, ending with Eritrean independence following a referendum in April 1993. Hostilities between Eritrea and Ethiopia persisted, leading to the Eritrean–Ethiopian War of 1998–2000 and further skirmishes with both Djibouti and Ethiopia. In the vacuum that followed the 1889 death of Emperor Yohannes II, Gen. Oreste Baratieri occupied the highlands along the Eritrean coast and Italy proclaimed the establishment of the new colony of Italian Eritrea, a colony of the Kingdom of Italy. In the Treaty of Wuchale (It. Uccialli) signed the same year, King Menelik of Shewa, a southern Ethiopian kingdom, recognized the Italian occupation of his rivals' lands of Bogos, Hamasien, Akkele Guzay, and Serae in exchange for guarantees of financial assistance and continuing access to European arms and ammunition. His subsequent victory over his rival kings and enthronement as Emperor Menelek II (r. 1889–1913) made the treaty formally binding upon the entire territory. In an attempt at reform, Eritrean government officials and NGO representatives have participated in numerous public meetings and dialogues. In these sessions they have answered questions as fundamental as, ""What are human rights?"", ""Who determines what are human rights?"", and ""What should take precedence, human or communal rights?"" In 2007, the Eritrean government also banned female genital mutilation. In Regional Assemblies and religious circles, Eritreans themselves speak out continuously against the use of female circumcision. They cite health concerns and individual freedom as being of primary concern when they say this. Furthermore, they implore rural peoples to cast away this ancient cultural practice. Additionally, a new movement called Citizens for Democratic Rights in Eritrea aimed at bringing about dialogue between the government and opposition was formed in early 2009. The group consists of ordinary citizens and some people close to the government. Disagreements following the war have resulted in stalemate punctuated by periods of elevated tension and renewed threats of war. The stalemate led the President of Eritrea to urge the UN to take action on Ethiopia with the Eleven Letters penned by the President to the United Nations Security Council. The situation has been further escalated by the continued efforts of the Eritrean and Ethiopian leaders in supporting opposition in one another's countries.[citation needed] In 2011, Ethiopia accused Eritrea of planting bombs at an African Union summit in Addis Ababa, which was later supported by a UN report. Eritrea denied the claims."
"Atlantic_City,_New_Jersey","As of the 2000 United States Census there were 40,517 people, 15,848 households, and 8,700 families residing in the city. The population density was 3,569.8 people per square mile (1,378.3/km2). There were 20,219 housing units at an average density of 1,781.4 per square mile (687.8/km2). The racial makeup of the city was 44.16% black or African American, 26.68% White, 0.48% Native American, 10.40% Asian, 0.06% Pacific Islander, 13.76% other races, and 4.47% from two or more races. 24.95% of the population were Hispanic or Latino of any race. 19.44% of the population was non-Hispanic whites. Boardwalk Hall, formally known as the ""Historic Atlantic City Convention Hall"", is an arena in Atlantic City along the boardwalk. Boardwalk Hall was Atlantic City's primary convention center until the opening of the Atlantic City Convention Center in 1997. The Atlantic City Convention Center includes 500,000 sq ft (46,000 m2) of showroom space, 5 exhibit halls, 45 meeting rooms with 109,000 sq ft (10,100 m2) of space, a garage with 1,400 parking spaces, and an adjacent Sheraton hotel. Both the Boardwalk Hall and Convention Center are operated by the Atlantic City Convention & Visitors Authority. As of September 2014, the greater Atlantic City area has one of the highest unemployment rates in the country at 13.8%, out of labor force of around 141,000. Summers are typically warm and humid with a July daily average of 75.6 °F (24.2 °C). During this time, the city gets a sea breeze off the ocean that often makes daytime temperatures much cooler than inland areas, making Atlantic City a prime place for beating the summer heat from June through September. Average highs even just a few miles west of Atlantic City exceed 85 °F (29 °C) in July. Near the coast, temperatures reach or exceed 90 °F (32 °C) on an average of only 6.8 days a year, but this reaches 21 days at nearby Atlantic City Int'l.[a] Winters are cool, with January averaging 35.5 °F (2 °C). Spring and autumn are erratic, although they are usually mild with low humidity. The average window for freezing temperatures is November 20 to March 25, allowing a growing season of 239 days. Extreme temperatures range from −9 °F (−23 °C) on February 9, 1934 to 104 °F (40 °C) on August 7, 1918.[b] By 1878, because of the growing popularity of the city, one railroad line could no longer keep up with demand. Soon, the Philadelphia and Atlantic City Railway was also constructed to transport tourists to Atlantic City. At this point massive hotels like The United States and Surf House, as well as smaller rooming houses, had sprung up all over town. The United States Hotel took up a full city block between Atlantic, Pacific, Delaware, and Maryland Avenues. These hotels were not only impressive in size, but featured the most updated amenities, and were considered quite luxurious for their time. The median income for a household in the city was $26,969, and the median income for a family was $31,997. Males had a median income of $25,471 versus $23,863 for females. The per capita income for the city was $15,402. About 19.1% of families and 23.6% of the population were below the poverty line, including 29.1% of those under age 18 and 18.9% of those age 65 or over. In 1903, Josiah White III bought a parcel of land near Ohio Avenue and the boardwalk and built the Queen Anne style Marlborough House. The hotel was a hit and, in 1905–06, he chose to expand the hotel and bought another parcel of land next door to his Marlborough House. In an effort to make his new hotel a source of conversation, White hired the architectural firm of Price and McLanahan. The firm made use of reinforced concrete, a new building material invented by Jean-Louis Lambot in 1848 (Joseph Monier received the patent in 1867). The hotel's Spanish and Moorish themes, capped off with its signature dome and chimneys, represented a step forward from other hotels that had a classically designed influence. White named the new hotel the Blenheim and merged the two hotels into the Marlborough-Blenheim. Bally's Atlantic City was later constructed at this location. The 1920s, with tourism at its peak, are considered by many historians as Atlantic City's golden age. During Prohibition, which was enacted nationally in 1919 and lasted until 1933, much liquor was consumed and gambling regularly took place in the back rooms of nightclubs and restaurants. It was during Prohibition that racketeer and political boss Enoch L. ""Nucky"" Johnson rose to power. Prohibition was largely unenforced in Atlantic City, and, because alcohol that had been smuggled into the city with the acquiescence of local officials could be readily obtained at restaurants and other establishments, the resort's popularity grew further. The city then dubbed itself as ""The World's Playground"". Nucky Johnson's income, which reached as much as $500,000 annually, came from the kickbacks he took on illegal liquor, gambling and prostitution operating in the city, as well as from kickbacks on construction projects. The city hosted the 1964 Democratic National Convention which nominated Lyndon Johnson for President and Hubert Humphrey as Vice President. The convention and the press coverage it generated, however, cast a harsh light on Atlantic City, which by then was in the midst of a long period of economic decline. Many felt that the friendship between Johnson and Governor of New Jersey Richard J. Hughes led Atlantic City to host the Democratic Convention. Atlantic City (sometimes referred to as ""Monopoly City"") has become well-known over the years for its portrayal in the U.S. version of the popular board game, Monopoly, in which properties on the board are named after locations in and near Atlantic City. While the original incarnation of the game did not feature Atlantic City, it was in Indianapolis that Ruth Hoskins learned the game, and took it back to Atlantic City. After she arrived, Hoskins made a new board with Atlantic City street names, and taught it to a group of local Quakers. The first road connecting the city to the mainland at Pleasantville was completed in 1870 and charged a 30-cent toll. Albany Avenue was the first road to the mainland that was available without a toll. Because of its location in South Jersey, hugging the Atlantic Ocean between marshlands and islands, Atlantic City was viewed by developers as prime real estate and a potential resort town. In 1853, the first commercial hotel, The Belloe House, located at Massachusetts and Atlantic Avenue, was built. Like many older east coast cities after World War II, Atlantic City became plagued with poverty, crime, corruption, and general economic decline in the mid-to-late 20th century. The neighborhood known as the ""Inlet"" became particularly impoverished. The reasons for the resort's decline were multi-layered. First of all, the automobile became more readily available to many Americans after the war. Atlantic City had initially relied upon visitors coming by train and staying for a couple of weeks. The car allowed them to come and go as they pleased, and many people would spend only a few days, rather than weeks. Also, the advent of suburbia played a huge role. With many families moving to their own private houses, luxuries such as home air conditioning and swimming pools diminished their interest in flocking to the luxury beach resorts during the hot summer. But perhaps the biggest factor in the decline in Atlantic City's popularity came from cheap, fast jet service to other premier resorts, such as Miami Beach and the Bahamas. In an effort at revitalizing the city, New Jersey voters in 1976 passed a referendum, approving casino gambling for Atlantic City; this came after a 1974 referendum on legalized gambling failed to pass. Immediately after the legislation passed, the owners of the Chalfonte-Haddon Hall Hotel began converting it into the Resorts International. It was the first legal casino in the eastern United States when it opened on May 26, 1978. Other casinos were soon constructed along the Boardwalk and, later, in the marina district for a total of eleven today. The introduction of gambling did not, however, quickly eliminate many of the urban problems that plagued Atlantic City. Many people have suggested that it only served to exacerbate those problems, as attested to by the stark contrast between tourism intensive areas and the adjacent impoverished working-class neighborhoods. In addition, Atlantic City has been less popular than Las Vegas, as a gambling city in the United States. Donald Trump helped bring big name boxing bouts to the city to attract customers to his casinos. The boxer Mike Tyson had most of his fights in Atlantic City in the 1980s, which helped Atlantic City achieve nationwide attention as a gambling resort. Numerous highrise condominiums were built for use as permanent residences or second homes. By end of the decade it was one of the most popular tourist destinations in the United States. Marvin Gardens, the leading yellow property on the board shown, is actually a misspelling of the original location name, ""Marven Gardens"". The misspelling was said to have been introduced by Charles Todd and passed on when his home-made Monopoly board was copied by Charles Darrow and thence Parker Brothers. It was not until 1995 that Parker Brothers acknowledged this mistake and formally apologized to the residents of Marven Gardens for the misspelling although the spelling error was not corrected. In 1883, salt water taffy was conceived in Atlantic City by David Bradley. The traditional story is that Bradley's shop was flooded after a major storm, soaking his taffy with salty Atlantic Ocean water. He sold some ""salt water taffy"" to a girl, who proudly walked down to the beach to show her friends. Bradley's mother was in the back of the store when the sale was made, and loved the name, and so salt water taffy was born. ""Superstorm Sandy"" struck Atlantic City on October 29, 2012, causing flooding and power-outages but left minimal damage to any of the tourist areas including the Boardwalk and casino resorts, despite widespread belief that the city's boardwalk had been destroyed. The source of the misinformation was a widely circulated photograph of a damaged section of the Boardwalk that was slated for repairs, prior to the storm, and incorrect news reports at the time of the disaster. The storm produced an all-time record low barometric pressure reading of 943 mb (27.85"") for not only Atlantic City, but the state of New Jersey. The city was incorporated in 1854, the same year in which the Camden and Atlantic Railroad train service began. Built on the edge of the bay, this served as the direct link of this remote parcel of land with Philadelphia, Pennsylvania. That same year, construction of the Absecon Lighthouse, designed by George Meade of the Corps of Topographical Engineers, was approved, with work initiated the next year. By 1874, almost 500,000 passengers a year were coming to Atlantic City by rail. In Boardwalk Empire: The Birth, High Times, and Corruption of Atlantic City, ""Atlantic City's Godfather"" Nelson Johnson describes the inspiration of Dr. Jonathan Pitney (the ""Father of Atlantic City"") to develop Atlantic City as a health resort, his efforts to convince the municipal authorities that a railroad to the beach would be beneficial, his successful alliance with Samuel Richards (entrepreneur and member of the most influential family in southern New Jersey at the time) to achieve that goal, the actual building of the railroad, and the experience of the first 600 riders, who ""were chosen carefully by Samuel Richards and Jonathan Pitney"": According to the United States Census Bureau, the city had a total area of 17.037 square miles (44.125 km2), including 10.747 square miles (27.835 km2) of land and 6.290 square miles (16.290 km2) of water (36.92%). In the wake of the United States' economic downturn and the legalization of gambling in adjacent and nearby states (including Delaware, Maryland, New York, and Pennsylvania), four casino closures took place in 2014: the Atlantic Club on January 13; the Showboat on August 31; the Revel, which was Atlantic City's second-newest casino, on September 2; and Trump Plaza, which originally opened in 1984, and was the poorest performing casino in the city, on September 16. At the 2010 United States Census, there were 39,558 people, 15,504 households, and 8,558 families residing in the city. The population density was 3,680.8 per square mile (1,421.2/km2). There were 20,013 housing units at an average density of 1,862.2 per square mile (719.0/km2). The racial makeup of the city was 26.65% (10,543) White, 38.29% (15,148) Black or African American, 0.61% (242) Native American, 15.55% (6,153) Asian, 0.05% (18) Pacific Islander, 14.03% (5,549) from other races, and 4.82% (1,905) from two or more races. Hispanics or Latinos of any race were 30.45% (12,044) of the population. Executives at Trump Entertainment Resorts, whose sole remaining property will be the Trump Taj Mahal, said in 2013 that they were considering the option of selling the Taj and winding down and exiting the gaming and hotel business. In the early part of the 20th century, Atlantic City went through a radical building boom. Many of the modest boarding houses that dotted the boardwalk were replaced with large hotels. Two of the city's most distinctive hotels were the Marlborough-Blenheim Hotel and the Traymore Hotel. In July 2010, Governor Chris Christie announced that a state takeover of the city and local government ""was imminent"". Comparing regulations in Atlantic City to an ""antique car"", Atlantic City regulatory reform is a key piece of Gov. Chris Christie's plan, unveiled on July 22, to reinvigorate an industry mired in a four-year slump in revenue and hammered by fresh competition from casinos in the surrounding states of Delaware, Pennsylvania, Connecticut, and more recently, Maryland. In January 2011, Chris Christie announced the Atlantic City Tourism District, a state-run district encompassing the boardwalk casinos, the marina casinos, the Atlantic City Outlets, and Bader Field. Fairleigh Dickinson University's PublicMind poll surveyed New Jersey voters' attitudes on the takeover. The February 16, 2011 survey showed that 43% opposed the measure while 29% favored direct state oversight. Interestingly, the poll also found that even South Jersey voters expressed opposition to the plan; 40% reported they opposed the measure and 37% reported they were in favor of it. Annual precipitation is 40 inches (1,020 mm) which is fairly spread throughout the year. Owing to its proximity to the Atlantic Ocean and its location in South Jersey, Atlantic City receives less snow than a good portion of the rest of New Jersey. Even at the airport, where low temperatures are often much lower than along the coast, snow averages only 16.5 inches (41.9 cm) each winter. It is very common for rain to fall in Atlantic City while the northern and western parts of the state are receiving snow. In the wake of the closures and declining revenue from casinos, Governor Christie said in September 2014 that the state would consider a 2015 referendum to end the 40-year-old monopoly that Atlantic City holds on casino gambling and allowing gambling in other municipalities. With casino revenue declining from $5.2 billion in 2006 to $2.9 billion in 2013, the state saw a drop in money from its 8% tax on those earnings, which is used to fund programs for senior citizens and the disabled. It is on Absecon Island, on the Atlantic coast. Atlantic City was incorporated on May 1, 1854, from portions of Egg Harbor Township and Galloway Township. The city borders Absecon, Brigantine, Pleasantville, Ventnor City and West Atlantic City. One by one, additional large hotels were constructed along the boardwalk, including the Brighton, Chelsea, Shelburne, Ambassador, Ritz Carlton, Mayflower, Madison House, and the Breakers. The Quaker-owned Chalfonte House, opened in 1868, and Haddon House, opened in 1869, flanked North Carolina Avenue at the beach end. Their original wood-frame structures would be enlarged, and even moved closer to the beach, over the years. The modern Chalfonte Hotel, eight stories tall, opened in 1904. The modern Haddon Hall was built in stages and was completed in 1929, at eleven stories. By this time, they were under the same ownership and merged into the Chalfonte-Haddon Hall Hotel, becoming the city's largest hotel with nearly 1,000 rooms. By 1930, the Claridge, the city's last large hotel before the casinos, opened its doors. The 400-room Claridge was built by a partnership that included renowned Philadelphia contractor John McShain. At 24 stories, it would become known as the ""Skyscraper By The Sea."" The city became known as the ""The World's Playground. In the city, 24.6% of the population were under the age of 18, 10.2% from 18 to 24, 26.8% from 25 to 44, 25.8% from 45 to 64, and 12.7% who were 65 years of age or older. The median age was 36.3 years. For every 100 females there were 96.2 males. For every 100 females age 18 and over, there were 94.4 males. The first boardwalk was built in 1870 along a portion of the beach in an effort to help hotel owners keep sand out of their lobbies. Businesses were restricted and the boardwalk was removed each year at the end of the peak season. Because of its effectiveness and popularity, the boardwalk was expanded in length and width, and modified several times in subsequent years. The historic length of the boardwalk, before the destructive 1944 Great Atlantic Hurricane, was about 7 miles (11 km) and it extended from Atlantic City to Longport, through Ventnor and Margate. Owing to economic conditions and the late 2000s recession, many of the proposed mega casinos never went further than the initial planning stages. One of these developers was Pinnacle Entertainment, who purchased the Sands Atlantic City, only to close it permanently November 11, 2006. The following year, the resort was demolished in a dramatic, Las Vegas styled implosion, the first of its kind in Atlantic City. While Pinnacle Entertainment intended to replace it with a $1.5–2 billion casino resort, the company canceled its construction plans and plans to sell the land. The biggest disappointment was when MGM Resorts International announced that it would pull out of all development for Atlantic City, effectively ending their plans for the MGM Grand Atlantic City. Although Wynn's plans for development in the city were scrapped in 2002, the tunnel opened in 2001. The new roadway prompted Boyd Gaming in partnership with MGM/Mirage to build Atlantic City's newest casino. The Borgata opened in July 2003, and its success brought an influx of developers to Atlantic City with plans for building grand Las Vegas style mega casinos to revitalize the aging city. The Traymore Hotel was located at the corner of Illinois Avenue and the boardwalk. Begun in 1879 as a small boarding house, the hotel grew through a series of uncoordinated expansions. By 1914, the hotel's owner, Daniel White, taking a hint from the Marlborough-Blenheim, commissioned the firm of Price and McLanahan to build an even bigger hotel. Rising 16 stories, the tan brick and gold-capped hotel would become one of the city's best-known landmarks. The hotel made use of ocean-facing hotel rooms by jutting its wings farther from the main portion of the hotel along Pacific Avenue. By the late 1960s, many of the resort's once great hotels were suffering from embarrassing vacancy rates. Most of them were either shut down, converted to cheap apartments, or converted to nursing home facilities by the end of the decade. Prior to and during the advent of legalized gaming, many of these hotels were demolished. The Breakers, the Chelsea, the Brighton, the Shelburne, the Mayflower, the Traymore, and the Marlborough-Blenheim were demolished in the 1970s and 1980s. Of the many pre-casino resorts that bordered the boardwalk, only the Claridge, the Dennis, the Ritz-Carlton, and the Haddon Hall survive to this day as parts of Bally's Atlantic City, a condo complex, and Resorts Atlantic City. The old Ambassador Hotel was purchased by Ramada in 1978 and was gutted to become the Tropicana Casino and Resort Atlantic City, only reusing the steelwork of the original building. Smaller hotels off the boardwalk, such as the Madison also survived. The Census Bureau's 2006–2010 American Community Survey showed that (in 2010 inflation-adjusted dollars) median household income was $30,237 (with a margin of error of +/- $2,354) and the median family income was $35,488 (+/- $2,607). Males had a median income of $32,207 (+/- $1,641) versus $29,298 (+/- $1,380) for females. The per capita income for the city was $20,069 (+/- $2,532). About 23.1% of families and 25.3% of the population were below the poverty line, including 36.6% of those under age 18 and 16.8% of those age 65 or over. There were 15,504 households, of which 27.3% had children under the age of 18 living with them, 25.9% were married couples living together, 22.2% had a female householder with no husband present, and 44.8% were non-families. 37.5% of all households were made up of individuals, and 14.3% had someone living alone who was 65 years of age or older. The average household size was 2.50 and the average family size was 3.34. From May 13 to May 16 in 1929, Johnson hosted a conference for organized crime figures from all across America. The men who called this meeting were Masseria family lieutenant Charles ""Lucky"" Luciano and former Chicago South Side Gang boss Johnny ""the Fox"" Torrio, with heads of the Bugs and Meyer Mob, Meyer Lansky and Benjamin Siegel, being used as muscle for the meeting. On April 29, 2011, the boundaries for the state-run tourism district were set. The district would include heavier police presence, as well as beautification and infrastructure improvements. The CRDA would oversee all functions of the district and will make changes to attract new businesses and attractions. New construction would be ambitious and may resort to eminent domain. During this time, Atlantic City was under the mayoral reign of Edward L. Bader, known for his contributions to the construction, athletics and aviation of Atlantic City. Despite the opposition of many others, he purchased land that became the city's municipal airport and high school football stadium, both of which were later named Bader Field in his honor. He led the initiative, in 1923, to construct the Atlantic City High School at Albany and Atlantic Avenues. Bader, in November 1923, initiated a public referendum, during the general election, at which time residents approved the construction of a Convention Center. The city passed an ordinance approving a bond issue for $1.5 million to be used for the purchase of land for Convention Hall, now known as the Boardwalk Hall, finalized September 30, 1924. Bader was also a driving force behind the creation of the Miss America competition. In the city the population was spread out with 25.7% under the age of 18, 8.9% from 18 to 24, 31.0% from 25 to 44, 20.2% from 45 to 64, and 14.2% who were 65 years of age or older. The median age was 35 years. For every 100 females there were 96.1 males. For every 100 females age 18 and over, there were 93.2 males. Atlantic City is considered as the ""Gambling Capital of the East Coast,"" and currently has eight large casinos and several smaller ones. In 2011, New Jersey's casinos employed approximately 33,000 employees, had 28.5 million visitors, made $3.3 billion in gaming revenue, and paid $278 million in taxes. They are regulated by the New Jersey Casino Control Commission and the New Jersey Division of Gaming Enforcement. There were 15,848 households out of which 27.7% had children under the age of 18 living with them, 24.8% were married couples living together, 23.2% had a female householder with no husband present, and 45.1% were non-families. 37.2% of all households were made up of individuals and 15.4% had someone living alone who was 65 years of age or older. The average household size was 2.46 and the average family size was 3.26. Unincorporated communities, localities and place names located partially or completely within the city include Chelsea, City Island, Great Island and Venice Park. Caesars Entertainment executives have been reconsidering the future of their three remaining Atlantic City properties (Bally's, Caesars and Harrah's), in the wake of a Chapter 11 bankruptcy filing by the company's casino operating unit in January 2015. The tourism district would comprise several key areas in the city; the Marina District, Ducktown, Chelsea, South Inlet, Bader Field, and Gardner's Basin. Also included are 10 roadways that lead into the district, including several in the city's northern end, or North Beach. Gardner's Basin, which is home to the Atlantic City Aquarium, was initially left out of the tourism district, while a residential neighborhood in the Chelsea section was removed from the final boundaries, owing to complaints from the city. Also, the inclusion of Bader Field in the district was controversial and received much scrutiny from mayor Lorenzo Langford, who cast the lone ""no"" vote on the creation of the district citing its inclusion. With the redevelopment of Las Vegas and the opening of two casinos in Connecticut in the early 1990s, along with newly built casinos in the nearby Philadelphia metro area in the 2000s, Atlantic City's tourism began to decline due to its failure to diversify away from gaming. Determined to expand, in 1999 the Atlantic City Redevelopment Authority partnered with Las Vegas casino mogul Steve Wynn to develop a new roadway to a barren section of the city near the Marina. Nicknamed ""The Tunnel Project"", Steve Wynn planned the proposed 'Mirage Atlantic City' around the idea that he would connect the $330 million tunnel stretching 2.5 miles (4.0 km) from the Atlantic City Expressway to his new resort. The roadway was later officially named the Atlantic City-Brigantine Connector, and funnels incoming traffic off of the expressway into the city's marina district and Brigantine, New Jersey. In 2006, Morgan Stanley purchased 20 acres (8.1 ha) directly north of the Showboat Atlantic City Hotel and Casino for a new $2 billion plus casino resort. Revel Entertainment Group was named as the project's developer for the Revel Casino. Revel was hindered with many problems, with the biggest setback to the company being in April 2010 when Morgan Stanley, the owner of 90% of Revel Entertainment Group, decided to discontinue funding for continued construction and put its stake in Revel up for sale. Early in 2010 the New Jersey state legislature passed a bill offering tax incentives to attract new investors and complete the job, but a poll by Fairleigh Dickinson University's PublicMind released in March 2010 showed that three of five voters (60%) opposed the legislation, and two of three of those who opposed it ""strongly"" opposed it. Ultimately, Governor Chris Christie offered Revel $261 million in state tax credits to assist the casino once it opened. As of March 2011[update], Revel had completed all of the exterior work and had continued work on the interior after finally receiving the funding necessary to complete construction. It had a soft opening in April 2012, and was fully open by May 2012. Ten months later, in February 2013, after serious losses and a write-down in the value of the resort from $2.4 billion to $450 million, Revel filed for Chapter 11 bankruptcy. It was restructured but still could not carry on and re-entered bankruptcy on June 19, 2014. It was put up for sale, however as no suitable bids were received the resort closed its doors on September 2, 2014."
Korean_War,"On Saturday, 24 June 1950, U.S. Secretary of State Dean Acheson informed President Truman that the North Koreans had invaded South Korea. Truman and Acheson discussed a U.S. invasion response and agreed that the United States was obligated to act, paralleling the North Korean invasion with Adolf Hitler's aggressions in the 1930s, with the conclusion being that the mistake of appeasement must not be repeated. Several U.S. industries were mobilized to supply materials, labor, capital, production facilities, and other services necessary to support the military objectives of the Korean War. However, President Truman later acknowledged that he believed fighting the invasion was essential to the American goal of the global containment of communism as outlined in the National Security Council Report 68 (NSC-68) (declassified in 1975): Korea was considered to be part of the Empire of Japan as an industrialized colony along with Taiwan, and both were part of the Greater East Asia Co-Prosperity Sphere. In 1937, the colonial Governor-General, General Jirō Minami, commanded the attempted cultural assimilation of Korea's 23.5 million people by banning the use and study of Korean language, literature, and culture, to be replaced with that of mandatory use and study of their Japanese counterparts. Starting in 1939, the populace was required to use Japanese names under the Sōshi-kaimei policy. Conscription of Koreans for labor in war industries began in 1939, with as many as 2 million Koreans conscripted into either the Japanese Army or into the Japanese labor force. The resultant South Korean government promulgated a national political constitution on 17 July 1948, and elected Syngman Rhee as President on 20 July 1948. The Republic of Korea (South Korea) was established on 15 August 1948. In the Russian Korean Zone of Occupation, the Soviet Union established a Communist North Korean government led by Kim Il-sung. President Rhee's régime excluded communists and leftists from southern politics. Disenfranchised, they headed for the hills, to prepare for guerrilla war against the US-sponsored ROK Government. A major consideration was the possible Soviet reaction in the event that the US intervened. The Truman administration was fretful that a war in Korea was a diversionary assault that would escalate to a general war in Europe once the United States committed in Korea. At the same time, ""[t]here was no suggestion from anyone that the United Nations or the United States could back away from [the conflict]"". Yugoslavia–a possible Soviet target because of the Tito-Stalin Split—was vital to the defense of Italy and Greece, and the country was first on the list of the National Security Council's post-North Korea invasion list of ""chief danger spots"". Truman believed if aggression went unchecked a chain reaction would be initiated that would marginalize the United Nations and encourage Communist aggression elsewhere. The UN Security Council approved the use of force to help the South Koreans and the US immediately began using what air and naval forces that were in the area to that end. The Administration still refrained from committing on the ground because some advisers believed the North Koreans could be stopped by air and naval power alone. During the Hungnam evacuation, about 193 shiploads of UN Command forces and matériel (approximately 105,000 soldiers, 98,000 civilians, 17,500 vehicles, and 350,000 tons of supplies) were evacuated to Pusan. The SS Meredith Victory was noted for evacuating 14,000 refugees, the largest rescue operation by a single ship, even though it was designed to hold 12 passengers. Before escaping, the UN Command forces razed most of Hungnam city, especially the port facilities; and on 16 December 1950, President Truman declared a national emergency with Presidential Proclamation No. 2914, 3 C.F.R. 99 (1953), which remained in force until 14 September 1978.[b] The next day (17 December 1950) Kim Il-sung was deprived of the right of command of KPA by China. After that, the leading part of the war became the Chinese army. Following that, on 1 February 1951, United Nations General Assembly adopted a draft resolution condemning China as an aggressor in the Korean War. By 1 October 1950, the UN Command repelled the KPA northwards past the 38th parallel; the ROK Army crossed after them, into North Korea. MacArthur made a statement demanding the KPA's unconditional surrender. Six days later, on 7 October, with UN authorization, the UN Command forces followed the ROK forces northwards. The X Corps landed at Wonsan (in southeastern North Korea) and Riwon (in northeastern North Korea), already captured by ROK forces. The Eighth U.S. Army and the ROK Army drove up western Korea and captured Pyongyang city, the North Korean capital, on 19 October 1950. The 187th Airborne Regimental Combat Team (""Rakkasans"") made their first of two combat jumps during the Korean War on 20 October 1950 at Sunchon and Sukchon. The missions of the 187th were to cut the road north going to China, preventing North Korean leaders from escaping from Pyongyang; and to rescue American prisoners of war. At month's end, UN forces held 135,000 KPA prisoners of war. As they neared the Sino-Korean border, the UN forces in the west were divided from those in the east by 50–100 miles of mountainous terrain. On 1 March 1951 Mao sent a cable to Stalin, in which he emphasized the difficulties faced by Chinese forces and the urgent need for air cover, especially over supply lines. Apparently impressed by the Chinese war effort, Stalin finally agreed to supply two air force divisions, three anti-aircraft divisions, and six thousand trucks. PVA troops in Korea continued to suffer severe logistical problems throughout the war. In late April Peng Dehuai sent his deputy, Hong Xuezhi, to brief Zhou Enlai in Beijing. What Chinese soldiers feared, Hong said, was not the enemy, but that they had nothing to eat, no bullets to shoot, and no trucks to transport them to the rear when they were wounded. Zhou attempted to respond to the PVA's logistical concerns by increasing Chinese production and improving methods of supply, but these efforts were never completely sufficient. At the same time, large-scale air defense training programs were carried out, and the Chinese Air Force began to participate in the war from September 1951 onward. The North Korean contributions to the Chinese Communist victory were not forgotten after the creation of the People's Republic of China in 1949. As a token of gratitude, between 50,000 and 70,000 Korean veterans that served in the PLA were sent back along with their weapons, and they later played a significant role in the initial invasion of South Korea. China promised to support the North Koreans in the event of a war against South Korea. The Chinese support created a deep division between the Korean Communists, and Kim Il-sung's authority within the Communist party was challenged by the Chinese faction led by Pak Il-yu, who was later purged by Kim. In the resulting Battle of Pusan Perimeter (August–September 1950), the U.S. Army withstood KPA attacks meant to capture the city at the Naktong Bulge, P'ohang-dong, and Taegu. The United States Air Force (USAF) interrupted KPA logistics with 40 daily ground support sorties that destroyed 32 bridges, halting most daytime road and rail traffic. KPA forces were forced to hide in tunnels by day and move only at night. To deny matériel to the KPA, the USAF destroyed logistics depots, petroleum refineries, and harbors, while the U.S. Navy air forces attacked transport hubs. Consequently, the over-extended KPA could not be supplied throughout the south. On 27 August, 67th Fighter Squadron aircraft mistakenly attacked facilities in Chinese territory and the Soviet Union called the UN Security Council's attention to China's complaint about the incident. The US proposed that a commission of India and Sweden determine what the US should pay in compensation but the Soviets vetoed the US proposal. Meanwhile, on 10 October 1950, the 89th Tank Battalion was attached to the 1st Cavalry Division, increasing the armor available for the Northern Offensive. On 15 October, after moderate KPA resistance, the 7th Cavalry Regiment and Charlie Company, 70th Tank Battalion captured Namchonjam city. On 17 October, they flanked rightwards, away from the principal road (to Pyongyang), to capture Hwangju. Two days later, the 1st Cavalry Division captured Pyongyang, the North's capital city, on 19 October 1950. Kim Il Sung and his government temporarily moved its capital to Sinuiju – although as UNC forces approached, the government again moved – this time to Kanggye. One facet of the changing attitude toward Korea and whether to get involved was Japan. Especially after the fall of China to the Communists, U.S. East Asian experts saw Japan as the critical counterweight to the Soviet Union and China in the region. While there was no United States policy that dealt with South Korea directly as a national interest, its proximity to Japan increased the importance of South Korea. Said Kim: ""The recognition that the security of Japan required a non-hostile Korea led directly to President Truman's decision to intervene... The essential point... is that the American response to the North Korean attack stemmed from considerations of US policy toward Japan."" By mid-1950, North Korean forces numbered between 150,000 and 200,000 troops, organized into 10 infantry divisions, one tank division, and one air force division, with 210 fighter planes and 280 tanks, who captured scheduled objectives and territory, among them Kaesong, Chuncheon, Uijeongbu, and Ongjin. Their forces included 274 T-34-85 tanks, 200 artillery pieces, 110 attack bombers, some 150 Yak fighter planes, 78 Yak trainers, and 35 reconnaissance aircraft. In addition to the invasion force, the North KPA had 114 fighters, 78 bombers, 105 T-34-85 tanks, and some 30,000 soldiers stationed in reserve in North Korea. Although each navy consisted of only several small warships, the North and South Korean navies fought in the war as sea-borne artillery for their in-country armies. By August, the KPA had pushed back the ROK Army and the Eighth United States Army to the vicinity of Pusan in southeast Korea. In their southward advance, the KPA purged the Republic of Korea's intelligentsia by killing civil servants and intellectuals. On 20 August, General MacArthur warned North Korean leader Kim Il-sung that he was responsible for the KPA's atrocities. By September, the UN Command controlled the Pusan perimeter, enclosing about 10% of Korea, in a line partially defined by the Nakdong River. Acting on State Secretary Acheson's recommendation, President Truman ordered General MacArthur to transfer matériel to the Army of the Republic of Korea while giving air cover to the evacuation of U.S. nationals. The President disagreed with advisers who recommended unilateral U.S. bombing of the North Korean forces, and ordered the US Seventh Fleet to protect the Republic of China (Taiwan), whose government asked to fight in Korea. The United States denied ROC's request for combat, lest it provoke a communist Chinese retaliation. Because the United States had sent the Seventh Fleet to ""neutralize"" the Taiwan Strait, Chinese premier Zhou Enlai criticized both the UN and U.S. initiatives as ""armed aggression on Chinese territory."" The initial assault by North Korean KPA forces were aided by the use of Soviet T-34-85 tanks. A North Korean tank corps equipped with about 120 T-34s spearheaded the invasion. These drove against a ROK Army with few anti-tank weapons adequate to deal with the Soviet T-34s. Additional Soviet armor was added as the offensive progressed. The North Korean tanks had a good deal of early successes against South Korean infantry, elements of the 24th Infantry Division, and those United States built M24 Chaffee light tanks that they encountered. Interdiction by ground attack aircraft was the only means of slowing the advancing Korean armor. The tide turned in favour of the United Nations forces in August 1950 when the North Koreans suffered major tank losses during a series of battles in which the UN forces brought heavier equipment to bear, including M4A3 Sherman medium tanks backed by U.S. M26 heavy tanks, along with the British Centurion, Churchill, and Cromwell tanks. After the Incheon landing, the 1st Cavalry Division began its northward advance from the Pusan Perimeter. ""Task Force Lynch"" (after Lieutenant Colonel James H. Lynch), 3rd Battalion, 7th Cavalry Regiment, and two 70th Tank Battalion units (Charlie Company and the Intelligence–Reconnaissance Platoon) effected the ""Pusan Perimeter Breakout"" through 106.4 miles (171.2 km) of enemy territory to join the 7th Infantry Division at Osan. The X Corps rapidly defeated the KPA defenders around Seoul, thus threatening to trap the main KPA force in Southern Korea. With Lieutenant-General Matthew Ridgway assuming the command of the U.S. Eighth Army on 26 December, the PVA and the KPA launched their Third Phase Offensive (also known as the ""Chinese New Year's Offensive"") on New Year's Eve of 1950. Utilizing night attacks in which UN Command fighting positions were encircled and then assaulted by numerically superior troops who had the element of surprise, the attacks were accompanied by loud trumpets and gongs, which fulfilled the double purpose of facilitating tactical communication and mentally disorienting the enemy. UN forces initially had no familiarity with this tactic, and as a result some soldiers panicked, abandoning their weapons and retreating to the south. The Chinese New Year's Offensive overwhelmed UN forces, allowing the PVA and KPA to conquer Seoul for the second time on 4 January 1951. The Demilitarized Zone runs northeast of the 38th parallel; to the south, it travels west. The old Korean capital city of Kaesong, site of the armistice negotiations, originally was in pre-war South Korea, but now is part of North Korea. The United Nations Command, supported by the United States, the North Korean People's Army, and the Chinese People's Volunteers, signed the Armistice Agreement on 27 July 1953 to end the fighting. The Armistice also called upon the governments of South Korea, North Korea, China and the United States to participate in continued peace talks. The war is considered to have ended at this point, even though there was no peace treaty. North Korea nevertheless claims that it won the Korean War. On 27 June 1950, two days after the KPA invaded and three months before the Chinese entered the war, President Truman dispatched the United States Seventh Fleet to the Taiwan Strait, to prevent hostilities between the Nationalist Republic of China (Taiwan) and the People's Republic of China (PRC). On 4 August 1950, with the PRC invasion of Taiwan aborted, Mao Zedong reported to the Politburo that he would intervene in Korea when the People's Liberation Army's (PLA) Taiwan invasion force was reorganized into the PLA North East Frontier Force. China justified its entry into the war as a response to ""American aggression in the guise of the UN"". On 25 November at the Korean western front, the PVA 13th Army Group attacked and overran the ROK II Corps at the Battle of the Ch'ongch'on River, and then decimated the US 2nd Infantry Division on the UN forces' right flank. The UN Command retreated; the U.S. Eighth Army's retreat (the longest in US Army history) was made possible because of the Turkish Brigade's successful, but very costly, rear-guard delaying action near Kunuri that slowed the PVA attack for two days (27–29 November). On 27 November at the Korean eastern front, a U.S. 7th Infantry Division Regimental Combat Team (3,000 soldiers) and the U.S. 1st Marine Division (12,000–15,000 marines) were unprepared for the PVA 9th Army Group's three-pronged encirclement tactics at the Battle of Chosin Reservoir, but they managed to escape under Air Force and X Corps support fire—albeit with some 15,000 collective casualties. During World War II, Japan used Korea's food, livestock, and metals for their war effort. Japanese forces in Korea increased from 46,000 soldiers in 1941 to 300,000 in 1945. Japanese Korea conscripted 2.6 million forced laborers controlled with a collaborationist Korean police force; some 723,000 people were sent to work in the overseas empire and in metropolitan Japan. By 1942, Korean men were being conscripted into the Imperial Japanese Army. By January 1945, Koreans made up 32% of Japan's labor force. At the end of the war, other world powers did not recognize Japanese rule in Korea and Taiwan. By 30 November, the PVA 13th Army Group managed to expel the U.S. Eighth Army from northwest Korea. Retreating from the north faster than they had counter-invaded, the Eighth Army crossed the 38th parallel border in mid December. UN morale hit rock bottom when commanding General Walton Walker of the U.S. Eighth Army was killed on 23 December 1950 in an automobile accident. In northeast Korea by 11 December, the U.S. X Corps managed to cripple the PVA 9th Army Group while establishing a defensive perimeter at the port city of Hungnam. The X Corps were forced to evacuate by 24 December in order to reinforce the badly depleted U.S. Eighth Army to the south. On 8 September 1945, U.S. Lt. Gen. John R. Hodge arrived in Incheon to accept the Japanese surrender south of the 38th parallel. Appointed as military governor, General Hodge directly controlled South Korea as head of the United States Army Military Government in Korea (USAMGIK 1945–48). He established control by restoring to power the key Japanese colonial administrators, but in the face of Korean protests he quickly reversed this decision. The USAMGIK refused to recognize the provisional government of the short-lived People's Republic of Korea (PRK) because it suspected it was communist. UN aerial reconnaissance had difficulty sighting PVA units in daytime, because their march and bivouac discipline minimized aerial detection. The PVA marched ""dark-to-dark"" (19:00–03:00), and aerial camouflage (concealing soldiers, pack animals, and equipment) was deployed by 05:30. Meanwhile, daylight advance parties scouted for the next bivouac site. During daylight activity or marching, soldiers were to remain motionless if an aircraft appeared, until it flew away; PVA officers were under order to shoot security violators. Such battlefield discipline allowed a three-division army to march the 286 miles (460 km) from An-tung, Manchuria, to the combat zone in some 19 days. Another division night-marched a circuitous mountain route, averaging 18 miles (29 km) daily for 18 days. After secretly crossing the Yalu River on 19 October, the PVA 13th Army Group launched the First Phase Offensive on 25 October, attacking the advancing UN forces near the Sino-Korean border. This military decision made solely by China changed the attitude of the Soviet Union. Twelve days after Chinese troops entered the war, Stalin allowed the Soviet Air Force to provide air cover, and supported more aid to China. After decimating the ROK II Corps at the Battle of Onjong, the first confrontation between Chinese and U.S. military occurred on 1 November 1950; deep in North Korea, thousands of soldiers from the PVA 39th Army encircled and attacked the U.S. 8th Cavalry Regiment with three-prong assaults—from the north, northwest, and west—and overran the defensive position flanks in the Battle of Unsan. The surprise assault resulted in the UN forces retreating back to the Ch'ongch'on River, while the Chinese unexpectedly disappeared into mountain hideouts following victory. It is unclear why the Chinese did not press the attack and follow up their victory. As an initial response, Truman called for a naval blockade of North Korea, and was shocked to learn that such a blockade could be imposed only 'on paper', since the U.S. Navy no longer had the warships with which to carry out his request. In fact, because of the extensive defense cuts and the emphasis placed on building a nuclear bomber force, none of the services were in a position to make a robust response with conventional military strength. General Omar Bradley, Chairman of the Joint Chiefs of Staff, was faced with re-organizing and deploying an American military force that was a shadow of its World War II counterpart. The impact of the Truman administration's defense budget cutbacks were now keenly felt, as American troops fought a series of costly rearguard actions. Lacking sufficient anti-tank weapons, artillery or armor, they were driven back down the Korean peninsula to Pusan. In a postwar analysis of the unpreparedness of U.S. Army forces deployed to Korea during the summer and fall of 1950, Army Major General Floyd L. Parks stated that ""Many who never lived to tell the tale had to fight the full range of ground warfare from offensive to delaying action, unit by unit, man by man ... [T]hat we were able to snatch victory from the jaws of defeat ... does not relieve us from the blame of having placed our own flesh and blood in such a predicament."" On 15 October 1950, President Truman and General MacArthur met at Wake Island in the mid-Pacific Ocean. This meeting was much publicized because of the General's discourteous refusal to meet the President on the continental United States. To President Truman, MacArthur speculated there was little risk of Chinese intervention in Korea, and that the PRC's opportunity for aiding the KPA had lapsed. He believed the PRC had some 300,000 soldiers in Manchuria, and some 100,000–125,000 soldiers at the Yalu River. He further concluded that, although half of those forces might cross south, ""if the Chinese tried to get down to Pyongyang, there would be the greatest slaughter"" without air force protection. On 25 September, Seoul was recaptured by South Korean forces. American air raids caused heavy damage to the KPA, destroying most of its tanks and much of its artillery. North Korean troops in the south, instead of effectively withdrawing north, rapidly disintegrated, leaving Pyongyang vulnerable. During the general retreat only 25,000 to 30,000 soldiers managed to rejoin the Northern KPA lines. On 27 September, Stalin convened an emergency session of the Politburo, in which he condemned the incompetence of the KPA command and held Soviet military advisers responsible for the defeat. In order to enlist Stalin's support, Zhou and a Chinese delegation left for Moscow on 8 October, arriving there on 10 October at which point they flew to Stalin's home at the Black Sea. There they conferred with the top Soviet leadership which included Joseph Stalin as well as Vyacheslav Molotov, Lavrentiy Beria and Georgi Malenkov. Stalin initially agreed to send military equipment and ammunition, but warned Zhou that the Soviet Union's air force would need two or three months to prepare any operations. In a subsequent meeting, Stalin told Zhou that he would only provide China with equipment on a credit basis, and that the Soviet air force would only operate over Chinese airspace, and only after an undisclosed period of time. Stalin did not agree to send either military equipment or air support until March 1951. Mao did not find Soviet air support especially useful, as the fighting was going to take place on the south side of the Yalu. Soviet shipments of matériel, when they did arrive, were limited to small quantities of trucks, grenades, machine guns, and the like. In China, the war is officially called the ""War to Resist U.S. Aggression and Aid Korea"" (simplified Chinese: 抗美援朝战争; traditional Chinese: 抗美援朝戰爭; pinyin: Kàngměiyuáncháo zhànzhēng), although the term ""Chaoxian (Korean) War"" (simplified Chinese: 朝鲜战争; traditional Chinese: 朝鮮戰爭; pinyin: Cháoxiǎn zhànzhēng) is also used in unofficial contexts, along with the term ""Korean Conflict"" (simplified Chinese: 韩战; traditional Chinese: 韓戰; pinyin: Hán Zhàn) more commonly used in regions such as Hong Kong and Macau. The Battle of Osan, the first significant American engagement of the Korean War, involved the 540-soldier Task Force Smith, which was a small forward element of the 24th Infantry Division which had been flown in from Japan. On 5 July 1950, Task Force Smith attacked the North Koreans at Osan but without weapons capable of destroying the North Koreans' tanks. They were unsuccessful; the result was 180 dead, wounded, or taken prisoner. The KPA progressed southwards, pushing back the US force at Pyongtaek, Chonan, and Chochiwon, forcing the 24th Division's retreat to Taejeon, which the KPA captured in the Battle of Taejon; the 24th Division suffered 3,602 dead and wounded and 2,962 captured, including the Division's Commander, Major General William F. Dean. While these preparations were underway in the North, there were frequent clashes along the 38th parallel, especially at Kaesong and Ongjin, many initiated by the South. The Republic of Korea Army (ROK Army) was being trained by the U.S. Korean Military Advisory Group (KMAG). On the eve of war, KMAG's commander General William Lynn Roberts voiced utmost confidence in the ROK Army and boasted that any North Korean invasion would merely provide ""target practice"". For his part, Syngman Rhee repeatedly expressed his desire to conquer the North, including when American diplomat John Foster Dulles visited Korea on 18 June. On 27 June, Rhee evacuated from Seoul with some of the government. On 28 June, at 2 am, the South Korean Army blew up the highway bridge across the Han River in an attempt to stop the North Korean army. The bridge was detonated while 4,000 refugees were crossing the bridge, and hundreds were killed. Destroying the bridge also trapped many South Korean military units north of the Han River. In spite of such desperate measures, Seoul fell that same day. A number of South Korean National Assemblymen remained in Seoul when it fell, and forty-eight subsequently pledged allegiance to the North. The Soviet Union challenged the legitimacy of the war for several reasons. The ROK Army intelligence upon which Resolution 83 was based came from U.S. Intelligence; North Korea was not invited as a sitting temporary member of the UN, which violated UN Charter Article 32; and the Korean conflict was beyond the UN Charter's scope, because the initial north–south border fighting was classed as a civil war. Because the Soviet Union was boycotting the Security Council at the time, legal scholars posited that deciding upon an action of this type required the unanimous vote of the five permanent members. In the last two weeks of February 1951, Operation Roundup was followed by Operation Killer, carried out by the revitalized Eighth Army. It was a full-scale, battlefront-length attack staged for maximum exploitation of firepower to kill as many KPA and PVA troops as possible. Operation Killer concluded with I Corps re-occupying the territory south of the Han River, and IX Corps capturing Hoengseong. On 7 March 1951, the Eighth Army attacked with Operation Ripper, expelling the PVA and the KPA from Seoul on 14 March 1951. This was the city's fourth conquest in a years' time, leaving it a ruin; the 1.5 million pre-war population was down to 200,000, and people were suffering from severe food shortages. The principal battles of the stalemate include the Battle of Bloody Ridge (18 August–15 September 1951), the Battle of the Punchbowl (31 August-21 September 1951), the Battle of Heartbreak Ridge (13 September–15 October 1951), the Battle of Old Baldy (26 June–4 August 1952), the Battle of White Horse (6–15 October 1952), the Battle of Triangle Hill (14 October–25 November 1952), the Battle of Hill Eerie (21 March–21 June 1952), the sieges of Outpost Harry (10–18 June 1953), the Battle of the Hook (28–29 May 1953), the Battle of Pork Chop Hill (23 March–16 July 1953), and the Battle of Kumsong (13–27 July 1953). Chinese troops suffered from deficient military equipment, serious logistical problems, overextended communication and supply lines, and the constant threat of UN bombers. All of these factors generally led to a rate of Chinese casualties that was far greater than the casualties suffered by UN troops. The situation became so serious that, on November 1951, Zhou Enlai called a conference in Shenyang to discuss the PVA's logistical problems. At the meeting it was decided to accelerate the construction of railways and airfields in the area, to increase the number of trucks available to the army, and to improve air defense by any means possible. These commitments did little to directly address the problems confronting PVA troops. Citing the inability of the Joint Commission to make progress, the U.S. government decided to hold an election under United Nations auspices with the aim of creating an independent Korea. The Soviet authorities and the Korean Communists refused to co-operate on the grounds it would not be fair, and many South Korean politicians also boycotted it. A general election was held in the South on 10 May 1948. It was marred by terrorism and sabotage resulting in 600 deaths. North Korea held parliamentary elections three months later on 25 August. In early February, the South Korean 11th Division ran the operation to destroy the guerrillas and their sympathizer citizens in Southern Korea. During the operation, the division and police conducted the Geochang massacre and Sancheong-Hamyang massacre. In mid-February, the PVA counterattacked with the Fourth Phase Offensive and achieved initial victory at Hoengseong. But the offensive was soon blunted by the IX Corps positions at Chipyong-ni in the center. Units of the U.S. 2nd Infantry Division and the French Battalion fought a short but desperate battle that broke the attack's momentum. The battle is sometimes known as the Gettysburg of the Korean War. The battle saw 5,600 Korean, American and French troops defeat a numerically superior Chinese force. Surrounded on all sides, the U.S. 2nd Infantry Division Warrior Division's 23rd Regimental Combat Team with an attached French Battalion was hemmed in by more than 25,000 Chinese Communist forces. United Nations forces had previously retreated in the face of large Communist forces instead of getting cut off, but this time they stood and fought at odds of roughly 15 to 1. With the end of the war with Japan, the Chinese Civil War resumed between the Chinese Communists and the Chinese Nationalists. While the Communists were struggling for supremacy in Manchuria, they were supported by the North Korean government with matériel and manpower. According to Chinese sources, the North Koreans donated 2,000 railway cars worth of matériel while thousands of Koreans served in the Chinese People's Liberation Army (PLA) during the war. North Korea also provided the Chinese Communists in Manchuria with a safe refuge for non-combatants and communications with the rest of China. UN forces retreated to Suwon in the west, Wonju in the center, and the territory north of Samcheok in the east, where the battlefront stabilized and held. The PVA had outrun its logistics capability and thus were unable to press on beyond Seoul as food, ammunition, and matériel were carried nightly, on foot and bicycle, from the border at the Yalu River to the three battle lines. In late January, upon finding that the PVA had abandoned their battle lines, General Ridgway ordered a reconnaissance-in-force, which became Operation Roundup (5 February 1951). A full-scale X Corps advance proceeded, which fully exploited the UN Command's air superiority, concluding with the UN reaching the Han River and recapturing Wonju. On 18 September, Stalin dispatched General H. M. Zakharov to Korea to advise Kim Il-sung to halt his offensive around the Pusan perimeter and to redeploy his forces to defend Seoul. Chinese commanders were not briefed on North Korean troop numbers or operational plans. As the overall commander of Chinese forces, Zhou Enlai suggested that the North Koreans should attempt to eliminate the enemy forces at Inchon only if they had reserves of at least 100,000 men; otherwise, he advised the North Koreans to withdraw their forces north. After a new wave of UN sanctions, on 11 March 2013, North Korea claimed that it had invalidated the 1953 armistice. On 13 March 2013, North Korea confirmed it ended the 1953 Armistice and declared North Korea ""is not restrained by the North-South declaration on non-aggression"". On 30 March 2013, North Korea stated that it had entered a ""state of war"" with South Korea and declared that ""The long-standing situation of the Korean peninsula being neither at peace nor at war is finally over"". Speaking on 4 April 2013, the U.S. Secretary of Defense, Chuck Hagel, informed the press that Pyongyang had ""formally informed"" the Pentagon that it had ""ratified"" the potential usage of a nuclear weapon against South Korea, Japan and the United States of America, including Guam and Hawaii. Hagel also stated that the United States would deploy the Terminal High Altitude Area Defense anti-ballistic missile system to Guam, because of a credible and realistic nuclear threat from North Korea. In 1952, the United States elected a new president, and on 29 November 1952, the president-elect, Dwight D. Eisenhower, went to Korea to learn what might end the Korean War. With the United Nations' acceptance of India's proposed Korean War armistice, the KPA, the PVA, and the UN Command ceased fire with the battle line approximately at the 38th parallel. Upon agreeing to the armistice, the belligerents established the Korean Demilitarized Zone (DMZ), which has since been patrolled by the KPA and ROKA, United States, and Joint UN Commands. In contrast, the ROK Army defenders were relatively unprepared and ill-equipped. In South to the Naktong, North to the Yalu (1961), R.E. Appleman reports the ROK forces' low combat readiness as of 25 June 1950. The ROK Army had 98,000 soldiers (65,000 combat, 33,000 support), no tanks (they had been requested from the U.S. military, but requests were denied), and a 22-piece air force comprising 12 liaison-type and 10 AT6 advanced-trainer airplanes. There were no large foreign military garrisons in Korea at the time of the invasion, but there were large U.S. garrisons and air forces in Japan. Meanwhile, U.S. garrisons in Japan continually dispatched soldiers and matériel to reinforce defenders in the Pusan Perimeter. Tank battalions deployed to Korea directly from the U.S. mainland from the port of San Francisco to the port of Pusan, the largest Korean port. By late August, the Pusan Perimeter had some 500 medium tanks battle-ready. In early September 1950, ROK Army and UN Command forces outnumbered the KPA 180,000 to 100,000 soldiers. The UN forces, once prepared, counterattacked and broke out of the Pusan Perimeter. The Chinese counterattacked in April 1951, with the Fifth Phase Offensive, also known as the Chinese Spring Offensive, with three field armies (approximately 700,000 men). The offensive's first thrust fell upon I Corps, which fiercely resisted in the Battle of the Imjin River (22–25 April 1951) and the Battle of Kapyong (22–25 April 1951), blunting the impetus of the offensive, which was halted at the ""No-name Line"" north of Seoul. On 15 May 1951, the Chinese commenced the second impulse of the Spring Offensive and attacked the ROK Army and the U.S. X Corps in the east at the Soyang River. After initial success, they were halted by 20 May. At month's end, the U.S. Eighth Army counterattacked and regained ""Line Kansas"", just north of the 38th parallel. The UN's ""Line Kansas"" halt and subsequent offensive action stand-down began the stalemate that lasted until the armistice of 1953. During most of the war, the UN navies patrolled the west and east coasts of North Korea, sinking supply and ammunition ships and denying the North Koreans the ability to resupply from the sea. Aside from very occasional gunfire from North Korean shore batteries, the main threat to United States and UN navy ships was from magnetic mines. During the war, five U.S. Navy ships were lost to mines: two minesweepers, two minesweeper escorts, and one ocean tug. Mines and gunfire from North Korean coastal artillery damaged another 87 U.S. warships, resulting in slight to moderate damage. The on-again, off-again armistice negotiations continued for two years, first at Kaesong, on the border between North and South Korea, and then at the neighbouring village of Panmunjom. A major, problematic negotiation point was prisoner of war (POW) repatriation. The PVA, KPA, and UN Command could not agree on a system of repatriation because many PVA and KPA soldiers refused to be repatriated back to the north, which was unacceptable to the Chinese and North Koreans. In the final armistice agreement, signed on 27 July 1953, a Neutral Nations Repatriation Commission, under the chairman Indian General K. S. Thimayya, was set up to handle the matter. For the remainder of the Korean War the UN Command and the PVA fought, but exchanged little territory; the stalemate held. Large-scale bombing of North Korea continued, and protracted armistice negotiations began 10 July 1951 at Kaesong. On the Chinese side, Zhou Enlai directed peace talks, and Li Kenong and Qiao Guanghua headed the negotiation team. Combat continued while the belligerents negotiated; the UN Command forces' goal was to recapture all of South Korea and to avoid losing territory. The PVA and the KPA attempted similar operations, and later effected military and psychological operations in order to test the UN Command's resolve to continue the war. The Chinese intervention in late October 1950 bolstered the Korean People's Air Force (KPAF) of North Korea with the MiG-15, one of the world's most advanced jet fighters. The fast, heavily armed MiG outflew first-generation UN jets such as the F-80 (United States Air Force) and Gloster Meteors (Royal Australian Air Force), posing a real threat to B-29 Superfortress bombers even under fighter escort. Fearful of confronting the United States directly, the Soviet Union denied involvement of their personnel in anything other than an advisory role, but air combat quickly resulted in Soviet pilots dropping their code signals and speaking over the wireless in Russian. This known direct Soviet participation was a casus belli that the UN Command deliberately overlooked, lest the war for the Korean peninsula expand to include the Soviet Union, and potentially escalate into atomic warfare. Against the rested and re-armed Pusan Perimeter defenders and their reinforcements, the KPA were undermanned and poorly supplied; unlike the UN Command, they lacked naval and air support. To relieve the Pusan Perimeter, General MacArthur recommended an amphibious landing at Inchon (now known as Incheon), near Seoul and well over 100 miles (160 km) behind the KPA lines. On 6 July, he ordered Major General Hobart R. Gay, Commander, 1st Cavalry Division, to plan the division's amphibious landing at Incheon; on 12–14 July, the 1st Cavalry Division embarked from Yokohama, Japan to reinforce the 24th Infantry Division inside the Pusan Perimeter. On 27 September, MacArthur received the top secret National Security Council Memorandum 81/1 from Truman reminding him that operations north of the 38th parallel were authorized only if ""at the time of such operation there was no entry into North Korea by major Soviet or Chinese Communist forces, no announcements of intended entry, nor a threat to counter our operations militarily..."" On 29 September MacArthur restored the government of the Republic of Korea under Syngman Rhee. On 30 September, Defense Secretary George Marshall sent an eyes-only message to MacArthur: ""We want you to feel unhampered tactically and strategically to proceed north of the 38th parallel."" During October, the ROK police executed people who were suspected to be sympathetic to North Korea, and similar massacres were carried out until early 1951. On 11 April 1951, Commander-in-Chief Truman relieved the controversial General MacArthur, the Supreme Commander in Korea. There were several reasons for the dismissal. MacArthur had crossed the 38th parallel in the mistaken belief that the Chinese would not enter the war, leading to major allied losses. He believed that whether or not to use nuclear weapons should be his own decision, not the President's. MacArthur threatened to destroy China unless it surrendered. While MacArthur felt total victory was the only honorable outcome, Truman was more pessimistic about his chances once involved in a land war in Asia, and felt a truce and orderly withdrawal from Korea could be a valid solution. MacArthur was the subject of congressional hearings in May and June 1951, which determined that he had defied the orders of the President and thus had violated the U.S. Constitution. A popular criticism of MacArthur was that he never spent a night in Korea, and directed the war from the safety of Tokyo. Korea was ruled by Japan from 1910 until the closing days of World War II. In August 1945, the Soviet Union declared war on Japan and—by agreement with the United States—occupied Korea north of the 38th parallel. U.S. forces subsequently occupied the south and Japan surrendered. By 1948, two separate governments had been set up. Both governments claimed to be the legitimate government of Korea, and neither side accepted the border as permanent. The conflict escalated into open warfare when North Korean forces—supported by the Soviet Union and China—invaded South Korea on 25 June 1950. On that day, the United Nations Security Council recognized this North Korean act as invasion and called for an immediate ceasefire. On 27 June, the Security Council adopted S/RES/83: Complaint of aggression upon the Republic of Korea and decided the formation and dispatch of the UN Forces in Korea. Twenty-one countries of the United Nations eventually contributed to the defense of South Korea, with the United States providing 88% of the UN's military personnel. Soon after the war began, General MacArthur had begun planning a landing at Incheon, but the Pentagon opposed him. When authorized, he activated a combined U.S. Army and Marine Corps, and ROK Army force. The X Corps, led by General Edward Almond, Commander, consisted of 40,000 men of the 1st Marine Division, the 7th Infantry Division and around 8,600 ROK Army soldiers. By 15 September, the amphibious assault force faced few KPA defenders at Incheon: military intelligence, psychological warfare, guerrilla reconnaissance, and protracted bombardment facilitated a relatively light battle. However, the bombardment destroyed most of the city of Incheon. On the night of 10 August in Washington, American Colonels Dean Rusk and Charles H. Bonesteel III were tasked with dividing the Korean Peninsula into Soviet and U.S. occupation zones and proposed the 38th parallel. This was incorporated into America's General Order No. 1 which responded to the Japanese surrender on 15 August. Explaining the choice of the 38th parallel, Rusk observed, ""even though it was further north than could be realistically reached by U.S. forces, in the event of Soviet disagreement...we felt it important to include the capital of Korea in the area of responsibility of American troops"". He noted that he was ""faced with the scarcity of US forces immediately available, and time and space factors, which would make it difficult to reach very far north, before Soviet troops could enter the area"". As Rusk's comments indicate, the Americans doubted whether the Soviet government would agree to this. Stalin, however, maintained his wartime policy of co-operation, and on 16 August the Red Army halted at the 38th parallel for three weeks to await the arrival of U.S. forces in the south. In a series of emergency meetings that lasted from 2–5 October, Chinese leaders debated whether to send Chinese troops into Korea. There was considerable resistance among many leaders, including senior military leaders, to confronting the U.S. in Korea. Mao strongly supported intervention, and Zhou was one of the few Chinese leaders who firmly supported him. After Lin Biao politely refused Mao's offer to command Chinese forces in Korea (citing his upcoming medical treatment), Mao decided that Peng Dehuai would be the commander of the Chinese forces in Korea after Peng agreed to support Mao's position. Mao then asked Peng to speak in favor of intervention to the rest of the Chinese leaders. After Peng made the case that if U.S. troops conquered Korea and reached the Yalu they might cross it and invade China the Politburo agreed to intervene in Korea. Later, the Chinese claimed that US bombers had violated PRC national airspace on three separate occasions and attacked Chinese targets before China intervened. On 8 October 1950, Mao Zedong redesignated the PLA North East Frontier Force as the Chinese People's Volunteer Army (PVA). The USAF countered the MiG-15 by sending over three squadrons of its most capable fighter, the F-86 Sabre. These arrived in December 1950. The MiG was designed as a bomber interceptor. It had a very high service ceiling—50,000 feet (15,000 m) and carried very heavy weaponry: one 37 mm cannon and two 23 mm cannons. They were fast enough to dive past the fighter escort of P-80 Shooting Stars and F9F Panthers and could reach and destroy the U.S. heavy bombers. B-29 losses could not be avoided, and the Air Force was forced to switch from a daylight bombing campaign to the necessarily less accurate nighttime bombing of targets. The MiGs were countered by the F-86 Sabres. They had a ceiling of 42,000 feet (13,000 m) and were armed with six .50 caliber (12.7 mm) machine guns, which were range adjusted by radar gunsights. If coming in at higher altitude the advantage of engaging or not went to the MiG. Once in a level flight dogfight, both swept-wing designs attained comparable maximum speeds of around 660 mph (1,100 km/h). The MiG climbed faster, but the Sabre turned and dived better. The Korean War (in South Korean Hangul: 한국전쟁, Hanja: 韓國戰爭, Hanguk Jeonjaeng, ""Korean War""; in North Korean Chosungul: 조국해방전쟁, Joguk Haebang Jeonjaeng, ""Fatherland Liberation War""; 25 June 1950 – 27 July 1953)[a] was started when North Korea invaded South Korea. The United Nations, with United States as the principal force, came to aid of South Korea. China, along with assistance from Soviet Union, came to aid of North Korea. The war arose from the division of Korea at the end of World War II and from the global tensions of the Cold War that developed immediately afterwards. After the war, Operation Glory was conducted from July to November 1954, to allow combatant countries to exchange their dead. The remains of 4,167 U.S. Army and U.S. Marine Corps dead were exchanged for 13,528 KPA and PVA dead, and 546 civilians dead in UN prisoner-of-war camps were delivered to the South Korean government. After Operation Glory, 416 Korean War unknown soldiers were buried in the National Memorial Cemetery of the Pacific (The Punchbowl), on the island of Oahu, Hawaii. Defense Prisoner of War/Missing Personnel Office (DPMO) records indicate that the PRC and the DPRK transmitted 1,394 names, of which 858 were correct. From 4,167 containers of returned remains, forensic examination identified 4,219 individuals. Of these, 2,944 were identified as American, and all but 416 were identified by name. From 1996 to 2006, the DPRK recovered 220 remains near the Sino-Korean border. In the months after the Shenyang conference Peng Dehuai went to Beijing several times to brief Mao and Zhou about the heavy casualties suffered by Chinese troops and the increasing difficulty of keeping the front lines supplied with basic necessities. Peng was convinced that the war would be protracted, and that neither side would be able to achieve victory in the near future. On 24 February 1952, the Military Commission, presided over by Zhou, discussed the PVA's logistical problems with members of various government agencies involved in the war effort. After the government representatives emphasized their inability to meet the demands of the war, Peng, in an angry outburst, shouted: ""You have this and that problem... You should go to the front and see with your own eyes what food and clothing the soldiers have! Not to speak of the casualties! For what are they giving their lives? We have no aircraft. We have only a few guns. Transports are not protected. More and more soldiers are dying of starvation. Can't you overcome some of your difficulties?"" The atmosphere became so tense that Zhou was forced to adjourn the conference. Zhou subsequently called a series of meetings, where it was agreed that the PVA would be divided into three groups, to be dispatched to Korea in shifts; to accelerate the training of Chinese pilots; to provide more anti-aircraft guns to the front lines; to purchase more military equipment and ammunition from the Soviet Union; to provide the army with more food and clothing; and, to transfer the responsibility of logistics to the central government. In April 1950, Stalin gave Kim permission to invade the South under the condition that Mao would agree to send reinforcements if they became needed. Stalin made it clear that Soviet forces would not openly engage in combat, to avoid a direct war with the Americans. Kim met with Mao in May 1950. Mao was concerned that the Americans would intervene but agreed to support the North Korean invasion. China desperately needed the economic and military aid promised by the Soviets. At that time, the Chinese were in the process of demobilizing half of the PLA's 5.6 million soldiers. However, Mao sent more ethnic Korean PLA veterans to Korea and promised to move an army closer to the Korean border. Once Mao's commitment was secured, preparations for war accelerated. On 7 June 1950, Kim Il-sung called for a Korea-wide election on 5–8 August 1950 and a consultative conference in Haeju on 15–17 June 1950. On 11 June, the North sent three diplomats to the South, as a peace overture that Rhee rejected. On 21 June, Kim Il-Sung revised his war plan to involve general attack across the 38th parallel, rather than a limited operation in the Ongjin peninsula. Kim was concerned that South Korean agents had learned about the plans and South Korean forces were strengthening their defenses. Stalin agreed to this change of plan. By spring 1950, Stalin believed the strategic situation had changed. The Soviets had detonated their first nuclear bomb in September 1949; American soldiers had fully withdrawn from Korea; the Americans had not intervened to stop the communist victory in China, and Stalin calculated that the Americans would be even less willing to fight in Korea—which had seemingly much less strategic significance. The Soviets had also cracked the codes used by the US to communicate with the US embassy in Moscow, and reading these dispatches convinced Stalin that Korea did not have the importance to the US that would warrant a nuclear confrontation. Stalin began a more aggressive strategy in Asia based on these developments, including promising economic and military aid to China through the Sino–Soviet Friendship, Alliance, and Mutual Assistance Treaty. At dawn on Sunday, 25 June 1950, the Korean People's Army crossed the 38th parallel behind artillery fire. The KPA justified its assault with the claim that ROK troops had attacked first, and that they were aiming to arrest and execute the ""bandit traitor Syngman Rhee"". Fighting began on the strategic Ongjin peninsula in the west. There were initial South Korean claims that they had captured the city of Haeju, and this sequence of events has led some scholars to argue that the South Koreans actually fired first. General Ridgway was appointed Supreme Commander, Korea; he regrouped the UN forces for successful counterattacks, while General James Van Fleet assumed command of the U.S. Eighth Army. Further attacks slowly depleted the PVA and KPA forces; Operations Courageous (23–28 March 1951) and Tomahawk (23 March 1951) were a joint ground and airborne infilltration meant to trap Chinese forces between Kaesong and Seoul. UN forces advanced to ""Line Kansas"", north of the 38th parallel. The 187th Airborne Regimental Combat Team's (""Rakkasans"") second of two combat jumps was on Easter Sunday, 1951, at Munsan-ni, South Korea, codenamed Operation Tomahawk. The mission was to get behind Chinese forces and block their movement north. The 60th Indian Parachute Field Ambulance provided the medical cover for the operations, dropping an ADS and a surgical team and treating over 400 battle casualties apart from the civilian casualties that formed the core of their objective as the unit was on a humanitarian mission. Although Kim's early successes had led him to predict that he would end the war by the end of August, Chinese leaders were more pessimistic. To counter a possible U.S. deployment, Zhou Enlai secured a Soviet commitment to have the Soviet Union support Chinese forces with air cover, and deployed 260,000 soldiers along the Korean border, under the command of Gao Gang. Zhou commanded Chai Chengwen to conduct a topographical survey of Korea, and directed Lei Yingfu, Zhou's military advisor in Korea, to analyze the military situation in Korea. Lei concluded that MacArthur would most likely attempt a landing at Incheon. After conferring with Mao that this would be MacArthur's most likely strategy, Zhou briefed Soviet and North Korean advisers of Lei's findings, and issued orders to Chinese army commanders deployed on the Korean border to prepare for American naval activity in the Korea Strait. Soviet generals with extensive combat experience from the Second World War were sent to North Korea as the Soviet Advisory Group. These generals completed the plans for the attack by May. The original plans called for a skirmish to be initiated in the Ongjin Peninsula on the west coast of Korea. The North Koreans would then launch a ""counterattack"" that would capture Seoul and encircle and destroy the South Korean army. The final stage would involve destroying South Korean government remnants, capturing the rest of South Korea, including the ports. After the first two months of the conflict, South Korean forces were on the point of defeat, forced back to the Pusan Perimeter. In September 1950, an amphibious UN counter-offensive was launched at Inchon, and cut off many of the North Korean attackers. Those that escaped envelopment and capture were rapidly forced back north all the way to the border with China at the Yalu River, or into the mountainous interior. At this point, in October 1950, Chinese forces crossed the Yalu and entered the war. Chinese intervention triggered a retreat of UN forces which continued until mid-1951. After these dramatic reversals of fortune, which saw Seoul change hands four times, the last two years of conflict became a war of attrition, with the front line close to the 38th parallel. The war in the air, however, was never a stalemate. North Korea was subject to a massive bombing campaign. Jet fighters confronted each other in air-to-air combat for the first time in history, and Soviet pilots covertly flew in defense of their Communist allies. On 23 September 1946, an 8,000-strong railroad worker strike began in Pusan. Civil disorder spread throughout the country in what became known as the Autumn uprising. On 1 October 1946, Korean police killed three students in the Daegu Uprising; protesters counter-attacked, killing 38 policemen. On 3 October, some 10,000 people attacked the Yeongcheon police station, killing three policemen and injuring some 40 more; elsewhere, some 20 landlords and pro-Japanese South Korean officials were killed. The USAMGIK declared martial law. Because neither Korea had a significant navy, the Korean War featured few naval battles. A skirmish between North Korea and the UN Command occurred on 2 July 1950; the U.S. Navy cruiser USS Juneau, the Royal Navy cruiser HMS Jamaica, and the frigate HMS Black Swan fought four North Korean torpedo boats and two mortar gunboats, and sank them. USS Juneau later sank several ammunition ships that had been present. The last sea battle of the Korean War occurred at Inchon, days before the Battle of Incheon; the ROK ship PC-703 sank a North Korean mine layer in the Battle of Haeju Island, near Inchon. Three other supply ships were sunk by PC-703 two days later in the Yellow Sea. Thereafter, vessels from the UN nations held undisputed control of the sea about Korea. The gun ships were used in shore bombardment, while the aircraft carriers provided air support to the ground forces. After the formation of the People's Republic of China in 1949, the Chinese government named the Western nations, led by the United States, as the biggest threat to its national security. Basing this judgment on China's century of humiliation beginning in the early 19th century, American support for the Nationalists during the Chinese Civil War, and the ideological struggles between revolutionaries and reactionaries, the Chinese leadership believed that China would become a critical battleground in the United States' crusade against Communism. As a countermeasure and to elevate China's standing among the worldwide Communist movements, the Chinese leadership adopted a foreign policy that actively promoted Communist revolutions throughout territories on China's periphery. On 30 September, Zhou Enlai warned the United States that China was prepared to intervene in Korea if the United States crossed the 38th parallel. Zhou attempted to advise North Korean commanders on how to conduct a general withdrawal by using the same tactics which had allowed Chinese communist forces to successfully escape Chiang Kai-shek's Encirclement Campaigns in the 1930s, but by some accounts North Korean commanders did not utilize these tactics effectively. Historian Bruce Cumings argues, however, the KPA's rapid withdrawal was strategic, with troops melting into the mountains from where they could launch guerrilla raids on the UN forces spread out on the coasts. On 25 June 1950, the United Nations Security Council unanimously condemned the North Korean invasion of the Republic of Korea, with UN Security Council Resolution 82. The Soviet Union, a veto-wielding power, had boycotted the Council meetings since January 1950, protesting that the Republic of China (Taiwan), not the People's Republic of China, held a permanent seat in the UN Security Council. After debating the matter, the Security Council, on 27 June 1950, published Resolution 83 recommending member states provide military assistance to the Republic of Korea. On 27 June President Truman ordered U.S. air and sea forces to help the South Korean regime. On 4 July the Soviet Deputy Foreign Minister accused the United States of starting armed intervention on behalf of South Korea."
San_Diego,"In 2005 two city council members, Ralph Inzunza and Deputy Mayor Michael Zucchet – who briefly took over as acting mayor when Murphy resigned – were convicted of extortion, wire fraud, and conspiracy to commit wire fraud for taking campaign contributions from a strip club owner and his associates, allegedly in exchange for trying to repeal the city's ""no touch"" laws at strip clubs. Both subsequently resigned. Inzunza was sentenced to 21 months in prison. In 2009, a judge acquitted Zucchet on seven out of the nine counts against him, and granted his petition for a new trial on the other two charges; the remaining charges were eventually dropped. San Diego's roadway system provides an extensive network of routes for travel by bicycle. The dry and mild climate of San Diego makes cycling a convenient and pleasant year-round option. At the same time, the city's hilly, canyon-like terrain and significantly long average trip distances—brought about by strict low-density zoning laws—somewhat restrict cycling for utilitarian purposes. Older and denser neighborhoods around the downtown tend to be utility cycling oriented. This is partly because of the grid street patterns now absent in newer developments farther from the urban core, where suburban style arterial roads are much more common. As a result, a vast majority of cycling-related activities are recreational. Testament to San Diego's cycling efforts, in 2006, San Diego was rated as the best city for cycling for U.S. cities with a population over 1 million. San Diego and its backcountry are subject to periodic wildfires. In October 2003, San Diego was the site of the Cedar Fire, which has been called the largest wildfire in California over the past century. The fire burned 280,000 acres (1,100 km2), killed 15 people, and destroyed more than 2,200 homes. In addition to damage caused by the fire, smoke resulted in a significant increase in emergency room visits due to asthma, respiratory problems, eye irritation, and smoke inhalation; the poor air quality caused San Diego County schools to close for a week. Wildfires four years later destroyed some areas, particularly within the communities of Rancho Bernardo, Rancho Santa Fe, and Ramona. Private colleges and universities in the city include University of San Diego (USD), Point Loma Nazarene University (PLNU), Alliant International University (AIU), National University, California International Business University (CIBU), San Diego Christian College, John Paul the Great Catholic University, California College San Diego, Coleman University, University of Redlands School of Business, Design Institute of San Diego (DISD), Fashion Institute of Design & Merchandising's San Diego campus, NewSchool of Architecture and Design, Pacific Oaks College San Diego Campus, Chapman University's San Diego Campus, The Art Institute of California – San Diego, Platt College, Southern States University (SSU), UEI College, and Woodbury University School of Architecture's satellite campus. During World War II, San Diego became a major hub of military and defense activity, due to the presence of so many military installations and defense manufacturers. The city's population grew rapidly during and after World War II, more than doubling between 1930 (147,995) and 1950 (333,865). During the final months of the war, the Japanese had a plan to target multiple U.S. cities for biological attack, starting with San Diego. The plan was called ""Operation Cherry Blossoms at Night"" and called for kamikaze planes filled with fleas infected with plague (Yersinia pestis) to crash into civilian population centers in the city, hoping to spread plague in the city and effectively kill tens of thousands of civilians. The plan was scheduled to launch on September 22, 1945, but was not carried out because Japan surrendered five weeks earlier. San Diego hosts several major producers of wireless cellular technology. Qualcomm was founded and is headquartered in San Diego, and is one of the largest private-sector employers in San Diego. Other wireless industry manufacturers headquartered here include Nokia, LG Electronics, Kyocera International., Cricket Communications and Novatel Wireless. The largest software company in San Diego is security software company Websense Inc. San Diego also has the U.S. headquarters for the Slovakian security company ESET. San Diego has been designated as an iHub Innovation Center for collaboration potentially between wireless and life sciences. Historically home to the Kumeyaay people, San Diego was the first site visited by Europeans on what is now the West Coast of the United States. Upon landing in San Diego Bay in 1542, Juan Rodríguez Cabrillo claimed the entire area for Spain, forming the basis for the settlement of Alta California 200 years later. The Presidio and Mission San Diego de Alcalá, founded in 1769, formed the first European settlement in what is now California. In 1821, San Diego became part of the newly-independent Mexico, which reformed as the First Mexican Republic two years later. In 1850, it became part of the United States following the Mexican–American War and the admission of California to the union. The radio stations in San Diego include nationwide broadcaster, Clear Channel Communications; CBS Radio, Midwest Television, Lincoln Financial Media, Finest City Broadcasting, and many other smaller stations and networks. Stations include: KOGO AM 600, KFMB AM 760, KCEO AM 1000, KCBQ AM 1170, K-Praise, KLSD AM 1360 Air America, KFSD 1450 AM, KPBS-FM 89.5, Channel 933, Star 94.1, FM 94/9, FM News and Talk 95.7, Q96 96.1, KyXy 96.5, Free Radio San Diego (AKA Pirate Radio San Diego) 96.9FM FRSD, KSON 97.3/92.1, KXSN 98.1, Jack-FM 100.7, 101.5 KGB-FM, KLVJ 102.1, Rock 105.3, and another Pirate Radio station at 106.9FM, as well as a number of local Spanish-language radio stations. Stockton and Kearny went on to recover Los Angeles and force the capitulation of Alta California with the ""Treaty of Cahuenga"" on January 13, 1847. As a result of the Mexican–American War of 1846–48, the territory of Alta California, including San Diego, was ceded to the United States by Mexico, under the terms of the Treaty of Guadalupe Hidalgo in 1848. The Mexican negotiators of that treaty tried to retain San Diego as part of Mexico, but the Americans insisted that San Diego was ""for every commercial purpose of nearly equal importance to us with that of San Francisco,"" and the Mexican-American border was eventually established to be one league south of the southernmost point of San Diego Bay, so as to include the entire bay within the United States. Rainfall along the coast averages about 10 inches (250 mm) of precipitation annually. The average (mean) rainfall is 10.65 inches (271 mm) and the median is 9.6 inches (240 mm). Most of the rainfall occurs during the cooler months. The months of December through March supply most of the rain, with February the only month averaging 2 inches (51 mm) or more of rain. The months of May through September tend to be almost completely dry. Though there are few wet days per month during the rainy period, rainfall can be heavy when it does fall. Rainfall is usually greater in the higher elevations of San Diego; some of the higher elevation areas of San Diego can receive 11–15 inches (280–380 mm) of rain a year. Variability of rainfall can be extreme: in the wettest years of 1883/1884 and 1940/1941 more than 24 inches (610 mm) fell in the city, whilst in the driest years as little as 3.2 inches (80 mm) has fallen for a full year. The wettest month on record has been December 1921 with 9.21 inches (234 mm). San Diego County has one of the highest counts of animal and plant species that appear on the endangered species list among counties in the United States. Because of its diversity of habitat and its position on the Pacific Flyway, San Diego County has recorded the presence of 492 bird species, more than any other region in the country. San Diego always scores very high in the number of bird species observed in the annual Christmas Bird Count, sponsored by the Audubon Society, and it is known as one of the ""birdiest"" areas in the United States. Tourism is a major industry owing to the city's climate, its beaches, and numerous tourist attractions such as Balboa Park, Belmont amusement park, San Diego Zoo, San Diego Zoo Safari Park, and SeaWorld San Diego. San Diego's Spanish and Mexican heritage is reflected in the many historic sites across the city, such as Mission San Diego de Alcala and Old Town San Diego State Historic Park. Also, the local craft brewing industry attracts an increasing number of visitors for ""beer tours"" and the annual San Diego Beer Week in November; San Diego has been called ""America's Craft Beer Capital."" San Diego was ranked as the 20th-safest city in America in 2013 by Business Insider. According to Forbes magazine, San Diego was the ninth-safest city in the top 10 list of safest cities in the U.S. in 2010. Like most major cities, San Diego had a declining crime rate from 1990 to 2000. Crime in San Diego increased in the early 2000s. In 2004, San Diego had the sixth lowest crime rate of any U.S. city with over half a million residents. From 2002 to 2006, the crime rate overall dropped 0.8%, though not evenly by category. While violent crime decreased 12.4% during this period, property crime increased 1.1%. Total property crimes per 100,000 people were lower than the national average in 2008. The city's primary commercial airport is the San Diego International Airport (SAN), also known as Lindbergh Field. It is the busiest single-runway airport in the United States. It served over 17 million passengers in 2005, and is dealing with an increasingly larger number every year. It is located on San Diego Bay three miles (4.8 km) from downtown. San Diego International Airport maintains scheduled flights to the rest of the United States including Hawaii, as well as to Mexico, Canada, Japan, and the United Kingdom. It is operated by an independent agency, the San Diego Regional Airport Authority. In addition, the city itself operates two general-aviation airports, Montgomery Field (MYF) and Brown Field (SDM). By 2015, the Tijuana Cross-border Terminal in Otay Mesa will give direct access to Tijuana International Airport, with passengers walking across the U.S.–Mexico border on a footbridge to catch their flight on the Mexican side. In July 2013, three former supporters of Mayor Bob Filner asked him to resign because of allegations of repeated sexual harassment. Over the ensuing six weeks, 18 women came forward to publicly claim that Filner had sexually harassed them, and multiple individuals and groups called for him to resign. On August 19 Filner and city representatives entered a mediation process, as a result of which Filner agreed to resign, effective August 30, 2013, while the city agreed to limit his legal and financial exposure. Filner subsequently pleaded guilty to one felony count of false imprisonment and two misdemeanor battery charges, and was sentenced to house arrest and probation. Numerous regional transportation projects have occurred in recent years to mitigate congestion in San Diego. Notable efforts are improvements to San Diego freeways, expansion of San Diego Airport, and doubling the capacity of the cruise ship terminal of the port. Freeway projects included expansion of Interstates 5 and 805 around ""The Merge,"" a rush-hour spot where the two freeways meet. Also, an expansion of Interstate 15 through the North County is underway with the addition of high-occupancy-vehicle (HOV) ""managed lanes"". There is a tollway (The South Bay Expressway) connecting SR 54 and Otay Mesa, near the Mexican border. According to a 2007 assessment, 37 percent of streets in San Diego were in acceptable driving condition. The proposed budget fell $84.6 million short of bringing the city's streets to an acceptable level. Port expansions included a second cruise terminal on Broadway Pier which opened in 2010. Airport projects include expansion of Terminal 2, currently under construction and slated for completion in summer 2013. As of January 1, 2008 estimates by the San Diego Association of Governments revealed that the household median income for San Diego rose to $66,715, up from $45,733, and that the city population rose to 1,336,865, up 9.3% from 2000. The population was 45.3% non-Hispanic whites, down from 78.9% in 1970, 27.7% Hispanics, 15.6% Asians/Pacific Islanders, 7.1% blacks, 0.4% American Indians, and 3.9% from other races. Median age of Hispanics was 27.5 years, compared to 35.1 years overall and 41.6 years among non-Hispanic whites; Hispanics were the largest group in all ages under 18, and non-Hispanic whites constituted 63.1% of population 55 and older. In 2000, the median income for a household in the city was $45,733, and the median income for a family was $53,060. Males had a median income of $36,984 versus $31,076 for females. The per capita income for the city was $23,609. According to Forbes in 2005, San Diego was the fifth wealthiest U.S. city but about 10.6% of families and 14.6% of the population were below the poverty line, including 20.0% of those under age 18 and 7.6% of those age 65 or over. Nonetheless, San Diego was rated the fifth-best place to live in the United States in 2006 by Money magazine. In 1821, Mexico won its independence from Spain, and San Diego became part of the Mexican territory of Alta California. In 1822, Mexico began attempting to extend its authority over the coastal territory of Alta California. The fort on Presidio Hill was gradually abandoned, while the town of San Diego grew up on the level land below Presidio Hill. The Mission was secularized by the Mexican government in 1833, and most of the Mission lands were sold to wealthy Californio settlers. The 432 residents of the town petitioned the governor to form a pueblo, and Juan María Osuna was elected the first alcalde (""municipal magistrate""), defeating Pío Pico in the vote. (See, List of pre-statehood mayors of San Diego.) However, San Diego had been losing population throughout the 1830s and in 1838 the town lost its pueblo status because its size dropped to an estimated 100 to 150 residents. Beyond town Mexican land grants expanded the number of California ranchos that modestly added to the local economy. The city shares a 15-mile (24 km) border with Mexico that includes two border crossings. San Diego hosts the busiest international border crossing in the world, in the San Ysidro neighborhood at the San Ysidro Port of Entry. A second, primarily commercial border crossing operates in the Otay Mesa area; it is the largest commercial crossing on the California-Baja California border and handles the third-highest volume of trucks and dollar value of trade among all United States-Mexico land crossings. The presence of the University of California, San Diego and other research institutions has helped to fuel biotechnology growth. In 2013, San Diego has the second-largest biotech cluster in the United States, below the Boston area and above the San Francisco Bay Area. There are more than 400 biotechnology companies in the area. In particular, the La Jolla and nearby Sorrento Valley areas are home to offices and research facilities for numerous biotechnology companies. Major biotechnology companies like Illumina and Neurocrine Biosciences are headquartered in San Diego, while many biotech and pharmaceutical companies have offices or research facilities in San Diego. San Diego is also home to more than 140 contract research organizations (CROs) that provide a variety of contract services for pharmaceutical and biotechnology companies. The first European to visit the region was Portuguese-born explorer Juan Rodríguez Cabrillo sailing under the flag of Castile. Sailing his flagship San Salvador from Navidad, New Spain, Cabrillo claimed the bay for the Spanish Empire in 1542, and named the site 'San Miguel'. In November 1602, Sebastián Vizcaíno was sent to map the California coast. Arriving on his flagship San Diego, Vizcaíno surveyed the harbor and what are now Mission Bay and Point Loma and named the area for the Catholic Saint Didacus, a Spaniard more commonly known as San Diego de Alcalá. On November 12, 1602, the first Christian religious service of record in Alta California was conducted by Friar Antonio de la Ascensión, a member of Vizcaíno's expedition, to celebrate the feast day of San Diego. The city is governed by a mayor and a 9-member city council. In 2006, the city's form of government changed from a council–manager government to a strong mayor government. The change was brought about by a citywide vote in 2004. The mayor is in effect the chief executive officer of the city, while the council is the legislative body. The City of San Diego is responsible for police, public safety, streets, water and sewer service, planning and zoning, and similar services within its borders. San Diego is a sanctuary city, however, San Diego County is a participant of the Secure Communities program. As of 2011[update], the city had one employee for every 137 residents, with a payroll greater than $733 million. In the early part of the 20th century, San Diego hosted two World's Fairs: the Panama-California Exposition in 1915 and the California Pacific International Exposition in 1935. Both expositions were held in Balboa Park, and many of the Spanish/Baroque-style buildings that were built for those expositions remain to this day as central features of the park. The buildings were intended to be temporary structures, but most remained in continuous use until they progressively fell into disrepair. Most were eventually rebuilt, using castings of the original façades to retain the architectural style. The menagerie of exotic animals featured at the 1915 exposition provided the basis for the San Diego Zoo. During the 1950s there was a citywide festival called Fiesta del Pacifico highlighting the area's Spanish and Mexican past. In the 2010s there was a proposal for a large-scale celebration of the 100th anniversary of Balboa Park, but the plans were abandoned when the organization tasked with putting on the celebration went out of business. San Diego is one of the top-ten best climates in the Farmers' Almanac and is one of the two best summer climates in America as scored by The Weather Channel. Under the Köppen–Geiger climate classification system, the San Diego area has been variously categorized as having either a semi-arid climate (BSh in the original classification and BSkn in modified Köppen classification) or a Mediterranean climate (Csa and Csb). San Diego's climate is characterized by warm, dry summers and mild winters with most of the annual precipitation falling between December and March. The city has a mild climate year-round, with an average of 201 days above 70 °F (21 °C) and low rainfall (9–13 inches [230–330 mm] annually). Dewpoints in the summer months range from 57.0 °F (13.9 °C) to 62.4 °F (16.9 °C). Major state highways include SR 94, which connects downtown with I-805, I-15 and East County; SR 163, which connects downtown with the northeast part of the city, intersects I-805 and merges with I-15 at Miramar; SR 52, which connects La Jolla with East County through Santee and SR 125; SR 56, which connects I-5 with I-15 through Carmel Valley and Rancho Peñasquitos; SR 75, which spans San Diego Bay as the San Diego-Coronado Bridge, and also passes through South San Diego as Palm Avenue; and SR 905, which connects I-5 and I-805 to the Otay Mesa Port of Entry. Like most of southern California, the majority of San Diego's current area was originally occupied by chaparral, a plant community made up mostly of drought-resistant shrubs. The endangered Torrey pine has the bulk of its population in San Diego in a stretch of protected chaparral along the coast. The steep and varied topography and proximity to the ocean create a number of different habitats within the city limits, including tidal marsh and canyons. The chaparral and coastal sage scrub habitats in low elevations along the coast are prone to wildfire, and the rates of fire have increased in the 20th century, due primarily to fires starting near the borders of urban and wild areas. The city had a population of 1,307,402 according to the 2010 census, distributed over a land area of 372.1 square miles (963.7 km2). The urban area of San Diego extends beyond the administrative city limits and had a total population of 2,956,746, making it the third-largest urban area in the state, after that of the Los Angeles metropolitan area and San Francisco metropolitan area. They, along with the Riverside–San Bernardino, form those metropolitan areas in California larger than the San Diego metropolitan area, with a total population of 3,095,313 at the 2010 census. Many popular museums, such as the San Diego Museum of Art, the San Diego Natural History Museum, the San Diego Museum of Man, the Museum of Photographic Arts, and the San Diego Air & Space Museum are located in Balboa Park, which is also the location of the San Diego Zoo. The Museum of Contemporary Art San Diego (MCASD) is located in La Jolla and has a branch located at the Santa Fe Depot downtown. The downtown branch consists of two building on two opposite streets. The Columbia district downtown is home to historic ship exhibits belonging to the San Diego Maritime Museum, headlined by the Star of India, as well as the unrelated San Diego Aircraft Carrier Museum featuring the USS Midway aircraft carrier. The members of the city council are each elected from single member districts within the city. The mayor and city attorney are elected directly by the voters of the entire city. The mayor, city attorney, and council members are elected to four-year terms, with a two-term limit. Elections are held on a non-partisan basis per California state law; nevertheless, most officeholders do identify themselves as either Democrats or Republicans. In 2007, registered Democrats outnumbered Republicans by about 7 to 6 in the city, and Democrats currently (as of 2015[update]) hold a 5-4 majority in the city council. The current mayor, Kevin Faulconer, is a Republican. Downtown San Diego is located on San Diego Bay. Balboa Park encompasses several mesas and canyons to the northeast, surrounded by older, dense urban communities including Hillcrest and North Park. To the east and southeast lie City Heights, the College Area, and Southeast San Diego. To the north lies Mission Valley and Interstate 8. The communities north of the valley and freeway, and south of Marine Corps Air Station Miramar, include Clairemont, Kearny Mesa, Tierrasanta, and Navajo. Stretching north from Miramar are the northern suburbs of Mira Mesa, Scripps Ranch, Rancho Peñasquitos, and Rancho Bernardo. The far northeast portion of the city encompasses Lake Hodges and the San Pasqual Valley, which holds an agricultural preserve. Carmel Valley and Del Mar Heights occupy the northwest corner of the city. To their south are Torrey Pines State Reserve and the business center of the Golden Triangle. Further south are the beach and coastal communities of La Jolla, Pacific Beach, Mission Beach, and Ocean Beach. Point Loma occupies the peninsula across San Diego Bay from downtown. The communities of South San Diego, such as San Ysidro and Otay Mesa, are located next to the Mexico–United States border, and are physically separated from the rest of the city by the cities of National City and Chula Vista. A narrow strip of land at the bottom of San Diego Bay connects these southern neighborhoods with the rest of the city. The development of skyscrapers over 300 feet (91 m) in San Diego is attributed to the construction of the El Cortez Hotel in 1927, the tallest building in the city from 1927 to 1963. As time went on multiple buildings claimed the title of San Diego's tallest skyscraper, including the Union Bank of California Building and Symphony Towers. Currently the tallest building in San Diego is One America Plaza, standing 500 feet (150 m) tall, which was completed in 1991. The downtown skyline contains no super-talls, as a regulation put in place by the Federal Aviation Administration in the 1970s set a 500 feet (152 m) limit on the height of buildings due to the proximity of San Diego International Airport. An iconic description of the skyline includes its skyscrapers being compared to the tools of a toolbox. The U.S. Census Bureau reported that in 2000, 24.0% of San Diego residents were under 18, and 10.5% were 65 and over. As of 2011[update] the median age was 35.6; more than a quarter of residents were under age 20 and 11% were over age 65. Millennials (ages 18 through 34) constitute 27.1% of San Diego's population, the second-highest percentage in a major U.S. city. The San Diego County regional planning agency, SANDAG, provides tables and graphs breaking down the city population into 5-year age groups. San Diego is served by the San Diego Trolley light rail system, by the SDMTS bus system, and by Coaster and Amtrak Pacific Surfliner commuter rail; northern San Diego county is also served by the Sprinter light rail line. The Trolley primarily serves downtown and surrounding urban communities, Mission Valley, east county, and coastal south bay. A planned Mid-Coast extension of the Trolley will operate from Old Town to University City and the University of California, San Diego along the I-5 Freeway, with planned operation by 2018. The Amtrak and Coaster trains currently run along the coastline and connect San Diego with Los Angeles, Orange County, Riverside, San Bernardino, and Ventura via Metrolink and the Pacific Surfliner. There are two Amtrak stations in San Diego, in Old Town and the Santa Fe Depot downtown. San Diego transit information about public transportation and commuting is available on the Web and by dialing ""511"" from any phone in the area. The city lies on approximately 200 deep canyons and hills separating its mesas, creating small pockets of natural open space scattered throughout the city and giving it a hilly geography. Traditionally, San Diegans have built their homes and businesses on the mesas, while leaving the urban canyons relatively wild. Thus, the canyons give parts of the city a segmented feel, creating gaps between otherwise proximate neighborhoods and contributing to a low-density, car-centered environment. The San Diego River runs through the middle of San Diego from east to west, creating a river valley which serves to divide the city into northern and southern segments. The river used to flow into San Diego Bay and its fresh water was the focus of the earliest Spanish explorers.[citation needed] Several reservoirs and Mission Trails Regional Park also lie between and separate developed areas of the city. The southern portion of the Point Loma peninsula was set aside for military purposes as early as 1852. Over the next several decades the Army set up a series of coastal artillery batteries and named the area Fort Rosecrans. Significant U.S. Navy presence began in 1901 with the establishment of the Navy Coaling Station in Point Loma, and expanded greatly during the 1920s. By 1930, the city was host to Naval Base San Diego, Naval Training Center San Diego, San Diego Naval Hospital, Camp Matthews, and Camp Kearny (now Marine Corps Air Station Miramar). The city was also an early center for aviation: as early as World War I, San Diego was proclaiming itself ""The Air Capital of the West"". The city was home to important airplane developers and manufacturers like Ryan Airlines (later Ryan Aeronautical), founded in 1925, and Consolidated Aircraft (later Convair), founded in 1923. Charles A. Lindbergh's plane The Spirit of St. Louis was built in San Diego in 1927 by Ryan Airlines. In May 1769, Gaspar de Portolà established the Fort Presidio of San Diego on a hill near the San Diego River. It was the first settlement by Europeans in what is now the state of California. In July of the same year, Mission San Diego de Alcalá was founded by Franciscan friars under Junípero Serra. By 1797, the mission boasted the largest native population in Alta California, with over 1,400 neophytes living in and around the mission proper. Mission San Diego was the southern anchor in California of the historic mission trail El Camino Real. Both the Presidio and the Mission are National Historic Landmarks. San Diego's first television station was KFMB, which began broadcasting on May 16, 1949. Since the Federal Communications Commission (FCC) licensed seven television stations in Los Angeles, two VHF channels were available for San Diego because of its relative proximity to the larger city. In 1952, however, the FCC began licensing UHF channels, making it possible for cities such as San Diego to acquire more stations. Stations based in Mexico (with ITU prefixes of XE and XH) also serve the San Diego market. Television stations today include XHTJB 3 (Once TV), XETV 6 (CW), KFMB 8 (CBS), KGTV 10 (ABC), XEWT 12 (Televisa Regional), KPBS 15 (PBS), KBNT-CD 17 (Univision), XHTIT-TDT 21 (Azteca 7), XHJK-TDT 27 (Azteca 13), XHAS 33 (Telemundo), K35DG-D 35 (UCSD-TV), KDTF-LD 51 (Telefutura), KNSD 39 (NBC), KZSD-LP 41 (Azteca America), KSEX-CD 42 (Infomercials), XHBJ-TDT 45 (Gala TV), XHDTV 49 (MNTV), KUSI 51 (Independent), XHUAA-TDT 57 (Canal de las Estrellas), and KSWB-TV 69 (Fox). San Diego has an 80.6 percent cable penetration rate. Due to the ratio of U.S. and Mexican-licensed stations, San Diego is the largest media market in the United States that is legally unable to support a television station duopoly between two full-power stations under FCC regulations, which disallow duopolies in metropolitan areas with fewer than nine full-power television stations and require that there must be eight unique station owners that remain once a duopoly is formed (there are only seven full-power stations on the California side of the San Diego-Tijuana market).[citation needed] Though the E. W. Scripps Company owns KGTV and KZSD-LP, they are not considered a duopoly under the FCC's legal definition as common ownership between full-power and low-power television stations in the same market is permitted regardless to the number of stations licensed to the area. As a whole, the Mexico side of the San Diego-Tijuana market has two duopolies and one triopoly (Entravision Communications owns both XHAS-TV and XHDTV-TV, Azteca owns XHJK-TV and XHTIT-TV, and Grupo Televisa owns XHUAA-TV and XHWT-TV along with being the license holder for XETV-TV, which is run by California-based subsidiary Bay City Television). With the automobile being the primary means of transportation for over 80 percent of its residents, San Diego is served by a network of freeways and highways. This includes Interstate 5, which runs south to Tijuana and north to Los Angeles; Interstate 8, which runs east to Imperial County and the Arizona Sun Corridor; Interstate 15, which runs northeast through the Inland Empire to Las Vegas and Salt Lake City; and Interstate 805, which splits from I-5 near the Mexican border and rejoins I-5 at Sorrento Valley. From the start of the 20th century through the 1970s, the American tuna fishing fleet and tuna canning industry were based in San Diego, ""the tuna capital of the world"". San Diego's first tuna cannery was founded in 1911, and by the mid-1930s the canneries employed more than 1,000 people. A large fishing fleet supported the canneries, mostly staffed by immigrant fishermen from Japan, and later from the Portuguese Azores and Italy whose influence is still felt in neighborhoods like Little Italy and Point Loma. Due to rising costs and foreign competition, the last of the canneries closed in the early 1980s. In 1846, the United States went to war against Mexico and sent a naval and land expedition to conquer Alta California. At first they had an easy time of it capturing the major ports including San Diego, but the Californios in southern Alta California struck back. Following the successful revolt in Los Angeles, the American garrison at San Diego was driven out without firing a shot in early October 1846. Mexican partisans held San Diego for three weeks until October 24, 1846, when the Americans recaptured it. For the next several months the Americans were blockaded inside the pueblo. Skirmishes occurred daily and snipers shot into the town every night. The Californios drove cattle away from the pueblo hoping to starve the Americans and their Californio supporters out. On December 1 the Americans garrison learned that the dragoons of General Stephen W. Kearney were at Warner's Ranch. Commodore Robert F. Stockton sent a mounted force of fifty under Captain Archibald Gillespie to march north to meet him. Their joint command of 150 men, returning to San Diego, encountered about 93 Californios under Andrés Pico. In the ensuing Battle of San Pasqual, fought in the San Pasqual Valley which is now part of the city of San Diego, the Americans suffered their worst losses in the campaign. Subsequently a column led by Lieutenant Gray arrived from San Diego, rescuing Kearny's battered and blockaded command. With an estimated population of 1,381,069 as of July 1, 2014, San Diego is the eighth-largest city in the United States and second-largest in California. It is part of the San Diego–Tijuana conurbation, the second-largest transborder agglomeration between the US and a bordering country after Detroit–Windsor, with a population of 4,922,723 people. San Diego is the birthplace of California and is known for its mild year-round climate, natural deep-water harbor, extensive beaches, long association with the United States Navy and recent emergence as a healthcare and biotechnology development center. The climate in San Diego, like most of Southern California, often varies significantly over short geographical distances resulting in microclimates. In San Diego, this is mostly because of the city's topography (the Bay, and the numerous hills, mountains, and canyons). Frequently, particularly during the ""May gray/June gloom"" period, a thick ""marine layer"" cloud cover will keep the air cool and damp within a few miles of the coast, but will yield to bright cloudless sunshine approximately 5–10 miles (8.0–16.1 km) inland. Sometimes the June gloom can last into July, causing cloudy skies over most of San Diego for the entire day. Even in the absence of June gloom, inland areas tend to experience much more significant temperature variations than coastal areas, where the ocean serves as a moderating influence. Thus, for example, downtown San Diego averages January lows of 50 °F (10 °C) and August highs of 78 °F (26 °C). The city of El Cajon, just 10 miles (16 km) inland from downtown San Diego, averages January lows of 42 °F (6 °C) and August highs of 88 °F (31 °C). The San Diego Surf of the American Basketball Association is located in the city. The annual Farmers Insurance Open golf tournament (formerly the Buick Invitational) on the PGA Tour occurs at Torrey Pines Golf Course. This course was also the site of the 2008 U.S. Open Golf Championship. The San Diego Yacht Club hosted the America's Cup yacht races three times during the period 1988 to 1995. The amateur beach sport Over-the-line was invented in San Diego, and the annual world Over-the-line championships are held at Mission Bay every year. The state of California was admitted to the United States in 1850. That same year San Diego was designated the seat of the newly established San Diego County and was incorporated as a city. Joshua H. Bean, the last alcalde of San Diego, was elected the first mayor. Two years later the city was bankrupt; the California legislature revoked the city's charter and placed it under control of a board of trustees, where it remained until 1889. A city charter was re-established in 1889 and today's city charter was adopted in 1931. As of the Census of 2010, there were 1,307,402 people living in the city of San Diego. That represents a population increase of just under 7% from the 1,223,400 people, 450,691 households, and 271,315 families reported in 2000. The estimated city population in 2009 was 1,306,300. The population density was 3,771.9 people per square mile (1,456.4/km2). The racial makeup of San Diego was 45.1% White, 6.7% African American, 0.6% Native American, 15.9% Asian (5.9% Filipino, 2.7% Chinese, 2.5% Vietnamese, 1.3% Indian, 1.0% Korean, 0.7% Japanese, 0.4% Laotian, 0.3% Cambodian, 0.1% Thai). 0.5% Pacific Islander (0.2% Guamanian, 0.1% Samoan, 0.1% Native Hawaiian), 12.3% from other races, and 5.1% from two or more races. The ethnic makeup of the city was 28.8% Hispanic or Latino (of any race); 24.9% of the total population were Mexican American, and 0.6% were Puerto Rican. The San Diego Symphony at Symphony Towers performs on a regular basis and is directed by Jahja Ling. The San Diego Opera at Civic Center Plaza, directed by Ian Campbell, was ranked by Opera America as one of the top 10 opera companies in the United States. Old Globe Theatre at Balboa Park produces about 15 plays and musicals annually. The La Jolla Playhouse at UCSD is directed by Christopher Ashley. Both the Old Globe Theatre and the La Jolla Playhouse have produced the world premieres of plays and musicals that have gone on to win Tony Awards or nominations on Broadway. The Joan B. Kroc Theatre at Kroc Center's Performing Arts Center is a 600-seat state-of-the-art theatre that hosts music, dance, and theatre performances. The San Diego Repertory Theatre at the Lyceum Theatres in Horton Plaza produces a variety of plays and musicals. Hundreds of movies and a dozen TV shows have been filmed in San Diego, a tradition going back as far as 1898. The original town of San Diego was located at the foot of Presidio Hill, in the area which is now Old Town San Diego State Historic Park. The location was not ideal, being several miles away from navigable water. In 1850, William Heath Davis promoted a new development by the Bay shore called ""New San Diego"", several miles south of the original settlement; however, for several decades the new development consisted only a few houses, a pier and an Army depot. In the late 1860s, Alonzo Horton promoted a move to the bayside area, which he called ""New Town"" and which became Downtown San Diego. Horton promoted the area heavily, and people and businesses began to relocate to New Town because of its location on San Diego Bay convenient to shipping. New Town soon eclipsed the original settlement, known to this day as Old Town, and became the economic and governmental heart of the city. Still, San Diego remained a relative backwater town until the arrival of a railroad connection in 1878."
Myanmar,"The Bronze Age arrived circa 1500 BC when people in the region were turning copper into bronze, growing rice and domesticating poultry and pigs; they were among the first people in the world to do so. Human remains and artifacts from this era were discovered in Monywa District in the Sagaing Division. The Iron Age began around 500 BC with the emergence of iron-working settlements in an area south of present-day Mandalay. Evidence also shows the presence of rice-growing settlements of large villages and small towns that traded with their surroundings as far as China between 500 BC and 200 AD. Iron Age Burmese cultures also had influences from outside sources such as India and Thailand, as seen in their funerary practices concerning child burials. This indicates some form of communication between groups in Myanmar and other places, possibly through trade. Burma is bordered in the northwest by the Chittagong Division of Bangladesh and the Mizoram, Manipur, Nagaland and Arunachal Pradesh states of India. Its north and northeast border is with the Tibet Autonomous Region and Yunnan province for a Sino-Burman border total of 2,185 km (1,358 mi). It is bounded by Laos and Thailand to the southeast. Burma has 1,930 km (1,200 mi) of contiguous coastline along the Bay of Bengal and Andaman Sea to the southwest and the south, which forms one quarter of its total perimeter. Under British administration, Myanmar was the second-wealthiest country in South-East Asia. It had been the world's largest exporter of rice. Myanmar also had a wealth of natural and labour resources. British Burma began exporting crude oil in 1853, making it one of the earliest petroleum producers in the world. It produced 75% of the world's teak and had a highly literate population. The wealth was however, mainly concentrated in the hands of Europeans. In 1930s, agricultural production fell dramatically as international rice prices declined, and did not recover for several decades. International human rights organisations including Human Rights Watch, Amnesty International and the American Association for the Advancement of Science have repeatedly documented and condemned widespread human rights violations in Myanmar. The Freedom in the World 2011 report by Freedom House notes, ""The military junta has ... suppressed nearly all basic rights; and committed human rights abuses with impunity."" In July 2013, the Assistance Association for Political Prisoners indicated that there were approximately 100 political prisoners being held in Burmese prisons. Restrictions on media censorship were significantly eased in August 2012 following demonstrations by hundreds of protesters who wore shirts demanding that the government ""Stop Killing the Press."" The most significant change has come in the form that media organisations will no longer have to submit their content to a censorship board before publication. However, as explained by one editorial in the exiled press The Irrawaddy, this new ""freedom"" has caused some Burmese journalists to simply see the new law as an attempt to create an environment of self-censorship as journalists ""are required to follow 16 guidelines towards protecting the three national causes — non-disintegration of the Union, non-disintegration of national solidarity, perpetuation of sovereignty — and ""journalistic ethics"" to ensure their stories are accurate and do not jeopardise national security."" In July 2014 five journalists were sentenced to 10 years in jail after publishing a report saying the country was planning to build a new chemical weapons plant. Journalists described the jailings as a blow to the recently-won news media freedoms that had followed five decades of censorship and persecution. The most popular available tourist destinations in Myanmar include big cities such as Yangon and Mandalay; religious sites in Mon State, Pindaya, Bago and Hpa-An; nature trails in Inle Lake, Kengtung, Putao, Pyin Oo Lwin; ancient cities such as Bagan and Mrauk-U; as well as beaches in Nabule, Ngapali, Ngwe-Saung, Mergui. Nevertheless, much of the country is off-limits to tourists, and interactions between foreigners and the people of Myanmar, particularly in the border regions, are subject to police scrutiny. They are not to discuss politics with foreigners, under penalty of imprisonment and, in 2001, the Myanmar Tourism Promotion Board issued an order for local officials to protect tourists and limit ""unnecessary contact"" between foreigners and ordinary Burmese people. The provisional results of the 2014 Myanmar Census show that the total population is 51,419,420. This figure includes an estimated 1,206,353 persons in parts of northern Rakhine State, Kachin State and Kayin State who were not counted. People who were out of the country at the time of the census are not included in these figures. There are over 600,000 registered migrant workers from Myanmar in Thailand, and millions more work illegally. Burmese migrant workers account for 80% of Thailand's migrant workers. Population density is 76 per square kilometre (200/sq mi), among the lowest in Southeast Asia. The major agricultural product is rice, which covers about 60% of the country's total cultivated land area. Rice accounts for 97% of total food grain production by weight. Through collaboration with the International Rice Research Institute 52 modern rice varieties were released in the country between 1966 and 1997, helping increase national rice production to 14 million tons in 1987 and to 19 million tons in 1996. By 1988, modern varieties were planted on half of the country's ricelands, including 98 percent of the irrigated areas. In 2008 rice production was estimated at 50 million tons. The government has responded by imposing curfews and by deploying troops in the regions. On 10 June 2012, a state of emergency was declared in Rakhine, allowing the military to participate in administration of the region. The Burmese army and police have been accused of targeting Rohingya Muslims through mass arrests and arbitrary violence. A number of monks' organisations that played a vital role in Myanmar's struggle for democracy have taken measures to block any humanitarian assistance to the Rohingya community. Mon, who form 2% of the population, are ethno-linguistically related to the Khmer. Overseas Indians are 2%. The remainder are Kachin, Chin, Rohingya, Anglo-Indians, Gurkha, Nepali and other ethnic minorities. Included in this group are the Anglo-Burmese. Once forming a large and influential community, the Anglo-Burmese left the country in steady streams from 1958 onwards, principally to Australia and the UK. It is estimated that 52,000 Anglo-Burmese remain in Myanmar. As of 2009[update], 110,000 Burmese refugees were living in refugee camps in Thailand. In 1988, unrest over economic mismanagement and political oppression by the government led to widespread pro-democracy demonstrations throughout the country known as the 8888 Uprising. Security forces killed thousands of demonstrators, and General Saw Maung staged a coup d'état and formed the State Law and Order Restoration Council (SLORC). In 1989, SLORC declared martial law after widespread protests. The military government finalised plans for People's Assembly elections on 31 May 1989. SLORC changed the country's official English name from the ""Socialist Republic of the Union of Burma"" to the ""Union of Myanmar"" in 1989. In May 2013, Thein Sein became the first Myanmar president to visit the White House in 47 years; the last Burmese leader to visit the White House was Ne Win in September 1966. President Barack Obama praised the former general for political and economic reforms, and the cessation of tensions between Myanmar and the United States. Political activists objected to the visit due to concerns over human rights abuses in Myanmar but Obama assured Thein Sein that Myanmar will receive US support. The two leaders discussed to release more political prisoners, the institutionalisation of political reform and rule of law, and ending ethnic conflict in Myanmar—the two governments agreed to sign a bilateral trade and investment framework agreement on 21 May 2013. A major battleground, Burma was devastated during World War II. By March 1942, within months after they entered the war, Japanese troops had advanced on Rangoon and the British administration had collapsed. A Burmese Executive Administration headed by Ba Maw was established by the Japanese in August 1942. Wingate's British Chindits were formed into long-range penetration groups trained to operate deep behind Japanese lines. A similar American unit, Merrill's Marauders, followed the Chindits into the Burmese jungle in 1943. Beginning in late 1944, allied troops launched a series of offensives that led to the end of Japanese rule in July 1945. The battles were intense with much of Burma laid waste by the fighting. Overall, the Japanese lost some 150,000 men in Burma. Only 1,700 prisoners were taken. Typical jungle animals, particularly tigers and leopards, occur sparsely in Myanmar. In upper Myanmar, there are rhinoceros, wild buffalo, wild boars, deer, antelope, and elephants, which are also tamed or bred in captivity for use as work animals, particularly in the lumber industry. Smaller mammals are also numerous, ranging from gibbons and monkeys to flying foxes and tapirs. The abundance of birds is notable with over 800 species, including parrots, peafowl, pheasants, crows, herons, and paddybirds. Among reptile species there are crocodiles, geckos, cobras, Burmese pythons, and turtles. Hundreds of species of freshwater fish are wide-ranging, plentiful and are very important food sources. For a list of protected areas, see List of protected areas of Myanmar. Many US and European jewellery companies, including Bulgari, Tiffany, and Cartier, refuse to import these stones based on reports of deplorable working conditions in the mines. Human Rights Watch has encouraged a complete ban on the purchase of Burmese gems based on these reports and because nearly all profits go to the ruling junta, as the majority of mining activity in the country is government-run. The government of Myanmar controls the gem trade by direct ownership or by joint ventures with private owners of mines. In English, the country is popularly known as either ""Burma"" or ""Myanmar"" i/ˈmjɑːnˌmɑːr/. Both these names are derived from the name of the majority Burmese Bamar ethnic group. Myanmar is considered to be the literary form of the name of the group, while Burma is derived from ""Bamar"", the colloquial form of the group's name. Depending on the register used, the pronunciation would be Bama (pronounced: [bəmà]) or Myamah (pronounced: [mjəmà]). The name Burma has been in use in English since the 18th century. The government has assembled a National Human Rights Commission that consists of 15 members from various backgrounds. Several activists in exile, including Thee Lay Thee Anyeint members, have returned to Myanmar after President Thein Sein's invitation to expatriates to return home to work for national development. In an address to the United Nations Security Council on 22 September 2011, Myanmar's Foreign Minister Wunna Maung Lwin confirmed the government's intention to release prisoners in the near future. According to the Crisis Group, since Myanmar transitioned to a new government in August 2011, the country's human rights record has been improving. Previously giving Myanmar its lowest rating of 7, the 2012 Freedom in the World report also notes improvement, giving Myanmar a 6 for improvements in civil liberties and political rights, the release of political prisoners, and a loosening of restrictions. In 2013, Myanmar improved yet again, receiving a score of five in civil liberties and a six in political freedoms Mohinga is the traditional breakfast dish and is Myanmar's national dish. Seafood is a common ingredient in coastal cities such as Sittwe, Kyaukpyu, Mawlamyaing (formerly Moulmein), Mergui (Myeik) and Dawei, while meat and poultry are more commonly used in landlocked cities like Mandalay. Freshwater fish and shrimp have been incorporated into inland cooking as a primary source of protein and are used in a variety of ways, fresh, salted whole or filleted, salted and dried, made into a salty paste, or fermented sour and pressed. Archaeological evidence shows that Homo erectus lived in the region now known as Myanmar as early as 400,000 years ago. The first evidence of Homo sapiens is dated to about 11,000 BC, in a Stone Age culture called the Anyathian with discoveries of stone tools in central Myanmar. Evidence of neolithic age domestication of plants and animals and the use of polished stone tools dating to sometime between 10,000 and 6,000 BC has been discovered in the form of cave paintings near the city of Taunggyi. The most common way for travellers to enter the country seems to be by air. According to the website Lonely Planet, getting into Myanmar is problematic: ""No bus or train service connects Myanmar with another country, nor can you travel by car or motorcycle across the border – you must walk across."", and states that, ""It is not possible for foreigners to go to/from Myanmar by sea or river."" There are a small number of border crossings that allow the passage of private vehicles, such as the border between Ruili (China) to Mu-se, the border between Htee Kee (Myanmar) and Ban Phu Nam Ron (Thailand) (the most direct border between Dawei and Kanchanaburi), and the border between Myawaddy (Myanmar) and Mae Sot (Thailand). At least one tourist company has successfully run commercial overland routes through these borders since 2013. ""From Mae Sai (Thailand) you can cross to Tachileik, but can only go as far as Kengtung. Those in Thailand on a visa run can cross to Kawthaung but cannot venture farther into Myanmar."" With Burma preoccupied by the Chinese threat, Ayutthaya recovered its territories by 1770, and went on to capture Lan Na by 1776. Burma and Siam went to war until 1855, but all resulted in a stalemate, exchanging Tenasserim (to Burma) and Lan Na (to Ayutthaya). Faced with a powerful China and a resurgent Ayutthaya in the east, King Bodawpaya turned west, acquiring Arakan (1785), Manipur (1814) and Assam (1817). It was the second-largest empire in Burmese history but also one with a long ill-defined border with British India. During World War II, the British destroyed the major government buildings, oil wells and mines for tungsten, tin, lead and silver to keep them from the Japanese. Myanmar was bombed extensively by both sides. After independence, the country was in ruins with its major infrastructure completely destroyed. After a parliamentary government was formed in 1948, Prime Minister U Nu embarked upon a policy of nationalisation and the state was declared the owner of all land. The government also tried to implement a poorly considered Eight-Year plan. By the 1950s, rice exports had fallen by two thirds and mineral exports by over 96% (as compared to the pre-World War II period). Plans were partly financed by printing money, which led to inflation. In May 2008, Cyclone Nargis caused extensive damage in the densely populated, rice-farming delta of the Irrawaddy Division. It was the worst natural disaster in Burmese history with reports of an estimated 200,000 people dead or missing, and damage totalled to 10 billion US Dollars, and as many as 1 million left homeless. In the critical days following this disaster, Myanmar's isolationist government was accused of hindering United Nations recovery efforts. Humanitarian aid was requested but concerns about foreign military or intelligence presence in the country delayed the entry of United States military planes delivering medicine, food, and other supplies. In October 2012 the number of ongoing conflicts in Myanmar included the Kachin conflict, between the Pro-Christian Kachin Independence Army and the government; a civil war between the Rohingya Muslims, and the government and non-government groups in Rakhine State; and a conflict between the Shan, Lahu and Karen minority groups, and the government in the eastern half of the country. In addition al-Qaeda signalled an intention to become involved in Myanmar. In a video released 3 September 2014 mainly addressed to India, the militant group's leader Ayman al-Zawahiri said al-Qaeda had not forgotten the Muslims of Myanmar and that the group was doing ""what they can to rescue you"". In response, the military raised its level of alertness while the Burmese Muslim Association issued a statement saying Muslims would not tolerate any threat to their motherland. Myanmar is home to four major language families: Sino-Tibetan, Tai–Kadai, Austro-Asiatic, and Indo-European. Sino-Tibetan languages are most widely spoken. They include Burmese, Karen, Kachin, Chin, and Chinese (mainly Hokkien). The primary Tai–Kadai language is Shan. Mon, Palaung, and Wa are the major Austroasiatic languages spoken in Myanmar. The two major Indo-European languages are Pali, the liturgical language of Theravada Buddhism, and English. Little known fact about Myanmar is there are more than 130 languages spoken by people in Myanmar. Since many of them are known only within small tribes around the country, they may have been lost (many if not all) after a few generations. Burmese resentment was strong and was vented in violent riots that paralysed Yangon (Rangoon) on occasion all the way until the 1930s. Some of the discontent was caused by a disrespect for Burmese culture and traditions such as the British refusal to remove shoes when they entered pagodas. Buddhist monks became the vanguards of the independence movement. U Wisara, an activist monk, died in prison after a 166-day hunger strike to protest against a rule that forbade him from wearing his Buddhist robes while imprisoned. Burmese, the mother tongue of the Bamar and official language of Myanmar, is related to Tibetan and Chinese language. It is written in a script consisting of circular and semi-circular letters, which were adapted from the Mon script, which in turn was developed from a southern Indian script in the 5th century. The earliest known inscriptions in the Burmese script date from the 11th century. It is also used to write Pali, the sacred language of Theravada Buddhism, as well as several ethnic minority languages, including Shan, several Karen dialects, and Kayah (Karenni), with the addition of specialised characters and diacritics for each language. British colonial rule introduced Western elements of culture to Burma. Burma's education system is modelled after that of the United Kingdom. Colonial architectural influences are most evident in major cities such as Yangon. Many ethnic minorities, particularly the Karen in the southeast and the Kachin and Chin who populate the north and northeast, practice Christianity. According to the The World Factbook, the Burman population is 68% and the ethnic groups constitute 32%. However, the exiled leaders and organisations claims that ethnic population is 40%, which is implicitly contrasted with CIA report (official US report). The Rohingya people have consistently faced human rights abuses by the Burmese regime that has refused to acknowledge them as Burmese citizens (despite some of them having lived in Burma for over three generations)—the Rohingya have been denied Burmese citizenship since the enactment of a 1982 citizenship law. The law created three categories of citizenship: citizenship, associate citizenship, and naturalised citizenship. Citizenship is given to those who belong to one of the national races such as Kachin, Kayah (Karenni), Karen, Chin, Burman, Mon, Rakhine, Shan, Kaman, or Zerbadee. Associate citizenship is given to those who cannot prove their ancestors settled in Myanmar before 1823, but can prove they have one grandparent, or pre-1823 ancestor, who was a citizen of another country, as well as people who applied for citizenship in 1948 and qualified then by those laws. Naturalized citizenship is only given to those who have at least one parent with one of these types of Burmese citizenship or can provide ""conclusive evidence"" that their parents entered and resided in Burma prior to independence in 1948. The Burmese regime has attempted to forcibly expel Rohingya and bring in non-Rohingyas to replace them—this policy has resulted in the expulsion of approximately half of the 800,000 Rohingya from Burma, while the Rohingya people have been described as ""among the world's least wanted"" and ""one of the world's most persecuted minorities."" But the origin of ‘most persecuted minority’ statement is unclear. Burma continues to be used in English by the governments of many countries, such as Australia, Canada and the United Kingdom. Official United States policy retains Burma as the country's name, although the State Department's website lists the country as ""Burma (Myanmar)"" and Barack Obama has referred to the country by both names. The Czech Republic uses officially Myanmar, although its Ministry of Foreign Affairs mentions both Myanmar and Burma on its website. The United Nations uses Myanmar, as do the Association of Southeast Asian Nations, Russia, Germany, China, India, Norway, and Japan. In 2008, India suspended military aid to Myanmar over the issue of human rights abuses by the ruling junta, although it has preserved extensive commercial ties, which provide the regime with much-needed revenue. The thaw in relations began on 28 November 2011, when Belarusian Prime Minister Mikhail Myasnikovich and his wife Ludmila arrived in the capital, Naypyidaw, the same day as the country received a visit by US Secretary of State Hillary Rodham Clinton, who also met with pro-democracy opposition leader Aung San Suu Kyi. International relations progress indicators continued in September 2012 when Aung San Suu Kyi visited to the US followed by Myanmar's reformist president visit to the United Nations. Myanmar (myan-MAR i/miɑːnˈmɑːr/ mee-ahn-MAR, /miˈɛnmɑːr/ mee-EN-mar or /maɪˈænmɑːr/ my-AN-mar (also with the stress on first syllable); Burmese pronunciation: [mjəmà]),[nb 1] officially the Republic of the Union of Myanmar and also known as Burma, is a sovereign state in Southeast Asia bordered by Bangladesh, India, China, Laos and Thailand. One-third of Myanmar's total perimeter of 1,930 km (1,200 miles) forms an uninterrupted coastline along the Bay of Bengal and the Andaman Sea. The country's 2014 census revealed a much lower population than expected, with 51 million people recorded. Myanmar is 676,578 square kilometres (261,227 sq mi) in size. Its capital city is Naypyidaw and its largest city is Yangon (Rangoon). Around the second century BC the first-known city-states emerged in central Myanmar. The city-states were founded as part of the southward migration by the Tibeto-Burman-speaking Pyu city-states, the earliest inhabitants of Myanmar of whom records are extant, from present-day Yunnan. The Pyu culture was heavily influenced by trade with India, importing Buddhism as well as other cultural, architectural and political concepts which would have an enduring influence on later Burmese culture and political organisation. The elections of 2010 resulted in a victory for the military-backed Union Solidarity and Development Party. Various foreign observers questioned the fairness of the elections. One criticism of the election was that only government sanctioned political parties were allowed to contest in it and the popular National League for Democracy was declared illegal. However, immediately following the elections, the government ended the house arrest of the democracy advocate and leader of the National League for Democracy, Aung San Suu Kyi, and her ability to move freely around the country is considered an important test of the military's movement toward more openness. After unexpected reforms in 2011, NLD senior leaders have decided to register as a political party and to field candidates in future by-elections. Child soldiers have and continue to play a major part in the Burmese Army as well as Burmese rebel movements. The Independent reported in June 2012 that ""Children are being sold as conscripts into the Burmese military for as little as $40 and a bag of rice or a can of petrol."" The UN's Special Representative of the Secretary-General for Children and Armed Conflict, Radhika Coomaraswamy, who stepped down from her position a week later, met representatives of the Government of Myanmar on 5 July 2012 and stated that she hoped the government's signing of an action plan would ""signal a transformation."" In September 2012, the Myanmar Armed Forces released 42 child soldiers and the International Labour Organization met with representatives of the government as well as the Kachin Independence Army to secure the release of more child soldiers. According to Samantha Power, a US delegation raised the issue of child soldiers with the government in October 2012. However, she did not comment on the government's progress towards reform in this area. Though the country's foreign relations, particularly with Western nations, have been strained, relations have thawed since the reforms following the 2010 elections. After years of diplomatic isolation and economic and military sanctions, the United States relaxed curbs on foreign aid to Myanmar in November 2011 and announced the resumption of diplomatic relations on 13 January 2012 The European Union has placed sanctions on Myanmar, including an arms embargo, cessation of trade preferences, and suspension of all aid with the exception of humanitarian aid. Political unification returned in the mid-16th century, due to the efforts of Taungoo, a former vassal state of Ava. Taungoo's young, ambitious king Tabinshwehti defeated the more powerful Hanthawaddy in the Toungoo–Hanthawaddy War (1534–41). His successor Bayinnaung went on to conquer a vast swath of mainland Southeast Asia including the Shan states, Lan Na, Manipur, Mong Mao, the Ayutthaya Kingdom, Lan Xang and southern Arakan. However, the largest empire in the history of Southeast Asia unravelled soon after Bayinnaung's death in 1581, completely collapsing by 1599. Ayutthaya seized Tenasserim and Lan Na, and Portuguese mercenaries established Portuguese rule at Thanlyin (Syriam). Opinions differ whether the transition to liberal democracy is underway. According to some reports, the military's presence continues as the label 'disciplined democracy' suggests. This label asserts that the Burmese military is allowing certain civil liberties while clandestinely institutionalising itself further into Burmese politics. Such an assertion assumes that reforms only occurred when the military was able to safeguard its own interests through the transition—here, ""transition"" does not refer to a transition to a liberal democracy, but transition to a quasi-military rule. The immediate cause of the riots is unclear, with many commentators citing the killing of ten Burmese Muslims by ethnic Rakhine after the rape and murder of a Rakhine woman as the main cause. Whole villages have been ""decimated"". Over 300 houses and a number of public buildings have been razed. According to Tun Khin, the president of the Burmese Rohingya Organisation UK (BROUK), as of 28 June 2012, 650 Rohingyas have been killed, 1,200 are missing, and more than 80,000 have been displaced. According to the Myanmar authorities, the violence, between ethnic Rakhine Buddhists and Rohingya Muslims, left 78 people dead, 87 injured, and thousands of homes destroyed. It displaced more than 52,000 people. There is consensus that the military regime in Myanmar is one of the world's most repressive and abusive regimes. In November 2012, Samantha Power, Barack Obama's Special Assistant to the President on Human Rights, wrote on the White House blog in advance of the president's visit that ""Serious human rights abuses against civilians in several regions continue, including against women and children."" Members of the United Nations and major international human rights organisations have issued repeated and consistent reports of widespread and systematic human rights violations in Myanmar. The United Nations General Assembly has repeatedly called on the Burmese Military Junta to respect human rights and in November 2009 the General Assembly adopted a resolution ""strongly condemning the ongoing systematic violations of human rights and fundamental freedoms"" and calling on the Burmese Military Regime ""to take urgent measures to put an end to violations of international human rights and humanitarian law."" Armed conflict between ethnic Chinese rebels and the Myanmar Armed Forces have resulted in the Kokang offensive in February 2015. The conflict had forced 40,000 to 50,000 civilians to flee their homes and seek shelter on the Chinese side of the border. During the incident the government of China was accused of giving military assistance to the ethnic Chinese rebels. Burmese officials have been historically 'manipulated' and pressured by the communist Chinese government throughout Burmese modern history to create closer and binding ties with China, creating a Chinese satellite state in Southeast Asia. Until 2005, the United Nations General Assembly annually adopted a detailed resolution about the situation in Myanmar by consensus. But in 2006 a divided United Nations General Assembly voted through a resolution that strongly called upon the government of Myanmar to end its systematic violations of human rights. In January 2007, Russia and China vetoed a draft resolution before the United Nations Security Council calling on the government of Myanmar to respect human rights and begin a democratic transition. South Africa also voted against the resolution. Like the Pagan Empire, Ava, Hanthawaddy and the Shan states were all multi-ethnic polities. Despite the wars, cultural synchronisation continued. This period is considered a golden age for Burmese culture. Burmese literature ""grew more confident, popular, and stylistically diverse"", and the second generation of Burmese law codes as well as the earliest pan-Burma chronicles emerged. Hanthawaddy monarchs introduced religious reforms that later spread to the rest of the country. Many splendid temples of Mrauk U were built during this period. In 2007 the German professor Bassam Tibi suggested that the Rohingya conflict may be driven by an Islamist political agenda to impose religious laws, while non-religious causes have also been raised, such as a lingering resentment over the violence that occurred during the Japanese occupation of Burma in World War II—during this time period the British allied themselves with the Rohingya and fought against the puppet government of Burma (composed mostly of Bamar Japanese) that helped to establish the Tatmadaw military organisation that remains in power as of March 2013. In 1 April 2012 by-elections the NLD won 43 of the 45 available seats; previously an illegal organisation, the NLD had never won a Burmese election until this time. The 2012 by-elections were also the first time that international representatives were allowed to monitor the voting process in Myanmar. Following announcement of the by-elections, the Freedom House organisation raised concerns about ""reports of fraud and harassment in the lead up to elections, including the March 23 deportation of Somsri Hananuntasuk, executive director of the Asian Network for Free Elections (ANFREL), a regional network of civil society organisations promoting democratization."" However, uncertainties exist as some other political prisoners have not been released and clashes between Burmese troops and local insurgent groups continue. Since the democratic transition began in 2011, there has been continuous violence as 280 people have been killed and 140,000 forced to flee from their homes in the Rakhine state. A UN envoy reported in March 2013 that unrest had re-emerged between Myanmar's Buddhist and Muslim communities, with violence spreading to towns that are located closer to Yangon. The BBC News media outlet obtained video footage of a man with severe burns who received no assistance from passers-by or police officers even though he was lying on the ground in a public area. The footage was filmed by members of the Burmese police force in the town of Meiktila and was used as evidence that Buddhists continued to kill Muslims after the European Union sanctions were lifted on 23 April 2013. Sanctions imposed by the United States and European countries against the former military government, coupled with boycotts and other direct pressure on corporations by supporters of the democracy movement, have resulted in the withdrawal from the country of most US and many European companies. On 13 April 2012 British Prime Minister David Cameron called for the economic sanctions on Myanmar to be suspended in the wake of the pro-democracy party gaining 43 seats out of a possible 45 in the 2012 by-elections with the party leader, Aung San Suu Kyi becoming a member of the Burmese parliament. Myanmar's armed forces are known as the Tatmadaw, which numbers 488,000. The Tatmadaw comprises the Army, the Navy, and the Air Force. The country ranked twelfth in the world for its number of active troops in service. The military is very influential in Myanmar, with all top cabinet and ministry posts usually held by military officials. Official figures for military spending are not available. Estimates vary widely because of uncertain exchange rates, but Myanmar's military forces' expenses are high. Myanmar imports most of its weapons from Russia, Ukraine, China and India. According to Pew Research, 7% of the population identifies as Christian; 4% as Muslim; 1% follows traditional animistic beliefs; and 2% follow other religions, including Mahayana Buddhism, Hinduism, and East Asian religions. However, according to a US State Department's 2010 international religious freedom report, official statistics are alleged to underestimate the non-Buddhist population. Independent researchers put the Muslim population at 6 to 10% of the population[citation needed]. Jehovah's Witnesses have been present since 1914 and have about 80 congregations around the country and a branch office in Yangon publishing in 16 languages. A tiny Jewish community in Rangoon had a synagogue but no resident rabbi to conduct services. In a traditional village, the monastery is the centre of cultural life. Monks are venerated and supported by the lay people. A novitiation ceremony called shinbyu is the most important coming of age events for a boy, during which he enters the monastery for a short time. All male children in Buddhist families are encouraged to be a novice (beginner for Buddhism) before the age of twenty and to be a monk after the age of twenty. Girls have ear-piercing ceremonies (နားသ) at the same time. Burmese culture is most evident in villages where local festivals are held throughout the year, the most important being the pagoda festival. Many villages have a guardian nat, and superstition and taboos are commonplace. The impact of the post-election reforms has been observed in numerous areas, including ASEAN's approval of Myanmar's bid for the position of ASEAN chair in 2014; the visit by United States Secretary of State Hillary Clinton in December 2011 for the encouragement of further progress—it was the first visit by a Secretary of State in more than fifty years (Clinton met with the Burmese president and former military commander Thein Sein, as well as opposition leader Aung San Suu Kyi); and the participation of Aung San Suu Kyi's National League for Democracy (NLD) party in the 2012 by-elections, facilitated by the government's abolition of the laws that previously barred the NLD. As of July 2013, about 100 political prisoners remain imprisoned, while conflict between the Burmese Army and local insurgent groups continues. Much of the country lies between the Tropic of Cancer and the Equator. It lies in the monsoon region of Asia, with its coastal regions receiving over 5,000 mm (196.9 in) of rain annually. Annual rainfall in the delta region is approximately 2,500 mm (98.4 in), while average annual rainfall in the Dry Zone in central Myanmar is less than 1,000 mm (39.4 in). The Northern regions of Myanmar are the coolest, with average temperatures of 21 °C (70 °F). Coastal and delta regions have an average maximum temperature of 32 °C (89.6 °F). In December 2014, Myanmar signed an agreement to set up its first stock exchange. The Yangon Stock Exchange Joint Venture Co. Ltd will be set up with Myanma Economic Bank sharing 51 percent, Japan's Daiwa Institute of Research Ltd 30.25 percent and Japan Exchange Group 18.75 percent. The Yangon Stock Exchange (YSX) officially opened for business on Friday, March 25, 2016. First Myanmar Investment Co., Ltd. (FMI) became the first stock to be traded after receiving approval for an opening price of 26,000 kyats ($22). The goal of the Burmese constitutional referendum of 2008, held on 10 May 2008, is the creation of a ""discipline-flourishing democracy"". As part of the referendum process, the name of the country was changed from the ""Union of Myanmar"" to the ""Republic of the Union of Myanmar"", and general elections were held under the new constitution in 2010. Observer accounts of the 2010 election describe the event as mostly peaceful; however, allegations of polling station irregularities were raised, and the United Nations (UN) and a number of Western countries condemned the elections as fraudulent. Myanmar has received extensive military aid from China in the past Myanmar has been a member of ASEAN since 1997. Though it gave up its turn to hold the ASEAN chair and host the ASEAN Summit in 2006, it chaired the forum and hosted the summit in 2014. In November 2008, Myanmar's political situation with neighbouring Bangladesh became tense as they began searching for natural gas in a disputed block of the Bay of Bengal. Controversy surrounding the Rohingya population also remains an issue between Bangladesh and Myanmar. The dynasty regrouped and defeated the Portuguese in 1613 and Siam in 1614. It restored a smaller, more manageable kingdom, encompassing Lower Myanmar, Upper Myanmar, Shan states, Lan Na and upper Tenasserim. The Restored Toungoo kings created a legal and political framework whose basic features would continue well into the 19th century. The crown completely replaced the hereditary chieftainships with appointed governorships in the entire Irrawaddy valley, and greatly reduced the hereditary rights of Shan chiefs. Its trade and secular administrative reforms built a prosperous economy for more than 80 years. From the 1720s onward, the kingdom was beset with repeated Meithei raids into Upper Myanmar and a nagging rebellion in Lan Na. In 1740, the Mon of Lower Myanmar founded the Restored Hanthawaddy Kingdom. Hanthawaddy forces sacked Ava in 1752, ending the 266-year-old Toungoo Dynasty. Following World War II, Aung San negotiated the Panglong Agreement with ethnic leaders that guaranteed the independence of Myanmar as a unified state. Aung Zan Wai, Pe Khin, Bo Hmu Aung, Sir Maung Gyi, Dr. Sein Mya Maung, Myoma U Than Kywe were among the negotiators of the historical Panglong Conference negotiated with Bamar leader General Aung San and other ethnic leaders in 1947. In 1947, Aung San became Deputy Chairman of the Executive Council of Myanmar, a transitional government. But in July 1947, political rivals assassinated Aung San and several cabinet members. A diverse range of indigenous cultures exist in Myanmar, the majority culture is primarily Buddhist and Bamar. Bamar culture has been influenced by the cultures of neighbouring countries. This is manifested in its language, cuisine, music, dance and theatre. The arts, particularly literature, have historically been influenced by the local form of Theravada Buddhism. Considered the national epic of Myanmar, the Yama Zatdaw, an adaptation of India's Ramayana, has been influenced greatly by Thai, Mon, and Indian versions of the play. Buddhism is practised along with nat worship, which involves elaborate rituals to propitiate one from a pantheon of 37 nats. Myanmar's first film was a documentary of the funeral of Tun Shein — a leading politician of the 1910s, who campaigned for Burmese independence in London. The first Burmese silent film Myitta Ne Thuya (Love and Liquor) in 1920 which proved a major success, despite its poor quality due to a fixed camera position and inadequate film accessories. During the 1920s and 1930s, many Burmese-owned film companies made and produced several films. The first Burmese sound film was produced in 1932 in Bombay, India with the title Ngwe Pay Lo Ma Ya (Money Can't Buy It). After World War II, Burmese cinema continued to address political themes. Many of the films produced in the early Cold War era had a strong propaganda element to them. Pagan's collapse was followed by 250 years of political fragmentation that lasted well into the 16th century. Like the Burmans four centuries earlier, Shan migrants who arrived with the Mongol invasions stayed behind. Several competing Shan States came to dominate the entire northwestern to eastern arc surrounding the Irrawaddy valley. The valley too was beset with petty states until the late 14th century when two sizeable powers, Ava Kingdom and Hanthawaddy Kingdom, emerged. In the west, a politically fragmented Arakan was under competing influences of its stronger neighbours until the Kingdom of Mrauk U unified the Arakan coastline for the first time in 1437. Many religions are practised in Myanmar. Religious edifices and orders have been in existence for many years. Festivals can be held on a grand scale. The Christian and Muslim populations do, however, face religious persecution and it is hard, if not impossible, for non-Buddhists to join the army or get government jobs, the main route to success in the country. Such persecution and targeting of civilians is particularly notable in Eastern Myanmar, where over 3000 villages have been destroyed in the past ten years. More than 200,000 Muslims have fled to Bangladesh over the last 20 years to escape Islamophobic persecution. The educational system of Myanmar is operated by the government agency, the Ministry of Education. The education system is based on the United Kingdom's system due to nearly a century of British and Christian presences in Myanmar. Nearly all schools are government-operated, but there has been a recent increase in privately funded English language schools. Schooling is compulsory until the end of elementary school, approximately about 9 years old, while the compulsory schooling age is 15 or 16 at international level. In August 2007, an increase in the price of diesel and petrol led to Saffron Revolution led by Buddhist monks that were dealt with harshly by the government. The government cracked down on them on 26 September 2007. The crackdown was harsh, with reports of barricades at the Shwedagon Pagoda and monks killed. There were also rumours of disagreement within the Burmese armed forces, but none was confirmed. The military crackdown against unarmed protesters was widely condemned as part of the International reactions to the Saffron Revolution and led to an increase in economic sanctions against the Burmese Government. For most of its independent years, the country has been engrossed in rampant ethnic strife and Burma's myriad ethnic groups have been involved in one of the world's longest-running ongoing civil wars. During this time, the United Nations and several other organisations have reported consistent and systematic human rights violations in the country. In 2011, the military junta was officially dissolved following a 2010 general election, and a nominally civilian government was installed. While former military leaders still wield enormous power in the country, Burmese Military have taken steps toward relinquishing control of the government. This, along with the release of Aung San Suu Kyi and political prisoners, has improved the country's human rights record and foreign relations, and has led to the easing of trade and other economic sanctions. There is, however, continuing criticism of the government's treatment of the Muslim Rohingya minority and its poor response to the religious clashes. In the landmark 2015 election, Aung San Suu Kyi's party won a majority in both houses, ending military rule. Flights are available from most countries, though direct flights are limited to mainly Thai and other ASEAN airlines. According to Eleven magazine, ""In the past, there were only 15 international airlines and increasing numbers of airlines have began launching direct flights from Japan, Qatar, Taiwan, South Korea, Germany and Singapore."" Expansions were expected in September 2013, but yet again are mainly Thai and other Asian-based airlines according to Eleven Media Group's Eleven, ""Thailand-based Nok Air and Business Airlines and Singapore-based Tiger Airline"". The government has also relaxed reporting laws, but these remain highly restrictive. In September 2011, several banned websites, including YouTube, Democratic Voice of Burma and Voice of America, were unblocked. A 2011 report by the Hauser Center for Nonprofit Organizations found that, while contact with the Myanmar government was constrained by donor restrictions, international humanitarian non-governmental organisations (NGOs) see opportunities for effective advocacy with government officials, especially at the local level. At the same time, international NGOs are mindful of the ethical quandary of how to work with the government without bolstering or appeasing it. Following Thein Sein's first ever visit to the UK and a meeting with Prime Minister David Cameron, the Myanmar president declared that all of his nation's political prisoners will be released by the end of 2013, in addition to a statement of support for the well-being of the Rohingya Muslim community. In a speech at Chatham House, he revealed that ""We [Myanmar government] are reviewing all cases. I guarantee to you that by the end of this year, there will be no prisoners of conscience in Myanmar."", in addition to expressing a desire to strengthen links between the UK and Myanmar's military forces. Despite Western isolation, Asian corporations have generally remained willing to continue investing in the country and to initiate new investments, particularly in natural resource extraction. The country has close relations with neighbouring India and China with several Indian and Chinese companies operating in the country. Under India's Look East policy, fields of co-operation between India and Myanmar include remote sensing, oil and gas exploration, information technology, hydro power and construction of ports and buildings. Konbaung kings extended Restored Toungoo's administrative reforms, and achieved unprecedented levels of internal control and external expansion. For the first time in history, the Burmese language and culture came to predominate the entire Irrawaddy valley. The evolution and growth of Burmese literature and theatre continued, aided by an extremely high adult male literacy rate for the era (half of all males and 5% of females). Nonetheless, the extent and pace of reforms were uneven and ultimately proved insufficient to stem the advance of British colonialism. Since the 2010 election, the government has embarked on a series of reforms to direct the country towards liberal democracy, a mixed economy, and reconciliation, although doubts persist about the motives that underpin such reforms. The series of reforms includes the release of pro-democracy leader Aung San Suu Kyi from house arrest, the establishment of the National Human Rights Commission, the granting of general amnesties for more than 200 political prisoners, new labour laws that permit labour unions and strikes, a relaxation of press censorship, and the regulation of currency practices. The Bamar form an estimated 68% of the population. 10% of the population are Shan. The Kayin make up 7% of the population. The Rakhine people constitute 4% of the population. Overseas Chinese form approximately 3% of the population. Myanmar's ethnic minority groups prefer the term ""ethnic nationality"" over ""ethnic minority"" as the term ""minority"" furthers their sense of insecurity in the face of what is often described as ""Burmanisation""—the proliferation and domination of the dominant Bamar culture over minority cultures. Early civilisations in Myanmar included the Tibeto-Burman-speaking Pyu city-states in Upper Burma and the Mon kingdoms in Lower Burma. In the 9th century, the Bamar people entered the upper Irrawaddy valley and, following the establishment of the Pagan Kingdom in the 1050s, the Burmese language, culture and Theravada Buddhism slowly became dominant in the country. The Pagan Kingdom fell due to the Mongol invasions and several warring states emerged. In the 16th century, reunified by the Taungoo Dynasty, the country was for a brief period the largest empire in the history of Southeast Asia. The early 19th century Konbaung Dynasty ruled over an area that included modern Myanmar and briefly controlled Manipur and Assam as well. The British conquered Myanmar after three Anglo-Burmese Wars in the 19th century and the country became a British colony. Myanmar became an independent nation in 1948, initially as a democratic nation and then, following a coup d'état in 1962, a military dictatorship."
Human_Development_Index,"The origins of the HDI are found in the annual Development Reports of the United Nations Development Programme (UNDP). These were devised and launched by Pakistani economist Mahbub ul Haq in 1990 and had the explicit purpose ""to shift the focus of development economics from national income accounting to people-centered policies"". To produce the Human Development Reports, Mahbub ul Haq formed a group of development economists including Paul Streeten, Frances Stewart, Gustav Ranis, Keith Griffin, Sudhir Anand, and Meghnad Desai. Working alongside Nobel laureate Amartya Sen, they worked on capabilities and functions that provided the underlying conceptual framework. Haq was sure that a simple composite measure of human development was needed in order to convince the public, academics, and politicians that they can and should evaluate development not only by economic advances but also improvements in human well-being. Sen initially opposed this idea, but he soon went on to help Haq develop the Index. Sen was worried that it was going to be difficult to capture the full complexity of human capabilities in a single index, but Haq persuaded him that only a single number would shift the immediate attention of politicians from economic to human well-being. The 2015 Human Development Report by the United Nations Development Program was released on December 14, 2015, and calculates HDI values based on estimates for 2014. Below is the list of the ""very high human development"" countries: Note: The green arrows (), red arrows (), and blue dashes () represent changes in rank when compared to the 2011 HDI list, for countries listed in both rankings. The formula defining the HDI is promulgated by the United Nations Development Programme (UNDP). In general, to transform a raw variable, say , into a unit-free index between 0 and 1 (which allows different indices to be added together), the following formula is used: Countries in the top quartile of HDI (""very high human development"" group) with a missing IHDI: New Zealand, Singapore, Hong Kong, Liechtenstein, Brunei, Qatar, Saudi Arabia, Andorra, United Arab Emirates, Bahrain, Cuba, and Kuwait. The 2010 Human Development Report by the United Nations Development Program was released on November 4, 2010, and calculates HDI values based on estimates for 2010. Below is the list of the ""very high human development"" countries: Some countries were not included for various reasons, mainly the unavailability of certain crucial data. The following United Nations Member States were not included in the 2011 report: North Korea, Marshall Islands, Monaco, Nauru, San Marino, South Sudan, Somalia and Tuvalu. Some countries were not included for various reasons, mainly the unavailability of certain crucial data. The following United Nations Member States were not included in the 2010 report. Cuba lodged a formal protest at its lack of inclusion. The UNDP explained that Cuba had been excluded due to the lack of an ""internationally reported figure for Cuba’s Gross National Income adjusted for Purchasing Power Parity"". All other indicators for Cuba were available, and reported by the UNDP, but the lack of one indicator meant that no ranking could be attributed to the country. The situation has been addressed and, in later years, Cuba has ranked as a High Human Development country. The Human Development Index (HDI) is a composite statistic of life expectancy, education, and income per capita indicators, which are used to rank countries into four tiers of human development. A country scores higher HDI when the life expectancy at birth is longer, the education period is longer, and the income per capita is higher. The HDI was developed by the Pakistani economist Mahbub ul Haq, often framed in terms of whether people are able to ""be"" and ""do"" desirable things in their life, and was published by the United Nations Development Programme. Note: The green arrows (), red arrows (), and blue dashes () represent changes in rank when compared to the 2010 HDI list, for countries listed in both rankings. A HDI of 0.8 or more is considered to represent ""high development"". This includes all developed countries, such as those in North America, Western Europe, Oceania, and Eastern Asia, as well as some developing countries in Eastern Europe, Central and South America, Southeast Asia, the Caribbean, and the oil-rich Arabian Peninsula. Seven countries were promoted to this category this year, leaving the ""medium development"" group: Albania, Belarus, Brazil, Libya, Macedonia, Russia and Saudi Arabia. The 2009 Human Development Report by UNDP was released on October 5, 2009, and covers the period up to 2007. It was titled ""Overcoming barriers: Human mobility and development"". The top countries by HDI were grouped in a new category called ""very high human development"". The report refers to these countries as developed countries. They are: The Human Development Index has been criticized on a number of grounds including alleged ideological biases towards egalitarianism and so-called ""Western models of development"", failure to include any ecological considerations, lack of consideration of technological development or contributions to the human civilization, focusing exclusively on national performance and ranking, lack of attention to development from a global perspective, measurement error of the underlying statistics, and on the UNDP's changes in formula which can lead to severe misclassification in the categorisation of 'low', 'medium', 'high' or 'very high' human development countries. Some countries were not included for various reasons, primarily the lack of necessary data. The following United Nations Member States were not included in the 2014 report: North Korea, Marshall Islands, Monaco, Nauru, San Marino, Somalia, India, Pakistan, South Sudan, and Tuvalu. Countries in the top quartile of HDI (""very high human development"" group) with a missing IHDI include: New Zealand, Liechtenstein, Japan, Hong Kong, Singapore, Taiwan, United Arab Emirates, Andorra, Brunei, Malta, Qatar, Bahrain, Chile, Argentina and Barbados. Note: The green arrows (), red arrows (), and blue dashes () represent changes in rank. The changes in rank are not relative to the HDI list above, but are according to the source (p. 168) calculated with the exclusion of countries which are missing IHDI data. The HDI has extended its geographical coverage: David Hastings, of the United Nations Economic and Social Commission for Asia and the Pacific, published a report geographically extending the HDI to 230+ economies, whereas the UNDP HDI for 2009 enumerates 182 economies and coverage for the 2010 HDI dropped to 169 countries. Below is a list of countries in the top quartile by Inequality-adjusted Human Development Index (IHDI). According to the report, the IHDI is a ""measure of the average level of human development of people in a society once inequality is taken into account."" The 2010 Human Development Report was the first to calculate an Inequality-adjusted Human Development Index (IHDI), which factors in inequalities in the three basic dimensions of human development (income, life expectancy, and education). Below is a list of countries in the top quartile by IHDI: Some countries were not included for various reasons, such as being a non-UN member or unable or unwilling to provide the necessary data at the time of publication. Besides the states with limited recognition, the following states were also not included. Economists Hendrik Wolff, Howard Chong and Maximilian Auffhammer discuss the HDI from the perspective of data error in the underlying health, education and income statistics used to construct the HDI. They identified three sources of data error which are due to (i) data updating, (ii) formula revisions and (iii) thresholds to classify a country’s development status and conclude that 11%, 21% and 34% of all countries can be interpreted as currently misclassified in the development bins due to the three sources of data error, respectively. The authors suggest that the United Nations should discontinue the practice of classifying countries into development bins because - they claim - the cut-off values seem arbitrary, can provide incentives for strategic behavior in reporting official statistics, and have the potential to misguide politicians, investors, charity donors and the public who use the HDI at large.[citation needed] In 2010 the UNDP reacted to the criticism and updated the thresholds to classify nations as low, medium, and high human development countries. In a comment to The Economist in early January 2011, the Human Development Report Office responded to a January 6, 2011 article in the magazine which discusses the Wolff et al. paper. The Human Development Report Office states that they undertook a systematic revision of the methods used for the calculation of the HDI and that the new methodology directly addresses the critique by Wolff et al. in that it generates a system for continuous updating of the human development categories whenever formula or data revisions take place. The 2013 Human Development Report by the United Nations Development Program was released on March 14, 2013, and calculates HDI values based on estimates for 2012. Below is the list of the ""very high human development"" countries: The 2010 Human Development Report introduced an Inequality-adjusted Human Development Index (IHDI). While the simple HDI remains useful, it stated that ""the IHDI is the actual level of human development (accounting for inequality),"" and ""the HDI can be viewed as an index of 'potential' human development (or the maximum IHDI that could be achieved if there were no inequality)."" Note: The green arrows (), red arrows (), and blue dashes () represent changes in rank when compared to the new 2012 data HDI for 2011 – published in the 2012 report. On the following table, green arrows () represent an increase in ranking over the previous study, while red arrows () represent a decrease in ranking. They are followed by the number of spaces they moved. Blue dashes () represent a nation that did not move in the rankings since the previous study. Countries in the top quartile of HDI (""very high human development"" group) with a missing IHDI: New Zealand, Chile, Japan, Hong Kong, Singapore, Taiwan, Liechtenstein, Brunei, Andorra, Qatar, Barbados, United Arab Emirates, and Seychelles. A new index was released on December 18, 2008. This so-called ""statistical update"" covered the period up to 2006 and was published without an accompanying Human Development Report. The update is relevant due to newly released estimates of purchasing power parities (PPP), implying substantial adjustments for many countries, resulting in changes in HDI values and, in many cases, HDI ranks. Some countries were not included for various reasons, such as being a non-UN member, unable, or unwilling to provide the necessary data at the time of publication. Besides the states with limited recognition, the following states were also not included. The 2011 Human Development Report was released on 2 November 2011, and calculated HDI values based on estimates for 2011. Below is the list of the ""very high human development"" countries (equal to the top quartile): Note: The green arrows (), red arrows (), and blue dashes () represent changes in rank when compared to the 2011 HDI data for 2010 – published in the 2011 report (p. 131). The list below displays the top-ranked country from each year of the Human Development Index. Norway has been ranked the highest twelve times, Canada eight times, followed by Japan which has been ranked highest three times. Iceland has been ranked highest twice. The report showed a small increase in world HDI in comparison with last year's report. This rise was fueled by a general improvement in the developing world, especially of the least developed countries group. This marked improvement at the bottom was offset with a decrease in HDI of high income countries. The Inequality-adjusted Human Development Index (IHDI) is a ""measure of the average level of human development of people in a society once inequality is taken into account."" A HDI below 0.5 is considered to represent ""low development"". All 22 countries in that category are located in Africa. The highest-scoring Sub-Saharan countries, Gabon and South Africa, are ranked 119th and 121st, respectively. Nine countries departed from this category this year and joined the ""medium development"" group. The Human Development Report for 2007/2008 was launched in Brasília, Brazil, on November 27, 2007. Its focus was on ""Fighting climate change: Human solidarity in a divided world."" Most of the data used for the report are derived largely from 2005 or earlier, thus indicating an HDI for 2005. Not all UN member states choose to or are able to provide the necessary statistics. The 2014 Human Development Report by the United Nations Development Program was released on July 24, 2014, and calculates HDI values based on estimates for 2013. Below is the list of the ""very high human development"" countries: LE: Life expectancy at birth
MYS: Mean years of schooling (Years that a person 25 years-of-age or older has spent in schools)
EYS: Expected years of schooling (Years that a 5-year-old child will spend in schools throughout his life)
GNIpc: Gross national income at purchasing power parity per capita"
Hokkien,"In general, Hokkien dialects have 5 to 7 phonemic tones. According to the traditional Chinese system, however, there are 7 to 9 ""tones"",[citation needed] more correctly termed tone classes since two of them are non-phonemic ""entering tones"" (see the discussion on Chinese tone). Tone sandhi is extensive. There are minor variations between the Quanzhou and Zhangzhou tone systems. Taiwanese tones follow the patterns of Amoy or Quanzhou, depending on the area of Taiwan. Many dialects have an additional phonemic tone (""tone 9"" according to the traditional reckoning), used only in special or foreign loan words. Hokkien originated from Quanzhou. After the Opium War in 1842, Xiamen (Amoy) became one of the major treaty ports to be opened for trade with the outside world. From mid-19th century onwards, Xiamen slowly developed to become the political and economical center of the Hokkien-speaking region in China. This caused Amoy dialect to gradually replace the position of dialect variants from Quanzhou and Zhangzhou. From mid-19th century until the end of World War II, western diplomats usually learned Amoy Hokkien as the preferred dialect if they were to communicate with the Hokkien-speaking populace in China or South-East Asia. In the 1940s and 1950s, Taiwan also held Amoy Hokkien as its standard and tended to incline itself towards Amoy dialect. Hokkien has one of the most diverse phoneme inventories among Chinese varieties, with more consonants than Standard Mandarin or Cantonese. Vowels are more-or-less similar to that of Standard Mandarin. Hokkien varieties retain many pronunciations that are no longer found in other Chinese varieties. These include the retention of the /t/ initial, which is now /tʂ/ (Pinyin 'zh') in Mandarin (e.g. 'bamboo' 竹 is tik, but zhú in Mandarin), having disappeared before the 6th century in other Chinese varieties. Hokkien /hɒˈkiɛn/ (traditional Chinese: 福建話; simplified Chinese: 福建话; pinyin: Fújiànhuà; Pe̍h-ōe-jī: Hok-kiàn oē) or Quanzhang (Quanzhou–Zhangzhou / Chinchew–Changchew; BP: Zuánziū–Ziāngziū) is a group of mutually intelligible Min Nan Chinese dialects spoken throughout Southeast Asia, Taiwan, and by many other overseas Chinese. Hokkien originated from a dialect in southern Fujian. It is closely related to the Teochew, though mutual comprehension is difficult, and is somewhat more distantly related to Hainanese. Besides Hokkien, there are also other Min and Hakka dialects in Fujian province, most of which are not mutually intelligible with Hokkien. In 677 (during the reign of Emperor Gaozong), Chen Zheng (陳政), together with his son Chen Yuanguang (陳元光), led a military expedition to pacify the rebellion in Fujian. They settled in Zhangzhou and brought the Middle Chinese phonology of northern China during the 7th century into Zhangzhou; In 885, (during the reign of Emperor Xizong of Tang), the two brothers Wang Chao (王潮) and Wang Shenzhi (王審知), led a military expedition force to pacify the Huang Chao rebellion. They brought the Middle Chinese phonology commonly spoken in Northern China into Zhangzhou. These two waves of migrations from the north generally brought the language of northern Middle Chinese into the Fujian region. This then gradually evolved into the Zhangzhou dialect. Min Nan texts, all Hokkien, can be dated back to the 16th century. One example is the Doctrina Christiana en letra y lengua china, presumably written after 1587 by the Spanish Dominicans in the Philippines. Another is a Ming Dynasty script of a play called Romance of the Lychee Mirror (1566), supposedly the earliest Southern Min colloquial text. Xiamen University has also developed an alphabet based on Pinyin, which has been published in a dictionary called the Minnan Fangyan-Putonghua Cidian (閩南方言普通話詞典) and a language teaching book, which is used to teach the language to foreigners and Chinese non-speakers. It is known as Pumindian. There are many Hokkien speakers among overseas Chinese in Southeast Asia as well as in the United States. Many ethnic Han Chinese emigrants to the region were Hoklo from southern Fujian, and brought the language to what is now Burma (Myanmar), Indonesia (the former Dutch East Indies) and present day Malaysia and Singapore (formerly Malaya and the British Straits Settlements). Many of the Hokkien dialects of this region are highly similar to Taiwanese and Amoy. Hokkien is reportedly the native language of up to 98.5% of the Chinese Filipino in the Philippines, among which is known locally as Lan-nang or Lán-lâng-oē (""Our people’s language""). Hokkien speakers form the largest group of Chinese in Singapore, Malaysia and Indonesia.[citation needed] During the Three Kingdoms period of ancient China, there was constant warfare occurring in the Central Plain of China. Northerners began to enter into Fujian region, causing the region to incorporate parts of northern Chinese dialects. However, the massive migration of northern Han Chinese into Fujian region mainly occurred after the Disaster of Yongjia. The Jìn court fled from the north to the south, causing large numbers of northern Han Chinese to move into Fujian region. They brought the old Chinese — spoken in Central Plain of China from prehistoric era to 3rd century — into Fujian. This then gradually evolved into the Quanzhou dialect. The Amoy dialect (Xiamen) is a hybrid of the Quanzhou and Zhangzhou dialects. Taiwanese is also a hybrid of these two dialects. Taiwanese in northern Taiwan tends to be based on the Quanzhou variety, whereas the Taiwanese spoken in southern Taiwan tends to be based on Zhangzhou speech. There are minor variations in pronunciation and vocabulary between Quanzhou and Zhangzhou dialects. The grammar is generally the same. Additionally, extensive contact with the Japanese language has left a legacy of Japanese loanwords in Taiwanese Hokkien. On the other hand, the variants spoken in Singapore and Malaysia have a substantial number of loanwords from Malay and to a lesser extent, from English and other Chinese varieties, such as the closely related Teochew and some Cantonese. Hokkien dialects are analytic; in a sentence, the arrangement of words is important to its meaning. A basic sentence follows the subject–verb–object pattern (i.e. a subject is followed by a verb then by an object), though this order is often violated because Hokkien dialects are topic-prominent. Unlike synthetic languages, seldom do words indicate time, gender and plural by inflection. Instead, these concepts are expressed through adverbs, aspect markers, and grammatical particles, or are deduced from the context. Different particles are added to a sentence to further specify its status or intonation. The existence of literary and colloquial readings (文白異讀), called tha̍k-im (讀音), is a prominent feature of some Hokkien dialects and indeed in many Sinitic varieties in the south. The bulk of literary readings (文讀, bûn-tha̍k), based on pronunciations of the vernacular during the Tang Dynasty, are mainly used in formal phrases and written language (e.g. philosophical concepts, surnames, and some place names), while the colloquial (or vernacular) ones (白讀, pe̍h-tha̍k) are basically used in spoken language and vulgar phrases. Literary readings are more similar to the pronunciations of the Tang standard of Middle Chinese than their colloquial equivalents. The pronounced divergence between literary and colloquial pronunciations found in Hokkien dialects is attributed to the presence of several strata in the Min lexicon. The earliest, colloquial stratum is traced to the Han dynasty (206 BCE - 220 CE); the second colloquial one comes from the period of the Southern and Northern Dynasties (420 - 589 CE); the third stratum of pronunciations (typically literary ones) comes from the Tang Dynasty (618–907 CE) and is based on the prestige dialect of Chang'an (modern day Xi'an), its capital. While most Hokkien morphemes have standard designated characters, they are not always etymological or phono-semantic. Similar-sounding, similar-meaning or rare characters are commonly borrowed or substituted to represent a particular morpheme. Examples include ""beautiful"" (美 bí is the literary form), whose vernacular morpheme suí is represented by characters like 媠 (an obsolete character), 婎 (a vernacular reading of this character) and even 水 (transliteration of the sound suí), or ""tall"" (高 ko is the literary form), whose morpheme kôan is 懸. Common grammatical particles are not exempt; the negation particle m̄ (not) is variously represented by 毋, 呣 or 唔, among others. In other cases, characters are invented to represent a particular morpheme (a common example is the character 𪜶 in, which represents the personal pronoun ""they""). In addition, some characters have multiple and unrelated pronunciations, adapted to represent Hokkien words. For example, the Hokkien word bah (""meat"") has been reduced to the character 肉, which has etymologically unrelated colloquial and literary readings (he̍k and jio̍k, respectively). Another case is the word 'to eat,' chia̍h, which is often transcribed in Taiwanese newspapers and media as 呷 (a Mandarin transliteration, xiā, to approximate the Hokkien term), even though its recommended character in dictionaries is 食. In 2002, the Taiwan Solidarity Union, a party with about 10% of the Legislative Yuan seats at the time, suggested making Taiwanese a second official language. This proposal encountered strong opposition not only from Mainlander groups but also from Hakka and Taiwanese aboriginal groups who felt that it would slight their home languages, as well as others including Hoklo who objected to the proposal on logistical grounds and on the grounds that it would increase ethnic tensions. Because of these objections, support for this measure was lukewarm among moderate Taiwan independence supporters, and the proposal did not pass. Xiamen dialect, sometimes known as Amoy, is the main dialect spoken in the Chinese city of Xiamen and its surrounding regions of Tong'an and Xiang'an, both of which are now included in the Greater Xiamen area. This dialect developed in the late Ming dynasty when Xiamen was increasingly taking over Quanzhou's position as the main port of trade in southeastern China. Quanzhou traders began travelling southwards to Xiamen to carry on their businesses while Zhangzhou peasants began traveling northwards to Xiamen in search of job opportunities. It is at this time when a need for a common language arose. The Quanzhou and Zhangzhou varieties are similar in many ways (as can be seen from the common place of Henan Luoyang where they originated), but due to differences in accents, communication can be a problem. Quanzhou businessmen considered their speech to be the prestige accent and considered Zhangzhou's to be a village dialect. Over the centuries, dialect leveling occurred and the two speeches mixed to produce the Amoy dialect. All Latin characters required by Pe̍h-ōe-jī can be represented using Unicode (or the corresponding ISO/IEC 10646: Universal Character Set), using precomposed or combining (diacritics) characters. Prior to June 2004, the vowel akin to but more open than o, written with a dot above right, was not encoded. The usual workaround was to use the (stand-alone; spacing) character Interpunct (U+00B7, ·) or less commonly the combining character dot above (U+0307). As these are far from ideal, since 1997 proposals have been submitted to the ISO/IEC working group in charge of ISO/IEC 10646—namely, ISO/IEC JTC1/SC2/WG2—to encode a new combining character dot above right. This is now officially assigned to U+0358 (see documents N1593, N2507, N2628, N2699, and N2713). Font support is expected to follow. Hokkien, especially Taiwanese, is sometimes written in the Latin script using one of several alphabets. Of these the most popular is Pe̍h-ōe-jī (traditional Chinese: 白話字; simplified Chinese: 白话字; pinyin: Báihuàzì). POJ was developed first by Presbyterian missionaries in China and later by the indigenous Presbyterian Church in Taiwan; use of this alphabet has been actively promoted since the late 19th century. The use of a mixed script of Han characters and Latin letters is also seen, though remains uncommon. Other Latin-based alphabets also exist. In the 1990s, marked by the liberalization of language development and mother tongue movement in Taiwan, Taiwanese Hokkien had undergone a fast pace in its development. In 1993, Taiwan became the first region in the world to implement the teaching of Taiwanese Hokkien in Taiwanese schools. In 2001, the local Taiwanese language program was further extended to all schools in Taiwan, and Taiwanese Hokkien became one of the compulsory local Taiwanese languages to be learned in schools. The mother tongue movement in Taiwan even influenced Xiamen (Amoy) to the point that in 2010, Xiamen also began to implement the teaching of Hokkien dialect in its schools. In 2007, the Ministry of Education in Taiwan also completed the standardization of Chinese characters used for writing Hokkien and developed Tai-lo as the standard Hokkien pronunciation and romanization guide. A number of universities in Taiwan also offer Hokkien degree courses for training Hokkien-fluent talents to work for the Hokkien media industry and education. Taiwan also has its own Hokkien literary and cultural circles whereby Hokkien poets and writers compose poetry or literature in Hokkien on a regular basis. The term Hokkien (福建; hɔk˥˥kɪɛn˨˩) is itself a term not used in Chinese to refer to the dialect, as it simply means Fujian province. In Chinese linguistics, these dialects are known by their classification under the Quanzhang Division (Chinese: 泉漳片; pinyin: Quánzhāng piàn) of Min Nan, which comes from the first characters of the two main Hokkien urban centers Quanzhou and Zhangzhou. The variety is also known by other terms such as the more general Min Nan (traditional Chinese: 閩南語, 閩南話; simplified Chinese: 闽南语, 闽南话; pinyin: Mǐnnányǔ, Mǐnnánhuà; Pe̍h-ōe-jī: Bân-lâm-gí,Bân-lâm-oē) or Southern Min, and Fulaohua (traditional Chinese: 福佬話; simplified Chinese: 福佬话; pinyin: Fúlǎohuà; Pe̍h-ōe-jī: Hō-ló-oē). The term Hokkien (Chinese: 福建話; Pe̍h-ōe-jī: hok-kiàn oē;Tâi-lô:Hok-kiàn-uē), on the other hand, is used commonly in South East Asia to refer to Min-nan dialects. Hokkien dialects are typically written using Chinese characters (漢字, Hàn-jī). However, the written script was and remains adapted to the literary form, which is based on classical Chinese, not the vernacular and spoken form. Furthermore, the character inventory used for Mandarin (standard written Chinese) does not correspond to Hokkien words, and there are a large number of informal characters (替字, thè-jī or thòe-jī; 'substitute characters') which are unique to Hokkien (as is the case with Cantonese). For instance, about 20 to 25% of Taiwanese morphemes lack an appropriate or standard Chinese character. Quite a few words from the variety of Old Chinese spoken in the state of Wu (where the ancestral language of Min and Wu dialect families originated and which was likely influenced by the Chinese spoken in the state of Chu which itself was not founded by Chinese speakers),[citation needed] and later words from Middle Chinese as well, have retained the original meanings in Hokkien, while many of their counterparts in Mandarin Chinese have either fallen out of daily use, have been substituted with other words (some of which are borrowed from other languages while others are new developments), or have developed newer meanings. The same may be said of Hokkien as well, since some lexical meaning evolved in step with Mandarin while others are wholly innovative developments."
The_Times,"In 1922, John Jacob Astor, son of the 1st Viscount Astor, bought The Times from the Northcliffe estate. The paper gained a measure of notoriety in the 1930s with its advocacy of German appeasement; then-editor Geoffrey Dawson was closely allied with those in the government who practised appeasement, most notably Neville Chamberlain. Several suitors appeared, including Robert Maxwell, Tiny Rowland and Lord Rothermere; however, only one buyer was in a position to meet the full Thomson remit, Australian media magnate Rupert Murdoch. Robert Holmes à Court, another Australian magnate had previously tried to buy The Times in 1980. The Thomson Corporation management were struggling to run the business due to the 1979 Energy Crisis and union demands. Management were left with no choice but to find a buyer who was in a position to guarantee the survival of both titles, and also one who had the resources and was committed to funding the introduction of modern printing methods. On 26 July 2012, to coincide with the official start of the London 2012 Olympics and the issuing of a series of souvenir front covers, The Times added the suffix ""of London"" to its masthead. The Times is the first newspaper to have borne that name, lending it to numerous other papers around the world, including The Times of India (founded in 1838), The Straits Times (Singapore) (1845), The New York Times (1851), The Irish Times (1859), Le Temps (France) (1861-1942), the Cape Times (South Africa) (1872), the Los Angeles Times (1881), The Seattle Times (1891), The Manila Times (1898), The Daily Times (Malawi) (1900), El Tiempo (Colombia) (1911), The Canberra Times (1926), and The Times (Malta) (1935). In these countries, the newspaper is often referred to as The London Times or The Times of London. The Times used contributions from significant figures in the fields of politics, science, literature, and the arts to build its reputation. For much of its early life, the profits of The Times were very large and the competition minimal, so it could pay far better than its rivals for information or writers. Beginning in 1814, the paper was printed on the new steam-driven cylinder press developed by Friedrich Koenig. In 1815, The Times had a circulation of 5,000. The Times occasionally makes endorsements for foreign elections. In November 2012, it endorsed a second term for Barack Obama although it also expressed reservations about his foreign policy. The Times faced financial extinction in 1890 under Arthur Fraser Walter, but it was rescued by an energetic editor, Charles Frederic Moberly Bell. During his tenure (1890–1911), The Times became associated with selling the Encyclopædia Britannica using aggressive American marketing methods introduced by Horace Everett Hooper and his advertising executive, Henry Haxton. Due to legal fights between the Britannica's two owners, Hooper and Walter Montgomery Jackson, The Times severed its connection in 1908 and was bought by pioneering newspaper magnate, Alfred Harmsworth, later Lord Northcliffe. In The Wombles, Uncle Bulgaria read The Times and asked for the other Wombles to bring him any copies that they found amongst the litter. The newspaper played a central role in the episode Very Behind the Times (Series 2, Episode 12). Historically, the paper was not overtly pro-Tory or Whig, but has been a long time bastion of the English Establishment and empire. The Times adopted a stance described as ""peculiarly detached"" at the 1945 general election; although it was increasingly critical of the Conservative Party's campaign, it did not advocate a vote for any one party. However, the newspaper reverted to the Tories for the next election five years later. It supported the Conservatives for the subsequent three elections, followed by support for both the Conservatives and the Liberal Party for the next five elections, expressly supporting a Con-Lib coalition in 1974. The paper then backed the Conservatives solidly until 1997, when it declined to make any party endorsement but supported individual (primarily Eurosceptic) candidates. Between 1941 and 1946, the left-wing British historian E.H. Carr was Assistant Editor. Carr was well known for the strongly pro-Soviet tone of his editorials. In December 1944, when fighting broke out in Athens between the Greek Communist ELAS and the British Army, Carr in a Times editorial sided with the Communists, leading Winston Churchill to condemn him and that leader in a speech to the House of Commons. As a result of Carr's editorial, The Times became popularly known during that stage of World War II as the threepenny Daily Worker (the price of the Daily Worker being one penny). The Times is a British daily national newspaper based in London. It began in 1785 under the title The Daily Universal Register and became The Times on 1 January 1788. The Times and its sister paper The Sunday Times (founded in 1821) are published by Times Newspapers, since 1981 a subsidiary of News UK, itself wholly owned by the News Corp group headed by Rupert Murdoch. The Times and The Sunday Times do not share editorial staff, were founded independently and have only had common ownership since 1967. Times Atlases have been produced since 1895. They are currently produced by the Collins Bartholomew imprint of HarperCollins Publishers. The flagship product is The Times Comprehensive Atlas of the World. This 164-page monthly magazine is sold separately from the newspaper of record and is Britain's best-selling travel magazine. The first issue of The Sunday Times Travel Magazine was in 2003, and it includes news, features and insider guides. In 1981, The Times and The Sunday Times were bought from Thomson by Rupert Murdoch's News International. The acquisition followed three weeks of intensive bargaining with the unions by company negotiators, John Collier and Bill O'Neill. The Sunday Times has a significantly higher circulation than The Times, and sometimes outsells The Sunday Telegraph. As of January 2013, The Times has a circulation of 399,339 and The Sunday Times of 885,612. The Times was the first newspaper to send war correspondents to cover particular conflicts. W. H. Russell, the paper's correspondent with the army in the Crimean War, was immensely influential with his dispatches back to England. The Times commissioned the serif typeface Times New Roman, created by Victor Lardent at the English branch of Monotype, in 1931. It was commissioned after Stanley Morison had written an article criticizing The Times for being badly printed and typographically antiquated. The font was supervised by Morison and drawn by Victor Lardent, an artist from the advertising department of The Times. Morison used an older font named Plantin as the basis for his design, but made revisions for legibility and economy of space. Times New Roman made its debut in the issue of 3 October 1932. After one year, the design was released for commercial sale. The Times stayed with Times New Roman for 40 years, but new production techniques and the format change from broadsheet to tabloid in 2004 have caused the newspaper to switch font five times since 1972. However, all the new fonts have been variants of the original New Roman font: Kim Philby, a Soviet double agent, was a correspondent for the newspaper in Spain during the Spanish Civil War of the late 1930s. Philby was admired for his courage in obtaining high-quality reporting from the front lines of the bloody conflict. He later joined MI6 during World War II, was promoted into senior positions after the war ended, then eventually defected to the Soviet Union in 1963. The Times's main supplement, every day, is the times2, featuring various lifestyle columns. It was discontinued on 1 March 2010 but reintroduced on 11 October 2010 after negative feedback. Its regular features include a puzzles section called Mind Games. Its previous incarnation began on 5 September 2005, before which it was called T2 and previously Times 2. Regular features include columns by a different columnist each weekday. There was a column by Marcus du Sautoy each Wednesday, for example. The back pages are devoted to puzzles and contain sudoku, ""Killer Sudoku"", ""KenKen"", word polygon puzzles, and a crossword simpler and more concise than the main ""Times Crossword"". The following year, when Philip Graves, the Constantinople (modern Istanbul) correspondent of The Times, exposed The Protocols as a forgery, The Times retracted the editorial of the previous year. In the James Bond series by Ian Fleming, James Bond, reads The Times. As described by Fleming in From Russia, with Love: ""The Times was the only paper that Bond ever read."" In a 2007 meeting with the House of Lords Select Committee on Communications, which was investigating media ownership and the news, Murdoch stated that the law and the independent board prevented him from exercising editorial control. In May 2008 printing of The Times switched from Wapping to new plants at Broxbourne on the outskirts of London, and Merseyside and Glasgow, enabling the paper to be produced with full colour on every page for the first time. In editorials published on 29 and 31 July 1914, Wickham Steed, the Times's Chief Editor, argued that the British Empire should enter World War I. On 8 May 1920, also under the editorship of Steed, The Times in an editorial endorsed the anti-Semitic fabrication The Protocols of the Learned Elders of Zion as a genuine document, and called Jews the world's greatest danger. In the leader entitled ""The Jewish Peril, a Disturbing Pamphlet: Call for Inquiry"", Steed wrote about The Protocols of the Elders of Zion: The Saturday edition of The Times contains a variety of supplements. These supplements were relaunched in January 2009 as: Sport, Weekend (including travel and lifestyle features), Saturday Review (arts, books, and ideas), The Times Magazine (columns on various topics), and Playlist (an entertainment listings guide). During the 19th century, it was not infrequent for the Foreign Office to approach The Times and ask for continental intelligence, which was often superior to that conveyed by official sources.[citation needed] The Times Magazine features columns touching on various subjects such as celebrities, fashion and beauty, food and drink, homes and gardens or simply writers' anecdotes. Notable contributors include Giles Coren, Food and Drink Writer of the Year in 2005 and Nadiya Hussain, winner of BBC's The Great British Bake Off. The Times and The Sunday Times have had an online presence since March 1999, originally at the-times.co.uk and sunday-times.co.uk, and later at timesonline.co.uk. There are now two websites: thetimes.co.uk is aimed at daily readers, and the thesundaytimes.co.uk site at providing weekly magazine-like content. There are also iPad and Android editions of both newspapers. Since July 2010, News UK has required readers who do not subscribe to the print edition to pay £2 per week to read The Times and The Sunday Times online. The Times Digital Archive (1785–2008) is freely accessible via Gale databases to readers affiliated with subscribing academic, public, and school libraries. Rex Stout's fictional detective Nero Wolfe is described as fond of solving the London Times' crossword puzzle at his New York home, in preference to those of American papers. In other events of the nineteenth century, The Times opposed the repeal of the Corn Laws until the number of demonstrations convinced the editorial board otherwise, and only reluctantly supported aid to victims of the Irish Potato Famine. It enthusiastically supported the Great Reform Bill of 1832, which reduced corruption and increased the electorate from 400,000 people to 800,000 people (still a small minority of the population). During the American Civil War, The Times represented the view of the wealthy classes, favouring the secessionists, but it was not a supporter of slavery. The Times Literary Supplement (TLS) first appeared in 1902 as a supplement to The Times, becoming a separately paid-for weekly literature and society magazine in 1914. The Times and the TLS have continued to be co-owned, and as of 2012 the TLS is also published by News International and cooperates closely with The Times, with its online version hosted on The Times website, and its editorial offices based in Times House, Pennington Street, London. In a 2009 national readership survey The Times was found to have the highest number of ABC1 25–44 readers and the largest numbers of readers in London of any of the ""quality"" papers. The Times, along with the British Film Institute, sponsors the ""The Times"" bfi London Film Festival. It also sponsors the Cheltenham Literature Festival and the Asia House Festival of Asian Literature at Asia House, London. The third John Walter, the founder's grandson, succeeded his father in 1847. The paper continued as more or less independent, but from the 1850s The Times was beginning to suffer from the rise in competition from the penny press, notably The Daily Telegraph and The Morning Post. Thomas Barnes was appointed general editor in 1817. In the same year, the paper's printer James Lawson, died and passed the business onto his son John Joseph Lawson(1802–1852). Under the editorship of Barnes and his successor in 1841, John Thadeus Delane, the influence of The Times rose to great heights, especially in politics and amongst the City of London. Peter Fraser and Edward Sterling were two noted journalists, and gained for The Times the pompous/satirical nickname 'The Thunderer' (from ""We thundered out the other day an article on social and political reform.""). The increased circulation and influence of the paper was based in part to its early adoption of the steam-driven rotary printing press. Distribution via steam trains to rapidly growing concentrations of urban populations helped ensure the profitability of the paper and its growing influence. After 14 years as editor, William Rees-Mogg resigned the post upon completion of the change of ownership. Murdoch began to make his mark on the paper by appointing Harold Evans as his replacement. One of his most important changes was the introduction of new technology and efficiency measures. In March–May 1982, following agreement with print unions, the hot-metal Linotype printing process used to print The Times since the 19th century was phased out and replaced by computer input and photo-composition. This allowed print room staff at The Times and The Sunday Times to be reduced by half. However, direct input of text by journalists (""single stroke"" input) was still not achieved, and this was to remain an interim measure until the Wapping dispute of 1986, when The Times moved from New Printing House Square in Gray's Inn Road (near Fleet Street) to new offices in Wapping. On 6 June 2005, The Times redesigned its Letters page, dropping the practice of printing correspondents' full postal addresses. Published letters were long regarded as one of the paper's key constituents. Author/solicitor David Green of Castle Morris Pembrokeshire has had more letters published on the main letters page than any other known contributor – 158 by 31 January 2008. According to its leading article, ""From Our Own Correspondents"", removal of full postal addresses was in order to fit more letters onto the page. The Times is the originator of the widely used Times Roman typeface, originally developed by Stanley Morison of The Times in collaboration with the Monotype Corporation for its legibility in low-tech printing. In November 2006 The Times began printing headlines in a new font, Times Modern. The Times was printed in broadsheet format for 219 years, but switched to compact size in 2004 in an attempt to appeal more to younger readers and commuters using public transport. The Sunday Times remains a broadsheet. Visits to the websites have decreased by 87% since the paywall was introduced, from 21 million unique users per month to 2.7 million. In April 2009, the timesonline site had a readership of 750,000 readers per day. As of October 2011, there were around 111,000 subscribers to The Times' digital products. In June 1990, The Times ceased its policy of using courtesy titles (""Mr"", ""Mrs"", or ""Miss"" prefixes) for living persons before full names on first reference, but it continues to use them before surnames on subsequent references. The more formal style is now confined to the ""Court and Social"" page, though ""Ms"" is now acceptable in that section, as well as before surnames in news sections. The Times features news for the first half of the paper, the Opinion/Comment section begins after the first news section with world news normally following this. The business pages begin on the centre spread, and are followed by The Register, containing obituaries, Court & Social section, and related material. The sport section is at the end of the main paper. The Times current prices are £1.20 for the daily edition and £1.50 for the Saturday edition. In the dystopian future world of George Orwell's Nineteen Eighty-Four, The Times has been transformed into the organ of the totalitarian ruling party, its editorials—of which several are quoted in the book—reflecting Big Brother's pronouncements. Robert Fisk, seven times British International Journalist of the Year, resigned as foreign correspondent in 1988 over what he saw as ""political censorship"" of his article on the shooting-down of Iran Air Flight 655 in July 1988. He wrote in detail about his reasons for resigning from the paper due to meddling with his stories, and the paper's pro-Israel stance. This makes it the most varied newspaper in terms of political support in British history. Some columnists in The Times are connected to the Conservative Party such as Daniel Finkelstein, Tim Montgomerie, Matthew Parris and Matt Ridley, but there are also columnists connected to the Labour Party such as David Aaronovitch, Phil Collins, Oliver Kamm and Jenni Russell. Though traditionally a moderate newspaper and sometimes a supporter of the Conservative Party, it supported the Labour Party in the 2001 and 2005 general elections. In 2004, according to MORI, the voting intentions of its readership were 40% for the Conservative Party, 29% for the Liberal Democrats, and 26% for Labour. The Times had an average daily circulation of 394,448 in March 2014; in the same period, The Sunday Times had an average daily circulation of 839,077. An American edition of The Times has been published since 6 June 2006. It has been heavily used by scholars and researchers because of its widespread availability in libraries and its detailed index. A complete historical file of the digitized paper is online from Gage Cengage publisher. At the time of Harold Evans' appointment as editor in 1981, The Times had an average daily sale of 282,000 copies in comparison to the 1.4 million daily sales of its traditional rival The Daily Telegraph. By November 2005 The Times sold an average of 691,283 copies per day, the second-highest of any British ""quality"" newspaper (after The Daily Telegraph, which had a circulation of 903,405 copies in the period), and the highest in terms of full-rate sales. By March 2014, average daily circulation of The Times had fallen to 394,448 copies, compared to The Daily Telegraph's 523,048, with the two retaining respectively the second-highest and highest circulations among British ""quality"" newspapers. In contrast The Sun, the highest-selling ""tabloid"" daily newspaper in the United Kingdom, sold an average of 2,069,809 copies in March 2014, and the Daily Mail, the highest-selling ""middle market"" British daily newspaper, sold an average of 1,708,006 copies in the period. In November 2003, News International began producing the newspaper in both broadsheet and tabloid sizes. On 13 September 2004, the weekday broadsheet was withdrawn from sale in Northern Ireland. Since 1 November 2004, the paper has been printed solely in tabloid format. For the 2001 general election The Times declared its support for Tony Blair's Labour government, which was re-elected by a landslide. It supported Labour again in 2005, when Labour achieved a third successive win, though with a reduced majority. For the 2010 general election, however, the newspaper declared its support for the Tories once again; the election ended in the Tories taking the most votes and seats but having to form a coalition with the Liberal Democrats in order to form a government as they had failed to gain an overall majority. The Game is included in the newspaper on Mondays, and details all the weekend's football activity (Premier League and Football League Championship, League One and League Two.) The Scottish edition of The Game also includes results and analysis from Scottish Premier League games. On 3 May 1966 it resumed printing news on the front page - previously the front page featured small advertisements, usually of interest to the moneyed classes in British society. In 1967, members of the Astor family sold the paper to Canadian publishing magnate Roy Thomson. His Thomson Corporation brought it under the same ownership as The Sunday Times to form Times Newspapers Limited. The Times was founded by publisher John Walter on 1 January 1785 as The Daily Universal Register, with Walter in the role of editor. Walter had lost his job by the end of 1784 after the insurance company where he was working went bankrupt because of the complaints of a Jamaican hurricane. Being unemployed, Walter decided to set a new business up. It was in that time when Henry Johnson invented the logography, a new typography that was faster and more precise (three years later, it was proved that it was not as efficient as had been said). Walter bought the logography's patent and to use it, he decided to open a printing house, where he would daily produce an advertising sheet. The first publication of the newspaper The Daily Universal Register in Great Britain was 1 January 1785. Unhappy because people always omitted the word Universal, Ellias changed the title after 940 editions on 1 January 1788 to The Times. In 1803, Walter handed ownership and editorship to his son of the same name. Walter Sr had spent sixteen months in Newgate Prison for libel printed in The Times, but his pioneering efforts to obtain Continental news, especially from France, helped build the paper's reputation among policy makers and financiers."
Nutrition,"The first recorded nutritional experiment with human subjects is found in the Bible's Book of Daniel. Daniel and his friends were captured by the king of Babylon during an invasion of Israel. Selected as court servants, they were to share in the king's fine foods and wine. But they objected, preferring vegetables (pulses) and water in accordance with their Jewish dietary restrictions. The king's chief steward reluctantly agreed to a trial. Daniel and his friends received their diet for 10 days and were then compared to the king's men. Appearing healthier, they were allowed to continue with their diet. Since the Industrial Revolution some two hundred years ago, the food processing industry has invented many technologies that both help keep foods fresh longer and alter the fresh state of food as they appear in nature. Cooling is the primary technology used to maintain freshness, whereas many more technologies have been invented to allow foods to last longer without becoming spoiled. These latter technologies include pasteurisation, autoclavation, drying, salting, and separation of various components, all of which appearing to alter the original nutritional contents of food. Pasteurisation and autoclavation (heating techniques) have no doubt improved the safety of many common foods, preventing epidemics of bacterial infection. But some of the (new) food processing technologies have downfalls as well. Nutrition is taught in schools in many countries. In England and Wales, the Personal and Social Education and Food Technology curricula include nutrition, stressing the importance of a balanced diet and teaching how to read nutrition labels on packaging. In many schools, a Nutrition class will fall within the Family and Consumer Science or Health departments. In some American schools, students are required to take a certain number of FCS or Health related classes. Nutrition is offered at many schools, and, if it is not a class of its own, nutrition is included in other FCS or Health classes such as: Life Skills, Independent Living, Single Survival, Freshmen Connection, Health etc. In many Nutrition classes, students learn about the food groups, the food pyramid, Daily Recommended Allowances, calories, vitamins, minerals, malnutrition, physical activity, healthful food choices, portion sizes, and how to live a healthy life. Cancer is now common in developing countries. According to a study by the International Agency for Research on Cancer, ""In the developing world, cancers of the liver, stomach and esophagus were more common, often linked to consumption of carcinogenic preserved foods, such as smoked or salted food, and parasitic infections that attack organs."" Lung cancer rates are rising rapidly in poorer nations because of increased use of tobacco. Developed countries ""tended to have cancers linked to affluence or a 'Western lifestyle' — cancers of the colon, rectum, breast and prostate — that can be caused by obesity, lack of exercise, diet and age."" The state of obesity clearly contributes to insulin resistance, which in turn can cause type 2 diabetes. Virtually all obese and most type 2 diabetic individuals have marked insulin resistance. Although the association between overweight and insulin resistance is clear, the exact (likely multifarious) causes of insulin resistance remain less clear. It is important to note that it has been demonstrated that appropriate exercise, more regular food intake, and reducing glycemic load (see below) all can reverse insulin resistance in overweight individuals (and thereby lower blood sugar levels in those with type 2 diabetes). Saturated fats (typically from animal sources) have been a staple in many world cultures for millennia. Unsaturated fats (e. g., vegetable oil) are considered healthier, while trans fats are to be avoided. Saturated and some trans fats are typically solid at room temperature (such as butter or lard), while unsaturated fats are typically liquids (such as olive oil or flaxseed oil). Trans fats are very rare in nature, and have been shown to be highly detrimental to human health, but have properties useful in the food processing industry, such as rancidity resistance.[citation needed] Research in the field of nutrition has greatly contributed in finding out the essential facts about how environmental depletion can lead to crucial nutrition-related health problems like contamination, spread of contagious diseases, malnutrition, etc. Moreover, environmental contamination due to discharge of agricultural as well as industrial chemicals like organocholrines, heavy metal, and radionucleotides may adversely affect the human and the ecosystem as a whole. As far as safety of the human health is concerned, then these environmental contaminants can reduce people's nutritional status and health. This could directly or indirectly cause drastic changes in their diet habits. Hence, food-based remedial as well as preventive strategies are essential to address global issues like hunger and malnutrition and to enable the susceptible people to adapt themselves to all these environmental as well as socio-economic alterations. Registered dietitian nutritionists (RDs or RDNs) are health professionals qualified to provide safe, evidence-based dietary advice which includes a review of what is eaten, a thorough review of nutritional health, and a personalized nutritional treatment plan. They also provide preventive and therapeutic programs at work places, schools and similar institutions. Certified Clinical Nutritionists or CCNs, are trained health professionals who also offer dietary advice on the role of nutrition in chronic disease, including possible prevention or remediation by addressing nutritional deficiencies before resorting to drugs. Government regulation especially in terms of licensing, is currently less universal for the CCN than that of RD or RDN. Another advanced Nutrition Professional is a Certified Nutrition Specialist or CNS. These Board Certified Nutritionists typically specialize in obesity and chronic disease. In order to become board certified, potential CNS candidate must pass an examination, much like Registered Dieticians. This exam covers specific domains within the health sphere including; Clinical Intervention and Human Health. Sometimes overlooked during his life, James Lind, a physician in the British navy, performed the first scientific nutrition experiment in 1747. Lind discovered that lime juice saved sailors that had been at sea for years from scurvy, a deadly and painful bleeding disorder. Between 1500 and 1800, an estimated two million sailors had died of scurvy. The discovery was ignored for forty years, after which British sailors became known as ""limeys."" The essential vitamin C within citrus fruits would not be identified by scientists until 1932. The macronutrients are carbohydrates, fats, protein, and water. The macronutrients (excluding fiber and water) provide structural material (amino acids from which proteins are built, and lipids from which cell membranes and some signaling molecules are built) and energy. Some of the structural material can be used to generate energy internally, and in either case it is measured in Joules or kilocalories (often called ""Calories"" and written with a capital C to distinguish them from little 'c' calories). Carbohydrates and proteins provide 17 kJ approximately (4 kcal) of energy per gram, while fats provide 37 kJ (9 kcal) per gram, though the net energy from either depends on such factors as absorption and digestive effort, which vary substantially from instance to instance. Vitamins, minerals, fiber, and water do not provide energy, but are required for other reasons. As cellular metabolism/energy production requires oxygen, potentially damaging (e.g., mutation causing) compounds known as free radicals can form. Most of these are oxidizers (i.e., acceptors of electrons) and some react very strongly. For the continued normal cellular maintenance, growth, and division, these free radicals must be sufficiently neutralized by antioxidant compounds. Recently, some researchers suggested an interesting theory of evolution of dietary antioxidants. Some are produced by the human body with adequate precursors (glutathione, Vitamin C), and those the body cannot produce may only be obtained in the diet via direct sources (Vitamin C in humans, Vitamin A, Vitamin K) or produced by the body from other compounds (Beta-carotene converted to Vitamin A by the body, Vitamin D synthesized from cholesterol by sunlight). Phytochemicals (Section Below) and their subgroup, polyphenols, make up the majority of antioxidants; about 4,000 are known. Different antioxidants are now known to function in a cooperative network. For example, Vitamin C can reactivate free radical-containing glutathione or Vitamin E by accepting the free radical itself. Some antioxidants are more effective than others at neutralizing different free radicals. Some cannot neutralize certain free radicals. Some cannot be present in certain areas of free radical development (Vitamin A is fat-soluble and protects fat areas, Vitamin C is water-soluble and protects those areas). When interacting with a free radical, some antioxidants produce a different free radical compound that is less dangerous or more dangerous than the previous compound. Having a variety of antioxidants allows any byproducts to be safely dealt with by more efficient antioxidants in neutralizing a free radical's butterfly effect. In the US, dietitians are registered (RD) or licensed (LD) with the Commission for Dietetic Registration and the American Dietetic Association, and are only able to use the title ""dietitian,"" as described by the business and professions codes of each respective state, when they have met specific educational and experiential prerequisites and passed a national registration or licensure examination, respectively. In California, registered dietitians must abide by the ""Business and Professions Code of Section 2585-2586.8"". Anyone may call themselves a nutritionist, including unqualified dietitians, as this term is unregulated. Some states, such as the State of Florida, have begun to include the title ""nutritionist"" in state licensure requirements. Most governments provide guidance on nutrition, and some also impose mandatory disclosure/labeling requirements for processed food manufacturers and restaurants to assist consumers in complying with such guidance. Some organizations have begun working with teachers, policymakers, and managed foodservice contractors to mandate improved nutritional content and increased nutritional resources in school cafeterias from primary to university level institutions. Health and nutrition have been proven to have close links with overall educational success. Currently, less than 10% of American college students report that they eat the recommended five servings of fruit and vegetables daily. Better nutrition has been shown to have an impact on both cognitive and spatial memory performance; a study showed those with higher blood sugar levels performed better on certain memory tests. In another study, those who consumed yogurt performed better on thinking tasks when compared to those that consumed caffeine-free diet soda or confections. Nutritional deficiencies have been shown to have a negative effect on learning behavior in mice as far back as 1951. As mentioned, lifestyle- and obesity-related diseases are becoming increasingly prevalent all around the world. There is little doubt that the increasingly widespread application of some modern food processing technologies has contributed to this development. The food processing industry is a major part of modern economy, and as such it is influential in political decisions (e.g., nutritional recommendations, agricultural subsidising). In any known profit-driven economy, health considerations are hardly a priority; effective production of cheap foods with a long shelf-life is more the trend. In general, whole, fresh foods have a relatively short shelf-life and are less profitable to produce and sell than are more processed foods. Thus, the consumer is left with the choice between more expensive, but nutritionally superior, whole, fresh foods, and cheap, usually nutritionally inferior, processed foods. Because processed foods are often cheaper, more convenient (in both purchasing, storage, and preparation), and more available, the consumption of nutritionally inferior foods has been increasing throughout the world along with many nutrition-related health complications. The conversion rate of omega-6 DGLA to AA largely determines the production of the prostaglandins PGE1 and PGE2. Omega-3 EPA prevents AA from being released from membranes, thereby skewing prostaglandin balance away from pro-inflammatory PGE2 (made from AA) toward anti-inflammatory PGE1 (made from DGLA). Moreover, the conversion (desaturation) of DGLA to AA is controlled by the enzyme delta-5-desaturase, which in turn is controlled by hormones such as insulin (up-regulation) and glucagon (down-regulation). The amount and type of carbohydrates consumed, along with some types of amino acid, can influence processes involving insulin, glucagon, and other hormones; therefore, the ratio of omega-3 versus omega-6 has wide effects on general health, and specific effects on immune function and inflammation, and mitosis (i.e., cell division). In the 1500s, Paracelsus was probably the first to criticize Galen publicly. Also in the 16th century, scientist and artist Leonardo da Vinci compared metabolism to a burning candle. Leonardo did not publish his works on this subject, but he was not afraid of thinking for himself and he definitely disagreed with Galen. Ultimately, 16th century works of Andreas Vesalius, sometimes called the father of modern medicine, overturned Galen's ideas. He was followed by piercing thought amalgamated with the era's mysticism and religion sometimes fueled by the mechanics of Newton and Galileo. Jan Baptist van Helmont, who discovered several gases such as carbon dioxide, performed the first quantitative experiment. Robert Boyle advanced chemistry. Sanctorius measured body weight. Physician Herman Boerhaave modeled the digestive process. Physiologist Albrecht von Haller worked out the difference between nerves and muscles. In the US, nutritional standards and recommendations are established jointly by the US Department of Agriculture and US Department of Health and Human Services. Dietary and physical activity guidelines from the USDA are presented in the concept of MyPlate, which superseded the food pyramid, which replaced the Four Food Groups. The Senate committee currently responsible for oversight of the USDA is the Agriculture, Nutrition and Forestry Committee. Committee hearings are often televised on C-SPAN. These statistics point to the complexities surrounding the lack of health/nutrition literacy and reveal the degree to which they are embedded in the social structure and interconnected with other problems. Among these problems are the lack of information about food choices, a lack of understanding of nutritional information and its application to individual circumstances, limited or difficult access to healthful foods, and a range of cultural influences and socioeconomic constraints such as low levels of education and high levels of poverty that decrease opportunities for healthful eating and living. Molecules of carbohydrates and fats consist of carbon, hydrogen, and oxygen atoms. Carbohydrates range from simple monosaccharides (glucose, fructose, galactose) to complex polysaccharides (starch). Fats are triglycerides, made of assorted fatty acid monomers bound to a glycerol backbone. Some fatty acids, but not all, are essential in the diet: they cannot be synthesized in the body. Protein molecules contain nitrogen atoms in addition to carbon, oxygen, and hydrogen. The fundamental components of protein are nitrogen-containing amino acids, some of which are essential in the sense that humans cannot make them internally. Some of the amino acids are convertible (with the expenditure of energy) to glucose and can be used for energy production, just as ordinary glucose, in a process known as gluconeogenesis. By breaking down existing protein, the carbon skeleton of the various amino acids can be metabolized to intermediates in cellular respiration; the remaining ammonia is discarded primarily as urea in urine. This occurs normally only during prolonged starvation. Malnutrition refers to insufficient, excessive, or imbalanced consumption of nutrients by an organism. In developed countries, the diseases of malnutrition are most often associated with nutritional imbalances or excessive consumption. In developing countries, malnutrition is more likely to be caused by poor access to a range of nutritious foods or inadequate knowledge. In Mali the International Crops Research Institute for the Semi-Arid Tropics (ICRISAT) and the Aga Khan Foundation, trained women's groups to make equinut, a healthy and nutritional version of the traditional recipe di-dèguè (comprising peanut paste, honey and millet or rice flour). The aim was to boost nutrition and livelihoods by producing a product that women could make and sell, and which would be accepted by the local community because of its local heritage. In the early 20th century, Carl von Voit and Max Rubner independently measured caloric energy expenditure in different species of animals, applying principles of physics in nutrition. In 1906, Edith G. Willcock and Frederick Hopkins showed that the amino acid tryptophan aids the well-being of mice but it did not assure their growth. In the middle of twelve years of attempts to isolate them, Hopkins said in a 1906 lecture that ""unsuspected dietetic factors,"" other than calories, protein, and minerals, are needed to prevent deficiency diseases. In 1907, Stephen M. Babcock and Edwin B. Hart conducted the single-grain experiment, which took nearly four years to complete. The EFSA panel also determined intakes for different populations. Recommended intake volumes in the elderly are the same as for adults as despite lower energy consumption, the water requirement of this group is increased due to a reduction in renal concentrating capacity. Pregnant and breastfeeding women require additional fluids to stay hydrated. The EFSA panel proposes that pregnant women should consume the same volume of water as non-pregnant women, plus an increase in proportion to the higher energy requirement, equal to 300 mL/day. To compensate for additional fluid output, breastfeeding women require an additional 700 mL/day above the recommended intake values for non-lactating women. It is possible with protein combinations of two incomplete protein sources (e.g., rice and beans) to make a complete protein source, and characteristic combinations are the basis of distinct cultural cooking traditions. However, complementary sources of protein do not need to be eaten at the same meal to be used together by the body. Excess amino acids from protein can be converted into glucose and used for fuel through a process called gluconeogenesis. The amino acids remaining after such conversion are discarded. Most fatty acids are non-essential, meaning the body can produce them as needed, generally from other fatty acids and always by expending energy to do so. However, in humans, at least two fatty acids are essential and must be included in the diet. An appropriate balance of essential fatty acids—omega-3 and omega-6 fatty acids—seems also important for health, although definitive experimental demonstration has been elusive. Both of these ""omega"" long-chain polyunsaturated fatty acids are substrates for a class of eicosanoids known as prostaglandins, which have roles throughout the human body. They are hormones, in some respects. The omega-3 eicosapentaenoic acid (EPA), which can be made in the human body from the omega-3 essential fatty acid alpha-linolenic acid (ALA), or taken in through marine food sources, serves as a building block for series 3 prostaglandins (e.g., weakly inflammatory PGE3). The omega-6 dihomo-gamma-linolenic acid (DGLA) serves as a building block for series 1 prostaglandins (e.g. anti-inflammatory PGE1), whereas arachidonic acid (AA) serves as a building block for series 2 prostaglandins (e.g. pro-inflammatory PGE 2). Both DGLA and AA can be made from the omega-6 linoleic acid (LA) in the human body, or can be taken in directly through food. An appropriately balanced intake of omega-3 and omega-6 partly determines the relative production of different prostaglandins, which is one reason why a balance between omega-3 and omega-6 is believed important for cardiovascular health. In industrialized societies, people typically consume large amounts of processed vegetable oils, which have reduced amounts of the essential fatty acids along with too much of omega-6 fatty acids relative to omega-3 fatty acids. Early recommendations for the quantity of water required for maintenance of good health suggested that 6–8 glasses of water daily is the minimum to maintain proper hydration. However the notion that a person should consume eight glasses of water per day cannot be traced to a credible scientific source. The original water intake recommendation in 1945 by the Food and Nutrition Board of the National Research Council read: ""An ordinary standard for diverse persons is 1 milliliter for each calorie of food. Most of this quantity is contained in prepared foods."" More recent comparisons of well-known recommendations on fluid intake have revealed large discrepancies in the volumes of water we need to consume for good health. Therefore, to help standardize guidelines, recommendations for water consumption are included in two recent European Food Safety Authority (EFSA) documents (2010): (i) Food-based dietary guidelines and (ii) Dietary reference values for water or adequate daily intakes (ADI). These specifications were provided by calculating adequate intakes from measured intakes in populations of individuals with “desirable osmolarity values of urine and desirable water volumes per energy unit consumed.” For healthful hydration, the current EFSA guidelines recommend total water intakes of 2.0 L/day for adult females and 2.5 L/day for adult males. These reference values include water from drinking water, other beverages, and from food. About 80% of our daily water requirement comes from the beverages we drink, with the remaining 20% coming from food. Water content varies depending on the type of food consumed, with fruit and vegetables containing more than cereals, for example. These values are estimated using country-specific food balance sheets published by the Food and Agriculture Organisation of the United Nations. Other guidelines for nutrition also have implications for the beverages we consume for healthy hydration- for example, the World Health Organization (WHO) recommend that added sugars should represent no more than 10% of total energy intake. Heart disease, cancer, obesity, and diabetes are commonly called ""Western"" diseases because these maladies were once rarely seen in developing countries. An international study in China found some regions had virtually no cancer or heart disease, while in other areas they reflected ""up to a 100-fold increase"" coincident with shifts from diets that were found to be entirely plant-based to heavily animal-based, respectively. In contrast, diseases of affluence like cancer and heart disease are common throughout the developed world, including the United States. Adjusted for age and exercise, large regional clusters of people in China rarely suffered from these ""Western"" diseases possibly because their diets are rich in vegetables, fruits, and whole grains, and have little dairy and meat products. Some studies show these to be, in high quantities, possible causes of some cancers. There are arguments for and against this controversial issue. Animal tissue consists of elements and compounds ingested, digested, absorbed, and circulated through the bloodstream to feed the cells of the body. Except in the unborn fetus, the digestive system is the first system involved[vague]. Digestive juices break chemical bonds in ingested molecules, and modify their conformations and energy states. Though some molecules are absorbed into the bloodstream unchanged, digestive processes release them from the matrix of foods. Unabsorbed matter, along with some waste products of metabolism, is eliminated from the body in the feces. An example of a state initiative to promote nutrition literacy is Smart Bodies, a public-private partnership between the state’s largest university system and largest health insurer, Louisiana State Agricultural Center and Blue Cross and Blue Shield of Louisiana Foundation. Launched in 2005, this program promotes lifelong healthful eating patterns and physically active lifestyles for children and their families. It is an interactive educational program designed to help prevent childhood obesity through classroom activities that teach children healthful eating habits and physical exercise. Obesity can unfavourably alter hormonal and metabolic status via resistance to the hormone leptin, and a vicious cycle may occur in which insulin/leptin resistance and obesity aggravate one another. The vicious cycle is putatively fuelled by continuously high insulin/leptin stimulation and fat storage, as a result of high intake of strongly insulin/leptin stimulating foods and energy. Both insulin and leptin normally function as satiety signals to the hypothalamus in the brain; however, insulin/leptin resistance may reduce this signal and therefore allow continued overfeeding despite large body fat stores. In addition, reduced leptin signalling to the brain may reduce leptin's normal effect to maintain an appropriately high metabolic rate. Modern separation techniques such as milling, centrifugation, and pressing have enabled concentration of particular components of food, yielding flour, oils, juices, and so on, and even separate fatty acids, amino acids, vitamins, and minerals. Inevitably, such large-scale concentration changes the nutritional content of food, saving certain nutrients while removing others. Heating techniques may also reduce food's content of many heat-labile nutrients such as certain vitamins and phytochemicals, and possibly other yet-to-be-discovered substances. Because of reduced nutritional value, processed foods are often 'enriched' or 'fortified' with some of the most critical nutrients (usually certain vitamins) that were lost during processing. Nonetheless, processed foods tend to have an inferior nutritional profile compared to whole, fresh foods, regarding content of both sugar and high GI starches, potassium/sodium, vitamins, fiber, and of intact, unoxidized (essential) fatty acids. In addition, processed foods often contain potentially harmful substances such as oxidized fats and trans fatty acids. As with the minerals discussed above, some vitamins are recognized as organic essential nutrients, necessary in the diet for good health. (Vitamin D is the exception: it can be synthesized in the skin, in the presence of UVB radiation.) Certain vitamin-like compounds that are recommended in the diet, such as carnitine, are thought useful for survival and health, but these are not ""essential"" dietary nutrients because the human body has some capacity to produce them from other compounds. Moreover, thousands of different phytochemicals have recently been discovered in food (particularly in fresh vegetables), which may have desirable properties including antioxidant activity (see below); however, experimental demonstration has been suggestive but inconclusive. Other essential nutrients that are not classified as vitamins include essential amino acids (see above), choline, essential fatty acids (see above), and the minerals discussed in the preceding section. Traditionally, simple carbohydrates are believed to be absorbed quickly, and therefore to raise blood-glucose levels more rapidly than complex carbohydrates. This, however, is not accurate. Some simple carbohydrates (e.g., fructose) follow different metabolic pathways (e.g., fructolysis) that result in only a partial catabolism to glucose, while, in essence, many complex carbohydrates may be digested at the same rate as simple carbohydrates. Glucose stimulates the production of insulin through food entering the bloodstream, which is grasped by the beta cells in the pancreas. One mustn't overlook the doctrines of Galen: In use from his life in the 1st century AD until the 17th century, it was heresy to disagree with him for 1500 years. Galen was physician to gladiators in Pergamon, and in Rome, physician to Marcus Aurelius and the three emperors who succeeded him. Most of Galen's teachings were gathered and enhanced in the late 11th century by Benedictine monks at the School of Salerno in Regimen sanitatis Salernitanum, which still had users in the 17th century. Galen believed in the bodily humours of Hippocrates, and he taught that pneuma is the source of life. Four elements (earth, air, fire and water) combine into ""complexion"", which combines into states (the four temperaments: sanguine, phlegmatic, choleric, and melancholic). The states are made up of pairs of attributes (hot and moist, cold and moist, hot and dry, and cold and dry), which are made of four humours: blood, phlegm, green (or yellow) bile, and black bile (the bodily form of the elements). Galen thought that for a person to have gout, kidney stones, or arthritis was scandalous, which Gratzer likens to Samuel Butler's Erehwon (1872) where sickness is a crime. In 1816, François Magendie discovered that dogs fed only carbohydrates (sugar), fat (olive oil), and water died evidently of starvation, but dogs also fed protein survived, identifying protein as an essential dietary component. William Prout in 1827 was the first person to divide foods into carbohydrates, fat, and protein. During the 19th century, Jean-Baptiste Dumas and Justus von Liebig quarrelled over their shared belief that animals get their protein directly from plants (animal and plant protein are the same and that humans do not create organic compounds). With a reputation as the leading organic chemist of his day but with no credentials in animal physiology, Liebig grew rich making food extracts like beef bouillon and infant formula that were later found to be of questionable nutritious value. In the 1860s, Claude Bernard discovered that body fat can be synthesized from carbohydrate and protein, showing that the energy in blood glucose can be stored as fat or as glycogen. A molecule of dietary fat typically consists of several fatty acids (containing long chains of carbon and hydrogen atoms), bonded to a glycerol. They are typically found as triglycerides (three fatty acids attached to one glycerol backbone). Fats may be classified as saturated or unsaturated depending on the detailed structure of the fatty acids involved. Saturated fats have all of the carbon atoms in their fatty acid chains bonded to hydrogen atoms, whereas unsaturated fats have some of these carbon atoms double-bonded, so their molecules have relatively fewer hydrogen atoms than a saturated fatty acid of the same length. Unsaturated fats may be further classified as monounsaturated (one double-bond) or polyunsaturated (many double-bonds). Furthermore, depending on the location of the double-bond in the fatty acid chain, unsaturated fatty acids are classified as omega-3 or omega-6 fatty acids. Trans fats are a type of unsaturated fat with trans-isomer bonds; these are rare in nature and in foods from natural sources; they are typically created in an industrial process called (partial) hydrogenation. There are nine kilocalories in each gram of fat. Fatty acids such as conjugated linoleic acid, catalpic acid, eleostearic acid and punicic acid, in addition to providing energy, represent potent immune modulatory molecules. Proteins are structural materials in much of the animal body (e.g. muscles, skin, and hair). They also form the enzymes that control chemical reactions throughout the body. Each protein molecule is composed of amino acids, which are characterized by inclusion of nitrogen and sometimes sulphur (these components are responsible for the distinctive smell of burning protein, such as the keratin in hair). The body requires amino acids to produce new proteins (protein retention) and to replace damaged proteins (maintenance). As there is no protein or amino acid storage provision, amino acids must be present in the diet. Excess amino acids are discarded, typically in the urine. For all animals, some amino acids are essential (an animal cannot produce them internally) and some are non-essential (the animal can produce them from other nitrogen-containing compounds). About twenty amino acids are found in the human body, and about ten of these are essential and, therefore, must be included in the diet. A diet that contains adequate amounts of amino acids (especially those that are essential) is particularly important in some situations: during early development and maturation, pregnancy, lactation, or injury (a burn, for instance). A complete protein source contains all the essential amino acids; an incomplete protein source lacks one or more of the essential amino acids. Another study examining the health and nutrition literacy status of residents of the lower Mississippi Delta found that 52 percent of participants had a high likelihood of limited literacy skills. While a precise comparison between the NAAL and Delta studies is difficult, primarily because of methodological differences, Zoellner et al. suggest that health literacy rates in the Mississippi Delta region are different from the U.S. general population and that they help establish the scope of the problem of health literacy among adults in the Delta region. For example, only 12 percent of study participants identified the My Pyramid graphic two years after it had been launched by the USDA. The study also found significant relationships between nutrition literacy and income level and nutrition literacy and educational attainment further delineating priorities for the region. Animal intestines contain a large population of gut flora. In humans, the four dominant phyla are Firmicutes, Bacteroidetes, Actinobacteria, and Proteobacteria. They are essential to digestion and are also affected by food that is consumed. Bacteria in the gut perform many important functions for humans, including breaking down and aiding in the absorption of otherwise indigestible food; stimulating cell growth; repressing the growth of harmful bacteria, training the immune system to respond only to pathogens; producing vitamin B12; and defending against some infectious diseases. Dietary minerals are inorganic chemical elements required by living organisms, other than the four elements carbon, hydrogen, nitrogen, and oxygen that are present in nearly all organic molecules. The term ""mineral"" is archaic, since the intent is to describe simply the less common elements in the diet. Some are heavier than the four just mentioned, including several metals, which often occur as ions in the body. Some dietitians recommend that these be supplied from foods in which they occur naturally, or at least as complex compounds, or sometimes even from natural inorganic sources (such as calcium carbonate from ground oyster shells). Some minerals are absorbed much more readily in the ionic forms found in such sources. On the other hand, minerals are often artificially added to the diet as supplements; the most famous is likely iodine in iodized salt which prevents goiter. A dramatic example of the effect of food processing on a population's health is the history of epidemics of beri-beri in people subsisting on polished rice. Removing the outer layer of rice by polishing it removes with it the essential vitamin thiamine, causing beri-beri. Another example is the development of scurvy among infants in the late 19th century in the United States. It turned out that the vast majority of sufferers were being fed milk that had been heat-treated (as suggested by Pasteur) to control bacterial disease. Pasteurisation was effective against bacteria, but it destroyed the vitamin C. The United Healthcare/Pacificare nutrition guideline recommends a whole plant food diet, and recommends using protein only as a condiment with meals. A National Geographic cover article from November 2005, entitled The Secrets of Living Longer, also recommends a whole plant food diet. The article is a lifestyle survey of three populations, Sardinians, Okinawans, and Adventists, who generally display longevity and ""suffer a fraction of the diseases that commonly kill people in other parts of the developed world, and enjoy more healthy years of life."" In sum, they offer three sets of 'best practices' to emulate. The rest is up to you. In common with all three groups is to ""Eat fruits, vegetables, and whole grains."" According to Walter Gratzer, the study of nutrition probably began during the 6th century BC. In China, the concept of Qi developed, a spirit or ""wind"" similar to what Western Europeans later called pneuma. Food was classified into ""hot"" (for example, meats, blood, ginger, and hot spices) and ""cold"" (green vegetables) in China, India, Malaya, and Persia. Humours developed perhaps first in China alongside qi. Ho the Physician concluded that diseases are caused by deficiencies of elements (Wu Xing: fire, water, earth, wood, and metal), and he classified diseases as well as prescribed diets. About the same time in Italy, Alcmaeon of Croton (a Greek) wrote of the importance of equilibrium between what goes in and what goes out, and warned that imbalance would result disease marked by obesity or emaciation. The list of nutrients that people are known to require is, in the words of Marion Nestle, ""almost certainly incomplete"". As of 2014, nutrients are thought to be of two types: macro-nutrients which are needed in relatively large amounts, and micronutrients which are needed in smaller quantities. A type of carbohydrate, dietary fiber, i.e. non-digestible material such as cellulose, is required, for both mechanical and biochemical reasons, although the exact reasons remain unclear. Other micronutrients include antioxidants and phytochemicals, which are said to influence (or protect) some body systems. Their necessity is not as well established as in the case of, for instance, vitamins. At the time of this entry, we were not able to identify any specific nutrition literacy studies in the U.S. at a national level. However, the findings of the 2003 National Assessment of Adult Literacy (NAAL) provide a basis upon which to frame the nutrition literacy problem in the U.S. NAAL introduced the first ever measure of ""the degree to which individuals have the capacity to obtain, process and understand basic health information and services needed to make appropriate health decisions"" – an objective of Healthy People 2010 and of which nutrition literacy might be considered an important subset. On a scale of below basic, basic, intermediate and proficient, NAAL found 13 percent of adult Americans have proficient health literacy, 44% have intermediate literacy, 29 percent have basic literacy and 14 percent have below basic health literacy. The study found that health literacy increases with education and people living below the level of poverty have lower health literacy than those above it. There is a debate about how and to what extent different dietary factors— such as intake of processed carbohydrates, total protein, fat, and carbohydrate intake, intake of saturated and trans fatty acids, and low intake of vitamins/minerals—contribute to the development of insulin and leptin resistance. In any case, analogous to the way modern man-made pollution may possess the potential to overwhelm the environment's ability to maintain homeostasis, the recent explosive introduction of high glycemic index and processed foods into the human diet may possess the potential to overwhelm the body's ability to maintain homeostasis and health (as evidenced by the metabolic syndrome epidemic). In 1913, Elmer McCollum discovered the first vitamins, fat-soluble vitamin A, and water-soluble vitamin B (in 1915; now known to be a complex of several water-soluble vitamins) and named vitamin C as the then-unknown substance preventing scurvy. Lafayette Mendel and Thomas Osborne also performed pioneering work on vitamins A and B. In 1919, Sir Edward Mellanby incorrectly identified rickets as a vitamin A deficiency because he could cure it in dogs with cod liver oil. In 1922, McCollum destroyed the vitamin A in cod liver oil, but found that it still cured rickets. Also in 1922, H.M. Evans and L.S. Bishop discover vitamin E as essential for rat pregnancy, originally calling it ""food factor X"" until 1925. Studies of nutritional status must take into account the state of the body before and after experiments, as well as the chemical composition of the whole diet and of all material excreted and eliminated from the body (in urine and feces). Comparing the food to the waste can help determine the specific compounds and elements absorbed and metabolized in the body. The effects of nutrients may only be discernible over an extended period, during which all food and waste must be analyzed. The number of variables involved in such experiments is high, making nutritional studies time-consuming and expensive, which explains why the science of animal nutrition is still slowly evolving. Carnivore and herbivore diets are contrasting, with basic nitrogen and carbon proportions vary for their particular foods. ""The nitrogen content of plant tissues averages about 2%, while in fungi, animals, and bacteria it averages about 5% to 10%."" Many herbivores rely on bacterial fermentation to create digestible nutrients from indigestible plant cellulose, while obligate carnivores must eat animal meats to obtain certain vitamins or nutrients their bodies cannot otherwise synthesize. All animals' diets must provide sufficient amounts of the basic building blocks they need, up to the point where their particular biology can synthesize the rest. Animal tissue contains chemical compounds, such as water, carbohydrates (sugar, starch, and fiber), amino acids (in proteins), fatty acids (in lipids), and nucleic acids (DNA and RNA). These compounds in turn consist of elements such as carbon, hydrogen, oxygen, nitrogen, phosphorus, calcium, iron, zinc, magnesium, manganese, and so on. All of these chemical compounds and elements occur in various forms and combinations (e.g. hormones, vitamins, phospholipids, hydroxyapatite). Dietary fiber is a carbohydrate that is incompletely absorbed in humans and in some animals. Like all carbohydrates, when it is metabolized it can produce four Calories (kilocalories) of energy per gram. However, in most circumstances it accounts for less than that because of its limited absorption and digestibility. Dietary fiber consists mainly of cellulose, a large carbohydrate polymer which is indigestible as humans do not have the required enzymes to disassemble it. There are two subcategories: soluble and insoluble fiber. Whole grains, fruits (especially plums, prunes, and figs), and vegetables are good sources of dietary fiber. There are many health benefits of a high-fiber diet. Dietary fiber helps reduce the chance of gastrointestinal problems such as constipation and diarrhea by increasing the weight and size of stool and softening it. Insoluble fiber, found in whole wheat flour, nuts and vegetables, especially stimulates peristalsis – the rhythmic muscular contractions of the intestines, which move digesta along the digestive tract. Soluble fiber, found in oats, peas, beans, and many fruits, dissolves in water in the intestinal tract to produce a gel that slows the movement of food through the intestines. This may help lower blood glucose levels because it can slow the absorption of sugar. Additionally, fiber, perhaps especially that from whole grains, is thought to possibly help lessen insulin spikes, and therefore reduce the risk of type 2 diabetes. The link between increased fiber consumption and a decreased risk of colorectal cancer is still uncertain. Plants uptake essential elements from the soil through their roots and from the air (consisting of mainly nitrogen and oxygen) through their leaves. Green plants obtain their carbohydrate supply from the carbon dioxide in the air by the process of photosynthesis. Carbon and oxygen are absorbed from the air, while other nutrients are absorbed from the soil. Nutrient uptake in the soil is achieved by cation exchange, wherein root hairs pump hydrogen ions (H+) into the soil through proton pumps. These hydrogen ions displace cations attached to negatively charged soil particles so that the cations are available for uptake by the root. In the leaves, stomata open to take in carbon dioxide and expel oxygen. The carbon dioxide molecules are used as the carbon source in photosynthesis. Excess water intake, without replenishment of sodium and potassium salts, leads to hyponatremia, which can further lead to water intoxication at more dangerous levels. A well-publicized case occurred in 2007, when Jennifer Strange died while participating in a water-drinking contest. More usually, the condition occurs in long-distance endurance events (such as marathon or triathlon competition and training) and causes gradual mental dulling, headache, drowsiness, weakness, and confusion; extreme cases may result in coma, convulsions, and death. The primary damage comes from swelling of the brain, caused by increased osmosis as blood salinity decreases. Effective fluid replacement techniques include water aid stations during running/cycling races, trainers providing water during team games, such as soccer, and devices such as Camel Baks, which can provide water for a person without making it too hard to drink the water. The relatively recent increased consumption of sugar has been linked to the rise of some afflictions such as diabetes, obesity, and more recently heart disease. Increased consumption of sugar has been tied to these three, among others. Obesity levels have more than doubled in the last 30 years among adults, going from 15% to 35% in the United States. Obesity and diet also happen to be high risk factors for diabetes. In the same time span that obesity doubled, diabetes numbers quadrupled in America. Increased weight, especially in the form of belly fat, and high sugar intake are also high risk factors for heart disease. Both sugar intake and fatty tissue increase the probability of elevated LDL cholesterol in the bloodstream. Elevated amounts of Low-density lipoprotein (LDL) cholesterol, is the primary factor in heart disease. In order to avoid all the dangers of sugar, moderate consumption is paramount. In the early 1880s, Kanehiro Takaki observed that Japanese sailors (whose diets consisted almost entirely of white rice) developed beriberi (or endemic neuritis, a disease causing heart problems and paralysis), but British sailors and Japanese naval officers did not. Adding various types of vegetables and meats to the diets of Japanese sailors prevented the disease, (not because of the increased protein as Takaki supposed but because it introduced a few parts per million of thiamine to the diet, later understood as a cure). Nutritionism is the view that excessive reliance on food science and the study of nutrition can lead to poor nutrition and to ill health. It was originally credited to Gyorgy Scrinis, and was popularized by Michael Pollan. Since nutrients are invisible, policy makers rely on nutrition experts to advise on food choices. Because science has an incomplete understanding of how food affects the human body, Pollan argues, nutritionism can be blamed for many of the health problems relating to diet in the Western World today. Several lines of evidence indicate lifestyle-induced hyperinsulinemia and reduced insulin function (i.e., insulin resistance) as a decisive factor in many disease states. For example, hyperinsulinemia and insulin resistance are strongly linked to chronic inflammation, which in turn is strongly linked to a variety of adverse developments such as arterial microinjuries and clot formation (i.e., heart disease) and exaggerated cell division (i.e., cancer). Hyperinsulinemia and insulin resistance (the so-called metabolic syndrome) are characterized by a combination of abdominal obesity, elevated blood sugar, elevated blood pressure, elevated blood triglycerides, and reduced HDL cholesterol. The negative impact of hyperinsulinemia on prostaglandin PGE1/PGE2 balance may be significant. In 1896, Eugen Baumann observed iodine in thyroid glands. In 1897, Christiaan Eijkman worked with natives of Java, who also suffered from beriberi. Eijkman observed that chickens fed the native diet of white rice developed the symptoms of beriberi but remained healthy when fed unprocessed brown rice with the outer bran intact. Eijkman cured the natives by feeding them brown rice, discovering that food can cure disease. Over two decades later, nutritionists learned that the outer rice bran contains vitamin B1, also known as thiamine."
Annelid,"However, the lifecycles of most living polychaetes, which are almost all marine animals, are unknown, and only about 25% of the 300+ species whose lifecycles are known follow this pattern. About 14% use a similar external fertilization but produce yolk-rich eggs, which reduce the time the larva needs to spend among the plankton, or eggs from which miniature adults emerge rather than larvae. The rest care for the fertilized eggs until they hatch – some by producing jelly-covered masses of eggs which they tend, some by attaching the eggs to their bodies and a few species by keeping the eggs within their bodies until they hatch. These species use a variety of methods for sperm transfer; for example, in some the females collect sperm released into the water, while in others the males have a penis that inject sperm into the female. There is no guarantee that this is a representative sample of polychaetes' reproductive patterns, and it simply reflects scientists' current knowledge. In 2007 Torsten Struck and colleagues compared 3 genes in 81 taxa, of which 9 were outgroups, in other words not considered closely related to annelids but included to give an indication of where the organisms under study are placed on the larger tree of life. For a cross-check the study used an analysis of 11 genes (including the original 3) in 10 taxa. This analysis agreed that clitellates, pogonophorans and echiurans were on various branches of the polychaete family tree. It also concluded that the classification of polychaetes into Scolecida, Canalipalpata and Aciculata was useless, as the members of these alleged groups were scattered all over the family tree derived from comparing the 81 taxa. In addition, it also placed sipunculans, generally regarded at the time as a separate phylum, on another branch of the polychaete tree, and concluded that leeches were a sub-group of oligochaetes rather than their sister-group among the clitellates. Rouse accepted the analyses based on molecular phylogenetics, and their main conclusions are now the scientific consensus, although the details of the annelid family tree remain uncertain. However, leeches and their closest relatives have a body structure that is very uniform within the group but significantly different from that of other annelids, including other members of the Clitellata. In leeches there are no septa, the connective tissue layer of the body wall is so thick that it occupies much of the body, and the two coelomata are widely separated and run the length of the body. They function as the main blood vessels, although they are side-by-side rather than upper and lower. However, they are lined with mesothelium, like the coelomata and unlike the blood vessels of other annelids. Leeches generally use suckers at their front and rear ends to move like inchworms. The anus is on the upper surface of the pygidium. Although many species can reproduce asexually and use similar mechanisms to regenerate after severe injuries, sexual reproduction is the normal method in species whose reproduction has been studied. The minority of living polychaetes whose reproduction and lifecycles are known produce trochophore larvae, that live as plankton and then sink and metamorphose into miniature adults. Oligochaetes are full hermaphrodites and produce a ring-like cocoon around their bodies, in which the eggs and hatchlings are nourished until they are ready to emerge. Some polychaetes breed only once in their lives, while others breed almost continuously or through several breeding seasons. While most polychaetes remain of one sex all their lives, a significant percentage of species are full hermaphrodites or change sex during their lives. Most polychaetes whose reproduction has been studied lack permanent gonads, and it is uncertain how they produce ova and sperm. In a few species the rear of the body splits off and becomes a separate individual that lives just long enough to swim to a suitable environment, usually near the surface, and spawn. Accounts of the use of leeches for the medically dubious practise of blood-letting have come from China around 30 AD, India around 200 AD, ancient Rome around 50 AD and later throughout Europe. In the 19th century medical demand for leeches was so high that some areas' stocks were exhausted and other regions imposed restrictions or bans on exports, and Hirudo medicinalis is treated as an endangered species by both IUCN and CITES. More recently leeches have been used to assist in microsurgery, and their saliva has provided anti-inflammatory compounds and several important anticoagulants, one of which also prevents tumors from spreading. The earliest good evidence for oligochaetes occurs in the Tertiary period, which began 65 million years ago, and it has been suggested that these animals evolved around the same time as flowering plants in the early Cretaceous, from 130 to 90 million years ago. A trace fossil consisting of a convoluted burrow partly filled with small fecal pellets may be evidence that earthworms were present in the early Triassic period from 251 to 245 million years ago. Body fossils going back to the mid Ordovician, from 472 to 461 million years ago, have been tentatively classified as oligochaetes, but these identifications are uncertain and some have been disputed. The annelids are bilaterally symmetrical, triploblastic, coelomate, invertebrate organisms. They also have parapodia for locomotion. Most textbooks still use the traditional division into polychaetes (almost all marine), oligochaetes (which include earthworms) and leech-like species. Cladistic research since 1997 has radically changed this scheme, viewing leeches as a sub-group of oligochaetes and oligochaetes as a sub-group of polychaetes. In addition, the Pogonophora, Echiura and Sipuncula, previously regarded as separate phyla, are now regarded as sub-groups of polychaetes. Annelids are considered members of the Lophotrochozoa, a ""super-phylum"" of protostomes that also includes molluscs, brachiopods, flatworms and nemerteans. It is thought that annelids were originally animals with two separate sexes, which released ova and sperm into the water via their nephridia. The fertilized eggs develop into trochophore larvae, which live as plankton. Later they sink to the sea-floor and metamorphose into miniature adults: the part of the trochophore between the apical tuft and the prototroch becomes the prostomium (head); a small area round the trochophore's anus becomes the pygidium (tail-piece); a narrow band immediately in front of that becomes the growth zone that produces new segments; and the rest of the trochophore becomes the peristomium (the segment that contains the mouth). The fluid in the coelomata contains coelomocyte cells that defend the animals against parasites and infections. In some species coelomocytes may also contain a respiratory pigment – red hemoglobin in some species, green chlorocruorin in others (dissolved in the plasma) – and provide oxygen transport within their segments. Respiratory pigment is also dissolved in the blood plasma. Species with well-developed septa generally also have blood vessels running all long their bodies above and below the gut, the upper one carrying blood forwards while the lower one carries it backwards. Networks of capillaries in the body wall and around the gut transfer blood between the main blood vessels and to parts of the segment that need oxygen and nutrients. Both of the major vessels, especially the upper one, can pump blood by contracting. In some annelids the forward end of the upper blood vessel is enlarged with muscles to form a heart, while in the forward ends of many earthworms some of the vessels that connect the upper and lower main vessels function as hearts. Species with poorly developed or no septa generally have no blood vessels and rely on the circulation within the coelom for delivering nutrients and oxygen. No single feature distinguishes Annelids from other invertebrate phyla, but they have a distinctive combination of features. Their bodies are long, with segments that are divided externally by shallow ring-like constrictions called annuli and internally by septa (""partitions"") at the same points, although in some species the septa are incomplete and in a few cases missing. Most of the segments contain the same sets of organs, although sharing a common gut, circulatory system and nervous system makes them inter-dependent. Their bodies are covered by a cuticle (outer covering) that does not contain cells but is secreted by cells in the skin underneath, is made of tough but flexible collagen and does not molt – on the other hand arthropods' cuticles are made of the more rigid α-chitin, and molt until the arthropods reach their full size. Most annelids have closed circulatory systems, where the blood makes its entire circuit via blood vessels. Most annelids have a pair of coelomata (body cavities) in each segment, separated from other segments by septa and from each other by vertical mesenteries. Each septum forms a sandwich with connective tissue in the middle and mesothelium (membrane that serves as a lining) from the preceding and following segments on either side. Each mesentery is similar except that the mesothelium is the lining of each of the pair of coelomata, and the blood vessels and, in polychaetes, the main nerve cords are embedded in it. The mesothelium is made of modified epitheliomuscular cells; in other words, their bodies form part of the epithelium but their bases extend to form muscle fibers in the body wall. The mesothelium may also form radial and circular muscles on the septa, and circular muscles around the blood vessels and gut. Parts of the mesothelium, especially on the outside of the gut, may also form chloragogen cells that perform similar functions to the livers of vertebrates: producing and storing glycogen and fat; producing the oxygen-carrier hemoglobin; breaking down proteins; and turning nitrogenous waste products into ammonia and urea to be excreted. Charles Darwin's book The Formation of Vegetable Mould through the Action of Worms (1881) presented the first scientific analysis of earthworms' contributions to soil fertility. Some burrow while others live entirely on the surface, generally in moist leaf litter. The burrowers loosen the soil so that oxygen and water can penetrate it, and both surface and burrowing worms help to produce soil by mixing organic and mineral matter, by accelerating the decomposition of organic matter and thus making it more quickly available to other organisms, and by concentrating minerals and converting them to forms that plants can use more easily. Earthworms are also important prey for birds ranging in size from robins to storks, and for mammals ranging from shrews to badgers, and in some cases conserving earthworms may be essential for conserving endangered birds. The setae (""hairs"") of annelids project out from the epidermis to provide traction and other capabilities. The simplest are unjointed and form paired bundles near the top and bottom of each side of each segment. The parapodia (""limbs"") of annelids that have them often bear more complex chetae at their tips – for example jointed, comb-like or hooked. Chetae are made of moderately flexible β-chitin and are formed by follicles, each of which has a chetoblast (""hair-forming"") cell at the bottom and muscles that can extend or retract the cheta. The chetoblasts produce chetae by forming microvilli, fine hair-like extensions that increase the area available for secreting the cheta. When the cheta is complete, the microvilli withdraw into the chetoblast, leaving parallel tunnels that run almost the full length of the cheta. Hence annelids' chetae are structurally different from the setae (""bristles"") of arthropods, which are made of the more rigid α-chitin, have a single internal cavity, and are mounted on flexible joints in shallow pits in the cuticle. Annelids are members of the protostomes, one of the two major superphyla of bilaterian animals – the other is the deuterostomes, which includes vertebrates. Within the protostomes, annelids used to be grouped with arthropods under the super-group Articulata (""jointed animals""), as segmentation is obvious in most members of both phyla. However, the genes that drive segmentation in arthropods do not appear to do the same in annelids. Arthropods and annelids both have close relatives that are unsegmented. It is at least as easy to assume that they evolved segmented bodies independently as it is to assume that the ancestral protostome or bilaterian was segmented and that segmentation disappeared in many descendant phyla. The current view is that annelids are grouped with molluscs, brachiopods and several other phyla that have lophophores (fan-like feeding structures) and/or trochophore larvae as members of Lophotrochozoa. Bryzoa may be the most basal phylum (the one that first became distinctive) within the Lophotrochozoa, and the relationships between the other members are not yet known. Arthropods are now regarded as members of the Ecdysozoa (""animals that molt""), along with some phyla that are unsegmented. Earthworms are Oligochaetes that support terrestrial food chains both as prey and in some regions are important in aeration and enriching of soil. The burrowing of marine polychaetes, which may constitute up to a third of all species in near-shore environments, encourages the development of ecosystems by enabling water and oxygen to penetrate the sea floor. In addition to improving soil fertility, annelids serve humans as food and as bait. Scientists observe annelids to monitor the quality of marine and fresh water. Although blood-letting is no longer in favor with doctors, some leech species are regarded as endangered species because they have been over-harvested for this purpose in the last few centuries. Ragworms' jaws are now being studied by engineers as they offer an exceptional combination of lightness and strength. The ""Lophotrochozoa"" hypothesis is also supported by the fact that many phyla within this group, including annelids, molluscs, nemerteans and flatworms, follow a similar pattern in the fertilized egg's development. When their cells divide after the 4-cell stage, descendants of these 4 cells form a spiral pattern. In these phyla the ""fates"" of the embryo's cells, in other words the roles their descendants will play in the adult animal, are the same and can be predicted from a very early stage. Hence this development pattern is often described as ""spiral determinate cleavage"". Terrestrial annelids can be invasive in some situations. In the glaciated areas of North America, for example, almost all native earthworms are thought to have been killed by the glaciers and the worms currently found in those areas are all introduced from other areas, primarily from Europe, and, more recently, from Asia. Northern hardwood forests are especially negatively impacted by invasive worms through the loss of leaf duff, soil fertility, changes in soil chemistry and the loss of ecological diversity. Especially of concern is Amynthas agrestis and at least one state (Wisconsin) has listed it as a prohibited species. Annelids' cuticles are made of collagen fibers, usually in layers that spiral in alternating directions so that the fibers cross each other. These are secreted by the one-cell deep epidermis (outermost skin layer). A few marine annelids that live in tubes lack cuticles, but their tubes have a similar structure, and mucus-secreting glands in the epidermis protect their skins. Under the epidermis is the dermis, which is made of connective tissue, in other words a combination of cells and non-cellular materials such as collagen. Below this are two layers of muscles, which develop from the lining of the coelom (body cavity): circular muscles make a segment longer and slimmer when they contract, while under them are longitudinal muscles, usually four distinct strips, whose contractions make the segment shorter and fatter. Some annelids also have oblique internal muscles that connect the underside of the body to each side. In addition to re-writing the classification of annelids and 3 previously independent phyla, the molecular phylogenetics analyses undermine the emphasis that decades of previous writings placed on the importance of segmentation in the classification of invertebrates. Polychaetes, which these analyses found to be the parent group, have completely segmented bodies, while polychaetes' echiurans and sipunculan offshoots are not segmented and pogonophores are segmented only in the rear parts of their bodies. It now seems that segmentation can appear and disappear much more easily in the course of evolution than was previously thought. The 2007 study also noted that the ladder-like nervous system, which is associated with segmentation, is less universal previously thought in both annelids and arthropods.[n 2] The brain generally forms a ring round the pharynx (throat), consisting of a pair of ganglia (local control centers) above and in front of the pharynx, linked by nerve cords either side of the pharynx to another pair of ganglia just below and behind it. The brains of polychaetes are generally in the prostomium, while those of clitellates are in the peristomium or sometimes the first segment behind the peristomium. In some very mobile and active polychaetes the brain is enlarged and more complex, with visible hindbrain, midbrain and forebrain sections. The rest of the central nervous system is generally ""ladder-like"", consisting of a pair of nerve cords that run through the bottom part of the body and have in each segment paired ganglia linked by a transverse connection. From each segmental ganglion a branching system of local nerves runs into the body wall and then encircles the body. However, in most polychaetes the two main nerve cords are fused, and in the tube-dwelling genus Owenia the single nerve chord has no ganglia and is located in the epidermis. Many annelids move by peristalsis (waves of contraction and expansion that sweep along the body), or flex the body while using parapodia to crawl or swim. In these animals the septa enable the circular and longitudinal muscles to change the shape of individual segments, by making each segment a separate fluid-filled ""balloon"". However, the septa are often incomplete in annelids that are semi-sessile or that do not move by peristalsis or by movements of parapodia – for example some move by whipping movements of the body, some small marine species move by means of cilia (fine muscle-powered hairs) and some burrowers turn their pharynges (throats) inside out to penetrate the sea-floor and drag themselves into it. Since annelids are soft-bodied, their fossils are rare – mostly jaws and the mineralized tubes that some of the species secreted. Although some late Ediacaran fossils may represent annelids, the oldest known fossil that is identified with confidence comes from about 518 million years ago in the early Cambrian period. Fossils of most modern mobile polychaete groups appeared by the end of the Carboniferous, about 299 million years ago. Palaeontologists disagree about whether some body fossils from the mid Ordovician, about 472 to 461 million years ago, are the remains of oligochaetes, and the earliest indisputable fossils of the group appear in the Tertiary period, which began 65 million years ago. Annelids with blood vessels use metanephridia to remove soluble waste products, while those without use protonephridia. Both of these systems use a two-stage filtration process, in which fluid and waste products are first extracted and these are filtered again to re-absorb any re-usable materials while dumping toxic and spent materials as urine. The difference is that protonephridia combine both filtration stages in the same organ, while metanephridia perform only the second filtration and rely on other mechanisms for the first – in annelids special filter cells in the walls of the blood vessels let fluids and other small molecules pass into the coelomic fluid, where it circulates to the metanephridia. In annelids the points at which fluid enters the protonephridia or metanephridia are on the forward side of a septum while the second-stage filter and the nephridiopore (exit opening in the body wall) are in the following segment. As a result, the hindmost segment (before the growth zone and pygidium) has no structure that extracts its wastes, as there is no following segment to filter and discharge them, while the first segment contains an extraction structure that passes wastes to the second, but does not contain the structures that re-filter and discharge urine. Earthworms make a significant contribution to soil fertility. The rear end of the Palolo worm, a marine polychaete that tunnels through coral, detaches in order to spawn at the surface, and the people of Samoa regard these spawning modules as a delicacy. Anglers sometimes find that worms are more effective bait than artificial flies, and worms can be kept for several days in a tin lined with damp moss. Ragworms are commercially important as bait and as food sources for aquaculture, and there have been proposals to farm them in order to reduce over-fishing of their natural populations. Some marine polychaetes' predation on molluscs causes serious losses to fishery and aquaculture operations. The gut is generally an almost straight tube supported by the mesenteries (vertical partitions within segments), and ends with the anus on the underside of the pygidium. However, in members of the tube-dwelling family Siboglinidae the gut is blocked by a swollen lining that houses symbiotic bacteria, which can make up 15% of the worms' total weight. The bacteria convert inorganic matter – such as hydrogen sulfide and carbon dioxide from hydrothermal vents, or methane from seeps – to organic matter that feeds themselves and their hosts, while the worms extend their palps into the gas flows to absorb the gases needed by the bacteria. Most of an annelid's body consists of segments that are practically identical, having the same sets of internal organs and external chaetae (Greek χαιτη, meaning ""hair"") and, in some species, appendages. However, the frontmost and rearmost sections are not regarded as true segments as they do not contain the standard sets of organs and do not develop in the same way as the true segments. The frontmost section, called the prostomium (Greek προ- meaning ""in front of"" and στομα meaning ""mouth"") contains the brain and sense organs, while the rearmost, called the pygidium (Greek πυγιδιον, meaning ""little tail"") or periproct contains the anus, generally on the underside. The first section behind the prostomium, called the peristomium (Greek περι- meaning ""around"" and στομα meaning ""mouth""), is regarded by some zoologists as not a true segment, but in some polychaetes the peristomium has chetae and appendages like those of other segments. Traditionally the annelids have been divided into two major groups, the polychaetes and clitellates. In turn the clitellates were divided into oligochaetes, which include earthworms, and hirudinomorphs, whose best-known members are leeches. For many years there was no clear arrangement of the approximately 80 polychaete families into higher-level groups. In 1997 Greg Rouse and Kristian Fauchald attempted a ""first heuristic step in terms of bringing polychaete systematics to an acceptable level of rigour"", based on anatomical structures, and divided polychaetes into: As in arthropods, each muscle fiber (cell) is controlled by more than one neuron, and the speed and power of the fiber's contractions depends on the combined effects of all its neurons. Vertebrates have a different system, in which one neuron controls a group of muscle fibers. Most annelids' longitudinal nerve trunks include giant axons (the output signal lines of nerve cells). Their large diameter decreases their resistance, which allows them to transmit signals exceptionally fast. This enables these worms to withdraw rapidly from danger by shortening their bodies. Experiments have shown that cutting the giant axons prevents this escape response but does not affect normal movement. The basic annelid form consists of multiple segments. Each segment has the same sets of organs and, in most polychaetes, has a pair of parapodia that many species use for locomotion. Septa separate the segments of many species, but are poorly defined or absent in others, and Echiura and Sipuncula show no obvious signs of segmentation. In species with well-developed septa, the blood circulates entirely within blood vessels, and the vessels in segments near the front ends of these species are often built up with muscles that act as hearts. The septa of such species also enable them to change the shapes of individual segments, which facilitates movement by peristalsis (""ripples"" that pass along the body) or by undulations that improve the effectiveness of the parapodia. In species with incomplete septa or none, the blood circulates through the main body cavity without any kind of pump, and there is a wide range of locomotory techniques – some burrowing species turn their pharynges inside out to drag themselves through the sediment. Feeding structures in the mouth region vary widely, and have little correlation with the animals' diets. Many polychaetes have a muscular pharynx that can be everted (turned inside out to extend it). In these animals the foremost few segments often lack septa so that, when the muscles in these segments contract, the sharp increase in fluid pressure from all these segments everts the pharynx very quickly. Two families, the Eunicidae and Phyllodocidae, have evolved jaws, which can be used for seizing prey, biting off pieces of vegetation, or grasping dead and decaying matter. On the other hand, some predatory polychaetes have neither jaws nor eversible pharynges. Selective deposit feeders generally live in tubes on the sea-floor and use palps to find food particles in the sediment and then wipe them into their mouths. Filter feeders use ""crowns"" of palps covered in cilia that wash food particles towards their mouths. Non-selective deposit feeders ingest soil or marine sediments via mouths that are generally unspecialized. Some clitellates have sticky pads in the roofs of their mouths, and some of these can evert the pads to capture prey. Leeches often have an eversible proboscis, or a muscular pharynx with two or three teeth. The sensors are primarily single cells that detect light, chemicals, pressure waves and contact, and are present on the head, appendages (if any) and other parts of the body. Nuchal (""on the neck"") organs are paired, ciliated structures found only in polychaetes, and are thought to be chemosensors. Some polychaetes also have various combinations of ocelli (""little eyes"") that detect the direction from which light is coming and camera eyes or compound eyes that can probably form images. The compound eyes probably evolved independently of arthropods' eyes. Some tube-worms use ocelli widely spread over their bodies to detect the shadows of fish, so that they can quickly withdraw into their tubes. Some burrowing and tube-dwelling polychaetes have statocysts (tilt and balance sensors) that tell them which way is down. A few polychaete genera have on the undersides of their heads palps that are used both in feeding and as ""feelers"", and some of these also have antennae that are structurally similar but probably are used mainly as ""feelers"". Nearly all polychaetes have parapodia that function as limbs, while other major annelid groups lack them. Parapodia are unjointed paired extensions of the body wall, and their muscles are derived from the circular muscles of the body. They are often supported internally by one or more large, thick chetae. The parapodia of burrowing and tube-dwelling polychaetes are often just ridges whose tips bear hooked chetae. In active crawlers and swimmers the parapodia are often divided into large upper and lower paddles on a very short trunk, and the paddles are generally fringed with chetae and sometimes with cirri (fused bundles of cilia) and gills. Most mature clitellates (the group that includes earthworms and leeches) are full hermaphrodites, although in a few leech species younger adults function as males and become female at maturity. All have well-developed gonads, and all copulate. Earthworms store their partners' sperm in spermathecae (""sperm stores"") and then the clitellum produces a cocoon that collects ova from the ovaries and then sperm from the spermathecae. Fertilization and development of earthworm eggs takes place in the cocoon. Leeches' eggs are fertilized in the ovaries, and then transferred to the cocoon. In all clitellates the cocoon also either produces yolk when the eggs are fertilized or nutrients while they are developing. All clitellates hatch as miniature adults rather than larvae. Since annelids are soft-bodied, their fossils are rare. Polychaetes' fossil record consists mainly of the jaws that some species had and the mineralized tubes that some secreted. Some Ediacaran fossils such as Dickinsonia in some ways resemble polychaetes, but the similarities are too vague for these fossils to be classified with confidence. The small shelly fossil Cloudina, from 549 to 542 million years ago, has been classified by some authors as an annelid, but by others as a cnidarian (i.e. in the phylum to which jellyfish and sea anemones belong). Until 2008 the earliest fossils widely accepted as annelids were the polychaetes Canadia and Burgessochaeta, both from Canada's Burgess Shale, formed about 505 million years ago in the early Cambrian. Myoscolex, found in Australia and a little older than the Burgess Shale, was possibly an annelid. However, it lacks some typical annelid features and has features which are not usually found in annelids and some of which are associated with other phyla. Then Simon Conway Morris and John Peel reported Phragmochaeta from Sirius Passet, about 518 million years old, and concluded that it was the oldest annelid known to date. There has been vigorous debate about whether the Burgess Shale fossil Wiwaxia was a mollusc or an annelid. Polychaetes diversified in the early Ordovician, about 488 to 474 million years ago. It is not until the early Ordovician that the first annelid jaws are found, thus the crown-group cannot have appeared before this date and probably appeared somewhat later. By the end of the Carboniferous, about 299 million years ago, fossils of most of the modern mobile polychaete groups had appeared. Many fossil tubes look like those made by modern sessile polychaetes  , but the first tubes clearly produced by polychaetes date from the Jurassic, less than 199 million years ago."
Oklahoma_City,"Downtown Oklahoma City, which has 7,600 residents, is currently seeing an influx of new private investment and large scale public works projects, which have helped to resuscitate a central business district left almost deserted by the Oil Bust of the early 1980s. The centerpiece of downtown is the newly renovated Crystal Bridge and Myriad Botanical Gardens, one of the few elements of the Pei Plan to be completed. In the next few years a massive new central park will link the gardens near the CBD and the new convention center to be built just south of it to the North Canadian River, as part of a massive works project known as Core to Shore; the new park is part of MAPS3, a collection of civic projects funded by a 1-cent temporary (seven-year) sales tax increase. Oklahoma City is the annual host of the Big 12 Baseball Tournament, the World Cup of Softball, and the annual NCAA Women's College World Series. The city has held the 2005 NCAA Men's Basketball First and Second round and hosted the Big 12 Men's and Women's Basketball Tournaments in 2007 and 2009. The major universities in the area – University of Oklahoma, Oklahoma City University, and Oklahoma State University – often schedule major basketball games and other sporting events at Chesapeake Energy Arena and Chickasaw Bricktown Ballpark, although most home games are played at their campus stadiums. Oklahoma City, lying in the Great Plains region, features one of the largest livestock markets in the world. Oil, natural gas, petroleum products and related industries are the largest sector of the local economy. The city is situated in the middle of an active oil field and oil derricks dot the capitol grounds. The federal government employs large numbers of workers at Tinker Air Force Base and the United States Department of Transportation's Mike Monroney Aeronautical Center (these two sites house several offices of the Federal Aviation Administration and the Transportation Department's Enterprise Service Center, respectively). The state of Oklahoma hosts a highly competitive high school football culture, with many teams in the Oklahoma City metropolitan area. The Oklahoma Secondary School Activities Association (OSSAA) organizes high school football into eight distinct classes based on the size of school enrollment. Beginning with the largest, the classes are: 6A, 5A, 4A, 3A, 2A, A, B, and C. Class 6A is broken into two divisions. Oklahoma City area schools in this division include: Edmond North, Mustang, Moore, Yukon, Edmond Memorial, Edmond Santa Fe, Norman North, Westmoore, Southmoore, Putnam City North, Norman, Putnam City, Putnam City West, U.S. Grant, Midwest City. The Oklahoma City National Memorial in the northern part of Oklahoma City's downtown was created as the inscription on its eastern gate of the Memorial reads, ""to honor the victims, survivors, rescuers, and all who were changed forever on April 19, 1995""; the memorial was built on the land formerly occupied by the Alfred P. Murrah Federal Building complex prior to its 1995 bombing. The outdoor Symbolic Memorial can be visited 24 hours a day for free, and the adjacent Memorial Museum, located in the former Journal Record building damaged by the bombing, can be entered for a small fee. The site is also home to the National Memorial Institute for the Prevention of Terrorism, a non-partisan, nonprofit think tank devoted to the prevention of terrorism. The ""Core-to-Shore"" project was created to relocate I-40 one mile (1.6 km) south and replace it with a boulevard to create a landscaped entrance to the city. This also allows the central portion of the city to expand south and connect with the shore of the Oklahoma River. Several elements of ""Core to Shore"" were included in the MAPS 3 proposal approved by voters in late 2009. Chesapeake Energy Arena in downtown is the principal multipurpose arena in the city which hosts concerts, NHL exhibition games, and many of the city's pro sports teams. In 2008, the Oklahoma City Thunder became the major tenant. Located nearby in Bricktown, the Chickasaw Bricktown Ballpark is the home to the city's baseball team, the Dodgers. ""The Brick"", as it is locally known, is considered one of the finest minor league parks in the nation.[citation needed] One of the more prominent landmarks downtown is the Crystal Bridge at the Myriad Botanical Gardens, a large downtown urban park. Designed by I. M. Pei, the Crystal Bridge is a tropical conservatory in the area. The park has an amphitheater, known as the Water Stage. In 2007, following a renovation of the stage, Oklahoma Shakespeare in the Park relocated to the Myriad Gardens. The Myriad Gardens will undergo a massive renovation in conjunction with the recently built Devon Tower directly north of it. Other professional sports clubs in Oklahoma City include the Oklahoma City Dodgers, the Triple-A affiliate of the Los Angeles Dodgers, the Oklahoma City Energy FC of the United Soccer League, and the Crusaders of Oklahoma Rugby Football Club USA Rugby. The Oklahoma City Thunder has been regarded by sports analysts as one of the elite franchises of the NBA's Western Conference and that of a media darling as the future of the league. Oklahoma City has earned Northwest Division titles every year since 2009 and has consistently improved its win record to 59-wins in 2014. The Thunder is led by first year head coach Billy Donovan and is anchored by several NBA superstars, including perennial All-Star point guard Russell Westbrook, 2014 MVP and four-time NBA scoring champion Kevin Durant, and Defensive Player of the Year nominee and shot-blocker Serge Ibaka. Oklahoma City is the capital and largest city of the state of Oklahoma. The county seat of Oklahoma County, the city ranks 27th among United States cities in population. The population grew following the 2010 Census, with the population estimated to have increased to 620,602 as of July 2014. As of 2014, the Oklahoma City metropolitan area had a population of 1,322,429, and the Oklahoma City-Shawnee Combined Statistical Area had a population of 1,459,758 (Chamber of Commerce) residents, making it Oklahoma's largest metropolitan area. Oklahoma City's city limits extend into Canadian, Cleveland, and Pottawatomie counties, though much of those areas outside of the core Oklahoma County area are suburban or rural (watershed). The city ranks as the eighth-largest city in the United States by land area (including consolidated city-counties; it is the largest city in the United States by land area whose government is not consolidated with that of a county or borough). There are numerous community and international newspapers locally that cater to the city's ethnic mosaic; such as The Black Chronicle, headquartered in the Eastside, the OK VIETIMES and Oklahoma Chinese Times, located in Asia District, and various Hispanic community publications. The Campus is the student newspaper at Oklahoma City University. Gay publications include The Gayly Oklahoman. The city is home to several colleges and universities. Oklahoma City University, formerly known as Epworth University, was founded by the United Methodist Church on September 1, 1904 and is renowned for its performing arts, science, mass communications, business, law, and athletic programs. OCU has its main campus in the north-central section of the city, near the city's chinatown area. OCU Law is located in the Midtown district near downtown, in the old Central High School building. The Midwest Regional Medical Center located in the suburb of Midwest City; other major hospitals in the city include the Oklahoma Heart Hospital and the Mercy Health Center. There are 347 physicians for every 100,000 people in the city. Oklahoma City and the surrounding metropolitan area are home to a number of health care facilities and specialty hospitals. In Oklahoma City's MidTown district near downtown resides the state's oldest and largest single site hospital, St. Anthony Hospital and Physicians Medical Center. The Museum of Osteology houses more than 300 real animal skeletons. Focusing on the form and function of the skeletal system, this 7,000 sq ft (650 m2) museum displays hundreds of skulls and skeletons from all corners of the world. Exhibits include adaptation, locomotion, classification and diversity of the vertebrate kingdom. The Museum of Osteology is the only one of its kind in America. With regards to Mexican drug cartels, Oklahoma City has traditionally been the territory of the notorious Juárez Cartel, but the Sinaloa Cartel has been reported as trying to establish a foothold in Oklahoma City. There are many rival gangs in Oklahoma City, one whose headquarters has been established in the city, the Southside Locos, traditionally known as Sureños. Major state expressways through the city include Lake Hefner Parkway (SH-74), the Kilpatrick Turnpike, Airport Road (SH-152), and Broadway Extension (US-77) which continues from I-235 connecting Central Oklahoma City to Edmond. Lake Hefner Parkway runs through northwest Oklahoma City, while Airport Road runs through southwest Oklahoma City and leads to Will Rogers World Airport. The Kilpatrick Turnpike loops around north and west Oklahoma City. Although technically not a university, the FAA's Mike Monroney Aeronautical Center has many aspects of an institution of higher learning. Its FAA Academy is accredited by the North Central Association of Colleges and Schools. Its Civil Aerospace Medical Institute (CAMI) has a medical education division responsible for aeromedical education in general as well as the education of aviation medical examiners in the U.S. and 93 other countries. In addition, The National Academy of Science offers Research Associateship Programs for fellowship and other grants for CAMI research. Walking trails line Lake Hefner and Lake Overholser in the northwest part of the city and downtown at the canal and the Oklahoma River. The majority of the east shore area is taken up by parks and trails, including a new leashless dog park and the postwar-era Stars and Stripes Park. Lake Stanley Draper is the city's largest and most remote lake. On December 2009, Oklahoma City voters passed MAPS 3, the $777 million (7-year 1-cent tax) initiative, which will include funding (appx $130M) for an estimated 5-to-6-mile (8.0 to 9.7 km) modern streetcar in downtown Oklahoma City and the establishment of a transit hub. It is believed the streetcar would begin construction in 2014 and be in operation around 2017. Oklahoma City has experienced significant population increases since the late 1990s. In May 2014, the U.S. Census announced Oklahoma City had an estimated population of 620,602 in 2014 and that it had grown 5.3 percent between April 2010 and June 2013. Since the official Census in 2000, Oklahoma City had grown 21 percent (a 114,470 raw increase) according to the Bureau estimates. The 2014 estimate of 620,602 is the largest population Oklahoma City has ever recorded. It is the first city in the state to record a population greater than 600,000 residents and the largest municipal population of the Great Plains region (OK, KS, NE, SD, ND). According to the United States Census Bureau, the city has a total area of 620.34 square miles (1,606.7 km2), of which, 601.11 square miles (1,556.9 km2) of it is land and 19.23 square miles (49.8 km2) of it is water. The total area is 3.09 percent water. Oklahoma City also has its share of very brutal crimes, particularly in the 1970s. The worst of which occurred in 1978, when six employees of a Sirloin Stockade restaurant on the city's south side were murdered execution-style in the restaurant's freezer. An intensive investigation followed, and the three individuals involved, who also killed three others in Purcell, Oklahoma, were identified. One, Harold Stafford, died in a motorcycle accident in Tulsa not long after the restaurant murders. Another, Verna Stafford, was sentenced to life without parole after being granted a new trial after she had previously been sentenced to death. Roger Dale Stafford, considered the mastermind of the murder spree, was executed by lethal injection at the Oklahoma State Penitentiary in 1995. Oklahoma City is an integral point on the United States Interstate Network, with three major interstate highways – Interstate 35, Interstate 40, and Interstate 44 – bisecting the city. Interstate 240 connects Interstate 40 and Interstate 44 in south Oklahoma City, while Interstate 235 spurs from Interstate 44 in north-central Oklahoma City into downtown. Oklahoma City was settled on April 22, 1889, when the area known as the ""Unassigned Lands"" was opened for settlement in an event known as ""The Land Run"". Some 10,000 homesteaders settled the area that would become the capital of Oklahoma. The town grew quickly; the population doubled between 1890 and 1900. Early leaders of the development of the city included Anton Classen, John Shartel, Henry Overholser and James W. Maney. Patience Latting was elected Mayor of Oklahoma City in 1971, becoming the city's first female mayor. Latting was also the first woman to serve as mayor of a U.S. city with over 350,000 residents. METRO Transit is the city's public transit company. The main transfer terminal is located downtown at NW 5th Street and Hudson Avenue. METRO Transit maintains limited coverage of the city's main street grid using a hub-and-spoke system from the main terminal, making many journeys impractical due to the rather small number of bus routes offered and that most trips require a transfer downtown. The city has recognized that transit as a major issue for the rapidly growing and urbanizing city and has initiated several studies in recent times to improve upon the existing bus system starting with a plan known as the Fixed Guideway Study. This study identified several potential commuter transit routes from the suburbs into downtown OKC as well as feeder-line bus and/or rail routes throughout the city. Other major sporting events include Thoroughbred and Quarter horse racing circuits at Remington Park and numerous horse shows and equine events that take place at the state fairgrounds each year. There are numerous golf courses and country clubs spread around the city. The City of Oklahoma City has operated under a council-manager form of city government since 1927. Mick Cornett serves as Mayor, having first been elected in 2004, and re-elected in 2006, 2010, and 2014. Eight councilpersons represent each of the eight wards of Oklahoma City. City Manager Jim Couch was appointed in late 2000. Couch previously served as assistant city manager, Metropolitan Area Projects Plan (MAPS) director and utilities director prior to his service as city manager. INTEGRIS Health owns several hospitals, including INTEGRIS Baptist Medical Center, the INTEGRIS Cancer Institute of Oklahoma, and the INTEGRIS Southwest Medical Center. INTEGRIS Health operates hospitals, rehabilitation centers, physician clinics, mental health facilities, independent living centers and home health agencies located throughout much of Oklahoma. INTEGRIS Baptist Medical Center was named in U.S. News & World Report's 2012 list of Best Hospitals. INTEGRIS Baptist Medical Center ranks high-performing in the following categories: Cardiology and Heart Surgery; Diabetes and Endocrinology; Ear, Nose and Throat; Gastroenterology; Geriatrics; Nephrology; Orthopedics; Pulmonology and Urology. The American Banjo Museum located in the Bricktown Entertainment district is dedicated to preserving and promoting the music and heritage of America's native musical instrument – the banjo. With a collection valued at $3.5 million it is truly a national treasure. An interpretive exhibits tells the evolution of the banjo from its humble roots in American slavery, to bluegrass, to folk and world music. With 19.48 inches of rainfall, May 2015 was by far Oklahoma City's record-wettest month since record keeping began in 1890. Across Oklahoma and Texas generally, there was record flooding in the latter part of the month  In the 2000 Census Oklahoma City's age composition was 25.5% under the age of 18, 10.7% from 18 to 24, 30.8% from 25 to 44, 21.5% from 45 to 64, and 11.5% who were 65 years of age or older. The median age was 34 years. For every 100 females there were 95.6 males. For every 100 females age 18 and over, there were 92.7 males. In the aftermath of Hurricane Katrina, the NBA's New Orleans Hornets (now the New Orleans Pelicans) temporarily relocated to the Ford Center, playing the majority of its home games there during the 2005–06 and 2006–07 seasons. The team became the first NBA franchise to play regular-season games in the state of Oklahoma.[citation needed] The team was known as the New Orleans/Oklahoma City Hornets while playing in Oklahoma City. The team ultimately returned to New Orleans full-time for the 2007–08 season. The Hornets played their final home game in Oklahoma City during the exhibition season on October 9, 2007 against the Houston Rockets. The Science Museum Oklahoma (formerly Kirkpatrick Science and Air Space Museum at Omniplex) houses exhibits on science, aviation, and an IMAX theater. The museum formerly housed the International Photography Hall of Fame (IPHF) that exhibits photographs and artifacts from a large collection of cameras and other artifacts preserving the history of photography. IPHF honors those who have made significant contributions to the art and/or science of photography and relocated to St. Louis, Missouri in 2013. Oklahoma City has a humid subtropical climate (Köppen: Cfa), with frequent variations in weather daily and seasonally, except during the consistently hot and humid summer months. Prolonged and severe droughts (sometimes leading to wildfires in the vicinity) as well as very heavy rainfall leading to flash flooding and flooding occur with some regularity. Consistent winds, usually from the south or south-southeast during the summer, help temper the hotter weather. Consistent northerly winds during the winter can intensify cold periods. Severe ice storms and snowstorms happen sporadically during the winter. The city is roughly bisected by the North Canadian River (recently renamed the Oklahoma River inside city limits). The North Canadian once had sufficient flow to flood every year, wreaking destruction on surrounding areas, including the central business district and the original Oklahoma City Zoo. In the 1940s, a dam was built on the river to manage the flood control and reduced its level. In the 1990s, as part of the citywide revitalization project known as MAPS, the city built a series of low-water dams, returning water to the portion of the river flowing near downtown. The city has three large lakes: Lake Hefner and Lake Overholser, in the northwestern quarter of the city; and the largest, Lake Stanley Draper, in the sparsely populated far southeast portion of the city. The Oklahoma City Thunder of the National Basketball Association (NBA) has called Oklahoma City home since the 2008–09 season, when owner Clayton Bennett relocated the franchise from Seattle, Washington. The Thunder plays home games at the Chesapeake Energy Arena in downtown Oklahoma City, known affectionately in the national media as 'the Peake' and 'Loud City'. The Thunder is known by several nicknames, including ""OKC Thunder"" and simply ""OKC"", and its mascot is Rumble the Bison. By the time Oklahoma was admitted to the Union in 1907, Oklahoma City had surpassed Guthrie, the territorial capital, as the population center and commercial hub of the new state. Soon after, the capital was moved from Guthrie to Oklahoma City. Oklahoma City was a major stop on Route 66 during the early part of the 20th century; it was prominently mentioned in Bobby Troup's 1946 jazz classic, ""(Get Your Kicks on) Route 66"", later made famous by artist Nat King Cole. Oklahoma City is the principal city of the eight-county Oklahoma City Metropolitan Statistical Area in Central Oklahoma and is the state's largest urbanized area. Based on population rank, the metropolitan area was the 42nd largest in the nation as of 2012. The Oklahoma City Police Department, has a uniformed force of 1,169 officers and 300+ civilian employees. The Department has a central police station and five substations covering 2,500 police reporting districts that average 1/4 square mile in size. According to the Oklahoma City Chamber of Commerce, the metropolitan area's economic output grew by 33 percent between 2001 and 2005 due chiefly to economic diversification. Its gross metropolitan product was $43.1 billion in 2005 and grew to $61.1 billion in 2009. In 2008, Forbes magazine named Oklahoma City the most ""recession proof city in America"". The magazine reported that the city had falling unemployment, one of the strongest housing markets in the country and solid growth in energy, agriculture and manufacturing. However, during the early 1980s, Oklahoma City had one of the worst job and housing markets due to the bankruptcy of Penn Square Bank in 1982 and then the post-1985 crash in oil prices.[citation needed] In April 2005, the Oklahoma City Skate Park at Wiley Post Park was renamed the Mat Hoffman Action Sports Park to recognize Mat Hoffman, an Oklahoma City area resident and businessman that was instrumental in the design of the skate park and is a 10-time BMX World Vert champion. In March 2009, the Mat Hoffman Action Sports Park was named by the National Geographic Society Travel Guide as one of the ""Ten Best."" Oklahoma City lies in the Sandstone Hills region of Oklahoma, known for hills of 250 to 400 feet (120 m) and two species of oak: blackjack oak (Quercus marilandica) and post oak (Q. stellata). The northeastern part of the city and its eastern suburbs fall into an ecological region known as the Cross Timbers. The major U.S. broadcast television networks have affiliates in the Oklahoma City market (ranked 41st for television by Nielsen and 48th for radio by Arbitron, covering a 34-county area serving the central, northern-central and west-central sections Oklahoma); including NBC affiliate KFOR-TV (channel 4), ABC affiliate KOCO-TV (channel 5), CBS affiliate KWTV-DT (channel 9, the flagship of locally based Griffin Communications), PBS station KETA-TV (channel 13, the flagship of the state-run OETA member network), Fox affiliate KOKH-TV (channel 25), CW affiliate KOCB (channel 34), independent station KAUT-TV (channel 43), MyNetworkTV affiliate KSBI-TV (channel 52), and Ion Television owned-and-operated station KOPX-TV (channel 62). The market is also home to several religious stations including TBN owned-and-operated station KTBO-TV (channel 14) and Norman-based Daystar owned-and-operated station KOCM (channel 46). The Oklahoma School of Science and Mathematics, a school for some of the state's most gifted math and science pupils, is also located in Oklahoma City. Other theaters include Lyric Theatre, Jewel Box Theatre, Kirkpatrick Auditorium, the Poteet Theatre, the Oklahoma City Community College Bruce Owen Theater and the 488-seat Petree Recital Hall, at the Oklahoma City University campus. The university also opened the Wanda L Bass School of Music and auditorium in April 2006. Oklahoma City was home to several pioneers in radio and television broadcasting. Oklahoma City's WKY Radio was the first radio station transmitting west of the Mississippi River and the third radio station in the United States. WKY received its federal license in 1921 and has continually broadcast under the same call letters since 1922. In 1928, WKY was purchased by E.K. Gaylord's Oklahoma Publishing Company and affiliated with the NBC Red Network; in 1949, WKY-TV (channel 4) went on the air and later became the first independently owned television station in the U.S. to broadcast in color. In mid-2002, WKY radio was purchased outright by Citadel Broadcasting, who was bought out by Cumulus Broadcasting in 2011. The Gaylord family earlier sold WKY-TV in 1976, which has gone through a succession of owners (what is now KFOR-TV is currently owned by Tribune Broadcasting as of December 2013). In 1993, the city passed a massive redevelopment package known as the Metropolitan Area Projects (MAPS), intended to rebuild the city's core with civic projects to establish more activities and life to downtown. The city added a new baseball park; central library; renovations to the civic center, convention center and fairgrounds; and a water canal in the Bricktown entertainment district. Water taxis transport passengers within the district, adding color and activity along the canal. MAPS has become one of the most successful public-private partnerships undertaken in the U.S., exceeding $3 billion in private investment as of 2010. As a result of MAPS, the population living in downtown housing has exponentially increased, together with demand for additional residential and retail amenities, such as grocery, services, and shops. An upscale lifestyle publication called Slice Magazine is circulated throughout the metropolitan area. In addition, there is a magazine published by Back40 Design Group called The Edmond Outlook. It contains local commentary and human interest pieces direct-mailed to over 50,000 Edmond residents. Before World War II, Oklahoma City developed major stockyards, attracting jobs and revenue formerly in Chicago and Omaha, Nebraska. With the 1928 discovery of oil within the city limits (including under the State Capitol), Oklahoma City became a major center of oil production. Post-war growth accompanied the construction of the Interstate Highway System, which made Oklahoma City a major interchange as the convergence of I-35, I-40 and I-44. It was also aided by federal development of Tinker Air Force Base. Oklahoma City is protected by the Oklahoma City Fire Department (OKCFD), which employs 1015 paid, professional firefighters. The current Chief of Department is G. Keith Bryant, the department is also commanded by three Deputy Chiefs, who – along with the department chief – oversee the Operational Services, Prevention Services, and Support Services bureaus. The OKCFD currently operates out of 37 fire stations, located throughout the city in six battalions. The OKCFD also operates a fire apparatus fleet of 36 engines (including 30 paramedic engines), 13 ladders, 16 brush patrol units, six water tankers, two hazardous materials units, one Technical Rescue Unit, one Air Supply Unit, six Arson Investigation Units, and one Rehabilitation Unit. Each engine is staffed with a driver, an officer, and one to two firefighters, while each ladder company is staffed with a driver, an officer, and one firefighter. Minimum staffing per shift is 213 personnel. The Oklahoma City Fire Department responds to over 70,000 emergency calls annually. The city is bisected geographically and culturally by the North Canadian River, which basically divides North Oklahoma City and South Oklahoma City. The two halves of the city were actually founded and plotted as separate cities, but soon grew together. The north side is characterized by very diverse and fashionable urban neighborhoods near the city center and sprawling suburbs further north. South Oklahoma City is generally more blue collar working class and significantly more industrial, having grown up around the Stockyards and meat packing plants at the turn of the century, and is currently the center of the city's rapidly growing Latino community. Oklahoma City has a major park in each quadrant of the city, going back to the first parks masterplan. Will Rogers Park, Lincoln Park, Trosper Park, and Woodson Park were once connected by the Grand Boulevard loop, some sections of which no longer exist. Martin Park Nature Center is a natural habitat in far northwest Oklahoma City. Will Rogers Park is home to the Lycan Conservatory, the Rose Garden, and Butterfly Garden, all built in the WPA era. Oklahoma City is home to the American Banjo Museum, which houses a large collection of highly decorated banjos from the early 20th century and exhibits on the history of the banjo and its place in American history. Concerts and lectures are also held there. As of the 2010 census, there were 579,999 people, 230,233 households, and 144,120 families residing in the city. The population density was 956.4 inhabitants per square mile (321.9/km²). There were 256,930 housing units at an average density of 375.9 per square mile (145.1/km²). The average temperature is 61.4 °F (16.3 °C), with the monthly daily average ranging from 39.2 °F (4.0 °C) in January to 83.0 °F (28.3 °C) in July. Extremes range from −17 °F (−27 °C) on February 12, 1899 to 113 °F (45 °C) on August 11, 1936 and August 3, 2012; the last sub-zero (°F) reading was −5 °F (−21 °C) on February 10, 2011. Temperatures reach 100 °F (38 °C) on 10.4 days of the year, 90 °F (32 °C) on nearly 70 days, and fail to rise above freezing on 8.3 days. The city receives about 35.9 inches (91.2 cm) of precipitation annually, of which 8.6 inches (21.8 cm) is snow. Private career and technology education schools in Oklahoma City include Oklahoma Technology Institute, Platt College, Vatterott College, and Heritage College. The Dale Rogers Training Center in Oklahoma City is a nonprofit vocational training center for individuals with disabilities. The National Cowboy & Western Heritage Museum has galleries of western art and is home to the Hall of Great Western Performers. In contrast, the city will also be home to The American Indian Cultural Center and Museum that began construction in 2009 (although completion of the facility has been held up due to insufficient funding), on the south side of Interstate 40, southeast from Bricktown. Residents of Oklahoma City suffered substantial losses on April 19, 1995 when Timothy McVeigh detonated a bomb in front of the Murrah building. The building was destroyed (the remnants of which had to be imploded in a controlled demolition later that year), more than 100 nearby buildings suffered severe damage, and 168 people were killed. The site has been commemorated as the Oklahoma City National Memorial and Museum. Since its opening in 2000, over three million people have visited. Every year on April 19, survivors, families and friends return to the memorial to read the names of each person lost. The Oklahoma History Center is the history museum of the state of Oklahoma. Located across the street from the governor's mansion at 800 Nazih Zuhdi Drive in northeast Oklahoma City, the museum opened in 2005 and is operated by the Oklahoma Historical Society. It preserves the history of Oklahoma from the prehistoric to the present day. After a lackluster arrival to Oklahoma City for the 2008–09 season, the Oklahoma City Thunder secured a berth (8th) in the 2010 NBA Playoffs the next year after boasting its first 50-win season, winning two games in the first round against the Los Angeles Lakers. In 2012, Oklahoma City made it to the NBA Finals, but lost to the Miami Heat in five games. In 2013 the Thunder reached the Western Conference semifinals without All-Star guard Russell Westbrook, who was injured in their first round series against the Houston Rockets, only to lose to the Memphis Grizzlies. In 2014 Oklahoma City again reached the NBA's Western Conference Finals but eventually lost to the San Antonio Spurs in six games. Oklahoma City is home to several professional sports teams, including the Oklahoma City Thunder of the National Basketball Association. The Thunder is the city's second ""permanent"" major professional sports franchise after the now-defunct AFL Oklahoma Wranglers and is the third major-league team to call the city home when considering the temporary hosting of the New Orleans/Oklahoma City Hornets for the 2005–06 and 2006–07 NBA seasons. OU Medicine, an academic medical institution located on the campus of The University of Oklahoma Health Sciences Center, is home to OU Medical Center. OU Medicine operates Oklahoma's only level-one trauma center at the OU Medical Center and the state's only level-one trauma center for children at Children's Hospital at OU Medicine, both of which are located in the Oklahoma Health Center district. Other medical facilities operated by OU Medicine include OU Physicians and OU Children's Physicians, the OU College of Medicine, the Oklahoma Cancer Center and OU Medical Center Edmond, the latter being located in the northern suburb of Edmond. Oklahoma City is on the I-35 Corridor and is one of the primary travel corridors into neighboring Texas and Mexico. Located in the Frontier Country region of the state, the city's northeast section lies in an ecological region known as the Cross Timbers. The city was founded during the Land Run of 1889, and grew to a population of over 10,000 within hours of its founding. The city was the scene of the April 19, 1995 bombing of the Alfred P. Murrah Federal Building, in which 168 people died. It was the deadliest terror attack in the history of the United States until the attacks of September 11, 2001, and remains the deadliest act of domestic terrorism in U.S. history. There were 230,233 households, 29.4% of which had children under the age of 18 living with them, 43.4% were married couples living together, 13.9% had a female householder with no husband present, and 37.4% were non-families. One person households account for 30.5% of all households and 8.7% of all households had someone living alone who is 65 years of age or older. The average household size was 2.47 and the average family size was 3.11. Oklahoma City has several public career and technology education schools associated with the Oklahoma Department of Career and Technology Education, the largest of which are Metro Technology Center and Francis Tuttle Technology Center. The University of Oklahoma has several institutions of higher learning in the city and metropolitan area, with OU Medicine and the University of Oklahoma Health Sciences Center campuses located east of downtown in the Oklahoma Health Center district, and the main campus located to the south in the suburb of Norman. The OU Medicine hosting the state's only Level-One trauma center. OU Health Sciences Center is one of the nation's largest independent medical centers, employing more than 12,000 people. OU is one of only four major universities in the nation to operate six medical schools.[clarification needed] The Oklahoma City Zoo and Botanical Garden is home to numerous natural habitats, WPA era architecture and landscaping, and hosts major touring concerts during the summer at its amphitheater. Oklahoma City also has two amusement parks, Frontier City theme park and White Water Bay water park. Frontier City is an 'Old West'-themed amusement park. The park also features a recreation of a western gunfight at the 'OK Corral' and many shops that line the ""Western"" town's main street. Frontier City also hosts a national concert circuit at its amphitheater during the summer. Oklahoma City also has a combination racetrack and casino open year-round, Remington Park, which hosts both Quarter horse (March – June) and Thoroughbred (August – December) seasons. The population density normally reported for Oklahoma City using the area of its city limits can be a bit misleading. Its urbanized zone covers roughly 244 sq mi (630 km2) resulting in a density of 2,500 per square mile (2013 est), compared with larger rural watershed areas incorporated by the city, which cover the remaining 377 sq mi (980 km2) of the city limits. Oklahoma City Community College in south Oklahoma City is the second-largest community college in the state. Rose State College is located east of Oklahoma City in suburban Midwest City. Oklahoma State University–Oklahoma City is located in the ""Furniture District"" on the Westside. Northeast of the city is Langston University, the state's historically black college (HBCU). Langston also has an urban campus in the eastside section of the city. Southern Nazarene University, which was founded by the Church of the Nazarene, is a university located in suburban Bethany, which is surrounded by the Oklahoma City city limits. Oklahoma City is home to the state's largest school district, Oklahoma City Public Schools. The district's Classen School of Advanced Studies and Harding Charter Preparatory High School rank high among public schools nationally according to a formula that looks at the number of Advanced Placement, International Baccalaureate and/or Cambridge tests taken by the school's students divided by the number of graduating seniors. In addition, OKCPS's Belle Isle Enterprise Middle School was named the top middle school in the state according to the Academic Performance Index, and recently received the Blue Ribbon School Award, in 2004 and again in 2011. KIPP Reach College Preparatory School in Oklahoma City received the 2012 National Blue Ribbon along with its school leader, Tracy McDaniel Sr., being awarded the Terrel H. Bell Award for Outstanding Leadership. In the American College of Sports Medicine's annual ranking of the United States' 50 most populous metropolitan areas on the basis of community health, Oklahoma City took last place in 2010, falling five places from its 2009 rank of 45. The ACSM's report, published as part of its American Fitness Index program, cited, among other things, the poor diet of residents, low levels of physical fitness, higher incidences of obesity, diabetes, and cardiovascular disease than the national average, low access to recreational facilities like swimming pools and baseball diamonds, the paucity of parks and low investment by the city in their development, the high percentage of households below the poverty level, and the lack of state-mandated physical education curriculum as contributing factors. Oklahoma City is served by two primary airports, Will Rogers World Airport and the much smaller Wiley Post Airport (incidentally, the two honorees died in the same plane crash in Alaska) Will Rogers World Airport is the state's busiest commercial airport, with over 3.6 million passengers annually. Tinker Air Force Base, in southeast Oklahoma City, is the largest military air depot in the nation; a major maintenance and deployment facility for the Navy and the Air Force, and the second largest military institution in the state (after Fort Sill in Lawton). The third-largest university in the state, the University of Central Oklahoma, is located just north of the city in the suburb of Edmond. Oklahoma Christian University, one of the state's private liberal arts institutions, is located just south of the Edmond border, inside the Oklahoma City limits. While not in Oklahoma City proper, other large employers within the MSA region include: Tinker Air Force Base (27,000); University of Oklahoma (11,900); University of Central Oklahoma (2,900); and Norman Regional Hospital (2,800). The Oklahoman is Oklahoma City's major daily newspaper and is the most widely circulated in the state. NewsOK.com is the Oklahoman's online presence. Oklahoma Gazette is Oklahoma City's independent newsweekly, featuring such staples as local commentary, feature stories, restaurant reviews and movie listings and music and entertainment. The Journal Record is the city's daily business newspaper and okcBIZ is a monthly publication that covers business news affecting those who live and work in Central Oklahoma. Oklahoma City also has several major national and state highways within its city limits. Shields Boulevard (US-77) continues from E.K. Gaylord Boulevard in downtown Oklahoma City and runs south eventually connecting to I-35 near the suburb of Moore. Northwest Expressway (Oklahoma State Highway 3) runs from North Classen Boulevard in north-central Oklahoma City to the northwestern suburbs. On April 19, 1995, the Alfred P. Murrah Federal Building was destroyed by a fertilizer bomb manufactured and detonated by Timothy McVeigh. The blast and catastrophic collapse killed 168 people and injured over 680. The blast shockwave destroyed or damaged 324 buildings within a 340-meter radius, destroyed or burned 86 cars, and shattered glass in 258 nearby buildings, causing at least an estimated $652 million worth of damage. The main suspect- Timothy McVeigh, was executed by lethal injection on June 11, 2001. It was the deadliest single domestic terrorist attack in US history, prior to 9/11. Since the MAPS projects' completion, the downtown area has seen continued development. Several downtown buildings are undergoing renovation/restoration. Notable among these was the restoration of the Skirvin Hotel in 2007. The famed First National Center is being renovated. Oklahoma City has a very active severe weather season from March through June, especially during April and May. Being in the center of what is colloquially referred to as Tornado Alley, it is prone to especially frequent and severe tornadoes, as well as very severe hailstorms and occasional derechoes. Tornadoes have occurred in every month of the year and a secondary smaller peak also occurs during autumn, especially October. The Oklahoma City metropolitan area is one of the most tornado-prone major cities in the world, with about 150 tornadoes striking within the city limits since 1890. Since the time weather records have been kept, Oklahoma City has been struck by thirteen violent tornadoes, eleven F/EF4s and two F/EF5. On May 3, 1999 parts of southern Oklahoma City and nearby suburban communities suffered from one of the most powerful tornadoes on record, an F5 on the Fujita scale, with wind speeds estimated by radar at 318 mph (510 km/h). On May 20, 2013, far southwest Oklahoma City, along with Newcastle and Moore, was hit again by a EF5 tornado; it was 0.5 to 1.3 miles (0.80 to 2.09 km) wide and killed 23 people. Less than two weeks later, on May 31, another outbreak affected the Oklahoma City area, including an EF1 and an EF0 within the city and a tornado several miles west of the city that was 2.6 miles (4.2 km) in width, the widest tornado ever recorded."
Baptists,"Baptist historian Bruce Gourley outlines four main views of Baptist origins: (1) The modern scholarly consensus that the movement traces its origin to the 17th century via the English Separatists, (2) the view that it was an outgrowth of Anabaptist traditions, (3) the perpetuity view which assumes that the Baptist faith and practice has existed since the time of Christ, and (4) the successionist view, or ""Baptist successionism"", which argues that Baptist churches actually existed in an unbroken chain since the time of Christ. Many Baptist churches choose to affiliate with organizational groups that provide fellowship without control. The largest such group is the Southern Baptist Convention. There also are a substantial number of smaller cooperative groups. Finally, there are Baptist churches that choose to remain autonomous and independent of any denomination, organization, or association. It has been suggested that a primary Baptist principle is that local Baptist Churches are independent and self-governing, and if so the term 'Baptist denomination' may be considered somewhat incongruous. Baptists, like other Christians, are defined by doctrine—some of it common to all orthodox and evangelical groups and a portion of it distinctive to Baptists. Through the years, different Baptist groups have issued confessions of faith—without considering them to be creeds—to express their particular doctrinal distinctions in comparison to other Christians as well as in comparison to other Baptists. Most Baptists are evangelical in doctrine, but Baptist beliefs can vary due to the congregational governance system that gives autonomy to individual local Baptist churches. Historically, Baptists have played a key role in encouraging religious freedom and separation of church and state. In 1609, while still there, Smyth wrote a tract titled ""The Character of the Beast,"" or ""The False Constitution of the Church."" In it he expressed two propositions: first, infants are not to be baptized; and second, ""Antichristians converted are to be admitted into the true Church by baptism."" Hence, his conviction was that a scriptural church should consist only of regenerate believers who have been baptized on a personal confession of faith. He rejected the Separatist movement's doctrine of infant baptism (paedobaptism). Shortly thereafter, Smyth left the group, and layman Thomas Helwys took over the leadership, leading the church back to England in 1611. Ultimately, Smyth became committed to believers' baptism as the only biblical baptism. He was convinced on the basis of his interpretation of Scripture that infants would not be damned should they die in infancy. As early as the late 18th century, black Baptists began to organize separate churches, associations and mission agencies, especially in the northern states. Not only did blacks set up some independent congregations in the South before the American Civil War, freedmen quickly separated from white congregations and associations after the war. They wanted to be free of white supervision. In 1866 the Consolidated American Baptist Convention, formed from black Baptists of the South and West, helped southern associations set up black state conventions, which they did in Alabama, Arkansas, Virginia, North Carolina, and Kentucky. In 1880 black state conventions united in the national Foreign Mission Convention, to support black Baptist missionary work. Two other national black conventions were formed, and in 1895 they united as the National Baptist Convention. This organization later went through its own changes, spinning off other conventions. It is the largest black religious organization and the second largest Baptist organization in the world. Baptists are numerically most dominant in the Southeast. In 2007, the Pew Research Center's Religious Landscape Survey found that 45% of all African-Americans identify with Baptist denominations, with the vast majority of those being within the historically black tradition. Elsewhere in the Americas, in the Caribbean in particular, Baptist missionaries took an active role in the anti-slavery movement. In Jamaica, for example, William Knibb, a prominent British Baptist missionary, worked toward the emancipation of slaves in the British West Indies (which took place in 1838). Knibb also protagonised the creation of ""Free Villages""; rural communities centred around a Baptist church where emancipated slaves could farm their own land. Baptists were likewise active in promoting the education of former slaves; for example, Jamaica's Calabar High School, named after the slave port of Calabar, was formed by Baptist missionaries. At the same time, during and after slavery, slaves and free formed their own Spiritual Baptist movements - breakaway spiritual movements which often expressed resistance to oppression. Baptists are individuals who comprise a group of Christian denominations and churches that subscribe to a doctrine that baptism should be performed only for professing believers (believer's baptism, as opposed to infant baptism), and that it must be done by complete immersion (as opposed to affusion or sprinkling). Other tenets of Baptist churches include soul competency (liberty), salvation through faith alone, Scripture alone as the rule of faith and practice, and the autonomy of the local congregation. Baptists recognize two ministerial offices, elders and deacons. Baptist churches are widely considered to be Protestant churches, though some Baptists disavow this identity. Leading up to the American Civil War, Baptists became embroiled in the controversy over slavery in the United States. Whereas in the First Great Awakening, Methodist and Baptist preachers had opposed slavery and urged manumission, over the decades they made more of an accommodation with the institution. They worked with slaveholders in the South to urge a paternalistic institution. Both denominations made direct appeals to slaves and free blacks for conversion. The Baptists particularly allowed them active roles in congregations. By the mid-19th century, northern Baptists tended to oppose slavery. As tensions increased, in 1844 the Home Mission Society refused to appoint a slaveholder as a missionary who had been proposed by Georgia. It noted that missionaries could not take servants with them, and also that the Board did not want to appear to condone slavery. Historians trace the earliest church labeled ""Baptist"" back to 1609 in Amsterdam, with English Separatist John Smyth as its pastor. In accordance with his reading of the New Testament, he rejected baptism of infants and instituted baptism only of believing adults. Baptist practice spread to England, where the General Baptists considered Christ's atonement to extend to all people, while the Particular Baptists believed that it extended only to the elect. In 1638, Roger Williams established the first Baptist congregation in the North American colonies. In the mid-18th century, the First Great Awakening increased Baptist growth in both New England and the South. The Second Great Awakening in the South in the early 19th century increased church membership, as did the preachers' lessening of support for abolition and manumission of slavery, which had been part of the 18th-century teachings. Baptist missionaries have spread their church to every continent. Historians trace the earliest Baptist church back to 1609 in Amsterdam, with John Smyth as its pastor. Three years earlier, while a Fellow of Christ's College, Cambridge, he had broken his ties with the Church of England. Reared in the Church of England, he became ""Puritan, English Separatist, and then a Baptist Separatist,"" and ended his days working with the Mennonites. He began meeting in England with 60–70 English Separatists, in the face of ""great danger."" The persecution of religious nonconformists in England led Smyth to go into exile in Amsterdam with fellow Separatists from the congregation he had gathered in Lincolnshire, separate from the established church (Anglican). Smyth and his lay supporter, Thomas Helwys, together with those they led, broke with the other English exiles because Smyth and Helwys were convinced they should be baptized as believers. In 1609 Smyth first baptized himself and then baptized the others. Another milestone in the early development of Baptist doctrine was in 1638 with John Spilsbury, a Calvinistic minister who helped to promote the strict practice of believer's baptism by immersion. According to Tom Nettles, professor of historical theology at Southern Baptist Theological Seminary, ""Spilsbury's cogent arguments for a gathered, disciplined congregation of believers baptized by immersion as constituting the New Testament church gave expression to and built on insights that had emerged within separatism, advanced in the life of John Smyth and the suffering congregation of Thomas Helwys, and matured in Particular Baptists."" On 20 June 1995, the Southern Baptist Convention voted to adopt a resolution renouncing its racist roots and apologizing for its past defense of slavery. More than 20,000 Southern Baptists registered for the meeting in Atlanta. The resolution declared that messengers, as SBC delegates are called, ""unwaveringly denounce racism, in all its forms, as deplorable sin"" and ""lament and repudiate historic acts of evil such as slavery from which we continue to reap a bitter harvest."" It offered an apology to all African-Americans for ""condoning and/or perpetuating individual and systemic racism in our lifetime"" and repentance for ""racism of which we have been guilty, whether consciously or unconsciously."" Although Southern Baptists have condemned racism in the past, this was the first time the predominantly white convention had dealt specifically with the issue of slavery. Modern Baptist churches trace their history to the English Separatist movement in the century after the rise of the original Protestant denominations. This view of Baptist origins has the most historical support and is the most widely accepted. Adherents to this position consider the influence of Anabaptists upon early Baptists to be minimal. It was a time of considerable political and religious turmoil. Both individuals and churches were willing to give up their theological roots if they became convinced that a more biblical ""truth"" had been discovered.[page needed] Smyth, convinced that his self-baptism was invalid, applied with the Mennonites for membership. He died while waiting for membership, and some of his followers became Mennonites. Thomas Helwys and others kept their baptism and their Baptist commitments. The modern Baptist denomination is an outgrowth of Smyth's movement. Baptists rejected the name Anabaptist when they were called that by opponents in derision. McBeth writes that as late as the 18th century, many Baptists referred to themselves as ""the Christians commonly—though falsely—called Anabaptists."" A minority view is that early seventeenth-century Baptists were influenced by (but not directly connected to) continental Anabaptists. According to this view, the General Baptists shared similarities with Dutch Waterlander Mennonites (one of many Anabaptist groups) including believer's baptism only, religious liberty, separation of church and state, and Arminian views of salvation, predestination and original sin. Representative writers including A.C. Underwood and William R. Estep. Gourley wrote that among some contemporary Baptist scholars who emphasize the faith of the community over soul liberty, the Anabaptist influence theory is making a comeback. Shared doctrines would include beliefs about one God; the virgin birth; miracles; atonement for sins through the death, burial, and bodily resurrection of Jesus; the Trinity; the need for salvation (through belief in Jesus Christ as the son of God, his death and resurrection, and confession of Christ as Lord); grace; the Kingdom of God; last things (eschatology) (Jesus Christ will return personally and visibly in glory to the earth, the dead will be raised, and Christ will judge everyone in righteousness); and evangelism and missions. Some historically significant Baptist doctrinal documents include the 1689 London Baptist Confession of Faith, 1742 Philadelphia Baptist Confession, the 1833 New Hampshire Baptist Confession of Faith, the Southern Baptist Convention's Baptist Faith and Message, and written church covenants which some individual Baptist churches adopt as a statement of their faith and beliefs. Southern Baptist Landmarkism sought to reset the ecclesiastical separation which had characterized the old Baptist churches, in an era when inter-denominational union meetings were the order of the day. James Robinson Graves was an influential Baptist of the 19th century and the primary leader of this movement. While some Landmarkers eventually separated from the Southern Baptist Convention, the influence of the movement on the Convention continued into the 20th century. Its influence continues to affect convention policies. In 2005, the Southern Baptist International Mission Board forbade its missionaries to receive alien immersions for baptism. Baptist missionary work in Canada began in the British colony of Nova Scotia (present day Nova Scotia and New Brunswick) in the 1760s. The first official record of a Baptist church in Canada was that of the Horton Baptist Church (now Wolfville) in Wolfville, Nova Scotia on 29 October 1778. The church was established with the assistance of the New Light evangelist Henry Alline. Many of Alline's followers, after his death, would convert and strengthen the Baptist presence in the Atlantic region.[page needed] Two major groups of Baptists formed the basis of the churches in the Maritimes. These were referred to as Regular Baptist (Calvinistic in their doctrine) and Free Will Baptists. Following similar conflicts over modernism, the Southern Baptist Convention adhered to conservative theology as its official position. Two new Baptist groups were formed by moderate Southern Baptists who disagreed with the direction in which the Southern Baptist Convention was heading: the Alliance of Baptists in 1987 and the Cooperative Baptist Fellowship in 1991. Members of both groups originally identified as Southern Baptist, but over time the groups ""became permanent new families of Baptists."" During the Protestant Reformation, the Church of England (Anglicans) separated from the Roman Catholic Church. There were some Christians who were not content with the achievements of the mainstream Protestant Reformation. There also were Christians who were disappointed that the Church of England had not made corrections of what some considered to be errors and abuses. Of those most critical of the Church's direction, some chose to stay and try to make constructive changes from within the Anglican Church. They became known as ""Puritans"" and are described by Gourley as cousins of the English Separatists. Others decided they must leave the Church because of their dissatisfaction and became known as the Separatists. Baptists have faced many controversies in their 400-year history, controversies of the level of crises. Baptist historian Walter Shurden says the word ""crisis"" comes from the Greek word meaning ""to decide."" Shurden writes that contrary to the presumed negative view of crises, some controversies that reach a crisis level may actually be ""positive and highly productive."" He claims that even schism, though never ideal, has often produced positive results. In his opinion crises among Baptists each have become decision-moments that shaped their future. Some controversies that have shaped Baptists include the ""missions crisis"", the ""slavery crisis"", the ""landmark crisis"", and the ""modernist crisis"". In May 1845, the Baptist congregations in the United States split over slavery and missions. The Home Mission Society prevented slaveholders from being appointed as missionaries. The split created the Southern Baptist Convention, while the northern congregations formed their own umbrella organization now called the American Baptist Churches USA (ABC-USA). The Methodist Episcopal Church, South had recently separated over the issue of slavery, and southern Presbyterians would do so shortly thereafter. Both Roger Williams and John Clarke, his compatriot and coworker for religious freedom, are variously credited as founding the earliest Baptist church in North America. In 1639, Williams established a Baptist church in Providence, Rhode Island, and Clarke began a Baptist church in Newport, Rhode Island. According to a Baptist historian who has researched the matter extensively, ""There is much debate over the centuries as to whether the Providence or Newport church deserved the place of 'first' Baptist congregation in America. Exact records for both congregations are lacking."""
Digestion,"In mammals, preparation for digestion begins with the cephalic phase in which saliva is produced in the mouth and digestive enzymes are produced in the stomach. Mechanical and chemical digestion begin in the mouth where food is chewed, and mixed with saliva to begin enzymatic processing of starches. The stomach continues to break food down mechanically and chemically through churning and mixing with both acids and enzymes. Absorption occurs in the stomach and gastrointestinal tract, and the process finishes with defecation. Underlying the process is muscle movement throughout the system through swallowing and peristalsis. Each step in digestion requires energy, and thus imposes an ""overhead charge"" on the energy made available from absorbed substances. Differences in that overhead cost are important influences on lifestyle, behavior, and even physical structures. Examples may be seen in humans, who differ considerably from other hominids (lack of hair, smaller jaws and musculature, different dentition, length of intestines, cooking, etc.). In the human digestive system, food enters the mouth and mechanical digestion of the food starts by the action of mastication (chewing), a form of mechanical digestion, and the wetting contact of saliva. Saliva, a liquid secreted by the salivary glands, contains salivary amylase, an enzyme which starts the digestion of starch in the food; the saliva also contains mucus, which lubricates the food, and hydrogen carbonate, which provides the ideal conditions of pH (alkaline) for amylase to work. After undergoing mastication and starch digestion, the food will be in the form of a small, round slurry mass called a bolus. It will then travel down the esophagus and into the stomach by the action of peristalsis. Gastric juice in the stomach starts protein digestion. Gastric juice mainly contains hydrochloric acid and pepsin. As these two chemicals may damage the stomach wall, mucus is secreted by the stomach, providing a slimy layer that acts as a shield against the damaging effects of the chemicals. At the same time protein digestion is occurring, mechanical mixing occurs by peristalsis, which is waves of muscular contractions that move along the stomach wall. This allows the mass of food to further mix with the digestive enzymes. Teeth (singular tooth) are small whitish structures found in the jaws (or mouths) of many vertebrates that are used to tear, scrape, milk and chew food. Teeth are not made of bone, but rather of tissues of varying density and hardness, such as enamel, dentine and cementum. Human teeth have a blood and nerve supply which enables proprioception. This is the ability of sensation when chewing, for example if we were to bite into something too hard for our teeth, such as a chipped plate mixed in food, our teeth send a message to our brain and we realise that it cannot be chewed, so we stop trying. Digestion is the breakdown of large insoluble food molecules into small water-soluble food molecules so that they can be absorbed into the watery blood plasma. In certain organisms, these smaller substances are absorbed through the small intestine into the blood stream. Digestion is a form of catabolism that is often divided into two processes based on how food is broken down: mechanical and chemical digestion. The term mechanical digestion refers to the physical breakdown of large pieces of food into smaller pieces which can subsequently be accessed by digestive enzymes. In chemical digestion, enzymes break down food into the small molecules the body can use. Digestion of some fats can begin in the mouth where lingual lipase breaks down some short chain lipids into diglycerides. However fats are mainly digested in the small intestine. The presence of fat in the small intestine produces hormones that stimulate the release of pancreatic lipase from the pancreas and bile from the liver which helps in the emulsification of fats for absorption of fatty acids. Complete digestion of one molecule of fat (a triglyceride) results a mixture of fatty acids, mono- and di-glycerides, as well as some undigested triglycerides, but no free glycerol molecules. In a channel transupport system, several proteins form a contiguous channel traversing the inner and outer membranes of the bacteria. It is a simple system, which consists of only three protein subunits: the ABC protein, membrane fusion protein (MFP), and outer membrane protein (OMP)[specify]. This secretion system transports various molecules, from ions, drugs, to proteins of various sizes (20 - 900 kDa). The molecules secreted vary in size from the small Escherichia coli peptide colicin V, (10 kDa) to the Pseudomonas fluorescens cell adhesion protein LapA of 900 kDa. Different phases of digestion take place including: the cephalic phase , gastric phase, and intestinal phase. The cephalic phase occurs at the sight, thought and smell of food, which stimulate the cerebral cortex. Taste and smell stimuli are sent to the hypothalamus and medulla oblongata. After this it is routed through the vagus nerve and release of acetylcholine. Gastric secretion at this phase rises to 40% of maximum rate. Acidity in the stomach is not buffered by food at this point and thus acts to inhibit parietal (secretes acid) and G cell (secretes gastrin) activity via D cell secretion of somatostatin. The gastric phase takes 3 to 4 hours. It is stimulated by distension of the stomach, presence of food in stomach and decrease in pH. Distention activates long and myenteric reflexes. This activates the release of acetylcholine, which stimulates the release of more gastric juices. As protein enters the stomach, it binds to hydrogen ions, which raises the pH of the stomach. Inhibition of gastrin and gastric acid secretion is lifted. This triggers G cells to release gastrin, which in turn stimulates parietal cells to secrete gastric acid. Gastric acid is about 0.5% hydrochloric acid (HCl), which lowers the pH to the desired pH of 1-3. Acid release is also triggered by acetylcholine and histamine. The intestinal phase has two parts, the excitatory and the inhibitory. Partially digested food fills the duodenum. This triggers intestinal gastrin to be released. Enterogastric reflex inhibits vagal nuclei, activating sympathetic fibers causing the pyloric sphincter to tighten to prevent more food from entering, and inhibits local reflexes. After some time (typically 1–2 hours in humans, 4–6 hours in dogs, 3–4 hours in house cats),[citation needed] the resulting thick liquid is called chyme. When the pyloric sphincter valve opens, chyme enters the duodenum where it mixes with digestive enzymes from the pancreas and bile juice from the liver and then passes through the small intestine, in which digestion continues. When the chyme is fully digested, it is absorbed into the blood. 95% of absorption of nutrients occurs in the small intestine. Water and minerals are reabsorbed back into the blood in the colon (large intestine) where the pH is slightly acidic about 5.6 ~ 6.9. Some vitamins, such as biotin and vitamin K (K2MK7) produced by bacteria in the colon are also absorbed into the blood in the colon. Waste material is eliminated from the rectum during defecation. Digestive systems take many forms. There is a fundamental distinction between internal and external digestion. External digestion developed earlier in evolutionary history, and most fungi still rely on it. In this process, enzymes are secreted into the environment surrounding the organism, where they break down an organic material, and some of the products diffuse back to the organism. Animals have a tube (gastrointestinal tract) in which internal digestion occurs, which is more efficient because more of the broken down products can be captured, and the internal chemical environment can be more efficiently controlled. The nitrogen fixing Rhizobia are an interesting case, wherein conjugative elements naturally engage in inter-kingdom conjugation. Such elements as the Agrobacterium Ti or Ri plasmids contain elements that can transfer to plant cells. Transferred genes enter the plant cell nucleus and effectively transform the plant cells into factories for the production of opines, which the bacteria use as carbon and energy sources. Infected plant cells form crown gall or root tumors. The Ti and Ri plasmids are thus endosymbionts of the bacteria, which are in turn endosymbionts (or parasites) of the infected plant. An earthworm's digestive system consists of a mouth, pharynx, esophagus, crop, gizzard, and intestine. The mouth is surrounded by strong lips, which act like a hand to grab pieces of dead grass, leaves, and weeds, with bits of soil to help chew. The lips break the food down into smaller pieces. In the pharynx, the food is lubricated by mucus secretions for easier passage. The esophagus adds calcium carbonate to neutralize the acids formed by food matter decay. Temporary storage occurs in the crop where food and calcium carbonate are mixed. The powerful muscles of the gizzard churn and mix the mass of food and dirt. When the churning is complete, the glands in the walls of the gizzard add enzymes to the thick paste, which helps chemically breakdown the organic matter. By peristalsis, the mixture is sent to the intestine where friendly bacteria continue chemical breakdown. This releases carbohydrates, protein, fat, and various vitamins and minerals for absorption into the body. Digestion begins in the mouth with the secretion of saliva and its digestive enzymes. Food is formed into a bolus by the mechanical mastication and swallowed into the esophagus from where it enters the stomach through the action of peristalsis. Gastric juice contains hydrochloric acid and pepsin which would damage the walls of the stomach and mucus is secreted for protection. In the stomach further release of enzymes break down the food further and this is combined with the churning action of the stomach. The partially digested food enters the duodenum as a thick semi-liquid chyme. In the small intestine, the larger part of digestion takes place and this is helped by the secretions of bile, pancreatic juice and intestinal juice. The intestinal walls are lined with villi, and their epithelial cells is covered with numerous microvilli to improve the absorption of nutrients by increasing the surface area of the intestine. In addition to the use of the multiprotein complexes listed above, Gram-negative bacteria possess another method for release of material: the formation of outer membrane vesicles. Portions of the outer membrane pinch off, forming spherical structures made of a lipid bilayer enclosing periplasmic materials. Vesicles from a number of bacterial species have been found to contain virulence factors, some have immunomodulatory effects, and some can directly adhere to and intoxicate host cells. While release of vesicles has been demonstrated as a general response to stress conditions, the process of loading cargo proteins seems to be selective. The abomasum is the fourth and final stomach compartment in ruminants. It is a close equivalent of a monogastric stomach (e.g., those in humans or pigs), and digesta is processed here in much the same way. It serves primarily as a site for acid hydrolysis of microbial and dietary protein, preparing these protein sources for further digestion and absorption in the small intestine. Digesta is finally moved into the small intestine, where the digestion and absorption of nutrients occurs. Microbes produced in the reticulo-rumen are also digested in the small intestine. Lactase is an enzyme that breaks down the disaccharide lactose to its component parts, glucose and galactose. Glucose and galactose can be absorbed by the small intestine. Approximately 65 percent of the adult population produce only small amounts of lactase and are unable to eat unfermented milk-based foods. This is commonly known as lactose intolerance. Lactose intolerance varies widely by ethnic heritage; more than 90 percent of peoples of east Asian descent are lactose intolerant, in contrast to about 5 percent of people of northern European descent. Other animals, such as rabbits and rodents, practise coprophagia behaviours - eating specialised faeces in order to re-digest food, especially in the case of roughage. Capybara, rabbits, hamsters and other related species do not have a complex digestive system as do, for example, ruminants. Instead they extract more nutrition from grass by giving their food a second pass through the gut. Soft faecal pellets of partially digested food are excreted and generally consumed immediately. They also produce normal droppings, which are not eaten. Protein digestion occurs in the stomach and duodenum in which 3 main enzymes, pepsin secreted by the stomach and trypsin and chymotrypsin secreted by the pancreas, break down food proteins into polypeptides that are then broken down by various exopeptidases and dipeptidases into amino acids. The digestive enzymes however are mostly secreted as their inactive precursors, the zymogens. For example, trypsin is secreted by pancreas in the form of trypsinogen, which is activated in the duodenum by enterokinase to form trypsin. Trypsin then cleaves proteins to smaller polypeptides."
Super_Nintendo_Entertainment_System,"To compete with the popular Family Computer in Japan, NEC Home Electronics launched the PC Engine in 1987, and Sega Enterprises followed suit with the Mega Drive in 1988. The two platforms were later launched in North America in 1989 as the TurboGrafx-16 and the Genesis respectively. Both systems were built on 16-bit architectures and offered improved graphics and sound over the 8-bit NES. However, it took several years for Sega's system to become successful. Nintendo executives were in no rush to design a new system, but they reconsidered when they began to see their dominance in the market slipping. Game players were not the only ones to notice the violence in this game; US Senators Herb Kohl and Joe Lieberman convened a Congressional hearing on December 9, 1993 to investigate the marketing of violent video games to children.[e] While Nintendo took the high ground with moderate success, the hearings led to the creation of the Interactive Digital Software Association and the Entertainment Software Rating Board, and the inclusion of ratings on all video games. With these ratings in place, Nintendo decided its censorship policies were no longer needed. Designed by Masayuki Uemura, the designer of the original Famicom, the Super Famicom was released in Japan on Wednesday, November 21, 1990 for ¥25,000 (US$210). It was an instant success; Nintendo's initial shipment of 300,000 units sold out within hours, and the resulting social disturbance led the Japanese government to ask video game manufacturers to schedule future console releases on weekends. The system's release also gained the attention of the Yakuza, leading to a decision to ship the devices at night to avoid robbery. The cartridge media of the console is officially referred to as Game Pak in most Western regions, and as Cassette (カセット, Kasetto?) in Japan and parts of Latin America. While the SNES can address 128 Mbit,[f] only 117.75 Mbit are actually available for cartridge use. A fairly normal mapping could easily address up to 95 Mbit of ROM data (48 Mbit at FastROM speed) with 8 Mbit of battery-backed RAM. However, most available memory access controllers only support mappings of up to 32 Mbit. The largest games released (Tales of Phantasia and Star Ocean) contain 48 Mbit of ROM data, while the smallest games contain only 2 Mbit. During the SNES's life, Nintendo contracted with two different companies to develop a CD-ROM-based peripheral for the console to compete with Sega's CD-ROM based addon, Mega-CD. Ultimately, deals with both Sony and Philips fell through, (although a prototype console was produced by Sony) with Philips gaining the right to release a series of titles based on Nintendo franchises for its CD-i multimedia player and Sony going on to develop its own console based on its initial dealings with Nintendo (the PlayStation). On August 23, 1991,[a] Nintendo released the Super Nintendo Entertainment System, a redesigned version of the Super Famicom, in North America for US$199. The SNES was released in the United Kingdom and Ireland in April 1992 for GB£150, with a German release following a few weeks later. Most of the PAL region versions of the console use the Japanese Super Famicom design, except for labeling and the length of the joypad leads. The Playtronic Super NES in Brazil, although PAL, uses the North American design. Both the NES and SNES were released in Brazil in 1993 by Playtronic, a joint venture between the toy company Estrela and consumer electronics company Gradiente. All versions of the SNES are predominantly gray, although the exact shade may differ. The original North American version, designed by Nintendo of America industrial designer Lance Barr (who previously redesigned the Famicom to become the NES), has a boxy design with purple sliding switches and a dark gray eject lever. The loading bay surface is curved, both to invite interaction and to prevent food or drinks from being placed on the console and spilling as had happened with the flat surfaced NES. The Japanese and European versions are more rounded, with darker gray accents and buttons. The North American SNS-101 model and the Japanese Super Famicom Jr. (the SHVC-101 model), all designed by Barr, are both smaller with a rounded contour; however, the SNS-101 buttons are purple where the Super Famicom Jr. buttons are gray. The European and American versions of the SNES controllers have much longer cables compared to the Japanese Super Famicom controllers. The Super Nintendo Entertainment System (officially abbreviated the Super NES[b] or SNES[c], and commonly shortened to Super Nintendo[d]) is a 16-bit home video game console developed by Nintendo that was released in 1990 in Japan and South Korea, 1991 in North America, 1992 in Europe and Australasia (Oceania), and 1993 in South America. In Japan, the system is called the Super Famicom (Japanese: スーパーファミコン, Hepburn: Sūpā Famikon?, officially adopting the abbreviated name of its predecessor, the Family Computer), or SFC for short. In South Korea, it is known as the Super Comboy (슈퍼 컴보이 Syupeo Keomboi) and was distributed by Hyundai Electronics. Although each version is essentially the same, several forms of regional lockout prevent the different versions from being compatible with one another. It was released in Brazil on September 2, 1992, by Playtronic. PAL consoles face another incompatibility when playing out-of-region cartridges: the NTSC video standard specifies video at 60 Hz while PAL operates at 50 Hz, resulting in approximately 16.7% slower gameplay. Additionally, PAL's higher resolution results in letterboxing of the output image. Some commercial PAL region releases exhibit this same problem and, therefore, can be played in NTSC systems without issue while others will face a 20% speedup if played in an NTSC console. To mostly correct this issue, a switch can be added to place the SNES PPU into a 60 Hz mode supported by most newer PAL televisions. Later games will detect this setting and refuse to run, requiring the switch to be thrown only after the check completes. In 2007, GameTrailers named the SNES as the second-best console of all time in their list of top ten consoles that ""left their mark on the history of gaming"", citing its graphic, sound, and library of top-quality games. In 2015, they also named it the best Nintendo console of all time, saying, ""The list of games we love from this console completely annihilates any other roster from the Big N."" Technology columnist Don Reisinger proclaimed ""The SNES is the greatest console of all time"" in January 2008, citing the quality of the games and the console's dramatic improvement over its predecessor; fellow technology columnist Will Greenwald replied with a more nuanced view, giving the SNES top marks with his heart, the NES with his head, and the PlayStation (for its controller) with his hands. GamingExcellence also gave the SNES first place in 2008, declaring it ""simply the most timeless system ever created"" with many games that stand the test of time and citing its innovation in controller design, graphics capabilities, and game storytelling. At the same time, GameDaily rated it fifth of ten for its graphics, audio, controllers, and games. In 2009, IGN named the Super Nintendo Entertainment System the fourth best video game console, complimenting its audio and ""concentration of AAA titles"". Nintendo of America took the same stance against the distribution of SNES ROM image files and the use of emulators as it did with the NES, insisting that they represented flagrant software piracy. Proponents of SNES emulation cite discontinued production of the SNES constituting abandonware status, the right of the owner of the respective game to make a personal backup via devices such as the Retrode, space shifting for private use, the desire to develop homebrew games for the system, the frailty of SNES ROM cartridges and consoles, and the lack of certain foreign imports. In October 1997, Nintendo released a redesigned model of the SNES (the SNS-101 model) in North America for US$99, which sometimes included the pack-in game Super Mario World 2: Yoshi's Island. Like the earlier redesign of the NES (the NES-101 model), the new model was slimmer and lighter than its predecessor, but it lacked S-Video and RGB output, and it was among the last major SNES-related releases in the region. A similarly redesigned Super Famicom Jr. was released in Japan at around the same time. The rivalry between Nintendo and Sega resulted in what has been described as one of the most notable console wars in video game history, in which Sega positioned the Genesis as the ""cool"" console, with more mature titles aimed at older gamers, and edgy advertisements that occasionally attacked the competition. Nintendo however, scored an early public relations advantage by securing the first console conversion of Capcom's arcade classic Street Fighter II for SNES, which took over a year to make the transition to Genesis. Despite the Genesis's head start, much larger library of games, and lower price point, the Genesis only represented an estimated 60% of the American 16-bit console market in June 1992, and neither console could maintain a definitive lead for several years. Donkey Kong Country is said to have helped establish the SNES's market prominence in the latter years of the 16-bit generation, and for a time, maintain against the PlayStation and Saturn. According to Nintendo, the company had sold more than 20 million SNES units in the U.S. According to a 2014 Wedbush Securities report based on NPD sales data, the SNES ultimately outsold the Genesis in the U.S. market. The standard SNES controller adds two additional face buttons (X and Y) to the design of the NES iteration, arranging the four in a diamond shape, and introduces two shoulder buttons. It also features an ergonomic design by Lance Barr, later used for the NES-102 model controllers, also designed by Barr. The Japanese and PAL region versions incorporate the colors of the four action buttons into system's logo. The North American version's buttons are colored to match the redesigned console; the X and Y buttons are lavender with concave faces, and the A and B buttons are purple with convex faces. Several later consoles derive elements of their controller design from the SNES, including the PlayStation, Dreamcast, Xbox, and Wii Classic Controller. During the NES era, Nintendo maintained exclusive control over titles released for the system—the company had to approve every game, each third-party developer could only release up to five games per year (but some third parties got around this by using different names, for example Konami's ""Ultra Games"" brand), those games could not be released on another console within two years, and Nintendo was the exclusive manufacturer and supplier of NES cartridges. However, competition from Sega's console brought an end to this practice; in 1991, Acclaim began releasing games for both platforms, with most of Nintendo's other licensees following suit over the next several years; Capcom (which licensed some games to Sega instead of producing them directly) and Square were the most notable holdouts. The ABS plastic used in the casing of some older SNES and Super Famicom consoles is particularly susceptible to oxidization on exposure to air, likely due to an incorrect mixture of the stabilizing or flame retarding additives. This, along with the particularly light color of the original plastic, causes affected consoles to quickly become yellow; if the sections of the casing came from different batches of plastic, a ""two-tone"" effect results. The color can sometimes be restored with UV light and a hydrogen peroxide solution. While Nintendo never released an adapter for playing NES games on the SNES (though the instructions included a way to connect both consoles to the same TV by either daisy chaining the RF switches or using AV outputs for one or both systems), the Super Game Boy adapter cartridge allows games designed for Nintendo's portable Game Boy system to be played on the SNES. The Super Game Boy touted several feature enhancements over the Game Boy, including palette substitution, custom screen borders, and (for specially enhanced games) access to the SNES console. Japan also saw the release of the Super Game Boy 2, which added a communication port to enable a second Game Boy to connect for multiplayer games. Japan saw the release of the Satellaview, a modem which attached to the Super Famicom's expansion port and connected to the St.GIGA satellite radio station. Users of the Satellaview could download gaming news and specially designed games, which were frequently either remakes of or sequels to older Famicom titles, released in installments. Satellaview signals were broadcast from April 23, 1995 through June 30, 2000. In the United States, the similar but relatively short-lived XBAND allowed users to connect to a network via a dial-up modem to compete against other players around the country. Internally, a regional lockout chip (CIC) within the console and in each cartridge prevents PAL region games from being played on Japanese or North American consoles and vice versa. The Japanese and North American machines have the same region chip. This can be overcome through the use of adapters, typically by inserting the imported cartridge in one slot and a cartridge with the correct region chip in a second slot. Alternatively, disconnecting one pin of the console's lockout chip will prevent it from locking the console; hardware in later games can detect this situation, so it later became common to install a switch to reconnect the lockout chip as needed. The company continued to carefully review submitted titles, giving them scores using a 40-point scale and allocating Nintendo's marketing resources accordingly. Each region performed separate evaluations. Nintendo of America also maintained a policy that, among other things, limited the amount of violence in the games on its systems. One game, Mortal Kombat, would challenge this policy. A surprise hit in arcades in 1992, Mortal Kombat features splashes of blood and finishing moves that often depict one character dismembering the other. Because the Genesis version retained the gore while the SNES version did not, it outsold the SNES version by a ratio of three or four-to-one. All versions incorporate a top-loading slot for game cartridges, although the shape of the slot differs between regions to match the different shapes of the cartridges. The MULTI OUT connector (later used on the Nintendo 64 and GameCube) can output composite video, S-Video and RGB signals, as well as RF with an external RF modulator. Original versions additionally include a 28-pin expansion port under a small cover on the bottom of the unit and a standard RF output with channel selection switch on the back; the redesigned models output composite video only, requiring an external modulator for RF. Emulation of the SNES is now available on handheld units, such as Android devices, Apple's iPhone and iPad, Sony's PlayStation Portable (PSP), the Nintendo DS and Game Boy Advance, the Gizmondo, the Dingoo and the GP2X by GamePark Holdings, as well as PDAs. While individual games have been included with emulators on some GameCube discs, Nintendo's Virtual Console service for the Wii marks the introduction of officially sanctioned general SNES emulation, though SNES9x GX, a port of SNES9x, has been made for the Wii. While other companies were moving on to 32-bit systems, Rare and Nintendo proved that the SNES was still a strong contender in the market. In November 1994, Rare released Donkey Kong Country, a platform game featuring 3D models and textures pre-rendered on SGI workstations. With its detailed graphics, fluid animation and high-quality music, Donkey Kong Country rivaled the aesthetic quality of games that were being released on newer 32-bit CD-based consoles. In the last 45 days of 1994, the game sold 6.1 million units, making it the fastest-selling video game in history to that date. This game sent a message that early 32-bit systems had little to offer over the SNES, and helped make way for the more advanced consoles on the horizon. Throughout the course of its life, a number of peripherals were released which added to the functionality of the SNES. Many of these devices were modeled after earlier add-ons for the NES: the Super Scope is a light gun functionally similar to the NES Zapper (though the Super Scope features wireless capabilities) and the Super Advantage is an arcade-style joystick with adjustable turbo settings akin to the NES Advantage. Nintendo also released the SNES Mouse in conjunction with its Mario Paint title. Hudson Soft, under license from Nintendo, released the Super Multitap, a multiplayer adapter for use with its popular series of Bomberman games. Some of the more unusual controllers include the BatterUP baseball bat, the Life Fitness Entertainment System (an exercise bike controller with built-in monitoring software), and the TeeV Golf golf club."
East_Prussia,"The population of the province in 1900 was 1,996,626 people, with a religious makeup of 1,698,465 Protestants, 269,196 Roman Catholics, and 13,877 Jews. The Low Prussian dialect predominated in East Prussia, although High Prussian was spoken in Warmia. The numbers of Masurians, Kursenieki and Prussian Lithuanians decreased over time due to the process of Germanization. The Polish-speaking population concentrated in the south of the province (Masuria and Warmia) and all German geographic atlases at the start of 20th century showed the southern part of East Prussia as Polish with the number of Poles estimated at the time to be 300,000. Kursenieki inhabited the areas around the Curonian lagoon, while Lithuanian-speaking Prussians concentrated in the northeast in (Lithuania Minor). The Old Prussian ethnic group became completely Germanized over time and the Old Prussian language died out in the 18th century. Through publicly funded emergency relief programs concentrating on agricultural land-improvement projects and road construction, the ""Erich Koch Plan"" for East Prussia allegedly made the province free of unemployment; on August 16, 1933 Koch reported to Hitler that unemployment had been banished entirely from East Prussia, a feat that gained admiration throughout the Reich. Koch's industrialization plans led him into conflict with R. Walther Darré, who held the office of the Reich Peasant Leader (Reichsbauernführer) and Minister of Agriculture. Darré, a neopaganist rural romantic, wanted to enforce his vision of an agricultural East Prussia. When his ""Land"" representatives challenged Koch's plans, Koch had them arrested. In 1938 the Nazis altered about one-third of the toponyms of the area, eliminating, Germanizing, or simplifying a number of Old Prussian names, as well as those Polish or Lithuanian names originating from colonists and refugees to Prussia during and after the Protestant Reformation. More than 1,500 places were ordered to be renamed by 16 July 1938 following a decree issued by Gauleiter and Oberpräsident Erich Koch and initiated by Adolf Hitler. Many who would not cooperate with the rulers of Nazi Germany were sent to concentration camps and held prisoner there until their death or liberation. At the beginning of World War I, East Prussia became a theatre of war when the Russian Empire invaded the country. The Russian Army encountered at first little resistance because the bulk of the German Army had been directed towards the Western Front according to the Schlieffen Plan. Despite early success and the capture of the towns of Rastenburg and Gumbinnen, in the Battle of Tannenberg in 1914 and the Second Battle of the Masurian Lakes in 1915, the Russians were decisively defeated and forced to retreat. The Russians were followed by the German Army advancing into Russian territory. After the disastrous defeat of the Prussian Army at the Battle of Jena-Auerstedt in 1806, Napoleon occupied Berlin and had the officials of the Prussian General Directory swear an oath of allegiance to him, while King Frederick William III and his consort Louise fled via Königsberg and the Curonian Spit to Memel. The French troops immediately took up pursuit but were delayed in the Battle of Eylau on 9 February 1807 by an East Prussian contingent under General Anton Wilhelm von L'Estocq. Napoleon had to stay at the Finckenstein Palace, but in May, after a siege of 75 days, his troops led by Marshal François Joseph Lefebvre were able to capture the city Danzig, which had been tenaciously defended by General Count Friedrich Adolf von Kalkreuth. On 14 June, Napoleon ended the War of the Fourth Coalition with his victory at the Battle of Friedland. Frederick William and Queen Louise met with Napoleon for peace negotiations, and on 9 July the Prussian king signed the Treaty of Tilsit. Approximately one-third of East Prussia's population died in the plague and famine of 1709–1711, including the last speakers of Old Prussian. The plague, probably brought by foreign troops during the Great Northern War, killed 250,000 East Prussians, especially in the province's eastern regions. Crown Prince Frederick William I led the rebuilding of East Prussia, founding numerous towns. Thousands of Protestants expelled from the Archbishopric of Salzburg were allowed to settle in depleted East Prussia. The province was overrun by Imperial Russian troops during the Seven Years' War. Following Nazi Germany's defeat in World War II in 1945, East Prussia was partitioned between Poland and the Soviet Union according to the Potsdam Conference. Southern East Prussia was placed under Polish administration, while northern East Prussia was divided between the Soviet republics of Russia (the Kaliningrad Oblast) and Lithuania (the constituent counties of the Klaipėda Region). The city of Königsberg was renamed Kaliningrad in 1946. The German population of the province largely evacuated during the war, but several hundreds of thousands died during the years 1944–46 and the remainder were subsequently expelled. Because the duchy was outside of the core Holy Roman Empire, the prince-electors of Brandenburg were able to proclaim themselves King of Prussia beginning in 1701. After the annexation of most of western Royal Prussia in the First Partition of the Polish-Lithuanian Commonwealth in 1772, eastern (ducal) Prussia was connected by land with the rest of the Prussian state and was reorganized as a province the following year (1773). Between 1829 and 1878, the Province of East Prussia was joined with West Prussia to form the Province of Prussia. Representatives of the Polish government officially took over the civilian administration of the southern part of East Prussia on 23 May 1945. Subsequently Polish expatriates from Polish lands annexed by the Soviet Union as well as Ukrainians and Lemkos from southern Poland, expelled in Operation Vistula in 1947, were settled in the southern part of East Prussia, now the Polish Warmian-Masurian Voivodeship. In 1950 the Olsztyn Voivodeship counted 689,000 inhabitants, 22.6% of them coming from areas annexed by the Soviet Union, 10% Ukrainians, and 18.5% of them pre-war inhabitants. The remaining pre-war population was treated as Germanized Poles and a policy of re-Polonization was pursued throughout the country Most of these ""Autochthones"" chose to emigrate to West Germany from the 1950s through 1970s (between 1970 and 1988 55,227 persons from Warmia and Masuria moved to Western Germany). Local toponyms were Polonised by the Polish Commission for the Determination of Place Names. Although Brandenburg was a part of the Holy Roman Empire, the Prussian lands were not within the Holy Roman Empire and were with the administration by the Teutonic Order grandmasters under jurisdiction of the Emperor. In return for supporting Emperor Leopold I in the War of the Spanish Succession, Elector Frederick III was allowed to crown himself ""King in Prussia"" in 1701. The new kingdom ruled by the Hohenzollern dynasty became known as the Kingdom of Prussia. The designation ""Kingdom of Prussia"" was gradually applied to the various lands of Brandenburg-Prussia. To differentiate from the larger entity, the former Duchy of Prussia became known as Altpreußen (""Old Prussia""), the province of Prussia, or ""East Prussia"". After the expulsion of the German population ethnic Russians, Belarusians, and Ukrainians were settled in the northern part. In the Soviet part of the region, a policy of eliminating all remnants of German history was pursued. All German place names were replaced by new Russian names. The exclave was a military zone, which was closed to foreigners; Soviet citizens could only enter with special permission. In 1967 the remnants of Königsberg Castle were demolished on the orders of Leonid Brezhnev to make way for a new ""House of the Soviets"". Shortly after the end of the war in May 1945, Germans who had fled in early 1945 tried to return to their homes in East Prussia. An estimated number of 800,000 Germans were living in East Prussia during the summer of 1945. Many more were prevented from returning,[citation needed] and the German population of East Prussia was almost completely expelled by the communist regimes. During the war and for some time thereafter 45 camps were established for about 200,000-250,000 forced labourers, the vast majority of whom were deported to the Soviet Union, including the Gulag camp system. The largest camp with about 48,000 inmates was established at Deutsch Eylau (Iława). Orphaned children who were left behind in the zone occupied by the Soviet Union were referred to as Wolf children. The Teutonic Order lost eastern Prussia when Grand Master Albert of Brandenburg-Ansbach converted to Lutheranism and secularized the Prussian branch of the Teutonic Order in 1525. Albert established himself as the first duke of the Duchy of Prussia and a vassal of the Polish crown by the Prussian Homage. Walter von Cronberg, the next Grand Master, was enfeoffed with the title to Prussia after the Diet of Augsburg in 1530, but the Order never regained possession of the territory. In 1569 the Hohenzollern prince-electors of the Margraviate of Brandenburg became co-regents with Albert's son, the feeble-minded Albert Frederick. Since 1875, with the strengthening of self-rule, the urban and rural districts (Kreise) within each province (sometimes within each governorate) formed a corporation with common tasks and assets (schools, traffic installations, hospitals, cultural institutions, jails etc.) called the Provinzialverband (provincial association). Initially the assemblies of the urban and rural districts elected representatives for the provincial diets (Provinziallandtage), which were thus indirectly elected. As of 1919 the provincial diets (or as to governorate diets, the so-called Kommunallandtage) were directly elected by the citizens of the provinces (or governorates, respectively). These parliaments legislated within the competences transferred to the provincial associations. The provincial diet of East Prussia elected a provincial executive body (government), the provincial committee (Provinzialausschuss), and a head of province, the Landeshauptmann (""Land Captain""; till the 1880s titled Landdirektor, land director). A similar fate befell the Curonians who lived in the area around the Curonian Lagoon. While many fled from the Red Army during the evacuation of East Prussia, Curonians that remained behind were subsequently expelled by the Soviet Union. Only 219 lived along the Curonian Spit in 1955. Many had German names such as Fritz or Hans, a cause for anti-German discrimination. The Soviet authorities considered the Curonians fascists. Because of this discrimination, many immigrated to West Germany in 1958, where the majority of Curonians now live. East Prussia enclosed the bulk of the ancestral lands of the Baltic Old Prussians. During the 13th century, the native Prussians were conquered by the crusading Teutonic Knights. The indigenous Balts who survived the conquest were gradually converted to Christianity. Because of Germanization and colonisation over the following centuries, Germans became the dominant ethnic group, while Poles and Lithuanians formed minorities. From the 13th century, East Prussia was part of the monastic state of the Teutonic Knights. After the Second Peace of Thorn in 1466 it became a fief of the Kingdom of Poland. In 1525, with the Prussian Homage, the province became the Duchy of Prussia. The Old Prussian language had become extinct by the 17th or early 18th century. In 1939 the Regierungsbezirk Zichenau was annexed by Germany and incorporated into East Prussia. Parts of it were transferred to other regions, e.g. Suwałki to Regierungsbezirk Gumbinnen and Soldau to Regierungsbezirk Allenstein. Despite Nazi propaganda presenting all of the regions annexed as possessing significant German populations that wanted reunification with Germany, the Reich's statistics of late 1939 show that only 31,000 out of 994,092 people in this territory were ethnic Germans.[citation needed] With the forced abdication of Emperor William II in 1918, Germany became a republic. Most of West Prussia and the former Prussian Province of Posen, territories annexed by Prussia in the 18th century Partitions of Poland, were ceded to the Second Polish Republic according to the Treaty of Versailles. East Prussia became an exclave, being separated from mainland Germany. After the Treaty of Versailles, East Prussia was separated from Germany as an exclave; the Memelland was also separated from the province. Because most of West Prussia became part of the Second Polish Republic as the Polish Corridor, the formerly West Prussian Marienwerder region became part of East Prussia (as Regierungsbezirk Westpreußen). Also Soldau district in Allenstein region was part of Second Polish Republic. The Seedienst Ostpreußen was established to provide an independent transport service to East Prussia. The Administrator of Prussia, the grandmaster of the Teutonic Order Maximilian III, son of emperor Maximilian II died in 1618. When Maximilian died, Albert's line died out, and the Duchy of Prussia passed to the Electors of Brandenburg, forming Brandenburg-Prussia. Taking advantage of the Swedish invasion of Poland in 1655, and instead of fulfilling his vassal's duties towards the Polish Kingdom, by joining forces with the Swedes and subsequent treaties of Wehlau, Labiau, and Oliva, Elector and Duke Frederick William succeeded in revoking king of Poland's sovereignty over the Duchy of Prussia in 1660. The absolutist elector also subdued the noble estates of Prussia. The succeeding Prussian reforms instigated by Heinrich Friedrich Karl vom und zum Stein and Karl August von Hardenberg included the implementation of an Oberlandesgericht appellation court at Königsberg, a municipal corporation, economic freedom as well as emancipation of the serfs and Jews. In the course of the Prussian restoration by the 1815 Congress of Vienna, the East Prussian territories were re-arranged in the Regierungsbezirke of Gumbinnen and Königsberg. From 1905, the southern districts of East Prussia formed the separate Regierungsbezirk of Allenstein. East and West Prussia were first united in personal union in 1824, and then merged in a real union in 1829 to form the Province of Prussia. The united province was again split into separate East and West Prussian provinces in 1878. Although the 1945–1949 expulsion of Germans from the northern part of former East Prussia was often conducted in a violent and aggressive way by Soviet officials, the present Russian inhabitants of the Kaliningrad Oblast have much less animosity towards Germans. German names have been revived in commercial Russian trade and there is sometimes talk of reverting Kaliningrad's name to its historic name of Königsberg. The city centre of Kaliningrad was completely rebuilt, as British bombs in 1944 and the Soviet siege in 1945 had left it in nothing but ruins. In the 1772 First Partition of Poland, the Prussian king Frederick the Great annexed neighboring Royal Prussia, i.e. the Polish voivodeships of Pomerania (Gdańsk Pomerania or Pomerelia), Malbork, Chełmno and the Prince-Bishopric of Warmia, thereby bridging the ""Polish Corridor"" between his Prussian and Farther Pomeranian lands and cutting remaining Poland off the Baltic Coast. The territory of Warmia was incorporated into the lands of former Ducal Prussia, which, by administrative deed of 31 January 1773 were named East Prussia. The former Polish Pomerelian lands beyond the Vistula River together with Malbork and Chełmno Land formed the Province of West Prussia with its capital at Marienwerder (Kwidzyn). The Polish Partition Sejm ratified the cession on 30 September 1773, whereafter Frederick officially went on to call himself a King ""of"" Prussia. In 1939 East Prussia had 2.49 million inhabitants, 85% of them ethnic Germans, the others Poles in the south who, according to Polish estimates numbered in the interwar period around 300,000-350,000, the Latvian speaking Kursenieki, and Lietuvininkai who spoke Lithuanian in the northeast. Most German East Prussians, Masurians, Kursieniki, and Lietuvininkai were Lutheran, while the population of Ermland was mainly Roman Catholic due to the history of its bishopric. The East Prussian Jewish Congregation declined from about 9,000 in 1933 to 3,000 in 1939, as most fled from Nazi rule. Those who remained were later deported and killed in the Holocaust. Erich Koch headed the East Prussian Nazi party from 1928. He led the district from 1932. This period was characterized by efforts to collectivize the local agriculture and ruthlessness in dealing with his critics inside and outside the Party. He also had long-term plans for mass-scale industrialization of the largely agricultural province. These actions made him unpopular among the local peasants. In 1932 the local paramilitary SA had already started to terrorise their political opponents. On the night of 31 July 1932 there was a bomb attack on the headquarters of the Social Democrats in Königsberg, the Otto-Braun-House. The Communist politician Gustav Sauf was killed; the executive editor of the Social Democrat ""Königsberger Volkszeitung"", Otto Wyrgatsch, and the German People's Party politician Max von Bahrfeldt were severely injured. Members of the Reichsbanner were attacked and the local Reichsbanner Chairman of Lötzen, Kurt Kotzan, was murdered on 6 August 1932. In April 1946, northern East Prussia became an official province of the Russian SFSR as the ""Kyonigsbergskaya Oblast"", with the Memel Territory becoming part of the Lithuanian SSR. In June 1946 114,070 German and 41,029 Soviet citizens were registered in the Oblast, with an unknown number of disregarded unregistered persons. In July of that year, the historic city of Königsberg was renamed Kaliningrad to honour Mikhail Kalinin and the area named the Kaliningrad Oblast. Between 24 August and 26 October 1948 21 transports with in total 42,094 Germans left the Oblast to the Soviet Occupation Zone (which became East Germany). The last remaining Germans left in November 1949 (1,401 persons) and January 1950 (7 persons). The Kingdom of Prussia became the leading state of the German Empire after its creation in 1871. However, the Treaty of Versailles following World War I granted West Prussia to Poland and made East Prussia an exclave of Weimar Germany (the new Polish Corridor separating East Prussia from the rest of Germany), while the Memel Territory was detached and was annexed by Lithuania in 1923. Following Nazi Germany's defeat in World War II in 1945, war-torn East Prussia was divided at Joseph Stalin's insistence between the Soviet Union (the Kaliningrad Oblast in the Russian SFSR and the constituent counties of the Klaipėda Region in the Lithuanian SSR) and the People's Republic of Poland (the Warmian-Masurian Voivodeship). The capital city Königsberg was renamed Kaliningrad in 1946. The German population of the province was largely evacuated during the war or expelled shortly thereafter in the expulsion of Germans after World War II. An estimated 300,000 (around one fifth of the population) died either in war time bombings raids or in the battles to defend the province.[citation needed] Upon the invitation of Duke Konrad I of Masovia, the Teutonic Knights took possession of Prussia in the 13th century and created a monastic state to administer the conquered Old Prussians. Local Old-Prussian (north) and Polish (south) toponyms were gradually Germanised. The Knights' expansionist policies, including occupation of Polish Pomerania with Gdańsk/Danzig and western Lithuania, brought them into conflict with the Kingdom of Poland and embroiled them in several wars, culminating in the Polish-Lithuanian-Teutonic War, whereby the united armies of Poland and Lithuania, defeated the Teutonic Order at the Battle of Grunwald (Tannenberg) in 1410. Its defeat was formalised in the Second Treaty of Thorn in 1466 ending the Thirteen Years' War, and leaving the former Polish region Pomerania/Pomerelia under Polish control. Together with Warmia it formed the province of Royal Prussia. Eastern Prussia remained under the Knights, but as a fief of Poland. 1466 and 1525 arrangements by kings of Poland were not verified by the Holy Roman Empire as well as the previous gains of the Teutonic Knights were not verified."
Grape,"Red wine may offer health benefits more so than white because potentially beneficial compounds are present in grape skin, and only red wine is fermented with skins. The amount of fermentation time a wine spends in contact with grape skins is an important determinant of its resveratrol content. Ordinary non-muscadine red wine contains between 0.2 and 5.8 mg/L, depending on the grape variety, because it is fermented with the skins, allowing the wine to absorb the resveratrol. By contrast, a white wine contains lower phenolic contents because it is fermented after removal of skins. There are several sources of the seedlessness trait, and essentially all commercial cultivators get it from one of three sources: Thompson Seedless, Russian Seedless, and Black Monukka, all being cultivars of Vitis vinifera. There are currently more than a dozen varieties of seedless grapes. Several, such as Einset Seedless, Benjamin Gunnels's Prime seedless grapes, Reliance, and Venus, have been specifically cultivated for hardiness and quality in the relatively cold climates of northeastern United States and southern Ontario. Anthocyanins tend to be the main polyphenolics in purple grapes whereas flavan-3-ols (i.e. catechins) are the more abundant phenolic in white varieties. Total phenolic content, a laboratory index of antioxidant strength, is higher in purple varieties due almost entirely to anthocyanin density in purple grape skin compared to absence of anthocyanins in white grape skin. It is these anthocyanins that are attracting the efforts of scientists to define their properties for human health. Phenolic content of grape skin varies with cultivar, soil composition, climate, geographic origin, and cultivation practices or exposure to diseases, such as fungal infections. In the Bible, grapes are first mentioned when Noah grows them on his farm (Genesis 9:20–21). Instructions concerning wine are given in the book of Proverbs and in the book of Isaiah, such as in Proverbs 20:1 and Isaiah 5:20–25. Deuteronomy 18:3–5,14:22–27,16:13–15 tell of the use of wine during Jewish feasts. Grapes were also significant to both the Greeks and Romans, and their god of agriculture, Dionysus, was linked to grapes and wine, being frequently portrayed with grape leaves on his head. Grapes are especially significant for Christians, who since the Early Church have used wine in their celebration of the Eucharist. Views on the significance of the wine vary between denominations. In Christian art, grapes often represent the blood of Christ, such as the grape leaves in Caravaggio’s John the Baptist. Comparing diets among Western countries, researchers have discovered that although the French tend to eat higher levels of animal fat, the incidence of heart disease remains low in France. This phenomenon has been termed the French paradox, and is thought to occur from protective benefits of regularly consuming red wine. Apart from potential benefits of alcohol itself, including reduced platelet aggregation and vasodilation, polyphenols (e.g., resveratrol) mainly in the grape skin provide other suspected health benefits, such as: The cultivation of the domesticated grape began 6,000–8,000 years ago in the Near East. Yeast, one of the earliest domesticated microorganisms, occurs naturally on the skins of grapes, leading to the innovation of alcoholic drinks such as wine. The earliest archeological evidence for a dominant position of wine-making in human culture dates from 8,000 years ago in Georgia. The oldest winery was found in Armenia, dating to around 4000 BC.[citation needed] By the 9th century AD the city of Shiraz was known to produce some of the finest wines in the Middle East. Thus it has been proposed that Syrah red wine is named after Shiraz, a city in Persia where the grape was used to make Shirazi wine.[citation needed] Ancient Egyptian hieroglyphics record the cultivation of purple grapes,[citation needed] and history attests to the ancient Greeks, Phoenicians, and Romans growing purple grapes for both eating and wine production[citation needed]. The growing of grapes would later spread to other regions in Europe, as well as North Africa, and eventually in North America. Commercially cultivated grapes can usually be classified as either table or wine grapes, based on their intended method of consumption: eaten raw (table grapes) or used to make wine (wine grapes). While almost all of them belong to the same species, Vitis vinifera, table and wine grapes have significant differences, brought about through selective breeding. Table grape cultivars tend to have large, seedless fruit (see below) with relatively thin skin. Wine grapes are smaller, usually seeded, and have relatively thick skins (a desirable characteristic in winemaking, since much of the aroma in wine comes from the skin). Wine grapes also tend to be very sweet: they are harvested at the time when their juice is approximately 24% sugar by weight. By comparison, commercially produced ""100% grape juice"", made from table grapes, is usually around 15% sugar by weight. Grapes are a type of fruit that grow in clusters of 15 to 300, and can be crimson, black, dark blue, yellow, green, orange, and pink. ""White"" grapes are actually green in color, and are evolutionarily derived from the purple grape. Mutations in two regulatory genes of white grapes turn off production of anthocyanins, which are responsible for the color of purple grapes. Anthocyanins and other pigment chemicals of the larger family of polyphenols in purple grapes are responsible for the varying shades of purple in red wines. Grapes are typically an ellipsoid shape resembling a prolate spheroid. Grape juice is obtained from crushing and blending grapes into a liquid. The juice is often sold in stores or fermented and made into wine, brandy, or vinegar. Grape juice that has been pasteurized, removing any naturally occurring yeast, will not ferment if kept sterile, and thus contains no alcohol. In the wine industry, grape juice that contains 7–23% of pulp, skins, stems and seeds is often referred to as ""must"". In North America, the most common grape juice is purple and made from Concord grapes, while white grape juice is commonly made from Niagara grapes, both of which are varieties of native American grapes, a different species from European wine grapes. In California, Sultana (known there as Thompson Seedless) grapes are sometimes diverted from the raisin or table market to produce white juice. The Catholic Church uses wine in the celebration of the Eucharist because it is part of the tradition passed down through the ages starting with Jesus Christ at the Last Supper, where Catholics believe the consecrated bread and wine literally become the body and blood of Jesus Christ, a dogma known as transubstantiation. Wine is used (not grape juice) both due to its strong Scriptural roots, and also to follow the tradition set by the early Christian Church. The Code of Canon Law of the Catholic Church (1983), Canon 924 says that the wine used must be natural, made from grapes of the vine, and not corrupt. In some circumstances, a priest may obtain special permission to use grape juice for the consecration, however this is extremely rare and typically requires sufficient impetus to warrant such a dispensation, such as personal health of the priest."
Florida,"The British divided Florida into the two colonies of British East Florida and British West Florida. The British government gave land grants to officers and soldiers who had fought in the French and Indian War in order to encourage settlement. In order to induce settlers to move to the two new colonies reports of the natural wealth of Florida were published in England. A large number of British colonists who were ""energetic and of good character"" moved to Florida, mostly coming from South Carolina, Georgia and England though there was also a group of settlers who came from the colony of Bermuda. This would be the first permanent English-speaking population in what is now Duval County, Baker County, St. Johns County and Nassau County. The British built good public roads and introduced the cultivation of sugar cane, indigo and fruits as well the export of lumber. ""By May 1539, Conquistador Hernando de Soto skirted the coast of Florida, searching for a deep harbor to land. He described seeing a thick wall of red mangroves spread mile after mile, some reaching as high as 70 feet (21 m), with intertwined and elevated roots making landing difficult. Very soon, 'many smokes' appeared 'along the whole coast', billowing against the sky, when the Native ancestors of the Seminole spotted the newcomers and spread the alarm by signal fires"". The Spanish introduced Christianity, cattle, horses, sheep, the Spanish language, and more to Florida.[full citation needed] Both the Spanish and French established settlements in Florida, with varying degrees of success. In 1559, Don Tristán de Luna y Arellano established a colony at present-day Pensacola, one of the first European settlements in the continental United States, but it was abandoned by 1561. Some sections of the state feature architectural styles including Spanish revival, Florida vernacular, and Mediterranean Revival Style. It has the largest collection of Art Deco and Streamline Moderne buildings in both the United States and the entire world, most of which are located in the Miami metropolitan area, especially Miami Beach's Art Deco District, constructed as the city was becoming a resort destination. A unique architectural design found only in Florida is the post-World War II Miami Modern, which can be seen in areas such as Miami's MiMo Historic District. By the early 1800s, Indian removal was a significant issue throughout the southeastern U.S. and also in Florida. In 1830, the U.S. Congress passed the Indian Removal Act and as settlement increased, pressure grew on the United States government to remove the Indians from Florida. Seminoles harbored runaway blacks, known as the Black Seminoles, and clashes between whites and Indians grew with the influx of new settlers. In 1832, the Treaty of Payne's Landing promised to the Seminoles lands west of the Mississippi River if they agreed to leave Florida. Many Seminole left at this time. As of 2010, those of Hispanic or Latino ancestry ancestry accounted for 22.5% (4,223,806) of Florida's population. Out of the 22.5%, the largest groups were 6.5% (1,213,438) Cuban, 4.5% (847,550) Puerto Rican, 3.3% (629,718) Mexican, and 1.6% (300,414) Colombian. Florida's Hispanic population includes large communities of Cuban Americans in Miami and Tampa, Puerto Ricans in Orlando and Tampa, and Mexican/Central American migrant workers. The Hispanic community continues to grow more affluent and mobile. As of 2011, 57.0% of Florida's children under the age of 1 belonged to minority groups. Florida has a large and diverse Hispanic population, with Cubans and Puerto Ricans being the largest groups in the state. Nearly 80% of Cuban Americans live in Florida, especially South Florida where there is a long-standing and affluent Cuban community. Florida has the second largest Puerto Rican population after New York, as well as the fastest-growing in the nation. Puerto Ricans are more widespread throughout the state, though the heaviest concentrations are in the Orlando area of Central Florida. These American settlers established a permanent foothold in the area and ignored Spanish officials. The British settlers who had remained also resented Spanish rule, leading to a rebellion in 1810 and the establishment for ninety days of the so-called Free and Independent Republic of West Florida on September 23. After meetings beginning in June, rebels overcame the Spanish garrison at Baton Rouge (now in Louisiana), and unfurled the flag of the new republic: a single white star on a blue field. This flag would later become known as the ""Bonnie Blue Flag"". In 1998, Democratic voters dominated areas of the state with a high percentage of racial minorities and transplanted white liberals from the northeastern United States, known colloquially as ""snowbirds"". South Florida and the Miami metropolitan area are dominated by both racial minorities and white liberals. Because of this, the area has consistently voted as one of the most Democratic areas of the state. The Daytona Beach area is similar demographically and the city of Orlando has a large Hispanic population, which has often favored Democrats. Republicans, made up mostly of white conservatives, have dominated throughout much of the rest of Florida, particularly in the more rural and suburban areas. This is characteristic of its voter base throughout the Deep South. Florida i/ˈflɒrɪdə/ (Spanish for ""flowery land"") is a state located in the southeastern region of the United States. The state is bordered to the west by the Gulf of Mexico, to the north by Alabama and Georgia, to the east by the Atlantic Ocean, and to the south by the Straits of Florida and the sovereign state of Cuba. Florida is the 22nd most extensive, the 3rd most populous, and the 8th most densely populated of the United States. Jacksonville is the most populous city in Florida, and the largest city by area in the contiguous United States. The Miami metropolitan area is the eighth-largest metropolitan area in the United States. Tallahassee is the state capital. The fast-growing I-4 corridor area, which runs through Central Florida and connects the cities of Daytona Beach, Orlando, and Tampa/St. Petersburg, has had a fairly even breakdown of Republican and Democratic voters. The area is often seen as a merging point of the conservative northern portion of the state and the liberal southern portion, making it the biggest swing area in the state. Since the late 20th century, the voting results in this area, containing 40% of Florida voters, has often determined who will win the state of Florida in presidential elections. From 1952 to 1964, most voters were registered Democrats, but the state voted for the Republican presidential candidate in every election except for 1964. The following year, Congress passed and President Lyndon B. Johnson signed the Voting Rights Act of 1965, providing for oversight of state practices and enforcement of constitutional voting rights for African Americans and other minorities in order to prevent the discrimination and disenfranchisement that had excluded most of them for decades from the political process. As of 2010, those of (non-Hispanic white) European ancestry accounted for 57.9% of Florida's population. Out of the 57.9%, the largest groups were 12.0% German (2,212,391), 10.7% Irish (1,979,058), 8.8% English (1,629,832), 6.6% Italian (1,215,242), 2.8% Polish (511,229), and 2.7% French (504,641). White Americans of all European backgrounds are present in all areas of the state. In 1970, non-Hispanic whites were nearly 80% of Florida's population. Those of English and Irish ancestry are present in large numbers in all the urban/suburban areas across the state. Some native white Floridians, especially those who have descended from long-time Florida families, may refer to themselves as ""Florida crackers""; others see the term as a derogatory one. Like whites in most of the other Southern states, they descend mainly from English and Scots-Irish settlers, as well as some other British American settlers. Extended systems of underwater caves, sinkholes and springs are found throughout the state and supply most of the water used by residents. The limestone is topped with sandy soils deposited as ancient beaches over millions of years as global sea levels rose and fell. During the last glacial period, lower sea levels and a drier climate revealed a much wider peninsula, largely savanna. The Everglades, an enormously wide, slow-flowing river encompasses the southern tip of the peninsula. Sinkhole damage claims on property in the state exceeded a total of $2 billion from 2006 through 2010. At the end of the third quarter in 2008, Florida had the highest mortgage delinquency rate in the country, with 7.8% of mortgages delinquent at least 60 days. A 2009 list of national housing markets that were hard hit in the real estate crash included a disproportionate number in Florida. The early 21st-century building boom left Florida with 300,000 vacant homes in 2009, according to state figures. In 2009, the US Census Bureau estimated that Floridians spent an average 49.1% of personal income on housing-related costs, the third highest percentage in the country. Americans of English descent and Americans of Scots-Irish descent began moving into northern Florida from the backwoods of Georgia and South Carolina. Though technically not allowed by the Spanish authorities, the Spanish were never able to effectively police the border region and the backwoods settlers from the United States would continue to migrate into Florida unchecked. These migrants, mixing with the already present British settlers who had remained in Florida since the British period, would be the progenitors of the population known as Florida Crackers. Florida is served by Amtrak, operating numerous lines throughout, connecting the state's largest cities to points north in the United States and Canada. The busiest Amtrak train stations in Florida in 2011 were: Sanford (259,944), Orlando (179,142), Tampa Union Station (140,785), Miami (94,556), and Jacksonville (74,733). Sanford, in Greater Orlando, is the southern terminus of the Auto Train, which originates at Lorton, Virginia, south of Washington, D.C.. Until 2005, Orlando was also the eastern terminus of the Sunset Limited, which travels across the southern United States via New Orleans, Houston, and San Antonio to its western terminus of Los Angeles. Florida is served by two additional Amtrak trains (the Silver Star and the Silver Meteor), which operate between New York City and Miami. Miami Central Station, the city's rapid transit, commuter rail, intercity rail, and bus hub, is under construction. In the closely contested 2000 election, the state played a pivotal role. Out of more than 5.8 million votes for the two main contenders Bush and Al Gore, around 500 votes separated the two candidates for the all-decisive Florida electoral votes that landed Bush the election win. Florida's felony disenfranchisement law is more severe than most European nations or other American states. A 2002 study in the American Sociological Review concluded that ""if the state’s 827,000 disenfranchised felons had voted at the same rate as other Floridians, Democratic candidate Al Gore would have won Florida—and the presidency—by more than 80,000 votes."" The Gross Domestic Product (GDP) of Florida in 2010 was $748 billion. Its GDP is the fourth largest economy in the United States. In 2010, it became the fourth largest exporter of trade goods. The major contributors to the state's gross output in 2007 were general services, financial services, trade, transportation and public utilities, manufacturing and construction respectively. In 2010–11, the state budget was $70.5 billion, having reached a high of $73.8 billion in 2006–07. Chief Executive Magazine name Florida the third ""Best State for Business"" in 2011. In 2010, 6.9% of the population (1,269,765) considered themselves to be of only American ancestry (regardless of race or ethnicity). Many of these were of English or Scotch-Irish descent; however, their families have lived in the state for so long, that they choose to identify as having ""American"" ancestry or do not know their ancestry. In the 1980 United States census the largest ancestry group reported in Florida was English with 2,232,514 Floridians claiming that they were of English or mostly English American ancestry. Some of their ancestry went back to the original thirteen colonies. As a result of these initiatives northeastern Florida prospered economically in a way it never did under Spanish rule. Furthermore, the British governors were directed to call general assemblies as soon as possible in order to make laws for the Floridas and in the meantime they were, with the advice of councils, to establish courts. This would be the first introduction of much of the English-derived legal system which Florida still has today including trial by jury, habeas corpus and county-based government. Neither East Florida nor West Florida would send any representatives to Philadelphia to draft the Declaration of Independence. Florida would remain a Loyalist stronghold for the duration of the American Revolution. As of 2010, those of African ancestry accounted for 16.0% of Florida's population, which includes African Americans. Out of the 16.0%, 4.0% (741,879) were West Indian or Afro-Caribbean American. During the early 1900s, black people made up nearly half of the state's population. In response to segregation, disfranchisement and agricultural depression, many African Americans migrated from Florida to northern cities in the Great Migration, in waves from 1910 to 1940, and again starting in the later 1940s. They moved for jobs, better education for their children and the chance to vote and participate in society. By 1960 the proportion of African Americans in the state had declined to 18%. Conversely large numbers of northern whites moved to the state.[citation needed] Today, large concentrations of black residents can be found in northern and central Florida. Aside from blacks descended from African slaves brought to the US south, there are also large numbers of blacks of West Indian, recent African, and Afro-Latino immigrant origins, especially in the Miami/South Florida area. In 2010, Florida had the highest percentage of West Indians in the United States, with 2.0% (378,926) from Haitian ancestry, and 1.3% (236,950) Jamaican. All other (non-Hispanic) Caribbean nations were well below 0.1% of Florida residents. After the watershed events of Hurricane Andrew in 1992, the state of Florida began investing in economic development through the Office of Trade, Tourism, and Economic Development. Governor Jeb Bush realized that watershed events such as Andrew negatively impacted Florida's backbone industry of tourism severely. The office was directed to target Medical/Bio-Sciences among others. Three years later, The Scripps Research Institute (TSRI) announced it had chosen Florida for its newest expansion. In 2003, TSRI announced plans to establish a major science center in Palm Beach, a 364,000 square feet (33,800 m2) facility on 100 acres (40 ha), which TSRI planned to occupy in 2006. The court ruled in 2014, after lengthy testimony, that at least two districts had to be redrawn because of gerrymandering. After this was appealed, in July 2015 the Florida Supreme Court ruled that lawmakers had followed an illegal and unconstitutional process overly influenced by party operatives, and ruled that at least eight districts had to be redrawn. On December 2, 2015, a 5-2 majority of the Court accepted a new map of congressional districts, some of which was drawn by challengers. Their ruling affirmed the map previously approved by Leon County Judge Terry Lewis, who had overseen the original trial. It particularly makes changes in South Florida. There are likely to be additional challenges to the map and districts. The United States Census Bureau estimates that the population of Florida was 20,271,272 on July 1, 2015, a 7.82% increase since the 2010 United States Census. The population of Florida in the 2010 census was 18,801,310. Florida was the seventh fastest-growing state in the U.S. in the 12-month period ending July 1, 2012. In 2010, the center of population of Florida was located between Fort Meade and Frostproof. The center of population has moved less than 5 miles (8 km) to the east and approximately 1 mile (1.6 km) to the north between 1980 and 2010 and has been located in Polk County since the 1960 census. The population exceeded 19.7 million by December 2014, surpassing the population of the state of New York for the first time. The first post-Reconstruction era Republican elected to Congress from Florida was William C. Cramer in 1954 from Pinellas County on the Gulf Coast, where demographic changes were underway. In this period, African Americans were still disenfranchised by the state's constitution and discriminatory practices; in the 19th century they had made up most of the Republican Party. Cramer built a different Republican Party in Florida, attracting local white conservatives and transplants from northern and midwestern states. In 1966 Claude R. Kirk, Jr. was elected as the first post-Reconstruction Republican governor, in an upset election. In 1968 Edward J. Gurney, also a white conservative, was elected as the state's first post-reconstruction Republican US Senator. In 1970 Democrats took the governorship and the open US Senate seat, and maintained dominance for years. Reapportionment following the 2010 United States Census gave the state two more seats in the House of Representatives. The legislature's redistricting, announced in 2012, was quickly challenged in court, on the grounds that it had unfairly benefited Republican interests. In 2015, the Florida Supreme Court ruled on appeal that the congressional districts had to be redrawn because of the legislature's violation of the Fair District Amendments to the state constitution passed in 2010; it accepted a new map in early December 2015. Florida's nickname is the ""Sunshine State"", but severe weather is a common occurrence in the state. Central Florida is known as the lightning capital of the United States, as it experiences more lightning strikes than anywhere else in the country. Florida has one of the highest average precipitation levels of any state, in large part because afternoon thunderstorms are common in much of the state from late spring until early autumn. A narrow eastern part of the state including Orlando and Jacksonville receives between 2,400 and 2,800 hours of sunshine annually. The rest of the state, including Miami, receives between 2,800 and 3,200 hours annually. From the 1930s through much of the 1960s, Florida was essentially a one-party state dominated by white conservative Democrats, who together with other Democrats of the Solid South, exercised considerable control in Congress. They gained federal money from national programs; like other southern states, Florida residents have received more federal monies than they pay in taxes: the state is a net beneficiary. Since the 1970s, the conservative white majority of voters in the state has largely shifted from the Democratic to the Republican Party. It has continued to support Republican presidential candidates through the 20th century, except in 1976 and 1996, when the Democratic nominee was from the South. They have had ""the luxury of voting for presidential candidates who pledge to cut taxes and halt the expansion of government while knowing that their congressional delegations will continue to protect federal spending."" A peninsula between the Gulf of Mexico, the Atlantic Ocean, and the Straits of Florida, it has the longest coastline in the contiguous United States, approximately 1,350 miles (2,170 km), and is the only state that borders both the Gulf of Mexico and the Atlantic Ocean. Much of the state is at or near sea level and is characterized by sedimentary soil. The climate varies from subtropical in the north to tropical in the south. The American alligator, American crocodile, Florida panther, and manatee can be found in the Everglades National Park. Florida had become a burden to Spain, which could not afford to send settlers or garrisons. Madrid therefore decided to cede the territory to the United States through the Adams-Onís Treaty, which took effect in 1821. President James Monroe was authorized on March 3, 1821 to take possession of East Florida and West Florida for the United States and provide for initial governance. Andrew Jackson served as military governor of the newly acquired territory, but only for a brief period. On March 30, 1822, the United States merged East Florida and part of West Florida into the Florida Territory. In 1763, Spain traded Florida to the Kingdom of Great Britain for control of Havana, Cuba, which had been captured by the British during the Seven Years' War. It was part of a large expansion of British territory following the country's victory in the Seven Years' War. Almost the entire Spanish population left, taking along most of the remaining indigenous population to Cuba. The British soon constructed the King's Road connecting St. Augustine to Georgia. The road crossed the St. Johns River at a narrow point, which the Seminole called Wacca Pilatka and the British named ""Cow Ford"", both names ostensibly reflecting the fact that cattle were brought across the river there. Seminole Indians based in East Florida began raiding Georgia settlements, and offering havens for runaway slaves. The United States Army led increasingly frequent incursions into Spanish territory, including the 1817–1818 campaign against the Seminole Indians by Andrew Jackson that became known as the First Seminole War. The United States now effectively controlled East Florida. Control was necessary according to Secretary of State John Quincy Adams because Florida had become ""a derelict open to the occupancy of every enemy, civilized or savage, of the United States, and serving no other earthly purpose than as a post of annoyance to them."". NASCAR (headquartered in Daytona Beach) begins all three of its major auto racing series in Florida at Daytona International Speedway in February, featuring the Daytona 500, and ends all three Series in November at Homestead-Miami Speedway. Daytona also has the Coke Zero 400 NASCAR race weekend around Independence Day in July. The 24 Hours of Daytona is one of the world's most prestigious endurance auto races. The Grand Prix of St. Petersburg and Grand Prix of Miami have held IndyCar races as well. The climate of Florida is tempered somewhat by the fact that no part of the state is distant from the ocean. North of Lake Okeechobee, the prevalent climate is humid subtropical (Köppen: Cfa), while areas south of the lake (including the Florida Keys) have a true tropical climate (Köppen: Aw). Mean high temperatures for late July are primarily in the low 90s Fahrenheit (32–34 °C). Mean low temperatures for early to mid January range from the low 40s Fahrenheit (4–7 °C) in northern Florida to above 60 °F (16 °C) from Miami on southward. With an average daily temperature of 70.7 °F (21.5 °C), it is the warmest state in the country. Hurricanes pose a severe threat each year during the June 1 to November 30 hurricane season, particularly from August to October. Florida is the most hurricane-prone state, with subtropical or tropical water on a lengthy coastline. Of the category 4 or higher storms that have struck the United States, 83% have either hit Florida or Texas. From 1851 to 2006, Florida was struck by 114 hurricanes, 37 of them major—category 3 and above. It is rare for a hurricane season to pass without any impact in the state by at least a tropical storm.[citation needed] Florida is among the three states with the most severe felony disenfranchisement laws. Florida requires felons to have completed sentencing, parole and/or probation, and then seven years later, to apply individually for restoration of voting privileges. As in other aspects of the criminal justice system, this law has disproportionate effects for minorities. As a result, according to Brent Staples, based on data from The Sentencing Project, the effect of Florida's law is such that in 2014 ""[m]ore than one in ten Floridians – and nearly one in four African-American Floridians – are shut out of the polls because of felony convictions."""
Black_people,"From the late 19th century, the South used a colloquial term, the one-drop rule, to classify as black a person of any known African ancestry. This practice of hypodescent was not put into law until the early 20th century. Legally the definition varied from state to state. Racial definition was more flexible in the 18th and 19th centuries before the American Civil War. For instance, President Thomas Jefferson held persons who were legally white (less than 25% black) according to Virginia law at the time, but, because they were born to slave mothers, they were born into slavery, according to the principle of partus sequitur ventrem, which Virginia adopted into law in 1662. In the first 200 years that black people were in the United States, they primarily identified themselves by their specific ethnic group (closely allied to language) and not by skin color. Individuals identified themselves, for example, as Ashanti, Igbo, Bakongo, or Wolof. However, when the first captives were brought to the Americas, they were often combined with other groups from West Africa, and individual ethnic affiliations were not generally acknowledged by English colonists. In areas of the Upper South, different ethnic groups were brought together. This is significant as the captives came from a vast geographic region: the West African coastline stretching from Senegal to Angola and in some cases from the south-east coast such as Mozambique. A new African-American identity and culture was born that incorporated elements of the various ethnic groups and of European cultural heritage, resulting in fusions such as the Black church and Black English. This new identity was based on provenance and slave status rather than membership in any one ethnic group.[citation needed] By contrast, slave records from Louisiana show that the French and Spanish colonists recorded more complete identities of the West Africans, including ethnicities and given tribal names. Historians estimate that between the advent of Islam in 650CE and the abolition of slavery in the Arabian Peninsula in the mid-20th century, 10 to 18 million sub-Saharan Black Africans were enslaved by Arab slave traders and transported to the Arabian Peninsula and neighboring countries. This number far exceeded the number of slaves who were taken to the Americas. Several factors affected the visibility of descendants of this diaspora in 21st-century Arab societies: The traders shipped more female slaves than males, as there was a demand for them to serve as concubines in harems in the Arabian Peninsula and neighboring countries. Male slaves were castrated in order to serve as harem guards. The death toll of Black African slaves from forced labor was high. The mixed-race children of female slaves and Arab owners were assimilated into the Arab owners' families under the patrilineal kinship system. As a result, few distinctive Afro-Arab black communities have survived in the Arabian Peninsula and neighboring countries. According to Dr. Carlos Moore, resident scholar at Brazil's University of the State of Bahia, in the 21st century Afro-multiracials in the Arab world, including Arabs in North Africa, self-identify in ways that resemble multi-racials in Latin America. He claims that black-looking Arabs, much like black-looking Latin Americans, consider themselves white because they have some distant white ancestry. The second half of the 20th century to the present has seen a gradual shift towards improved human rights for Aboriginal people. In a 1967 referendum over 90% of the Australian population voted to end constitutional discrimination and to include Aborigines in the national census. During this period many Aboriginal activists began to embrace the term ""black"" and use their ancestry as a source of pride. Activist Bob Maza said: Some succeeded their fathers as rulers, such as Sultan Ahmad al-Mansur, who ruled Morocco from 1578 to 1608. He was not technically considered as a mixed-race child of a slave; his mother was Fulani and a concubine of his father. Such tolerance for black persons, even when technically ""free"", was not so common in Morocco. The long association of sub-Saharan peoples as slaves is shown in the term abd (Arabic: عبد‎,) (meaning ""slave""); it is still frequently used in the Arabic-speaking world as a term for black people. Additionally, there are around 60,000 non-Jewish African immigrants in Israel, some of whom have sought asylum. Most of the migrants are from communities in Sudan and Eritrea, particularly the Niger-Congo-speaking Nuba groups of the southern Nuba Mountains; some are illegal immigrants. The concept of blackness in the United States has been described as the degree to which one associates themselves with mainstream African-American culture, politics, and values. To a certain extent, this concept is not so much about race but more about political orientation, culture and behavior. Blackness can be contrasted with ""acting white"", where black Americans are said to behave with assumed characteristics of stereotypical white Americans with regard to fashion, dialect, taste in music, and possibly, from the perspective of a significant number of black youth, academic achievement. According to the Office for National Statistics, at the 2001 census there were over a million black people in the United Kingdom; 1% of the total population described themselves as ""Black Caribbean"", 0.8% as ""Black African"", and 0.2% as ""Black other"". Britain encouraged the immigration of workers from the Caribbean after World War II; the first symbolic movement was those who came on the ship the Empire Windrush. The preferred official umbrella term is ""black and minority ethnic"" (BME), but sometimes the term ""black"" is used on its own, to express unified opposition to racism, as in the Southall Black Sisters, which started with a mainly British Asian constituency, and the National Black Police Association, which has a membership of ""African, African-Caribbean and Asian origin"". In the post-apartheid era, the Constitution of South Africa has declared the country to be a ""Non-racial democracy"". In an effort to redress past injustices, the ANC government has introduced laws in support of affirmative action policies for Blacks; under these they define ""Black"" people to include ""Africans"", ""Coloureds"" and ""Asians"". Some affirmative action policies favor ""Africans"" over ""Coloureds"" in terms of qualifying for certain benefits. Some South Africans categorized as ""African Black"" say that ""Coloureds"" did not suffer as much as they did during apartheid. ""Coloured"" South Africans are known to discuss their dilemma by saying, ""we were not white enough under apartheid, and we are not black enough under the ANC (African National Congress)"".[citation needed] The Negritos are believed to be the first inhabitants of Southeast Asia. Once inhabiting Taiwan, Vietnam, and various other parts of Asia, they are now confined primarily to Thailand, the Malay Archipelago, and the Andaman and Nicobar Islands. Negrito means ""little black people"" in Spanish (negrito is the Spanish diminutive of negro, i.e., ""little black person""); it is what the Spaniards called the short-statured, hunter-gatherer autochthones that they encountered in the Philippines. Despite this, Negritos are never referred to as black today, and doing so would cause offense. The term Negrito itself has come under criticism in countries like Malaysia, where it is now interchangeable with the more acceptable Semang, although this term actually refers to a specific group. The common Thai word for Negritos literally means ""frizzy hair"". By that time, the majority of black people in the United States were native-born, so the use of the term ""African"" became problematic. Though initially a source of pride, many blacks feared that the use of African as an identity would be a hindrance to their fight for full citizenship in the US. They also felt that it would give ammunition to those who were advocating repatriating black people back to Africa. In 1835, black leaders called upon Black Americans to remove the title of ""African"" from their institutions and replace it with ""Negro"" or ""Colored American"". A few institutions chose to keep their historic names, such as the African Methodist Episcopal Church. African Americans popularly used the terms ""Negro"" or ""colored"" for themselves until the late 1960s. Alan Dershowitz described Sudan as an example of a government that ""actually deserve(s)"" the appellation ""apartheid."" Former Canadian Minister of Justice Irwin Cotler echoed the accusation. In South Africa, the period of colonization resulted in many unions and marriages between European men and African women from various tribes, resulting in mixed-race children. As the Europeans acquired territory and imposed rule over the Africans, they generally pushed mixed-race and Africans into second-class status. During the first half of the 20th century, the Afrikaaner-dominated government classified the population according to four main racial groups: Black, White, Asian (mostly Indian), and Coloured. The Coloured group included people of mixed Bantu, Khoisan, and European descent (with some Malay ancestry, especially in the Western Cape). The Coloured definition occupied an intermediary political position between the Black and White definitions in South Africa. It imposed a system of legal racial segregation, a complex of laws known as apartheid. By the 2000 census, demographic changes including the end to slavery, immigration from Europe and Asia, assimilation of multiracial persons, and other factors resulted in a population in which 6.2% of the population identified as black, 40% as pardo, and 55% as white. Essentially most of the black population was absorbed into the multi-racial category by intermixing. A 2007 genetic study found that at least 29% of the middle-class, white Brazilian population had some recent (since 1822 and the end of the colonial period) African ancestry. Being identified as either ""black"" or ""white"" in Australia during the 19th and early 20th centuries was critical in one's employment and social prospects. Various state-based Aboriginal Protection Boards were established which had virtually complete control over the lives of Indigenous Australians – where they lived, their employment, marriage, education and included the power to separate children from their parents. Aborigines were not allowed to vote and were often confined to reserves and forced into low paid or effectively slave labour. The social position of mixed-race or ""half-caste"" individuals varied over time. A 1913 report by Sir Baldwin Spencer states that: Numerous communities of dark-skinned peoples are present in North Africa, some dating from prehistoric communities. Others are descendants of the historical Trans-Saharan trade in peoples and/or, and after the Arab invasions of North Africa in the 7th century, descendants of slaves from the Arab Slave Trade in North Africa. In the 18th century, the Moroccan Sultan Moulay Ismail ""the Bloodthirsty"" (1672–1727) raised a corps of 150,000 black slaves, called his Black Guard, who coerced the country into submission. This nationwide acceptance and recognition of Aboriginal people led to a significant increase in the number of people self-identifying as Aboriginal or Torres Strait Islander. The reappropriation of the term ""black"" with a positive and more inclusive meaning has resulted in its widespread use in mainstream Australian culture, including public media outlets, government agencies, and private companies. In 2012, a number of high-profile cases highlighted the legal and community attitude that identifying as Aboriginal or Torres Strait Islander is not dependent on skin colour, with a well-known boxer Anthony Mundine being widely criticised for questioning the ""blackness"" of another boxer and journalist Andrew Bolt being successfully sued for publishing discriminatory comments about Aboriginals with light skin. Scholars disagree over the effects of social status on racial classifications in Brazil. It is generally believed that achieving upward mobility and education results in individuals being classified as a category of lighter skin. The popular claim is that in Brazil, poor whites are considered black and wealthy blacks are considered white. Some scholars disagree, arguing that ""whitening"" of one's social status may be open to people of mixed race, a large part of the population known as pardo, but a person perceived as preto (black) will continue to be classified as black regardless of wealth or social status. The Romans interacted with and later conquered parts of Mauretania, an early state that covered modern Morocco, western Algeria, and the Spanish cities Ceuta and Melilla during the classical period. The people of the region were noted in Classical literature as Mauri, which was subsequently rendered as Moors in English. From the years 1500 to 1850, an estimated 3.5 million captives were forcibly shipped from West/Central Africa to Brazil; the territory received the highest number of slaves of any country in the Americas. Scholars estimate that more than half of the Brazilian population is at least in part descended from these individuals. Brazil has the largest population of Afro-descendants outside of Africa. In contrast to the US, during the slavery period and after, the Portuguese colonial government and later Brazilian government did not pass formal anti-miscegenation or segregation laws. As in other Latin countries, intermarriage was prevalent during the colonial period and continued afterward. In addition, people of mixed race (pardo) often tended to marry white, and their descendants became accepted as white. As a result, some of the European descended population also has West African or Amerindian blood. According to the last census of the 20th century, in which Brazilians could choose from five color/ethnic categories with which they identified, 54% of individuals identified as white, 6.2% identified as black, and 39.5% identified as pardo (brown) — a broad multi-racial category, including tri-racial persons. These patterns of discrimination against non-whites have led some academic and other activists to advocate for use of the Portuguese term negro to encompass all African-descended people, in order to stimulate a ""black"" consciousness and identity. This proposal has been criticized since the term pardo is considered to include a wide range of multiracial people, such as caboclos (mestizos), assimilated Amerindians and tri-racials, not only people of partial African and European descent. Trying to identify this entire group as ""black"" would be a false imposition of a different identity from outside the culture and deny people their other, equally valid, ancestries and cultures. It seems a one-drop rule in reverse. In early 1991, non-Arabs of the Zaghawa tribe of Sudan attested that they were victims of an intensifying Arab apartheid campaign, segregating Arabs and non-Arabs (specifically people of sub-Saharan African descent). Sudanese Arabs, who controlled the government, were widely referred to as practicing apartheid against Sudan's non-Arab citizens. The government was accused of ""deftly manipulat(ing) Arab solidarity"" to carry out policies of apartheid and ethnic cleansing. Due to the Ottoman slave trade that had flourished in the Balkans, the coastal town of Ulcinj in Montenegro had its own black community. As a consequence of the slave trade and privateer activity, it is told how until 1878 in Ulcinj 100 black people lived. The Ottoman Army also deployed an estimated 30,000 Black African troops and cavalrymen to its expedition in Hungary during the Austro-Turkish War of 1716–18. Black people is a term used in certain countries, often in socially based systems of racial classification or of ethnicity, to describe persons who are perceived to be dark-skinned compared to other given populations. As such, the meaning of the expression varies widely both between and within societies, and depends significantly on context. For many other individuals, communities and countries, ""black"" is also perceived as a derogatory, outdated, reductive or otherwise unrepresentative label, and as a result is neither used nor defined. In 2008, the High Court in South Africa ruled that Chinese South Africans who were residents during the apartheid era (and their descendants) are to be reclassified as ""Black people,"" solely for the purposes of accessing affirmative action benefits, because they were also ""disadvantaged"" by racial discrimination. Chinese people who arrived in the country after the end of apartheid do not qualify for such benefits. The apartheid bureaucracy devised complex (and often arbitrary) criteria in the Population Registration Act of 1945 to determine who belonged in which group. Minor officials administered tests to enforce the classifications. When it was unclear from a person's physical appearance whether the individual should be considered Coloured or Black, the ""pencil test"" was used. A pencil was inserted into a person's hair to determine if the hair was kinky enough to hold the pencil, rather than having it pass through, as it would with smoother hair. If so, the person was classified as Black. Such classifications sometimes divided families. The U.S. census race definitions says a ""black"" is a person having origins in any of the black (sub-Saharan) racial groups of Africa. It includes people who indicate their race as ""Black, African Am., or Negro"" or who provide written entries such as African American, Afro-American, Kenyan, Nigerian, or Haitian. The Census Bureau notes that these classifications are socio-political constructs and should not be interpreted as scientific or anthropological. Most African Americans also have European ancestry in varying amounts; a lesser proportion have some Native American ancestry. For instance, genetic studies of African Americans show an ancestry that is on average 17–18% European. Critics note that people of color have limited media visibility. The Brazilian media has been accused of hiding or overlooking the nation's Black, Indigenous, Multiracial and East Asian populations. For example, the telenovelas or soaps are criticized for featuring actors who resemble northern Europeans rather than actors of the more prevalent Southern European features) and light-skinned mulatto and mestizo appearance. (Pardos may achieve ""white"" status if they have attained the middle-class or higher social status). Approximately 12 million Africans were shipped to the Americas during the Atlantic slave trade from 1492 to 1888, with 11.5 million of those shipped to South America and the Caribbean. Brazil was the largest importer in the Americas, with 5.5 million African slaves imported, followed by the British Caribbean with 2.76 million, the Spanish Caribbean and Spanish Mainland with 1.59 million Africans, and the French Caribbean with 1.32 million. Today their descendants number approximately 150 million in South America and the Caribbean. In addition to skin color, other physical characteristics such as facial features and hair texture are often variously used in classifying peoples as black in South America and the Caribbean. In South America and the Caribbean, classification as black is also closely tied to social status and socioeconomic variables, especially in light of social conceptions of ""blanqueamiento"" (racial whitening) and related concepts. The concept of race in Brazil is complex. A Brazilian child was never automatically identified with the racial type of one or both of his or her parents, nor were there only two categories to choose from. Between an individual of unmixed West African descent and a very light mulatto individual, more than a dozen racial categories were acknowledged, based on various combinations of hair color, hair texture, eye color, and skin color. These types grade into each other like the colors of the spectrum, and no one category stands significantly isolated from the rest. In Brazil, people are classified by appearance, not heredity. In 1988, the civil rights leader Jesse Jackson urged Americans to use instead the term ""African American"" because it had a historical cultural base and was a construction similar to terms used by European descendants, such as German American, Italian American, etc. Since then, African American and black have often had parallel status. However, controversy continues over which if any of the two terms is more appropriate. Maulana Karenga argues that the term African-American is more appropriate because it accurately articulates their geographical and historical origin.[citation needed] Others have argued that ""black"" is a better term because ""African"" suggests foreignness, although Black Americans helped found the United States. Still others believe that the term black is inaccurate because African Americans have a variety of skin tones. Some surveys suggest that the majority of Black Americans have no preference for ""African American"" or ""Black"", although they have a slight preference for ""black"" in personal settings and ""African American"" in more formal settings. During the apartheid era, those classed as ""Coloured"" were oppressed and discriminated against. But, they had limited rights and overall had slightly better socioeconomic conditions than those classed as ""Black"". The government required that Blacks and Coloureds live in areas separate from Whites, creating large townships located away from the cities as areas for Blacks. In July 2012, Ancestry.com reported on historic and DNA research by its staff that discovered that Obama is likely a descendant through his mother of John Punch, considered by some historians to be the first African slave in the Virginia colony. An indentured servant, he was ""bound for life"" in 1640 after trying to escape. The story of him and his descendants is that of multi-racial America since it appeared he and his sons married or had unions with white women, likely indentured servants and working-class like them. Their multi-racial children were free because they were born to free English women. Over time, Obama's line of the Bunch family (as they became known) were property owners and continued to ""marry white""; they became part of white society, likely by the early to mid-18th century. Though Brazilians of at least partial African heritage make up a large percentage of the population, few blacks have been elected as politicians. The city of Salvador, Bahia, for instance, is 80% people of color, but voters have not elected a mayor of color. Journalists like to say that US cities with black majorities, such as Detroit and New Orleans, have not elected white mayors since after the civil rights movement, when the Voting Rights Act of 1965 protected the franchise for minorities, and blacks in the South regained the power to vote for the first time since the turn of the 20th century. New Orleans elected its first black mayor in the 1970s. New Orleans elected a white mayor after the widescale disruption and damage of Hurricane Katrina in 2005. Egyptian President Anwar Sadat had a mother who was a dark-skinned Nubian Sudanese woman and a father who was a lighter-skinned Egyptian. In response to an advertisement for an acting position, as a young man he said, ""I am not white but I am not exactly black either. My blackness is tending to reddish"". Beginning several centuries ago, during the period of the Ottoman Empire, tens of thousands of Black Africans were brought by slave traders to plantations and agricultural areas situated between Antalya and Istanbul in present-day Turkey. Some of their descendants remained in situ, and many migrated to larger cities and towns. Other blacks slaves were transported to Crete, from where they or their descendants later reached the İzmir area through the population exchange between Greece and Turkey in 1923, or indirectly from Ayvalık in pursuit of work. Indigenous Australians have been referred to as ""black people"" in Australia since the early days of European settlement. While originally related to skin colour, the term is used to today to indicate Aboriginal or Torres Strait Islander ancestry in general and can refer to people of any skin pigmentation. The official policy became one of biological and cultural assimilation: ""Eliminate the full-blood and permit the white admixture to half-castes and eventually the race will become white"". This led to different treatment for ""black"" and ""half-caste"" individuals, with lighter-skinned individuals targeted for removal from their families to be raised as ""white"" people, restricted from speaking their native language and practising traditional customs, a process now known as the Stolen Generation. By the 1900s, nigger had become a pejorative word in the United States. In its stead, the term colored became the mainstream alternative to negro and its derived terms. After the African-American Civil rights movement, the terms colored and negro gave way to ""black"". Negro had superseded colored as the most polite word for African Americans at a time when black was considered more offensive. This term was accepted as normal, including by people classified as Negroes, until the later Civil Rights movement in the late 1960s. One well-known example is the identification by Reverend Martin Luther King, Jr. of his own race as ""Negro"" in his famous speech of 1963, I Have a Dream. During the American Civil Rights movement of the 1950s and 1960s, some African-American leaders in the United States, notably Malcolm X, objected to the word Negro because they associated it with the long history of slavery, segregation, and discrimination that treated African Americans as second-class citizens, or worse. Malcolm X preferred Black to Negro, but later gradually abandoned that as well for Afro-American after leaving the Nation of Islam. The term ""Moors"" has been used in Europe in a broader, somewhat derogatory sense to refer to Muslims, especially those of Arab or Berber descent, whether living in North Africa or Iberia. Moors were not a distinct or self-defined people. Medieval and early modern Europeans applied the name to Muslim Arabs, Berbers, Black Africans and Europeans alike. American University economist George Ayittey accused the Arab government of Sudan of practicing acts of racism against black citizens. According to Ayittey, ""In Sudan... the Arabs monopolized power and excluded blacks – Arab apartheid."" Many African commentators joined Ayittey in accusing Sudan of practising Arab apartheid. Genetic studies have found significant African female-mediated gene flow in Arab communities in the Arabian Peninsula and neighboring countries, with an average of 38% of maternal lineages in Yemen are of direct African descent, 16% in Oman-Qatar, and 10% in Saudi Arabia-United Arab Emirates. Different societies apply differing criteria regarding who is classified as ""black"", and these social constructs have also changed over time. In a number of countries, societal variables affect classification as much as skin color, and the social criteria for ""blackness"" vary. For example, in North America the term black people is not necessarily an indicator of skin color or majority ethnic ancestry, but it is instead a socially based racial classification related to being African American, with a family history associated with institutionalized slavery. In South Africa and Latin America, for instance, mixed-race people are generally not classified as ""black."" In South Pacific regions such as Australia and Melanesia, European colonists applied the term ""black"" or it was used by populations with different histories and ethnic origin. As African states became independent in the 1960s, the Soviet Union offered many of their citizens the chance to study in Russia. Over a period of 40 years, about 400,000 African students from various countries moved to Russia to pursue higher studies, including many Black Africans. This extended beyond the Soviet Union to many countries of the Eastern bloc. Due to the patriarchal nature of Arab society, Arab men, including during the slave trade in North Africa, enslaved more black women than men. They used more black female slaves in domestic service and agriculture than males. The men interpreted the Qur'an to permit sexual relations between a male master and his female slave outside of marriage (see Ma malakat aymanukum and sex), leading to many mixed-race children. When an enslaved woman became pregnant with her Arab master's child, she was considered as umm walad or ""mother of a child"", a status that granted her privileged rights. The child was given rights of inheritance to the father's property, so mixed-race children could share in any wealth of the father. Because the society was patrilineal, the children took their fathers' social status at birth and were born free. In the US, African Americans, who include multiracial people, earn 75% of what white people earn. In Brazil, people of color earn less than 50% of what whites earn. Some have posited that the facts of lower socioeconomic status for people of color suggest that Brazil practices a kind of one-drop rule, or discrimination against people who are not visibly European in ancestry. The gap in income between blacks and other non-whites is relatively small compared to the large gap between whites and all people of color. Other social factors, such as illiteracy and education levels, show the same patterns of disadvantage for people of color. Some commentators observe that the United States practice of segregation and white supremacy in the South, and discrimination in many areas outside that region, forced many African Americans to unite in the civil rights struggle. They suggest that the fluid nature of race in Brazil has divided individuals of African descent, between those with more or less ancestry. As a result, they have not united for a stronger civil rights movement.[citation needed] After the First World War, however, it became apparent that the number of mixed-race people was growing at a faster rate than the white population, and by 1930 fear of the ""half-caste menace"" undermining the White Australia ideal from within was being taken as a serious concern. Dr. Cecil Cook, the Northern Territory Protector of Natives, noted that: In 1978 Aboriginal writer Kevin Gilbert received the National Book Council award for his book Living Black: Blacks Talk to Kevin Gilbert, a collection of Aboriginal people's stories, and in 1998 was awarded (but refused to accept) the Human Rights Award for Literature for Inside Black Australia, a poetry anthology and exhibition of Aboriginal photography. In contrast to previous definitions based solely on the degree of Aboriginal ancestry, in 1990 the Government changed the legal definition of Aboriginal to include any: Distinctive and self-identified black communities have been reported in countries such as Iraq, with a reported 1.2 million black people, and they attest to a history of discrimination. African-Iraquis have sought minority status from the government, which would reserve some seats in Parliament for representatives of their population. According to Alamin M. Mazrui et al., generally in the Arabian Peninsula and neighboring countries, most of those of visible African descent are still classified and identify as Arab, not black. Afro-Spaniards are Spanish nationals of West/Central African descent. They today mainly come from Angola, Brazil, Cameroon, Cape Verde, Equatorial Guinea, Ghana, Gambia, Guinea-Bissau, Mali, Nigeria and Senegal. Additionally, many Afro-Spaniards born in Spain are from the former Spanish colony Equatorial Guinea. Today, there are an estimated 683,000 Afro-Spaniards in Spain. About 150,000 East African and black people live in Israel, amounting to just over 2% of the nation's population. The vast majority of these, some 120,000, are Beta Israel, most of whom are recent immigrants who came during the 1980s and 1990s from Ethiopia. In addition, Israel is home to over 5,000 members of the African Hebrew Israelites of Jerusalem movement that are descendants of African Americans who emigrated to Israel in the 20th century, and who reside mainly in a distinct neighborhood in the Negev town of Dimona. Unknown numbers of black converts to Judaism reside in Israel, most of them converts from the United Kingdom, Canada, and the United States. The Siddi are an ethnic group inhabiting India and Pakistan whose members are descended from Bantu peoples from Southeast Africa that were brought to the Indian subcontinent as slaves by Arab and Portuguese merchants. Although it is commonly believed locally that ""Siddi"" derives from a word meaning ""black"", the term is actually derived from ""Sayyid"", the title borne by the captains of the Arab vessels that first brought Siddi settlers to the area. In the Makran strip of the Sindh and Balochistan provinces in southwestern Pakistan, these Bantu descendants are known as the Makrani. There was a brief ""Black Power"" movement in Sindh in the 1960s and many Siddi are proud of and celebrate their African ancestry. Isidore of Seville, writing in the 7th century, claimed that the Latin word Maurus was derived from the Greek mauron, μαύρον, which is the Greek word for black. Indeed, by the time Isidore of Seville came to write his Etymologies, the word Maurus or ""Moor"" had become an adjective in Latin, ""for the Greeks call black, mauron"". ""In Isidore’s day, Moors were black by definition…"" In the Colonial America of 1619, John Rolfe used negars in describing the slaves who were captured from West Africa and then shipped to the Virginia colony. Later American English spellings, neger and neggar, prevailed in a northern colony, New York under the Dutch, and in metropolitan Philadelphia's Moravian and Pennsylvania Dutch communities; the African Burial Ground in New York City originally was known by the Dutch name ""Begraafplaats van de Neger"" (Cemetery of the Negro); an early US occurrence of neger in Rhode Island, dates from 1625. Thomas Jefferson also used the term ""black"" in his Notes on the State of Virginia in allusion to the slave populations. Other than by appearance, ""Coloureds"" can usually be distinguished from ""Blacks"" by language. Most speak Afrikaans or English as a first language, as opposed to Bantu languages such as Zulu or Xhosa. They also tend to have more European-sounding names than Bantu names. Sandra Laing is a South African woman who was classified as Coloured by authorities during the apartheid era, due to her skin colour and hair texture, although her parents could prove at least three generations of European ancestors. At age 10, she was expelled from her all-white school. The officials' decisions based on her anomalous appearance disrupted her family and adult life. She was the subject of the 2008 biographical dramatic film Skin, which won numerous awards. Because of the acceptance of miscegenation, Brazil has avoided the binary polarization of society into black and white. In addition, it abolished slavery without a civil war. The bitter and sometimes violent racial tensions that have divided the US are notably absent in Brazil. According to the 2010 census, 6.7% of Brazilians said they were black, compared with 6.2% in 2000, and 43.1% said they were racially mixed, up from 38.5%. In 2010, Elio Ferreira de Araujo, Brazil's minister for racial equality, attributed the increases to growing pride among his country's black and indigenous communities. Due to the often political and cultural contours of blackness in the United States, the notion of blackness can also be extended to non-black people. Toni Morrison once described Bill Clinton as the first black President of the United States, because, as she put it, he displayed ""almost every trope of blackness"". Christopher Hitchens was offended by the notion of Clinton as the first black president, noting, ""Mr Clinton, according to Toni Morrison, the Nobel Prize-winning novelist, is our first black President, the first to come from the broken home, the alcoholic mother, the under-the-bridge shadows of our ranking systems. Thus, we may have lost the mystical power to divine diabolism, but we can still divine blackness by the following symptoms: broken homes, alcoholic mothers, under-the-bridge habits and (presumable from the rest of [Arthur] Miller's senescent musings) the tendency to sexual predation and to shameless perjury about same."" Some black activists were also offended, claiming that Clinton used his knowledge of black culture to exploit black people for political gain as no other president had before, while not serving black interests. They cite the lack of action during the Rwandan Genocide and his welfare reform, which Larry Roberts said had led to the worst child poverty since the 1960s. Others cited that the number of black people in jail increased during his administration. The US racial or ethnic classification ""black"" refers to people with all possible kinds of skin pigmentation, from the darkest through to the very lightest skin colors, including albinos, if they are believed by others to have West African ancestry (in any discernible percentage), or to exhibit cultural traits associated with being ""African American"". As a result, in the United States the term ""black people"" is not an indicator of skin color or ethnic origin but is instead a socially based racial classification related to being African American, with a family history associated with institutionalized slavery. Relatively dark-skinned people can be classified as white if they fulfill other social criteria of ""whiteness"", and relatively light-skinned people can be classified as black if they fulfill the social criteria for ""blackness"" in a particular setting."
MP3,"Encoder / decoder overall delay is not defined, which means there is no official provision for gapless playback. However, some encoders such as LAME can attach additional metadata that will allow players that can handle it to deliver seamless playback. An MP3 file is made up of MP3 frames, which consist of a header and a data block. This sequence of frames is called an elementary stream. Due to the ""byte reservoir"", frames are not independent items and cannot usually be extracted on arbitrary frame boundaries. The MP3 Data blocks contain the (compressed) audio information in terms of frequencies and amplitudes. The diagram shows that the MP3 Header consists of a sync word, which is used to identify the beginning of a valid frame. This is followed by a bit indicating that this is the MPEG standard and two bits that indicate that layer 3 is used; hence MPEG-1 Audio Layer 3 or MP3. After this, the values will differ, depending on the MP3 file. ISO/IEC 11172-3 defines the range of values for each section of the header along with the specification of the header. Most MP3 files today contain ID3 metadata, which precedes or follows the MP3 frames, as noted in the diagram. In November 1997, the website mp3.com was offering thousands of MP3s created by independent artists for free. The small size of MP3 files enabled widespread peer-to-peer file sharing of music ripped from CDs, which would have previously been nearly impossible. The first large peer-to-peer filesharing network, Napster, was launched in 1999. Perceived quality can be influenced by listening environment (ambient noise), listener attention, and listener training and in most cases by listener audio equipment (such as sound cards, speakers and headphones). A ""tag"" in an audio file is a section of the file that contains metadata such as the title, artist, album, track number or other information about the file's contents. The MP3 standards do not define tag formats for MP3 files, nor is there a standard container format that would support metadata and obviate the need for tags. Alcatel-Lucent has asserted several MP3 coding and compression patents, allegedly inherited from AT&T-Bell Labs, in litigation of its own. In November 2006, before the companies' merger, Alcatel sued Microsoft for allegedly infringing seven patents. On 23 February 2007, a San Diego jury awarded Alcatel-Lucent US $1.52 billion in damages for infringement of two of them. The court subsequently tossed the award, however, finding that one patent had not been infringed and that the other was not even owned by Alcatel-Lucent; it was co-owned by AT&T and Fraunhofer, who had licensed it to Microsoft, the judge ruled. That defense judgment was upheld on appeal in 2008. See Alcatel-Lucent v. Microsoft for more information. The use of lossy compression is designed to greatly reduce the amount of data required to represent the audio recording and still sound like a faithful reproduction of the original uncompressed audio for most listeners. An MP3 file that is created using the setting of 128 kbit/s will result in a file that is about 1/11 the size of the CD file created from the original audio source (44,100 samples per second × 16 bits per sample × 2 channels = 1,411,200 bit/s; MP3 compressed at 128 kbit/s: 128,000 bit/s [1 k = 1,000, not 1024, because it is a bit rate]. Ratio: 1,411,200/128,000 = 11.025). An MP3 file can also be constructed at higher or lower bit rates, with higher or lower resulting quality. The immediate predecessors of MP3 were ""Optimum Coding in the Frequency Domain"" (OCF), and Perceptual Transform Coding (PXFM). These two codecs, along with block-switching contributions from Thomson-Brandt, were merged into a codec called ASPEC, which was submitted to MPEG, and which won the quality competition, but that was mistakenly rejected as too complex to implement. The first practical implementation of an audio perceptual coder (OCF) in hardware (Krasner's hardware was too cumbersome and slow for practical use), was an implementation of a psychoacoustic transform coder based on Motorola 56000 DSP chips. As sound scholar Jonathan Sterne notes, ""An Australian hacker acquired l3enc using a stolen credit card. The hacker then reverse-engineered the software, wrote a new user interface, and redistributed it for free, naming it ""thank you Fraunhofer"""". As a doctoral student at Germany's University of Erlangen-Nuremberg, Karlheinz Brandenburg began working on digital music compression in the early 1980s, focusing on how people perceive music. He completed his doctoral work in 1989. MP3 is directly descended from OCF and PXFM, representing the outcome of the collaboration of Brandenburg—working as a postdoc at AT&T-Bell Labs with James D. Johnston (""JJ"") of AT&T-Bell Labs—with the Fraunhofer Institut for Integrated Circuits, Erlangen, with relatively minor contributions from the MP2 branch of psychoacoustic sub-band coders. In 1990, Brandenburg became an assistant professor at Erlangen-Nuremberg. While there, he continued to work on music compression with scientists at the Fraunhofer Society (in 1993 he joined the staff of the Fraunhofer Institute). Technicolor (formerly called Thomson Consumer Electronics) claims to control MP3 licensing of the Layer 3 patents in many countries, including the United States, Japan, Canada and EU countries. Technicolor has been actively enforcing these patents. With too low a bit rate, compression artifacts (i.e., sounds that were not present in the original recording) may be audible in the reproduction. Some audio is hard to compress because of its randomness and sharp attacks. When this type of audio is compressed, artifacts such as ringing or pre-echo are usually heard. A sample of applause compressed with a relatively low bit rate provides a good example of compression artifacts. ReplayGain is a standard for measuring and storing the loudness of an MP3 file (audio normalization) in its metadata tag, enabling a ReplayGain-compliant player to automatically adjust the overall playback volume for each file. MP3Gain may be used to reversibly modify files based on ReplayGain measurements so that adjusted playback can be achieved on players without ReplayGain capability. A more sophisticated MP3 encoder can produce variable bitrate audio. MPEG audio may use bitrate switching on a per-frame basis, but only layer III decoders must support it. VBR is used when the goal is to achieve a fixed level of quality. The final file size of a VBR encoding is less predictable than with constant bitrate. Average bitrate is VBR implemented as a compromise between the two: the bitrate is allowed to vary for more consistent quality, but is controlled to remain near an average value chosen by the user, for predictable file sizes. Although an MP3 decoder must support VBR to be standards compliant, historically some decoders have bugs with VBR decoding, particularly before VBR encoders became widespread. Further work on MPEG audio was finalized in 1994 as part of the second suite of MPEG standards, MPEG-2, more formally known as international standard ISO/IEC 13818-3 (a.k.a. MPEG-2 Part 3 or backwards compatible MPEG-2 Audio or MPEG-2 Audio BC), originally published in 1995. MPEG-2 Part 3 (ISO/IEC 13818-3) defined additional bit rates and sample rates for MPEG-1 Audio Layer I, II and III. The new sampling rates are exactly half that of those originally defined in MPEG-1 Audio. This reduction in sampling rate serves to cut the available frequency fidelity in half while likewise cutting the bitrate by 50%. MPEG-2 Part 3 also enhanced MPEG-1's audio by allowing the coding of audio programs with more than two channels, up to 5.1 multichannel. Other lossy formats exist. Among these, mp3PRO, AAC, and MP2 are all members of the same technological family as MP3 and depend on roughly similar psychoacoustic models. The Fraunhofer Gesellschaft owns many of the basic patents underlying these formats as well, with others held by Dolby Labs, Sony, Thomson Consumer Electronics, and AT&T. Besides the bit rate of an encoded piece of audio, the quality of MP3 files also depends on the quality of the encoder itself, and the difficulty of the signal being encoded. As the MP3 standard allows quite a bit of freedom with encoding algorithms, different encoders may feature quite different quality, even with identical bit rates. As an example, in a public listening test featuring two different MP3 encoders at about 128 kbit/s, one scored 3.66 on a 1–5 scale, while the other scored only 2.22. Due to the tree structure of the filter bank, pre-echo problems are made worse, as the combined impulse response of the two filter banks does not, and cannot, provide an optimum solution in time/frequency resolution. Additionally, the combining of the two filter banks' outputs creates aliasing problems that must be handled partially by the ""aliasing compensation"" stage; however, that creates excess energy to be coded in the frequency domain, thereby decreasing coding efficiency.[citation needed] A working group consisting of Leon van de Kerkhof (The Netherlands), Gerhard Stoll (Germany), Leonardo Chiariglione (Italy), Yves-François Dehery (France), Karlheinz Brandenburg (Germany) and James D. Johnston (USA) took ideas from ASPEC, integrated the filter bank from Layer 2, added some of their own ideas and created MP3, which was designed to achieve the same quality at 128 kbit/s as MP2 at 192 kbit/s. Besides lossy compression methods, lossless formats are a significant alternative to MP3 because they provide unaltered audio content, though with an increased file size compared to lossy compression. Lossless formats include FLAC (Free Lossless Audio Codec), Apple Lossless and many others. When performing lossy audio encoding, such as creating an MP3 file, there is a trade-off between the amount of space used and the sound quality of the result. Typically, the creator is allowed to set a bit rate, which specifies how many kilobits the file may use per second of audio. The higher the bit rate, the larger the compressed file will be, and, generally, the closer it will sound to the original file. MPEG-1 or MPEG-2 Audio Layer III, more commonly referred to as MP3, is an audio coding format for digital audio which uses a form of lossy data compression. It is a common audio format for consumer audio streaming or storage, as well as a de facto standard of digital audio compression for the transfer and playback of music on most digital audio players. An additional extension to MPEG-2 is named MPEG-2.5 audio, as MPEG-3 already had a different meaning. This extension was developed at Fraunhofer IIS, the registered MP3 patent holders. Like MPEG-2, MPEG-2.5 adds new sampling rates exactly half of that previously possible with MPEG-2. It thus widens the scope of MP3 to include human speech and other applications requiring only 25% of the frequency reproduction possible with MPEG-1. While not an ISO recognized standard, MPEG-2.5 is widely supported by both inexpensive and brand name digital audio players as well as computer software based MP3 encoders and decoders. A sample rate comparison between MPEG-1, 2 and 2.5 is given further down. MPEG-2.5 was not developed by MPEG and was never approved as an international standard. MPEG-2.5 is thus an unofficial or proprietary extension to the MP3 format. A test given to new students by Stanford University Music Professor Jonathan Berger showed that student preference for MP3-quality music has risen each year. Berger said the students seem to prefer the 'sizzle' sounds that MP3s bring to music. Karlheinz Brandenburg used a CD recording of Suzanne Vega's song ""Tom's Diner"" to assess and refine the MP3 compression algorithm. This song was chosen because of its nearly monophonic nature and wide spectral content, making it easier to hear imperfections in the compression format during playbacks. Some refer to Suzanne Vega as ""The mother of MP3"". This particular track has an interesting property in that the two channels are almost, but not completely, the same, leading to a case where Binaural Masking Level Depression causes spatial unmasking of noise artifacts unless the encoder properly recognizes the situation and applies corrections similar to those detailed in the MPEG-2 AAC psychoacoustic model. Some more critical audio excerpts (glockenspiel, triangle, accordion, etc.) were taken from the EBU V3/SQAM reference compact disc and have been used by professional sound engineers to assess the subjective quality of the MPEG Audio formats. Uncompressed audio as stored on an audio-CD has a bit rate of 1,411.2 kbit/s, (16 bit/sample × 44100 samples/second × 2 channels / 1000 bits/kilobit), so the bitrates 128, 160 and 192 kbit/s represent compression ratios of approximately 11:1, 9:1 and 7:1 respectively. However, several de facto standards for tag formats exist. As of 2010, the most widespread are ID3v1 and ID3v2, and the more recently introduced APEv2. These tags are normally embedded at the beginning or end of MP3 files, separate from the actual MP3 frame data. MP3 decoders either extract information from the tags, or just treat them as ignorable, non-MP3 junk data. Unauthorized MP3 file sharing continues on next-generation peer-to-peer networks. Some authorized services, such as Beatport, Bleep, Juno Records, eMusic, Zune Marketplace, Walmart.com, Rhapsody, the recording industry approved re-incarnation of Napster, and Amazon.com sell unrestricted music in the MP3 format. Compression efficiency of encoders is typically defined by the bit rate, because compression ratio depends on the bit depth and sampling rate of the input signal. Nevertheless, compression ratios are often published. They may use the Compact Disc (CD) parameters as references (44.1 kHz, 2 channels at 16 bits per channel or 2×16 bit), or sometimes the Digital Audio Tape (DAT) SP parameters (48 kHz, 2×16 bit). Compression ratios with this latter reference are higher, which demonstrates the problem with use of the term compression ratio for lossy encoders. The basic MP3 decoding and encoding technology is patent-free in the European Union, all patents having expired there. In the United States, the technology will be substantially patent-free on 31 December 2017 (see below). The majority of MP3 patents expired in the US between 2007 and 2015. A sample rate of 44.1 kHz is almost always used, because this is also used for CD audio, the main source used for creating MP3 files. A greater variety of bit rates are used on the Internet. The rate of 128 kbit/s is commonly used, at a compression ratio of 11:1, offering adequate audio quality in a relatively small space. As Internet bandwidth availability and hard drive sizes have increased, higher bit rates up to 320 kbit/s are widespread. Non-standard bit rates up to 640 kbit/s can be achieved with the LAME encoder and the freeformat option, although few MP3 players can play those files. According to the ISO standard, decoders are only required to be able to decode streams up to 320 kbit/s. In February 2007, Texas MP3 Technologies sued Apple, Samsung Electronics and Sandisk in eastern Texas federal court, claiming infringement of a portable MP3 player patent that Texas MP3 said it had been assigned. Apple, Samsung, and Sandisk all settled the claims against them in January 2009. During encoding, 576 time-domain samples are taken and are transformed to 576 frequency-domain samples.[clarification needed] If there is a transient, 192 samples are taken instead of 576. This is done to limit the temporal spread of quantization noise accompanying the transient. (See psychoacoustics.) In the past, many organizations have claimed ownership of patents related to MP3 decoding or encoding. These claims led to a number of legal threats and actions from a variety of sources. As a result, uncertainty about which patents must be licensed in order to create MP3 products without committing patent infringement in countries that allow software patents was a common feature of the early stages of adoption of the technology. An in-depth study of MP3 audio quality, sound artist and composer Ryan Maguire's project ""The Ghost in the MP3"" isolates the sounds lost during MP3 compression. In 2015, he released the track ""moDernisT"" (an anagram of ""Tom's Diner""), composed exclusively from the sounds deleted during MP3 compression of the song ""Tom's Diner"", the track originally used in the formulation of the MP3 standard. A detailed account of the techniques used to isolate the sounds deleted during MP3 compression, along with the conceptual motivation for the project, was published in the 2014 Proceedings of the International Computer Music Conference. In September 2006, German officials seized MP3 players from SanDisk's booth at the IFA show in Berlin after an Italian patents firm won an injunction on behalf of Sisvel against SanDisk in a dispute over licensing rights. The injunction was later reversed by a Berlin judge, but that reversal was in turn blocked the same day by another judge from the same court, ""bringing the Patent Wild West to Germany"" in the words of one commentator. In 1991, there were only two proposals available that could be completely assessed for an MPEG audio standard: Musicam (Masking pattern adapted Universal Subband Integrated Coding And Multiplexing) and ASPEC (Adaptive Spectral Perceptual Entropy Coding). The Musicam technique, as proposed by Philips (the Netherlands), CCETT (France) and Institut für Rundfunktechnik (Germany) was chosen due to its simplicity and error robustness, as well as its low computational power associated with the encoding of high quality compressed audio. The Musicam format, based on sub-band coding, was the basis of the MPEG Audio compression format (sampling rates, structure of frames, headers, number of samples per frame). Several bit rates are specified in the MPEG-1 Audio Layer III standard: 32, 40, 48, 56, 64, 80, 96, 112, 128, 160, 192, 224, 256 and 320 kbit/s, with available sampling frequencies of 32, 44.1 and 48 kHz. MPEG-2 Audio Layer III allows bit rates of 8, 16, 24, 32, 40, 48, 56, 64, 80, 96, 112, 128, 144, 160 kbit/s with sampling frequencies of 16, 22.05 and 24 kHz. MPEG-2.5 Audio Layer III is restricted to bit rates of 8, 16, 24, 32, 40, 48, 56 and 64 kbit/s with sampling frequencies of 8, 11.025, and 12 kHz.[citation needed] Because of the Nyquist–Shannon sampling theorem, frequency reproduction is always strictly less than half of the sampling frequency, and imperfect filters requires a larger margin for error (noise level versus sharpness of filter), so 8 kHz sampling rate limits the maximum frequency to 4 kHz, while 48 kHz maximum sampling rate limits an MP3 to 24 kHz sound reproduction. ASPEC was the joint proposal of AT&T Bell Laboratories, Thomson Consumer Electronics, Fraunhofer Society and CNET. It provided the highest coding efficiency. Sisvel S.p.A. and its U.S. subsidiary Audio MPEG, Inc. previously sued Thomson for patent infringement on MP3 technology, but those disputes were resolved in November 2005 with Sisvel granting Thomson a license to their patents. Motorola followed soon after, and signed with Sisvel to license MP3-related patents in December 2005. Except for three patents, the US patents administered by Sisvel had all expired in 2015, however (the exceptions are: U.S. Patent 5,878,080, expires February 2017, U.S. Patent 5,850,456, expires February 2017 and U.S. Patent 5,960,037, expires 9. April 2017. Much of its technology and ideas were incorporated into the definition of ISO MPEG Audio Layer I and Layer II and the filter bank alone into Layer III (MP3) format as part of the computationally inefficient hybrid filter bank. Under the chairmanship of Professor Musmann (University of Hannover) the editing of the standard was made under the responsibilities of Leon van de Kerkhof (Layer I) and Gerhard Stoll (Layer II). An exception is the United States, where patents filed prior to 8 June 1995 expire 17 years after the publication date of the patent, but application extensions make it possible for a patent to issue much later than normally expected (see submarine patents). The various MP3-related patents expire on dates ranging from 2007 to 2017 in the U.S. Patents filed for anything disclosed in ISO CD 11172 a year or more after its publication are questionable. If only the known MP3 patents filed by December 1992 are considered, then MP3 decoding has been patent-free in the US since 22 September 2015 when U.S. Patent 5,812,672 expired which had a PCT filing in October 1992. If the longest-running patent mentioned in the aforementioned references is taken as a measure, then the MP3 technology will be patent-free in the United States on 30 December 2017 when U.S. Patent 5,703,999, held by the Fraunhofer-Gesellschaft and administered by Technicolor, expires. The psychoacoustic masking codec was first proposed in 1979, apparently independently, by Manfred R. Schroeder, et al. from Bell Telephone Laboratories, Inc. in Murray Hill, NJ, and M. A. Krasner both in the United States. Krasner was the first to publish and to produce hardware for speech (not usable as music bit compression), but the publication of his results as a relatively obscure Lincoln Laboratory Technical Report did not immediately influence the mainstream of psychoacoustic codec development. Manfred Schroeder was already a well-known and revered figure in the worldwide community of acoustical and electrical engineers, but his paper was not much noticed, since it described negative results due to the particular nature of speech and the linear predictive coding (LPC) gain present in speech. Both Krasner and Schroeder built upon the work performed by Eberhard F. Zwicker in the areas of tuning and masking of critical bands, that in turn built on the fundamental research in the area from Bell Labs of Harvey Fletcher and his collaborators. A wide variety of (mostly perceptual) audio compression algorithms were reported in IEEE's refereed Journal on Selected Areas in Communications. That journal reported in February 1988 on a wide range of established, working audio bit compression technologies, some of them using auditory masking as part of their fundamental design, and several showing real-time hardware implementations. The compression works by reducing the accuracy of certain parts of a sound that are considered to be beyond the auditory resolution ability of most people. This method is commonly referred to as perceptual coding. It uses psychoacoustic models to discard or reduce precision of components less audible to human hearing, and then records the remaining information in an efficient manner. There is no scale factor band 21 (sfb21) for frequencies above approx 16 kHz, forcing the encoder to choose between less accurate representation in band 21 or less efficient storage in all bands below band 21, the latter resulting in wasted bitrate in VBR encoding. On 7 July 1994, the Fraunhofer Society released the first software MP3 encoder called l3enc. The filename extension .mp3 was chosen by the Fraunhofer team on 14 July 1995 (previously, the files had been named .bit). With the first real-time software MP3 player WinPlay3 (released 9 September 1995) many people were able to encode and play back MP3 files on their PCs. Because of the relatively small hard drives back in that time (~ 500–1000 MB) lossy compression was essential to store non-instrument based (see tracker and MIDI) music for playback on computer. The song ""Tom's Diner"" by Suzanne Vega was the first song used by Karlheinz Brandenburg to develop the MP3. Brandenburg adopted the song for testing purposes, listening to it again and again each time refining the scheme, making sure it did not adversely affect the subtlety of Vega's voice. In the second half of '90s, MP3 files began to spread on the Internet. The popularity of MP3s began to rise rapidly with the advent of Nullsoft's audio player Winamp, released in 1997. In 1998, the first portable solid state digital audio player MPMan, developed by SaeHan Information Systems which is headquartered in Seoul, South Korea, was released and the Rio PMP300 was sold afterwards in 1998, despite legal suppression efforts by the RIAA. The ease of creating and sharing MP3s resulted in widespread copyright infringement. Major record companies argued that this free sharing of music reduced sales, and called it ""music piracy"". They reacted by pursuing lawsuits against Napster (which was eventually shut down and later sold) and against individual users who engaged in file sharing. The simplest type of MP3 file uses one bit rate for the entire file: this is known as Constant Bit Rate (CBR) encoding. Using a constant bit rate makes encoding simpler and faster. However, it is also possible to create files where the bit rate changes throughout the file. These are known as Variable Bit Rate (VBR) files. The idea behind this is that, in any piece of audio, some parts will be much easier to compress, such as silence or music containing only a few instruments, while others will be more difficult to compress. So, the overall quality of the file may be increased by using a lower bit rate for the less complex passages and a higher one for the more complex parts. With some encoders, it is possible to specify a given quality, and the encoder will vary the bit rate accordingly. Users who know a particular ""quality setting"" that is transparent to their ears can use this value when encoding all of their music, and generally speaking not need to worry about performing personal listening tests on each piece of music to determine the correct bit rate. Decoding, on the other hand, is carefully defined in the standard. Most decoders are ""bitstream compliant"", which means that the decompressed output that they produce from a given MP3 file will be the same, within a specified degree of rounding tolerance, as the output specified mathematically in the ISO/IEC high standard document (ISO/IEC 11172-3). Therefore, comparison of decoders is usually based on how computationally efficient they are (i.e., how much memory or CPU time they use in the decoding process). There are also open compression formats like Opus and Vorbis that are available free of charge and without any known patent restrictions. Some of the newer audio compression formats, such as AAC, WMA Pro and Vorbis, are free of some limitations inherent to the MP3 format that cannot be overcome by any MP3 encoder. The initial near-complete MPEG-1 standard (parts 1, 2 and 3) was publicly available on 6 December 1991 as ISO CD 11172. In most countries, patents cannot be filed after prior art has been made public, and patents expire 20 years after the initial filing date, which can be up to 12 months later for filings in other countries. As a result, patents required to implement MP3 expired in most countries by December 2012, 21 years after the publication of ISO CD 11172. All algorithms for MPEG-1 Audio Layer I, II and III were approved in 1991 and finalized in 1992 as part of MPEG-1, the first standard suite by MPEG, which resulted in the international standard ISO/IEC 11172-3 (a.k.a. MPEG-1 Audio or MPEG-1 Part 3), published in 1993. A reference simulation software implementation, written in the C language and later known as ISO 11172-5, was developed (in 1991–1996) by the members of the ISO MPEG Audio committee in order to produce bit compliant MPEG Audio files (Layer 1, Layer 2, Layer 3). It was approved as a committee draft of ISO/IEC technical report in March 1994 and printed as document CD 11172-5 in April 1994. It was approved as a draft technical report (DTR/DIS) in November 1994, finalized in 1996 and published as international standard ISO/IEC TR 11172-5:1998 in 1998. The reference software in C language was later published as a freely available ISO standard. Working in non-real time on a number of operating systems, it was able to demonstrate the first real time hardware decoding (DSP based) of compressed audio. Some other real time implementation of MPEG Audio encoders were available for the purpose of digital broadcasting (radio DAB, television DVB) towards consumer receivers and set top boxes. In September 1998, the Fraunhofer Institute sent a letter to several developers of MP3 software stating that a license was required to ""distribute and/or sell decoders and/or encoders"". The letter claimed that unlicensed products ""infringe the patent rights of Fraunhofer and Thomson. To make, sell and/or distribute products using the [MPEG Layer-3] standard and thus our patents, you need to obtain a license under these patents from us."" The MP3 lossy audio data compression algorithm takes advantage of a perceptual limitation of human hearing called auditory masking. In 1894, the American physicist Alfred M. Mayer reported that a tone could be rendered inaudible by another tone of lower frequency. In 1959, Richard Ehmer described a complete set of auditory curves regarding this phenomenon. Ernst Terhardt et al. created an algorithm describing auditory masking with high accuracy. This work added to a variety of reports from authors dating back to Fletcher, and to the work that initially determined critical ratios and critical bandwidths. The MPEG-1 standard does not include a precise specification for an MP3 encoder, but does provide example psychoacoustic models, rate loop, and the like in the non-normative part of the original standard. At present, these suggested implementations are quite dated. Implementers of the standard were supposed to devise their own algorithms suitable for removing parts of the information from the audio input. As a result, there are many different MP3 encoders available, each producing files of differing quality. Comparisons are widely available, so it is easy for a prospective user of an encoder to research the best choice. An encoder that is proficient at encoding at higher bit rates (such as LAME) is not necessarily as good at lower bit rates. MP3 was designed by the Moving Picture Experts Group (MPEG) as part of its MPEG-1 standard and later extended in the MPEG-2 standard. The first subgroup for audio was formed by several teams of engineers at Fraunhofer IIS, University of Hannover, AT&T-Bell Labs, Thomson-Brandt, CCETT, and others. MPEG-1 Audio (MPEG-1 Part 3), which included MPEG-1 Audio Layer I, II and III was approved as a committee draft of ISO/IEC standard in 1991, finalised in 1992 and published in 1993 (ISO/IEC 11172-3:1993). Backwards compatible MPEG-2 Audio (MPEG-2 Part 3) with additional bit rates and sample rates was published in 1995 (ISO/IEC 13818-3:1995). Layer III audio can also use a ""bit reservoir"", a partially full frame's ability to hold part of the next frame's audio data, allowing temporary changes in effective bitrate, even in a constant bitrate stream. Internal handling of the bit reservoir increases encoding delay.[citation needed] Early MPEG Layer III encoders used what is now called Constant Bit Rate (CBR). The software was only able to use a uniform bitrate on all frames in an MP3 file. Later more sophisticated MP3 encoders were able to use the bit reservoir to target an average bit rate selecting the encoding rate for each frame based on the complexity of the sound in that portion of the recording."
Dwight_D._Eisenhower,"In late 1952 Eisenhower went to Korea and discovered a military and political stalemate. Once in office, when the Chinese began a buildup in the Kaesong sanctuary, he threatened to use nuclear force if an armistice was not concluded. His earlier military reputation in Europe was effective with the Chinese. The National Security Council, the Joint Chiefs of Staff, and the Strategic Air Command (SAC) devised detailed plans for nuclear war against China. With the death of Stalin in early March 1953, Russian support for a Chinese hard-line weakened and China decided to compromise on the prisoner issue. French cooperation was deemed necessary to the campaign, and Eisenhower encountered a ""preposterous situation"" with the multiple rival factions in France. His primary objective was to move forces successfully into Tunisia, and intending to facilitate that objective, he gave his support to François Darlan as High Commissioner in North Africa, despite Darlan's previous high offices of state in Vichy France and his continued role as commander-in-chief of the French armed forces. The Allied leaders were ""thunderstruck"" by this from a political standpoint, though none of them had offered Eisenhower guidance with the problem in the course of planning the operation. Eisenhower was severely criticized for the move. Darlan was assassinated on December 24 by Fernand Bonnier de La Chapelle. Eisenhower did not take action to prevent the arrest and extrajudicial execution of Bonnier de La Chapelle by associates of Darlan acting without authority from either Vichy or the Allies, considering it a criminal rather than a military matter. Eisenhower later appointed General Henri Giraud as High Commissioner, who had been installed by the Allies as Darlan's commander-in-chief, and who had refused to postpone the execution. The D-Day Normandy landings on June 6, 1944, were costly but successful. A month later, the invasion of Southern France took place, and control of forces in the southern invasion passed from the AFHQ to the SHAEF. Many prematurely considered that victory in Europe would come by summer's end—however the Germans did not capitulate for almost a year. From then until the end of the war in Europe on May 8, 1945, Eisenhower, through SHAEF, commanded all Allied forces, and through his command of ETOUSA had administrative command of all U.S. forces on the Western Front north of the Alps. He was ever mindful of the inevitable loss of life and suffering that would be experienced on an individual level by the troops under his command and their families. This prompted him to make a point of visiting every division involved in the invasion. Eisenhower's sense of responsibility was underscored by his draft of a statement to be issued if the invasion failed. It has been called one of the great speeches of history: In 1953, the Republican Party's Old Guard presented Eisenhower with a dilemma by insisting he disavow the Yalta Agreements as beyond the constitutional authority of the Executive Branch; however, the death of Joseph Stalin in March 1953 made the matter a practical moot point. At this time Eisenhower gave his Chance for Peace speech in which he attempted, unsuccessfully, to forestall the nuclear arms race with the Soviet Union by suggesting multiple opportunities presented by peaceful uses of nuclear materials. Biographer Stephen Ambrose opined that this was the best speech of Eisenhower's presidency. While President Truman had begun the process of desegregating the Armed Forces in 1948, actual implementation had been slow. Eisenhower made clear his stance in his first State of the Union address in February 1953, saying ""I propose to use whatever authority exists in the office of the President to end segregation in the District of Columbia, including the Federal Government, and any segregation in the Armed Forces"". When he encountered opposition from the services, he used government control of military spending to force the change through, stating ""Wherever Federal Funds are expended ..., I do not see how any American can justify ... a discrimination in the expenditure of those funds"". In November 1942, he was also appointed Supreme Commander Allied Expeditionary Force of the North African Theater of Operations (NATOUSA) through the new operational Headquarters Allied (Expeditionary) Force Headquarters (A(E)FHQ). The word ""expeditionary"" was dropped soon after his appointment for security reasons. The campaign in North Africa was designated Operation Torch and was planned underground within the Rock of Gibraltar. Eisenhower was the first non-British person to command Gibraltar in 200 years. Eisenhower began smoking cigarettes at West Point, often two or three packs a day. Eisenhower stated that he ""gave [himself] an order"" to stop cold turkey in March 1949 while at Columbia. He was probably the first president to release information about his health and medical records while in office. On September 24, 1955, while vacationing in Colorado, he had a serious heart attack that required six weeks' hospitalization, during which time Nixon, Dulles, and Sherman Adams assumed administrative duties and provided communication with the President. He was treated by Dr. Paul Dudley White, a cardiologist with a national reputation, who regularly informed the press of the President's progress. Instead of eliminating him as a candidate for a second term as President, his physician recommended a second term as essential to his recovery. The development of the appreciation medals was initiated by the White House and executed by the Bureau of the Mint through the U.S. Mint in Philadelphia. The medals were struck from September 1958 through October 1960. A total of twenty designs are cataloged with a total mintage of 9,858. Each of the designs incorporates the text ""with appreciation"" or ""with personal and official gratitude"" accompanied with Eisenhower's initials ""D.D.E."" or facsimile signature. The design also incorporates location, date, and/or significant event. Prior to the end of his second term as President, 1,451 medals were turned-in to the Bureau of the Mint and destroyed. The Eisenhower appreciation medals are part of the Presidential Medal of Appreciation Award Medal Series. During the late 1920s and early 1930s, Eisenhower's career in the post-war army stalled somewhat, as military priorities diminished; many of his friends resigned for high-paying business jobs. He was assigned to the American Battle Monuments Commission directed by General Pershing, and with the help of his brother Milton Eisenhower, then a journalist at the Agriculture Department, he produced a guide to American battlefields in Europe. He then was assigned to the Army War College and graduated in 1928. After a one-year assignment in France, Eisenhower served as executive officer to General George V. Mosely, Assistant Secretary of War, from 1929 to February 1933. Major Dwight D. Eisenhower graduated from the Army Industrial College (Washington, DC) in 1933 and later served on the faculty (it was later expanded to become the Industrial College of the Armed Services and is now known as the Dwight D. Eisenhower School for National Security and Resource Strategy). Operation Torch also served as a valuable training ground for Eisenhower's combat command skills; during the initial phase of Generalfeldmarschall Erwin Rommel's move into the Kasserine Pass, Eisenhower created some confusion in the ranks by some interference with the execution of battle plans by his subordinates. He also was initially indecisive in his removal of Lloyd Fredendall, commanding U.S. II Corps. He became more adroit in such matters in later campaigns. In February 1943, his authority was extended as commander of AFHQ across the Mediterranean basin to include the British Eighth Army, commanded by General Sir Bernard Montgomery. The Eighth Army had advanced across the Western Desert from the east and was ready for the start of the Tunisia Campaign. Eisenhower gained his fourth star and gave up command of ETOUSA to become commander of NATOUSA. Whittaker was unsuited for the role and soon retired. Stewart and Harlan were conservative Republicans, while Brennan was a Democrat who became a leading voice for liberalism. In selecting a Chief Justice, Eisenhower looked for an experienced jurist who could appeal to liberals in the party as well as law-and-order conservatives, noting privately that Warren ""represents the kind of political, economic, and social thinking that I believe we need on the Supreme Court ... He has a national name for integrity, uprightness, and courage that, again, I believe we need on the Court"". In the next few years Warren led the Court in a series of liberal decisions that revolutionized the role of the Court. In recognition of his senior position in the Allied command, on December 20, 1944, he was promoted to General of the Army, equivalent to the rank of Field Marshal in most European armies. In this and the previous high commands he held, Eisenhower showed his great talents for leadership and diplomacy. Although he had never seen action himself, he won the respect of front-line commanders. He interacted adeptly with allies such as Winston Churchill, Field Marshal Bernard Montgomery and General Charles de Gaulle. He had serious disagreements with Churchill and Montgomery over questions of strategy, but these rarely upset his relationships with them. He dealt with Soviet Marshal Zhukov, his Russian counterpart, and they became good friends. Following the German unconditional surrender, Eisenhower was appointed Military Governor of the U.S. Occupation Zone, based at the IG Farben Building in Frankfurt am Main. He had no responsibility for the other three zones, controlled by Britain, France and the Soviet Union, except for the city of Berlin, which was managed by the Four-Power Authorities through the Allied Kommandatura as the governing body. Upon discovery of the Nazi concentration camps, he ordered camera crews to document evidence of the atrocities in them for use in the Nuremberg Trials. He reclassified German prisoners of war (POWs) in U.S. custody as Disarmed Enemy Forces (DEFs), who were no longer subject to the Geneva Convention. Eisenhower followed the orders laid down by the Joint Chiefs of Staff (JCS) in directive JCS 1067, but softened them by bringing in 400,000 tons of food for civilians and allowing more fraternization. In response to the devastation in Germany, including food shortages and an influx of refugees, he arranged distribution of American food and medical equipment. His actions reflected the new American attitudes of the German people as Nazi victims not villains, while aggressively purging the ex-Nazis. That evening, Eisenhower's body was placed onto a train en route to Abilene, Kansas, the last time a funeral train has been used as part of funeral proceedings of an American president. His body arrived on April 2, and was interred later that day in a small chapel on the grounds of the Eisenhower Presidential Library. The president's body was buried as a General of the Army. The family used an $80 standard soldier's casket, and dressed Eisenhower's body in his famous short green jacket. His only medals worn were: the Army Distinguished Service Medal with three oak leaf clusters, the Navy Distinguished Service Medal, and the Legion of Merit. Eisenhower is buried alongside his son Doud, who died at age 3 in 1921. His wife Mamie was buried next to him after her death in 1979. In June 1943 a visiting politician had suggested to Eisenhower that he might become President of the United States after the war. Believing that a general should not participate in politics, one author later wrote that ""figuratively speaking, [Eisenhower] kicked his political-minded visitor out of his office"". As others asked him about his political future, Eisenhower told one that he could not imagine wanting to be considered for any political job ""from dogcatcher to Grand High Supreme King of the Universe"", and another that he could not serve as Army Chief of Staff if others believed he had political ambitions. In 1945 Truman told Eisenhower during the Potsdam Conference that if desired, the president would help the general win the 1948 election, and in 1947 he offered to run as Eisenhower's running mate on the Democratic ticket if MacArthur won the Republican nomination. In the 1960 election to choose his successor, Eisenhower endorsed his own Vice President, Republican Richard Nixon against Democrat John F. Kennedy. He told friends, ""I will do almost anything to avoid turning my chair and country over to Kennedy."" He actively campaigned for Nixon in the final days, although he may have done Nixon some harm. When asked by reporters at the end of a televised press conference to list one of Nixon's policy ideas he had adopted, Eisenhower joked, ""If you give me a week, I might think of one. I don't remember."" Kennedy's campaign used the quote in one of its campaign commercials. Nixon narrowly lost to Kennedy. Eisenhower, who was the oldest president in history at that time (then 70), was succeeded by the youngest elected president, as Kennedy was 43. The Eisenhowers had two sons. Doud Dwight ""Icky"" Eisenhower was born September 24, 1917, and died of scarlet fever on January 2, 1921, at the age of three; Eisenhower was mostly reticent to discuss his death. Their second son, John Eisenhower (1922–2013), was born in Denver Colorado. John served in the United States Army, retired as a brigadier general, became an author and served as U.S. Ambassador to Belgium from 1969 to 1971. Coincidentally, John graduated from West Point on D-Day, June 6, 1944. He married Barbara Jean Thompson on June 10, 1947. John and Barbara had four children: David, Barbara Ann, Susan Elaine and Mary Jean. David, after whom Camp David is named, married Richard Nixon's daughter Julie in 1968. John died on December 21, 2013. Eisenhower, as well as the officers and troops under him, had learned valuable lessons in their previous operations, and their skills had all strengthened in preparation for the next most difficult campaign against the Germans—a beach landing assault. His first struggles, however, were with Allied leaders and officers on matters vital to the success of the Normandy invasion; he argued with Roosevelt over an essential agreement with De Gaulle to use French resistance forces in covert and sabotage operations against the Germans in advance of Overlord. Admiral Ernest J. King fought with Eisenhower over King's refusal to provide additional landing craft from the Pacific. He also insisted that the British give him exclusive command over all strategic air forces to facilitate Overlord, to the point of threatening to resign unless Churchill relented, as he did. Eisenhower then designed a bombing plan in France in advance of Overlord and argued with Churchill over the latter's concern with civilian casualties; de Gaulle interjected that the casualties were justified in shedding the yoke of the Germans, and Eisenhower prevailed. He also had to skillfully manage to retain the services of the often unruly George S. Patton, by severely reprimanding him when Patton earlier had slapped a subordinate, and then when Patton gave a speech in which he made improper comments about postwar policy. In 1948, Eisenhower became President of Columbia University, an Ivy League university in New York City. The assignment was described as not being a good fit in either direction. During that year Eisenhower's memoir, Crusade in Europe, was published. Critics regarded it as one of the finest U.S. military memoirs, and it was a major financial success as well. Eisenhower's profit on the book was substantially aided by an unprecedented ruling by the U.S. Department of the Treasury that Eisenhower was not a professional writer, but rather, marketing the lifetime asset of his experiences, and thus he only had to pay capital gains tax on his $635,000 advance instead of the much higher personal tax rate. This ruling saved Eisenhower about $400,000. Eisenhower was a golf enthusiast later in life, and joined the Augusta National Golf Club in 1948. He played golf frequently during and after his presidency and was unreserved in expressing his passion for the game, to the point of golfing during winter; he ordered his golf balls painted black so he could see them better against snow on the ground. He had a small, basic golf facility installed at Camp David, and became close friends with the Augusta National Chairman Clifford Roberts, inviting Roberts to stay at the White House on several occasions. Roberts, an investment broker, also handled the Eisenhower family's investments. Roberts also advised Eisenhower on tax aspects of publishing his memoirs, which proved financially lucrative. The Interstate Highway System is officially known as the 'Dwight D. Eisenhower National System of Interstate and Defense Highways' in his honor. It was inspired in part by Eisenhower's own Army experiences in World War II, where he recognized the advantages of the autobahn systems in Germany, Austria, and Switzerland. Commemorative signs reading ""Eisenhower Interstate System"" and bearing Eisenhower's permanent 5-star rank insignia were introduced in 1993 and are currently displayed throughout the Interstate System. Several highways are also named for him, including the Eisenhower Expressway (Interstate 290) near Chicago and the Eisenhower Tunnel on Interstate 70 west of Denver. In November 1956, Eisenhower forced an end to the combined British, French and Israeli invasion of Egypt in response to the Suez Crisis, receiving praise from Egyptian president Gamal Abdel Nasser. Simultaneously he condemned the brutal Soviet invasion of Hungary in response to the Hungarian Revolution of 1956. He publicly disavowed his allies at the United Nations, and used financial and diplomatic pressure to make them withdraw from Egypt. Eisenhower explicitly defended his strong position against Britain and France in his memoirs, which were published in 1965. On the domestic front, he covertly opposed Joseph McCarthy and contributed to the end of McCarthyism by openly invoking the modern expanded version of executive privilege. He otherwise left most political activity to his Vice President, Richard Nixon. He was a moderate conservative who continued New Deal agencies and expanded Social Security. He also launched the Interstate Highway System, the Defense Advanced Research Projects Agency (DARPA), the establishment of strong science education via the National Defense Education Act, and encouraged peaceful use of nuclear power via amendments to the Atomic Energy Act. He assumed duties again at Camp Meade, Maryland, commanding a battalion of tanks, where he remained until 1922. His schooling continued, focused on the nature of the next war and the role of the tank in it. His new expertise in tank warfare was strengthened by a close collaboration with George S. Patton, Sereno E. Brett, and other senior tank leaders. Their leading-edge ideas of speed-oriented offensive tank warfare were strongly discouraged by superiors, who considered the new approach too radical and preferred to continue using tanks in a strictly supportive role for the infantry. Eisenhower was even threatened with court martial for continued publication of these proposed methods of tank deployment, and he relented. Eisenhower responded to the French defeat with the formation of the SEATO (Southeast Asia Treaty Organization) Alliance with the U.K., France, New Zealand and Australia in defense of Vietnam against communism. At that time the French and Chinese reconvened Geneva peace talks; Eisenhower agreed the U.S. would participate only as an observer. After France and the Communists agreed to a partition of Vietnam, Eisenhower rejected the agreement, offering military and economic aid to southern Vietnam. Ambrose argues that Eisenhower, by not participating in the Geneva agreement, had kept the U.S out of Vietnam; nevertheless, with the formation of SEATO, he had in the end put the U.S. back into the conflict. Historians have concluded that this assignment provided valuable preparation for handling the challenging personalities of Winston Churchill, George S. Patton, George Marshall, and General Montgomery during World War II. Eisenhower later emphasized that too much had been made of the disagreements with MacArthur, and that a positive relationship endured. While in Manila, Mamie suffered a life-threatening stomach ailment but recovered fully. Eisenhower was promoted to the rank of permanent lieutenant colonel in 1936. He also learned to fly, making a solo flight over the Philippines in 1937 and obtained his private pilot's license in 1939 at Fort Lewis. Also around this time, he was offered a post by the Philippine Commonwealth Government, namely by then Philippine President Manuel L. Quezon on recommendations by MacArthur, to become the chief of police of a new capital being planned, now named Quezon City, but he declined the offer. As a consequence of his heart attack, Eisenhower developed a left ventricular aneurysm, which was in turn the cause of a mild stroke on November 25, 1957. This incident occurred during a cabinet meeting when Eisenhower suddenly found himself unable to speak or move his right hand. The stroke had caused an aphasia. The president also suffered from Crohn's disease, chronic inflammatory condition of the intestine, which necessitated surgery for a bowel obstruction on June 9, 1956. To treat the intestinal block, surgeons bypassed about ten inches of his small intestine. His scheduled meeting with Indian Prime Minister Jawaharlal Nehru was postponed so he could recover from surgery at his farm in Gettysburg, Pennsylvania. He was still recovering from this operation during the Suez Crisis. Eisenhower's health issues forced him to give up smoking and make some changes to his dietary habits, but he still indulged in alcohol. During a visit to England he complained of dizziness and had to have his blood pressure checked on August 29, 1959; however, before dinner at Chequers on the next day his doctor General Howard Snyder recalled Eisenhower ""drank several gin-and-tonics, and one or two gins on the rocks ... three or four wines with the dinner"". Throughout his presidency, Eisenhower adhered to a political philosophy of dynamic conservatism. A self-described ""progressive conservative,"" he continued all the major New Deal programs still in operation, especially Social Security. He expanded its programs and rolled them into a new cabinet-level agency, the Department of Health, Education and Welfare, while extending benefits to an additional ten million workers. He implemented integration in the Armed Services in two years, which had not been completed under Truman. When the U.S. entered World War I he immediately requested an overseas assignment but was again denied and then assigned to Ft. Leavenworth, Kansas. In February 1918 he was transferred to Camp Meade in Maryland with the 65th Engineers. His unit was later ordered to France but to his chagrin he received orders for the new tank corps, where he was promoted to brevet Lieutenant Colonel in the National Army. He commanded a unit that trained tank crews at Camp Colt – his first command – at the site of ""Pickett's Charge"" on the Gettysburg, Pennsylvania Civil War battleground. Though Eisenhower and his tank crews never saw combat, he displayed excellent organizational skills, as well as an ability to accurately assess junior officers' strengths and make optimal placements of personnel. Among Ike's objectives in not directly confronting McCarthy was to prevent McCarthy from dragging the Atomic Energy Commission (AEC) into McCarthy's witch hunt for communists, which would interfere with, and perhaps delay, the AEC's important work on H-bombs. The administration had discovered through its own investigations that one of the leading scientists on the AEC, J. Robert Oppenheimer, had urged that the H-bomb work be delayed. Eisenhower removed him from the agency and revoked his security clearance, though he knew this would create fertile ground for McCarthy. In the general election, against the advice of his advisors, Eisenhower insisted on campaigning in the South, refusing to surrender the region to the Democratic Party. The campaign strategy, dubbed ""K1C2"", was to focus on attacking the Truman and Roosevelt administrations on three issues: Korea, Communism and corruption. In an effort to accommodate the right, he stressed that the liberation of Eastern Europe should be by peaceful means only; he also distanced himself from his former boss President Truman. Eisenhower did provide France with bombers and non-combat personnel. After a few months with no success by the French, he added other aircraft to drop napalm for clearing purposes. Further requests for assistance from the French were agreed to but only on conditions Eisenhower knew were impossible to meet – allied participation and congressional approval. When the French fortress of Dien Bien Phu fell to the Vietnamese Communists in May 1954, Eisenhower refused to intervene despite urgings from the Chairman of the Joint Chiefs, the Vice President and the head of NCS. With Eisenhower's leadership and Dulles' direction, CIA activities increased under the pretense of resisting the spread of communism in poorer countries; the CIA in part deposed the leaders of Iran in Operation Ajax, of Guatemala through Operation Pbsuccess, and possibly the newly independent Republic of the Congo (Léopoldville). In 1954 Eisenhower wanted to increase surveillance inside the Soviet Union. With Dulles' recommendation, he authorized the deployment of thirty Lockheed U-2's at a cost of $35 million. The Eisenhower administration also planned the Bay of Pigs Invasion to overthrow Fidel Castro in Cuba, which John F. Kennedy was left to carry out."" The Germans launched a surprise counter offensive, in the Battle of the Bulge in December 1944, which the Allies turned back in early 1945 after Eisenhower repositioned his armies and improved weather allowed the Air Force to engage. German defenses continued to deteriorate on both the eastern front with the Soviets and the western front with the Allies. The British wanted Berlin, but Eisenhower decided it would be a military mistake for him to attack Berlin, and said orders to that effect would have to be explicit. The British backed down, but then wanted Eisenhower to move into Czechoslovakia for political reasons. Washington refused to support Churchill's plan to use Eisenhower's army for political maneuvers against Moscow. The actual division of Germany followed the lines that Roosevelt, Churchill and Stalin had previously agreed upon. The Soviet Red Army captured Berlin in a very large-scale bloody battle, and the Germans finally surrendered on May 7, 1945. After the Japanese attack on Pearl Harbor, Eisenhower was assigned to the General Staff in Washington, where he served until June 1942 with responsibility for creating the major war plans to defeat Japan and Germany. He was appointed Deputy Chief in charge of Pacific Defenses under the Chief of War Plans Division (WPD), General Leonard T. Gerow, and then succeeded Gerow as Chief of the War Plans Division. Next, he was appointed Assistant Chief of Staff in charge of the new Operations Division (which replaced WPD) under Chief of Staff General George C. Marshall, who spotted talent and promoted accordingly. Eisenhower's goal to create improved highways was influenced by difficulties encountered during his involvement in the U.S. Army's 1919 Transcontinental Motor Convoy. He was assigned as an observer for the mission, which involved sending a convoy of U.S. Army vehicles coast to coast. His subsequent experience with encountering German autobahn limited-access road systems during the concluding stages of World War II convinced him of the benefits of an Interstate Highway System. Noticing the improved ability to move logistics throughout the country, he thought an Interstate Highway System in the U.S. would not only be beneficial for military operations, but provide a measure of continued economic growth. The legislation initially stalled in the Congress over the issuance of bonds to finance the project, but the legislative effort was renewed and the law was signed by Eisenhower in June 1956. In the immediate years after Eisenhower left office, his reputation declined. He was widely seen by critics as an inactive, uninspiring, golf-playing president compared to his vigorous young successor. Despite his unprecedented use of Army troops to enforce a federal desegregation order at Central High School in Little Rock, Eisenhower was criticized for his reluctance to support the civil rights movement to the degree that activists wanted. Eisenhower also attracted criticism for his handling of the 1960 U-2 incident and the associated international embarrassment, for the Soviet Union's perceived leadership in the nuclear arms race and the Space Race, and for his failure to publicly oppose McCarthyism. On the morning of March 28, 1969, at the age of 78, Eisenhower died in Washington, D.C. of congestive heart failure at Walter Reed Army Medical Center. The following day his body was moved to the Washington National Cathedral's Bethlehem Chapel, where he lay in repose for 28 hours. On March 30, his body was brought by caisson to the United States Capitol, where he lay in state in the Capitol Rotunda. On March 31, Eisenhower's body was returned to the National Cathedral, where he was given an Episcopal Church funeral service. This prevented Eisenhower from openly condemning Joseph McCarthy's highly criticized methods against communism. To facilitate relations with Congress, Eisenhower decided to ignore McCarthy's controversies and thereby deprive them of more energy from involvement of the White House. This position drew criticism from a number of corners. In late 1953 McCarthy declared on national television that the employment of communists within the government was a menace and would be a pivotal issue in the 1954 Senate elections. Eisenhower was urged to respond directly and specify the various measures he had taken to purge the government of communists. Nevertheless, he refused. In December 1943, President Roosevelt decided that Eisenhower – not Marshall – would be Supreme Allied Commander in Europe. The following month, he resumed command of ETOUSA and the following month was officially designated as the Supreme Allied Commander of the Allied Expeditionary Force (SHAEF), serving in a dual role until the end of hostilities in Europe in May 1945. He was charged in these positions with planning and carrying out the Allied assault on the coast of Normandy in June 1944 under the code name Operation Overlord, the liberation of Western Europe and the invasion of Germany. In the years that followed, Eisenhower increased the number of U.S. military advisors in South Vietnam to 900 men. This was due to North Vietnam's support of ""uprisings"" in the south and concern the nation would fall. In May 1957 Diem, then President of South Vietnam, made a state visit to the United States for ten days. President Eisenhower pledged his continued support, and a parade was held in Diem's honor in New York City. Although Diem was publicly praised, in private Secretary of State John Foster Dulles conceded that Diem had been selected because there were no better alternatives. Due to a complete estrangement between the two as a result of campaigning, Truman and Eisenhower had minimal discussions about the transition of administrations. After selecting his budget director, Joseph M. Dodge, Eisenhower asked Herbert Brownell and Lucius Clay to make recommendations for his cabinet appointments. He accepted their recommendations without exception; they included John Foster Dulles and George M. Humphrey with whom he developed his closest relationships, and one woman, Oveta Culp Hobby. Eisenhower's cabinet, consisting of several corporate executives and one labor leader, was dubbed by one journalist, ""Eight millionaires and a plumber."" The cabinet was notable for its lack of personal friends, office seekers, or experienced government administrators. He also upgraded the role of the National Security Council in planning all phases of the Cold War. As the 1954 congressional elections approached, and it became evident that the Republicans were in danger of losing their thin majority in both houses, Eisenhower was among those blaming the Old Guard for the losses, and took up the charge to stop suspected efforts by the right wing to take control of the GOP. Eisenhower then articulated his position as a moderate, progressive Republican: ""I have just one purpose ... and that is to build up a strong progressive Republican Party in this country. If the right wing wants a fight, they are going to get it ... before I end up, either this Republican Party will reflect progressivism or I won't be with them anymore."" A loblolly pine, known as the ""Eisenhower Pine"", was located on Augusta's 17th hole, approximately 210 yards (192 m) from the Masters tee. President Dwight D. Eisenhower, an Augusta National member, hit the tree so many times that, at a 1956 club meeting, he proposed that it be cut down. Not wanting to offend the president, the club's chairman, Clifford Roberts, immediately adjourned the meeting rather than reject the request. The tree was removed in February 2014 after an ice storm caused it significant damage. At the end of May 1942, Eisenhower accompanied Lt. Gen. Henry H. Arnold, commanding general of the Army Air Forces, to London to assess the effectiveness of the theater commander in England, Maj. Gen. James E. Chaney. He returned to Washington on June 3 with a pessimistic assessment, stating he had an ""uneasy feeling"" about Chaney and his staff. On June 23, 1942, he returned to London as Commanding General, European Theater of Operations (ETOUSA), based in London and with a house on Coombe, Kingston upon Thames, and replaced Chaney. He was promoted to lieutenant general on July 7. Within months of beginning his tenure as the president of the university, Eisenhower was requested to advise U.S. Secretary of Defense James Forrestal on the unification of the armed services. About six months after his appointment, he became the informal Chairman of the Joint Chiefs of Staff in Washington. Two months later he fell ill, and he spent over a month in recovery at the Augusta National Golf Club. He returned to his post in New York in mid-May, and in July 1949 took a two-month vacation out-of-state. Because the American Assembly had begun to take shape, he traveled around the country during mid-to-late 1950, building financial support from Columbia Associates, an alumni association. Speaker Martin concluded that Eisenhower worked too much through subordinates in dealing with Congress, with results, ""often the reverse of what he has desired"" because Members of Congress, ""resent having some young fellow who was picked up by the White House without ever having been elected to office himself coming around and telling them 'The Chief wants this'. The administration never made use of many Republicans of consequence whose services in one form or another would have been available for the asking."" In November 1945, Eisenhower returned to Washington to replace Marshall as Chief of Staff of the Army. His main role was rapid demobilization of millions of soldiers, a slow job that was delayed by lack of shipping. Eisenhower was convinced in 1946 that the Soviet Union did not want war and that friendly relations could be maintained; he strongly supported the new United Nations and favored its involvement in the control of atomic bombs. However, in formulating policies regarding the atomic bomb and relations with the Soviets, Truman was guided by the U.S. State Department and ignored Eisenhower and the Pentagon. Indeed, Eisenhower had opposed the use of the atomic bomb against the Japanese, writing, ""First, the Japanese were ready to surrender and it wasn't necessary to hit them with that awful thing. Second, I hated to see our country be the first to use such a weapon."" Initially, Eisenhower was characterized by hopes for cooperation with the Soviets. He even visited Warsaw in 1945. Invited by Bolesław Bierut and decorated with the highest military decoration, he was shocked by the scale of destruction in the city. However, by mid-1947, as East–West tensions over economic recovery in Germany and the Greek Civil War escalated, Eisenhower gave up and agreed with a containment policy to stop Soviet expansion. The Democrats gained a majority in both houses in the 1954 election. Eisenhower had to work with the Democratic Majority Leader Lyndon B. Johnson (later U.S. president) in the Senate and Speaker Sam Rayburn in the House, both from Texas. Joe Martin, the Republican Speaker from 1947 to 1949 and again from 1953 to 1955, wrote that Eisenhower ""never surrounded himself with assistants who could solve political problems with professional skill. There were exceptions, Leonard W. Hall, for example, who as chairman of the Republican National Committee tried to open the administration's eyes to the political facts of life, with occasional success. However, these exceptions were not enough to right the balance."" Angels in the Outfield was Eisenhower's favorite movie. His favorite reading material for relaxation were the Western novels of Zane Grey. With his excellent memory and ability to focus, Eisenhower was skilled at card games. He learned poker, which he called his ""favorite indoor sport,"" in Abilene. Eisenhower recorded West Point classmates' poker losses for payment after graduation, and later stopped playing because his opponents resented having to pay him. A classmate reported that after learning to play contract bridge at West Point, Eisenhower played the game six nights a week for five months. Once the coastal assault had succeeded, Eisenhower insisted on retaining personal control over the land battle strategy, and was immersed in the command and supply of multiple assaults through France on Germany. Field Marshal Montgomery insisted priority be given to his 21st Army Group's attack being made in the north, while Generals Bradley (12th U.S. Army Group) and Devers (Sixth U.S. Army Group) insisted they be given priority in the center and south of the front (respectively). Eisenhower worked tirelessly to address the demands of the rival commanders to optimize Allied forces, often by giving them tactical, though sometimes ineffective, latitude; many historians conclude this delayed the Allied victory in Europe. However, due to Eisenhower's persistence, the pivotal supply port at Antwerp was successfully, albeit belatedly, opened in late 1944, and victory became a more distinct probability. The last three years of Eisenhower's second term in office were ones of relatively good health. Eventually after leaving the White House, he suffered several additional and ultimately crippling heart attacks. A severe heart attack in August 1965 largely ended his participation in public affairs. In August 1966 he began to show symptoms of cholecystitis, for which he underwent surgery on December 12, 1966, when his gallbladder was removed, containing 16 gallstones. After Eisenhower's death in 1969 (see below), an autopsy unexpectedly revealed an adrenal pheochromocytoma, a benign adrenaline-secreting tumor that may have made the President more vulnerable to heart disease. Eisenhower suffered seven heart attacks in total from 1955 until his death. In 1957, the state of Arkansas refused to honor a federal court order to integrate their public school system stemming from the Brown decision. Eisenhower demanded that Arkansas governor Orval Faubus obey the court order. When Faubus balked, the president placed the Arkansas National Guard under federal control and sent in the 101st Airborne Division. They escorted and protected nine black students' entry to Little Rock Central High School, an all-white public school, for the first time since the Reconstruction Era. Martin Luther King Jr. wrote to Eisenhower to thank him for his actions, writing ""The overwhelming majority of southerners, Negro and white, stand firmly behind your resolute action to restore law and order in Little Rock"". Prior to his inauguration, Eisenhower led a meeting of advisors at Pearl Harbor addressing foremost issues; agreed objectives were to balance the budget during his term, to bring the Korean War to an end, to defend vital interests at lower cost through nuclear deterrent, and to end price and wage controls. Eisenhower also conducted the first pre-inaugural cabinet meeting in history in late 1952; he used this meeting to articulate his anti-communist Russia policy. His inaugural address, as well, was exclusively devoted to foreign policy and included this same philosophy, as well as a commitment to foreign trade and the United Nations. His primary duty was planning for the next war, which proved most difficult in the midst of the Great Depression. He then was posted as chief military aide to General MacArthur, Army Chief of Staff. In 1932, he participated in the clearing of the Bonus March encampment in Washington, D.C. Although he was against the actions taken against the veterans and strongly advised MacArthur against taking a public role in it, he later wrote the Army's official incident report, endorsing MacArthur's conduct. Initially Eisenhower planned on serving only one term, but as with other decisions, he maintained a position of maximum flexibility in case leading Republicans wanted him to run again. During his recovery from a heart attack late in 1955, he huddled with his closest advisors to evaluate the GOP's potential candidates; the group, in addition to his doctor, concluded a second term was well advised, and he announced in February 1956 he would run again. Eisenhower was publicly noncommittal about Nixon's repeating as the Vice President on his ticket; the question was an especially important one in light of his heart condition. He personally favored Robert B. Anderson, a Democrat, who rejected his offer; Eisenhower then resolved to leave the matter in the hands of the party. In 1956, Eisenhower faced Adlai Stevenson again and won by an even larger landslide, with 457 of 531 electoral votes and 57.6% of the popular vote. The level of campaigning was curtailed out of health considerations. Eisenhower's stint as the president of Columbia University was punctuated by his activity within the Council on Foreign Relations, a study group he led as president concerning the political and military implications of the Marshall Plan, and The American Assembly, Eisenhower's ""vision of a great cultural center where business, professional and governmental leaders could meet from time to time to discuss and reach conclusions concerning problems of a social and political nature"". His biographer Blanche Wiesen Cook suggested that this period served as ""the political education of General Eisenhower"", since he had to prioritize wide-ranging educational, administrative, and financial demands for the university. Through his involvement in the Council on Foreign Relations, he also gained exposure to economic analysis, which would become the bedrock of his understanding in economic policy. ""Whatever General Eisenhower knows about economics, he has learned at the study group meetings,"" one Aid to Europe member claimed. Soviet Premier Nikita Khrushchev announced that a ""spy-plane"" had been shot down but intentionally made no reference to the pilot. As a result, the Eisenhower Administration, thinking the pilot had died in the crash, authorized the release of a cover story claiming that the plane was a ""weather research aircraft"" which had unintentionally strayed into Soviet airspace after the pilot had radioed ""difficulties with his oxygen equipment"" while flying over Turkey. The Soviets put Captain Powers on trial and displayed parts of the U-2, which had been recovered almost fully intact. Dwight David ""Ike"" Eisenhower (/ˈaɪzənˌhaʊ.ər/ EYE-zən-HOW-ər; October 14, 1890 – March 28, 1969) was an American politician and general who served as the 34th President of the United States from 1953 until 1961. He was a five-star general in the United States Army during World War II and served as Supreme Commander of the Allied Forces in Europe. He was responsible for planning and supervising the invasion of North Africa in Operation Torch in 1942–43 and the successful invasion of France and Germany in 1944–45 from the Western Front. In 1951, he became the first Supreme Commander of NATO. Once again his spirits were raised when the unit under his command received orders overseas to France. This time his wishes were thwarted when the armistice was signed, just a week before departure. Completely missing out on the warfront left him depressed and bitter for a time, despite being given the Distinguished Service Medal for his work at home.[citation needed] In World War II, rivals who had combat service in the first great war (led by Gen. Bernard Montgomery) sought to denigrate Eisenhower for his previous lack of combat duty, despite his stateside experience establishing a camp, completely equipped, for thousands of troops, and developing a full combat training schedule. His parents set aside specific times at breakfast and at dinner for daily family Bible reading. Chores were regularly assigned and rotated among all the children, and misbehavior was met with unequivocal discipline, usually from David. His mother, previously a member (with David) of the River Brethren sect of the Mennonites, joined the International Bible Students Association, later known as Jehovah's Witnesses. The Eisenhower home served as the local meeting hall from 1896 to 1915, though Eisenhower never joined the International Bible Students. His later decision to attend West Point saddened his mother, who felt that warfare was ""rather wicked,"" but she did not overrule him. While speaking of himself in 1948, Eisenhower said he was ""one of the most deeply religious men I know"" though unattached to any ""sect or organization"". He was baptized in the Presbyterian Church in 1953. As the election approached, other prominent citizens and politicians from both parties urged Eisenhower to run for president. In January 1948, after learning of plans in New Hampshire to elect delegates supporting him for the forthcoming Republican National Convention, Eisenhower stated through the Army that he was ""not available for and could not accept nomination to high political office""; ""life-long professional soldiers"", he wrote, ""in the absence of some obvious and overriding reason, [should] abstain from seeking high political office"". Eisenhower maintained no political party affiliation during this time. Many believed he was forgoing his only opportunity to be president; Republican Thomas E. Dewey was considered the other probable winner, would presumably serve two terms, and Eisenhower, at age 66 in 1956, would then be too old. Eisenhower attended Abilene High School and graduated with the class of 1909. As a freshman, he injured his knee and developed a leg infection that extended into his groin, and which his doctor diagnosed as life-threatening. The doctor insisted that the leg be amputated but Dwight refused to allow it, and miraculously recovered, though he had to repeat his freshman year. He and brother Edgar both wanted to attend college, though they lacked the funds. They made a pact to take alternate years at college while the other worked to earn the tuitions. In July 1953, an armistice took effect with Korea divided along approximately the same boundary as in 1950. The armistice and boundary remain in effect today, with American soldiers stationed there to guarantee it. The armistice, concluded despite opposition from Secretary Dulles, South Korean President Syngman Rhee, and also within Eisenhower's party, has been described by biographer Ambrose as the greatest achievement of the administration. Eisenhower had the insight to realize that unlimited war in the nuclear age was unthinkable, and limited war unwinnable. From 1920, Eisenhower served under a succession of talented generals – Fox Conner, John J. Pershing, Douglas MacArthur and George Marshall. He first became executive officer to General Conner in the Panama Canal Zone, where, joined by Mamie, he served until 1924. Under Conner's tutelage, he studied military history and theory (including Carl von Clausewitz's On War), and later cited Conner's enormous influence on his military thinking, saying in 1962 that ""Fox Conner was the ablest man I ever knew."" Conner's comment on Eisenhower was, ""[He] is one of the most capable, efficient and loyal officers I have ever met."" On Conner's recommendation, in 1925–26 he attended the Command and General Staff College at Fort Leavenworth, Kansas, where he graduated first in a class of 245 officers. He then served as a battalion commander at Fort Benning, Georgia, until 1927. After golf, oil painting was Eisenhower's second hobby. While at Columbia University, Eisenhower began the art after watching Thomas E. Stephens paint Mamie's portrait. Eisenhower painted about 260 oils during the last 20 years of his life to relax, mostly landscapes but also portraits of subjects such as Mamie, their grandchildren, General Montgomery, George Washington, and Abraham Lincoln. Wendy Beckett stated that Eisenhower's work, ""simple and earnest, rather cause us to wonder at the hidden depths of this reticent president"". A conservative in both art and politics, he in a 1962 speech denounced modern art as ""a piece of canvas that looks like a broken-down Tin Lizzie, loaded with paint, has been driven over it."" On the whole, Eisenhower's support of the nation's fledgling space program was officially modest until the Soviet launch of Sputnik in 1957, gaining the Cold War enemy enormous prestige around the world. He then launched a national campaign that funded not just space exploration but a major strengthening of science and higher education. His Open Skies Policy attempted to legitimize illegal Lockheed U-2 flyovers and Project Genetrix while paving the way for spy satellite technology to orbit over sovereign territory, created NASA as a civilian space agency, signed a landmark science education law, and fostered improved relations with American scientists. After the Soviet Union launched the world's first artificial satellite in 1957, Eisenhower authorized the establishment of NASA, which led to the space race. During the Suez Crisis of 1956, Eisenhower condemned the Israeli, British and French invasion of Egypt, and forced them to withdraw. He also condemned the Soviet invasion during the Hungarian Revolution of 1956 but took no action. In 1958, Eisenhower sent 15,000 U.S. troops to Lebanon to prevent the pro-Western government from falling to a Nasser-inspired revolution. Near the end of his term, his efforts to set up a summit meeting with the Soviets collapsed because of the U-2 incident. In his January 17, 1961 farewell address to the nation, Eisenhower expressed his concerns about the dangers of massive military spending, particularly deficit spending and government contracts to private military manufacturers, and coined the term ""military–industrial complex"". Edgar took the first turn at school, and Dwight was employed as a night supervisor at the Belle Springs Creamery. Edgar asked for a second year, Dwight consented and worked for a second year. At that time, a friend ""Swede"" Hazlet was applying to the Naval Academy and urged Dwight to apply to the school, since no tuition was required. Eisenhower requested consideration for either Annapolis or West Point with his U.S. Senator, Joseph L. Bristow. Though Eisenhower was among the winners of the entrance-exam competition, he was beyond the age limit for the Naval Academy. He then accepted an appointment to West Point in 1911. The contacts gained through university and American Assembly fund-raising activities would later become important supporters in Eisenhower's bid for the Republican party nomination and the presidency. Meanwhile, Columbia University's liberal faculty members became disenchanted with the university president's ties to oilmen and businessmen, including Leonard McCollum, the president of Continental Oil; Frank Abrams, the chairman of Standard Oil of New Jersey; Bob Kleberg, the president of the King Ranch; H. J. Porter, a Texas oil executive; Bob Woodruff, the president of the Coca-Cola Corporation; and Clarence Francis, the chairman of General Foods. In May 1955, McCarthy threatened to issue subpoenas to White House personnel. Eisenhower was furious, and issued an order as follows: ""It is essential to efficient and effective administration that employees of the Executive Branch be in a position to be completely candid in advising with each other on official matters ... it is not in the public interest that any of their conversations or communications, or any documents or reproductions, concerning such advice be disclosed."" This was an unprecedented step by Eisenhower to protect communication beyond the confines of a cabinet meeting, and soon became a tradition known as executive privilege. Ike's denial of McCarthy's access to his staff reduced McCarthy's hearings to rants about trivial matters, and contributed to his ultimate downfall. Since the 19th century, many if not all presidents were assisted by a central figure or ""gatekeeper"", sometimes described as the President's Private Secretary, sometimes with no official title at all. Eisenhower formalized this role, introducing the office of White House Chief of Staff – an idea he borrowed from the United States Army. Every president after Lyndon Johnson has also appointed staff to this position. Initially, Gerald Ford and Jimmy Carter tried to operate without a chief of staff, but each eventually appointed one. Eisenhower was the last president born in the 19th century, and at age 62, was the oldest man elected President since James Buchanan in 1856 (President Truman stood at 64 in 1948 as the incumbent president at the time of his election four years earlier). Eisenhower was the only general to serve as President in the 20th century and the most recent President to have never held elected office prior to the Presidency (The other Presidents who did not have prior elected office were Zachary Taylor, Ulysses S. Grant, William Howard Taft and Herbert Hoover). President Truman, symbolizing a broad-based desire for an Eisenhower candidacy for president, again in 1951 pressed him to run for the office as a Democrat. It was at this time that Eisenhower voiced his disagreements with the Democratic party and declared himself and his family to be Republicans. A ""Draft Eisenhower"" movement in the Republican Party persuaded him to declare his candidacy in the 1952 presidential election to counter the candidacy of non-interventionist Senator Robert A. Taft. The effort was a long struggle; Eisenhower had to be convinced that political circumstances had created a genuine duty for him to offer himself as a candidate, and that there was a mandate from the populace for him to be their President. Henry Cabot Lodge, who served as his campaign manager, and others succeeded in convincing him, and in June 1952 he resigned his command at NATO to campaign full-time. Eisenhower defeated Taft for the nomination, having won critical delegate votes from Texas. Eisenhower's campaign was noted for the simple but effective slogan, ""I Like Ike"". It was essential to his success that Eisenhower express opposition to Roosevelt's policy at Yalta and against Truman's policies in Korea and China—matters in which he had once participated. In defeating Taft for the nomination, it became necessary for Eisenhower to appease the right wing Old Guard of the Republican Party; his selection of Richard M. Nixon as the Vice-President on the ticket was designed in part for that purpose. Nixon also provided a strong anti-communist presence as well as some youth to counter Ike's more advanced age. After the capitulation of Axis forces in North Africa, Eisenhower oversaw the highly successful invasion of Sicily. Once Mussolini, the Italian leader, had fallen in Italy, the Allies switched their attention to the mainland with Operation Avalanche. But while Eisenhower argued with President Roosevelt and British Prime Minister Churchill, who both insisted on unconditional terms of surrender in exchange for helping the Italians, the Germans pursued an aggressive buildup of forces in the country – making the job more difficult, by adding 19 divisions and initially outnumbering the Allied forces 2 to 1; nevertheless, the invasion of Italy was highly successful. Eisenhower told District of Columbia officials to make Washington a model for the rest of the country in integrating black and white public school children. He proposed to Congress the Civil Rights Act of 1957 and of 1960 and signed those acts into law. The 1957 act for the first time established a permanent civil rights office inside the Justice Department and a Civil Rights Commission to hear testimony about abuses of voting rights. Although both acts were much weaker than subsequent civil rights legislation, they constituted the first significant civil rights acts since 1875. In 1954, Eisenhower articulated the domino theory in his outlook towards communism in Southeast Asia and also in Central America. He believed that if the communists were allowed to prevail in Vietnam, this would cause a succession of countries to fall to communism, from Laos through Malaysia and Indonesia ultimately to India. Likewise, the fall of Guatemala would end with the fall of neighboring Mexico. That year the loss of North Vietnam to the communists and the rejection of his proposed European Defence Community (EDC) were serious defeats, but he remained optimistic in his opposition to the spread of communism, saying ""Long faces don't win wars"". As he had threatened the French in their rejection of EDC, he afterwards moved to restore West Germany, as a full NATO partner. In late 1954, Gen. J. Lawton Collins was made ambassador to ""Free Vietnam"" (the term South Vietnam came into use in 1955), effectively elevating the country to sovereign status. Collins' instructions were to support the leader Ngo Dinh Diem in subverting communism, by helping him to build an army and wage a military campaign. In February 1955, Eisenhower dispatched the first American soldiers to Vietnam as military advisors to Diem's army. After Diem announced the formation of the Republic of Vietnam (RVN, commonly known as South Vietnam) in October, Eisenhower immediately recognized the new state and offered military, economic, and technical assistance. In 1955 American nuclear arms policy became one aimed primarily at arms control as opposed to disarmament. The failure of negotiations over arms until 1955 was due mainly to the refusal of the Russians to permit any sort of inspections. In talks located in London that year, they expressed a willingness to discuss inspections; the tables were then turned on Eisenhower, when he responded with an unwillingness on the part of the U.S. to permit inspections. In May of that year the Russians agreed to sign a treaty giving independence to Austria, and paved the way for a Geneva summit with the U.S., U.K. and France. At the Geneva Conference Eisenhower presented a proposal called ""Open Skies"" to facilitate disarmament, which included plans for Russia and the U.S. to provide mutual access to each other's skies for open surveillance of military infrastructure. Russian leader Nikita Khrushchev dismissed the proposal out of hand. Over New York City in 1953, Eastern Airlines Flight 8610, a commercial flight, had a near miss with Air Force Flight 8610, a Lockheed C-121 Constellation known as Columbine II, while the latter was carrying President Eisenhower. This prompted the adoption of the unique call sign Air Force One, to be used whenever the president is on board any US Air Force aircraft. Columbine II is the only presidential aircraft to have ever been sold to the public and is the only remaining presidential aircraft left unrestored and not on public display. Eisenhower's main goals in office were to keep pressure on the Soviet Union and reduce federal deficits. In the first year of his presidency, he threatened the use of nuclear weapons in an effort to conclude the Korean War; his New Look policy of nuclear deterrence prioritized inexpensive nuclear weapons while reducing funding for conventional military forces. He ordered coups in Iran and Guatemala. Eisenhower refused to give major aid to help France in Vietnam. He gave strong financial support to the new nation of South Vietnam. Congress agreed to his request in 1955 for the Formosa Resolution, which obliged the U.S. to militarily support the pro-Western Republic of China in Taiwan and continue the isolation of the People's Republic of China. Eisenhower returned to the U.S. in December 1939 and was assigned as a battalion commander and regimental executive officer of the 15th Infantry at Fort Lewis, Washington. In March 1941 he was promoted to colonel and assigned as chief of staff of the newly activated IX Corps under Major General Kenyon Joyce. In June 1941, he was appointed Chief of Staff to General Walter Krueger, Commander of the 3rd Army, at Fort Sam Houston in San Antonio, Texas. After successfully participating in the Louisiana Maneuvers, he was promoted to brigadier general on October 3, 1941. Although his administrative abilities had been noticed, on the eve of the U.S. entry into World War II he had never held an active command above a battalion and was far from being considered by many as a potential commander of major operations. Eisenhower retired to the place where he and Mamie had spent much of their post-war time, a working farm adjacent to the battlefield at Gettysburg, Pennsylvania, only 70 miles from his ancestral home in Elizabethville, Dauphin County, Pennsylvania. In 1967 the Eisenhowers donated the farm to the National Park Service. In retirement, the former president did not completely retreat from political life; he spoke at the 1964 Republican National Convention and appeared with Barry Goldwater in a Republican campaign commercial from Gettysburg. However, his endorsement came somewhat reluctantly because Goldwater had attacked the former president as ""a dime-store New Dealer"". Early in 1953, the French asked Eisenhower for help in French Indochina against the Communists, supplied from China, who were fighting the First Indochina War. Eisenhower sent Lt. General John W. ""Iron Mike"" O'Daniel to Vietnam to study and assess the French forces there. Chief of Staff Matthew Ridgway dissuaded the President from intervening by presenting a comprehensive estimate of the massive military deployment that would be necessary. Eisenhower stated prophetically that ""this war would absorb our troops by divisions."" The 1960 Four Power Paris Summit between President Dwight Eisenhower, Nikita Khrushchev, Harold Macmillan and Charles de Gaulle collapsed because of the incident. Eisenhower refused to accede to Khrushchev's demands that he apologize. Therefore, Khrushchev would not take part in the summit. Up until this event, Eisenhower felt he had been making progress towards better relations with the Soviet Union. Nuclear arms reduction and Berlin were to have been discussed at the summit. Eisenhower stated it had all been ruined because of that ""stupid U-2 business"". On January 17, 1961, Eisenhower gave his final televised Address to the Nation from the Oval Office. In his farewell speech, Eisenhower raised the issue of the Cold War and role of the U.S. armed forces. He described the Cold War: ""We face a hostile ideology global in scope, atheistic in character, ruthless in purpose and insidious in method ..."" and warned about what he saw as unjustified government spending proposals and continued with a warning that ""we must guard against the acquisition of unwarranted influence, whether sought or unsought, by the military–industrial complex."" The trustees of Columbia University refused to accept Eisenhower's resignation in December 1950, when he took an extended leave from the university to become the Supreme Commander of the North Atlantic Treaty Organization (NATO), and he was given operational command of NATO forces in Europe. Eisenhower retired from active service as an Army general on May 31, 1952, and he resumed his presidency of Columbia. He held this position until January 20, 1953, when he became the President of the United States. The U.N. speech was well received but the Soviets never acted upon it, due to an overarching concern for the greater stockpiles of nuclear weapons in the U.S. arsenal. Indeed, Eisenhower embarked upon a greater reliance on the use of nuclear weapons, while reducing conventional forces, and with them the overall defense budget, a policy formulated as a result of Project Solarium and expressed in NSC 162/2. This approach became known as the ""New Look"", and was initiated with defense cuts in late 1953. On May 1, 1960, a U.S. one-man U-2 spy plane was reportedly shot down at high altitude over Soviet Union airspace. The flight was made to gain photo intelligence before the scheduled opening of an East–West summit conference, which had been scheduled in Paris, 15 days later. Captain Francis Gary Powers had bailed out of his aircraft and was captured after parachuting down onto Russian soil. Four days after Powers disappeared, the Eisenhower Administration had NASA issue a very detailed press release noting that an aircraft had ""gone missing"" north of Turkey. It speculated that the pilot might have fallen unconscious while the autopilot was still engaged, and falsely claimed that ""the pilot reported over the emergency frequency that he was experiencing oxygen difficulties."" Two controversies during the campaign tested him and his staff, but did not affect the campaign. One involved a report that Nixon had improperly received funds from a secret trust. Nixon spoke out adroitly to avoid potential damage, but the matter permanently alienated the two candidates. The second issue centered on Eisenhower's relented decision to confront the controversial methods of Joseph McCarthy on his home turf in a Wisconsin appearance. Just two weeks prior to the election, Eisenhower vowed to go to Korea and end the war there. He promised to maintain a strong commitment against Communism while avoiding the topic of NATO; finally, he stressed a corruption-free, frugal administration at home."
Solar_energy,"In 2011, a report by the International Energy Agency found that solar energy technologies such as photovoltaics, solar hot water and concentrated solar power could provide a third of the world’s energy by 2060 if politicians commit to limiting climate change. The energy from the sun could play a key role in de-carbonizing the global economy alongside improvements in energy efficiency and imposing costs on greenhouse gas emitters. ""The strength of solar is the incredible variety and flexibility of applications, from small scale to big scale"". Concentrating Solar Power (CSP) systems use lenses or mirrors and tracking systems to focus a large area of sunlight into a small beam. The concentrated heat is then used as a heat source for a conventional power plant. A wide range of concentrating technologies exists; the most developed are the parabolic trough, the concentrating linear fresnel reflector, the Stirling dish and the solar power tower. Various techniques are used to track the Sun and focus light. In all of these systems a working fluid is heated by the concentrated sunlight, and is then used for power generation or energy storage. Beginning with the surge in coal use which accompanied the Industrial Revolution, energy consumption has steadily transitioned from wood and biomass to fossil fuels. The early development of solar technologies starting in the 1860s was driven by an expectation that coal would soon become scarce. However, development of solar technologies stagnated in the early 20th century in the face of the increasing availability, economy, and utility of coal and petroleum. The Earth receives 174,000 terawatts (TW) of incoming solar radiation (insolation) at the upper atmosphere. Approximately 30% is reflected back to space while the rest is absorbed by clouds, oceans and land masses. The spectrum of solar light at the Earth's surface is mostly spread across the visible and near-infrared ranges with a small part in the near-ultraviolet. Most people around the world live in areas with insolation levels of 150 to 300 watts per square meter or 3.5 to 7.0 kWh/m2 per day. Solar radiation is absorbed by the Earth's land surface, oceans – which cover about 71% of the globe – and atmosphere. Warm air containing evaporated water from the oceans rises, causing atmospheric circulation or convection. When the air reaches a high altitude, where the temperature is low, water vapor condenses into clouds, which rain onto the Earth's surface, completing the water cycle. The latent heat of water condensation amplifies convection, producing atmospheric phenomena such as wind, cyclones and anti-cyclones. Sunlight absorbed by the oceans and land masses keeps the surface at an average temperature of 14 °C. By photosynthesis green plants convert solar energy into chemically stored energy, which produces food, wood and the biomass from which fossil fuels are derived. Greenhouses convert solar light to heat, enabling year-round production and the growth (in enclosed environments) of specialty crops and other plants not naturally suited to the local climate. Primitive greenhouses were first used during Roman times to produce cucumbers year-round for the Roman emperor Tiberius. The first modern greenhouses were built in Europe in the 16th century to keep exotic plants brought back from explorations abroad. Greenhouses remain an important part of horticulture today, and plastic transparent materials have also been used to similar effect in polytunnels and row covers. In addition, land availability has a large effect on the available solar energy because solar panels can only be set up on land that is unowned and suitable for solar panels. Roofs have been found to be a suitable place for solar cells, as many people have discovered that they can collect energy directly from their homes this way. Other areas that are suitable for solar cells are lands that are unowned by businesses where solar plants can be established. Active solar techniques use photovoltaics, concentrated solar power, solar thermal collectors, pumps, and fans to convert sunlight into useful outputs. Passive solar techniques include selecting materials with favorable thermal properties, designing spaces that naturally circulate air, and referencing the position of a building to the Sun. Active solar technologies increase the supply of energy and are considered supply side technologies, while passive solar technologies reduce the need for alternate resources and are generally considered demand side technologies. Urban heat islands (UHI) are metropolitan areas with higher temperatures than that of the surrounding environment. The higher temperatures are a result of increased absorption of the Solar light by urban materials such as asphalt and concrete, which have lower albedos and higher heat capacities than those in the natural environment. A straightforward method of counteracting the UHI effect is to paint buildings and roads white and plant trees. Using these methods, a hypothetical ""cool communities"" program in Los Angeles has projected that urban temperatures could be reduced by approximately 3 °C at an estimated cost of US$1 billion, giving estimated total annual benefits of US$530 million from reduced air-conditioning costs and healthcare savings. The International Energy Agency has said that solar energy can make considerable contributions to solving some of the most urgent problems the world now faces: In 2000, the United Nations Development Programme, UN Department of Economic and Social Affairs, and World Energy Council published an estimate of the potential solar energy that could be used by humans each year that took into account factors such as insolation, cloud cover, and the land that is usable by humans. The estimate found that solar energy has a global potential of 1,575–49,837 EJ per year (see table below). In the last two decades, photovoltaics (PV), also known as solar PV, has evolved from a pure niche market of small scale applications towards becoming a mainstream electricity source. A solar cell is a device that converts light directly into electricity using the photoelectric effect. The first solar cell was constructed by Charles Fritts in the 1880s. In 1931 a German engineer, Dr Bruno Lange, developed a photo cell using silver selenide in place of copper oxide. Although the prototype selenium cells converted less than 1% of incident light into electricity, both Ernst Werner von Siemens and James Clerk Maxwell recognized the importance of this discovery. Following the work of Russell Ohl in the 1940s, researchers Gerald Pearson, Calvin Fuller and Daryl Chapin created the crystalline silicon solar cell in 1954. These early solar cells cost 286 USD/watt and reached efficiencies of 4.5–6%. By 2012 available efficiencies exceed 20% and the maximum efficiency of research photovoltaics is over 40%. A solar balloon is a black balloon that is filled with ordinary air. As sunlight shines on the balloon, the air inside is heated and expands causing an upward buoyancy force, much like an artificially heated hot air balloon. Some solar balloons are large enough for human flight, but usage is generally limited to the toy market as the surface-area to payload-weight ratio is relatively high. The total solar energy absorbed by Earth's atmosphere, oceans and land masses is approximately 3,850,000 exajoules (EJ) per year. In 2002, this was more energy in one hour than the world used in one year. Photosynthesis captures approximately 3,000 EJ per year in biomass. The amount of solar energy reaching the surface of the planet is so vast that in one year it is about twice as much as will ever be obtained from all of the Earth's non-renewable resources of coal, oil, natural gas, and mined uranium combined, Solar distillation can be used to make saline or brackish water potable. The first recorded instance of this was by 16th-century Arab alchemists. A large-scale solar distillation project was first constructed in 1872 in the Chilean mining town of Las Salinas. The plant, which had solar collection area of 4,700 m2 (51,000 sq ft), could produce up to 22,700 L (5,000 imp gal; 6,000 US gal) per day and operate for 40 years. Individual still designs include single-slope, double-slope (or greenhouse type), vertical, conical, inverted absorber, multi-wick, and multiple effect. These stills can operate in passive, active, or hybrid modes. Double-slope stills are the most economical for decentralized domestic purposes, while active multiple effect units are more suitable for large-scale applications. Solar power is the conversion of sunlight into electricity, either directly using photovoltaics (PV), or indirectly using concentrated solar power (CSP). CSP systems use lenses or mirrors and tracking systems to focus a large area of sunlight into a small beam. PV converts light into electric current using the photoelectric effect. As of 2007, the total installed capacity of solar hot water systems is approximately 154 thermal gigawatt (GWth). China is the world leader in their deployment with 70 GWth installed as of 2006 and a long-term goal of 210 GWth by 2020. Israel and Cyprus are the per capita leaders in the use of solar hot water systems with over 90% of homes using them. In the United States, Canada and Australia heating swimming pools is the dominant application of solar hot water with an installed capacity of 18 GWth as of 2005. Off-grid PV systems have traditionally used rechargeable batteries to store excess electricity. With grid-tied systems, excess electricity can be sent to the transmission grid, while standard grid electricity can be used to meet shortfalls. Net metering programs give household systems a credit for any electricity they deliver to the grid. This is handled by 'rolling back' the meter whenever the home produces more electricity than it consumes. If the net electricity use is below zero, the utility then rolls over the kilowatt hour credit to the next month. Other approaches involve the use of two meters, to measure electricity consumed vs. electricity produced. This is less common due to the increased installation cost of the second meter. Most standard meters accurately measure in both directions, making a second meter unnecessary. In the United States, heating, ventilation and air conditioning (HVAC) systems account for 30% (4.65 EJ/yr) of the energy used in commercial buildings and nearly 50% (10.1 EJ/yr) of the energy used in residential buildings. Solar heating, cooling and ventilation technologies can be used to offset a portion of this energy. Solar water disinfection (SODIS) involves exposing water-filled plastic polyethylene terephthalate (PET) bottles to sunlight for several hours. Exposure times vary depending on weather and climate from a minimum of six hours to two days during fully overcast conditions. It is recommended by the World Health Organization as a viable method for household water treatment and safe storage. Over two million people in developing countries use this method for their daily drinking water. The large magnitude of solar energy available makes it a highly appealing source of electricity. The United Nations Development Programme in its 2000 World Energy Assessment found that the annual potential of solar energy was 1,575–49,837 exajoules (EJ). This is several times larger than the total world energy consumption, which was 559.8 EJ in 2012. Thermal mass is any material that can be used to store heat—heat from the Sun in the case of solar energy. Common thermal mass materials include stone, cement and water. Historically they have been used in arid climates or warm temperate regions to keep buildings cool by absorbing solar energy during the day and radiating stored heat to the cooler atmosphere at night. However, they can be used in cold temperate areas to maintain warmth as well. The size and placement of thermal mass depend on several factors such as climate, daylighting and shading conditions. When properly incorporated, thermal mass maintains space temperatures in a comfortable range and reduces the need for auxiliary heating and cooling equipment. Solar energy may be used in a water stabilisation pond to treat waste water without chemicals or electricity. A further environmental advantage is that algae grow in such ponds and consume carbon dioxide in photosynthesis, although algae may produce toxic chemicals that make the water unusable. Development of a solar-powered car has been an engineering goal since the 1980s. The World Solar Challenge is a biannual solar-powered car race, where teams from universities and enterprises compete over 3,021 kilometres (1,877 mi) across central Australia from Darwin to Adelaide. In 1987, when it was founded, the winner's average speed was 67 kilometres per hour (42 mph) and by 2007 the winner's average speed had improved to 90.87 kilometres per hour (56.46 mph). The North American Solar Challenge and the planned South African Solar Challenge are comparable competitions that reflect an international interest in the engineering and development of solar powered vehicles. Agriculture and horticulture seek to optimize the capture of solar energy in order to optimize the productivity of plants. Techniques such as timed planting cycles, tailored row orientation, staggered heights between rows and the mixing of plant varieties can improve crop yields. While sunlight is generally considered a plentiful resource, the exceptions highlight the importance of solar energy to agriculture. During the short growing seasons of the Little Ice Age, French and English farmers employed fruit walls to maximize the collection of solar energy. These walls acted as thermal masses and accelerated ripening by keeping plants warm. Early fruit walls were built perpendicular to the ground and facing south, but over time, sloping walls were developed to make better use of sunlight. In 1699, Nicolas Fatio de Duillier even suggested using a tracking mechanism which could pivot to follow the Sun. Applications of solar energy in agriculture aside from growing crops include pumping water, drying crops, brooding chicks and drying chicken manure. More recently the technology has been embraced by vinters, who use the energy generated by solar panels to power grape presses. Commercial solar water heaters began appearing in the United States in the 1890s. These systems saw increasing use until the 1920s but were gradually replaced by cheaper and more reliable heating fuels. As with photovoltaics, solar water heating attracted renewed attention as a result of the oil crises in the 1970s but interest subsided in the 1980s due to falling petroleum prices. Development in the solar water heating sector progressed steadily throughout the 1990s and growth rates have averaged 20% per year since 1999. Although generally underestimated, solar water heating and cooling is by far the most widely deployed solar technology with an estimated capacity of 154 GW as of 2007. In 1974, the unmanned AstroFlight Sunrise plane made the first solar flight. On 29 April 1979, the Solar Riser made the first flight in a solar-powered, fully controlled, man carrying flying machine, reaching an altitude of 40 feet (12 m). In 1980, the Gossamer Penguin made the first piloted flights powered solely by photovoltaics. This was quickly followed by the Solar Challenger which crossed the English Channel in July 1981. In 1990 Eric Scott Raymond in 21 hops flew from California to North Carolina using solar power. Developments then turned back to unmanned aerial vehicles (UAV) with the Pathfinder (1997) and subsequent designs, culminating in the Helios which set the altitude record for a non-rocket-propelled aircraft at 29,524 metres (96,864 ft) in 2001. The Zephyr, developed by BAE Systems, is the latest in a line of record-breaking solar aircraft, making a 54-hour flight in 2007, and month-long flights were envisioned by 2010. As of 2015, Solar Impulse, an electric aircraft, is currently circumnavigating the globe. It is a single-seat plane powered by solar cells and capable of taking off under its own power. The designed allows the aircraft to remain airborne for 36 hours. In 1975, the first practical solar boat was constructed in England. By 1995, passenger boats incorporating PV panels began appearing and are now used extensively. In 1996, Kenichi Horie made the first solar powered crossing of the Pacific Ocean, and the sun21 catamaran made the first solar powered crossing of the Atlantic Ocean in the winter of 2006–2007. There were plans to circumnavigate the globe in 2010. Solar energy is radiant light and heat from the Sun harnessed using a range of ever-evolving technologies such as solar heating, photovoltaics, solar thermal energy, solar architecture and artificial photosynthesis. The 1973 oil embargo and 1979 energy crisis caused a reorganization of energy policies around the world and brought renewed attention to developing solar technologies. Deployment strategies focused on incentive programs such as the Federal Photovoltaic Utilization Program in the US and the Sunshine Program in Japan. Other efforts included the formation of research facilities in the US (SERI, now NREL), Japan (NEDO), and Germany (Fraunhofer Institute for Solar Energy Systems ISE). Solar cookers use sunlight for cooking, drying and pasteurization. They can be grouped into three broad categories: box cookers, panel cookers and reflector cookers. The simplest solar cooker is the box cooker first built by Horace de Saussure in 1767. A basic box cooker consists of an insulated container with a transparent lid. It can be used effectively with partially overcast skies and will typically reach temperatures of 90–150 °C (194–302 °F). Panel cookers use a reflective panel to direct sunlight onto an insulated container and reach temperatures comparable to box cookers. Reflector cookers use various concentrating geometries (dish, trough, Fresnel mirrors) to focus light on a cooking container. These cookers reach temperatures of 315 °C (599 °F) and above but require direct light to function properly and must be repositioned to track the Sun. The International Organization for Standardization has established a number of standards relating to solar energy equipment. For example, ISO 9050 relates to glass in building while ISO 10217 relates to the materials used in solar water heaters. Shuman built the world’s first solar thermal power station in Maadi, Egypt, between 1912 and 1913. Shuman’s plant used parabolic troughs to power a 45–52 kilowatts (60–70 hp) engine that pumped more than 22,000 litres (4,800 imp gal; 5,800 US gal) of water per minute from the Nile River to adjacent cotton fields. Although the outbreak of World War I and the discovery of cheap oil in the 1930s discouraged the advancement of solar energy, Shuman’s vision and basic design were resurrected in the 1970s with a new wave of interest in solar thermal energy. In 1916 Shuman was quoted in the media advocating solar energy's utilization, saying: Deciduous trees and plants have been promoted as a means of controlling solar heating and cooling. When planted on the southern side of a building in the northern hemisphere or the northern side in the southern hemisphere, their leaves provide shade during the summer, while the bare limbs allow light to pass during the winter. Since bare, leafless trees shade 1/3 to 1/2 of incident solar radiation, there is a balance between the benefits of summer shading and the corresponding loss of winter heating. In climates with significant heating loads, deciduous trees should not be planted on the Equator facing side of a building because they will interfere with winter solar availability. They can, however, be used on the east and west sides to provide a degree of summer shading without appreciably affecting winter solar gain. Solar concentrating technologies such as parabolic dish, trough and Scheffler reflectors can provide process heat for commercial and industrial applications. The first commercial system was the Solar Total Energy Project (STEP) in Shenandoah, Georgia, USA where a field of 114 parabolic dishes provided 50% of the process heating, air conditioning and electrical requirements for a clothing factory. This grid-connected cogeneration system provided 400 kW of electricity plus thermal energy in the form of 401 kW steam and 468 kW chilled water, and had a one-hour peak load thermal storage. Evaporation ponds are shallow pools that concentrate dissolved solids through evaporation. The use of evaporation ponds to obtain salt from sea water is one of the oldest applications of solar energy. Modern uses include concentrating brine solutions used in leach mining and removing dissolved solids from waste streams. Clothes lines, clotheshorses, and clothes racks dry clothes through evaporation by wind and sunlight without consuming electricity or gas. In some states of the United States legislation protects the ""right to dry"" clothes. Unglazed transpired collectors (UTC) are perforated sun-facing walls used for preheating ventilation air. UTCs can raise the incoming air temperature up to 22 °C (40 °F) and deliver outlet temperatures of 45–60 °C (113–140 °F). The short payback period of transpired collectors (3 to 12 years) makes them a more cost-effective alternative than glazed collection systems. As of 2003, over 80 systems with a combined collector area of 35,000 square metres (380,000 sq ft) had been installed worldwide, including an 860 m2 (9,300 sq ft) collector in Costa Rica used for drying coffee beans and a 1,300 m2 (14,000 sq ft) collector in Coimbatore, India, used for drying marigolds. Solar power is anticipated to become the world's largest source of electricity by 2050, with solar photovoltaics and concentrated solar power contributing 16 and 11 percent to the global overall consumption, respectively. A solar chimney (or thermal chimney, in this context) is a passive solar ventilation system composed of a vertical shaft connecting the interior and exterior of a building. As the chimney warms, the air inside is heated causing an updraft that pulls air through the building. Performance can be improved by using glazing and thermal mass materials in a way that mimics greenhouses. The potential solar energy that could be used by humans differs from the amount of solar energy present near the surface of the planet because factors such as geography, time variation, cloud cover, and the land available to humans limits the amount of solar energy that we can acquire. Hydrogen production technologies been a significant area of solar chemical research since the 1970s. Aside from electrolysis driven by photovoltaic or photochemical cells, several thermochemical processes have also been explored. One such route uses concentrators to split water into oxygen and hydrogen at high temperatures (2,300–2,600 °C or 4,200–4,700 °F). Another approach uses the heat from solar concentrators to drive the steam reformation of natural gas thereby increasing the overall hydrogen yield compared to conventional reforming methods. Thermochemical cycles characterized by the decomposition and regeneration of reactants present another avenue for hydrogen production. The Solzinc process under development at the Weizmann Institute uses a 1 MW solar furnace to decompose zinc oxide (ZnO) at temperatures above 1,200 °C (2,200 °F). This initial reaction produces pure zinc, which can subsequently be reacted with water to produce hydrogen. Geography effects solar energy potential because areas that are closer to the equator have a greater amount of solar radiation. However, the use of photovoltaics that can follow the position of the sun can significantly increase the solar energy potential in areas that are farther from the equator. Time variation effects the potential of solar energy because during the nighttime there is little solar radiation on the surface of the Earth for solar panels to absorb. This limits the amount of energy that solar panels can absorb in one day. Cloud cover can effect the potential of solar panels because clouds block incoming light from the sun and reduce the light available for solar cells. Phase change materials such as paraffin wax and Glauber's salt are another thermal storage media. These materials are inexpensive, readily available, and can deliver domestically useful temperatures (approximately 64 °C or 147 °F). The ""Dover House"" (in Dover, Massachusetts) was the first to use a Glauber's salt heating system, in 1948. Solar energy can also be stored at high temperatures using molten salts. Salts are an effective storage medium because they are low-cost, have a high specific heat capacity and can deliver heat at temperatures compatible with conventional power systems. The Solar Two used this method of energy storage, allowing it to store 1.44 terajoules (400,000 kWh) in its 68 cubic metres storage tank with an annual storage efficiency of about 99%. In 2011, the International Energy Agency said that ""the development of affordable, inexhaustible and clean solar energy technologies will have huge longer-term benefits. It will increase countries’ energy security through reliance on an indigenous, inexhaustible and mostly import-independent resource, enhance sustainability, reduce pollution, lower the costs of mitigating global warming, and keep fossil fuel prices lower than otherwise. These advantages are global. Hence the additional costs of the incentives for early deployment should be considered learning investments; they must be wisely spent and need to be widely shared"". Commercial CSP plants were first developed in the 1980s. Since 1985 the eventually 354 MW SEGS CSP installation, in the Mojave Desert of California, is the largest solar power plant in the world. Other large CSP plants include the 150 MW Solnova Solar Power Station and the 100 MW Andasol solar power station, both in Spain. The 250 MW Agua Caliente Solar Project, in the United States, and the 221 MW Charanka Solar Park in India, are the world’s largest photovoltaic plants. Solar projects exceeding 1 GW are being developed, but most of the deployed photovoltaics are in small rooftop arrays of less than 5 kW, which are grid connected using net metering and/or a feed-in tariff. In 2013 solar generated less than 1% of the worlds total grid electricity. The common features of passive solar architecture are orientation relative to the Sun, compact proportion (a low surface area to volume ratio), selective shading (overhangs) and thermal mass. When these features are tailored to the local climate and environment they can produce well-lit spaces that stay in a comfortable temperature range. Socrates' Megaron House is a classic example of passive solar design. The most recent approaches to solar design use computer modeling tying together solar lighting, heating and ventilation systems in an integrated solar design package. Active solar equipment such as pumps, fans and switchable windows can complement passive design and improve system performance. Solar hot water systems use sunlight to heat water. In low geographical latitudes (below 40 degrees) from 60 to 70% of the domestic hot water use with temperatures up to 60 °C can be provided by solar heating systems. The most common types of solar water heaters are evacuated tube collectors (44%) and glazed flat plate collectors (34%) generally used for domestic hot water; and unglazed plastic collectors (21%) used mainly to heat swimming pools. Solar chemical processes use solar energy to drive chemical reactions. These processes offset energy that would otherwise come from a fossil fuel source and can also convert solar energy into storable and transportable fuels. Solar induced chemical reactions can be divided into thermochemical or photochemical. A variety of fuels can be produced by artificial photosynthesis. The multielectron catalytic chemistry involved in making carbon-based fuels (such as methanol) from reduction of carbon dioxide is challenging; a feasible alternative is hydrogen production from protons, though use of water as the source of electrons (as plants do) requires mastering the multielectron oxidation of two water molecules to molecular oxygen. Some have envisaged working solar fuel plants in coastal metropolitan areas by 2050 –  the splitting of sea water providing hydrogen to be run through adjacent fuel-cell electric power plants and the pure water by-product going directly into the municipal water system. Another vision involves all human structures covering the earth's surface (i.e., roads, vehicles and buildings) doing photosynthesis more efficiently than plants. Solar technologies are broadly characterized as either passive or active depending on the way they capture, convert and distribute sunlight and enable solar energy to be harnessed at different levels around the world, mostly depending on distance from the equator. Although solar energy refers primarily to the use of solar radiation for practical ends, all renewable energies, other than geothermal and tidal, derive their energy from the Sun in a direct or indirect way. In 1897, Frank Shuman, a U.S. inventor, engineer and solar energy pioneer built a small demonstration solar engine that worked by reflecting solar energy onto square boxes filled with ether, which has a lower boiling point than water, and were fitted internally with black pipes which in turn powered a steam engine. In 1908 Shuman formed the Sun Power Company with the intent of building larger solar power plants. He, along with his technical advisor A.S.E. Ackermann and British physicist Sir Charles Vernon Boys, developed an improved system using mirrors to reflect solar energy upon collector boxes, increasing heating capacity to the extent that water could now be used instead of ether. Shuman then constructed a full-scale steam engine powered by low-pressure water, enabling him to patent the entire solar engine system by 1912. Thermal mass systems can store solar energy in the form of heat at domestically useful temperatures for daily or interseasonal durations. Thermal storage systems generally use readily available materials with high specific heat capacities such as water, earth and stone. Well-designed systems can lower peak demand, shift time-of-use to off-peak hours and reduce overall heating and cooling requirements. It is an important source of renewable energy and its technologies are broadly characterized as either passive solar or active solar depending on the way they capture and distribute solar energy or convert it into solar power. Active solar techniques include the use of photovoltaic systems, concentrated solar power and solar water heating to harness the energy. Passive solar techniques include orienting a building to the Sun, selecting materials with favorable thermal mass or light dispersing properties, and designing spaces that naturally circulate air. Pumped-storage hydroelectricity stores energy in the form of water pumped when energy is available from a lower elevation reservoir to a higher elevation one. The energy is recovered when demand is high by releasing the water, with the pump becoming a hydroelectric power generator. Sunlight has influenced building design since the beginning of architectural history. Advanced solar architecture and urban planning methods were first employed by the Greeks and Chinese, who oriented their buildings toward the south to provide light and warmth."
Antenna_(radio),"It is possible to use the impedance matching concepts to construct vertical antennas substantially shorter than the 1⁄4 wavelength at which the antenna is resonant. By adding an inductance in series with the antenna, a so-called loading coil, the capacitive reactance of this antenna can be cancelled leaving a pure resistance which can then be matched to the transmission line. Sometimes the resulting resonant frequency of such a system (antenna plus matching network) is described using the construct of electrical length and the use of a shorter antenna at a lower frequency than its resonant frequency is termed electrical lengthening. An antenna (plural antennae or antennas), or aerial, is an electrical device which converts electric power into radio waves, and vice versa. It is usually used with a radio transmitter or radio receiver. In transmission, a radio transmitter supplies an electric current oscillating at radio frequency (i.e. a high frequency alternating current (AC)) to the antenna's terminals, and the antenna radiates the energy from the current as electromagnetic waves (radio waves). In reception, an antenna intercepts some of the power of an electromagnetic wave in order to produce a tiny voltage at its terminals, that is applied to a receiver to be amplified. The quarter-wave elements imitate a series-resonant electrical element due to the standing wave present along the conductor. At the resonant frequency, the standing wave has a current peak and voltage node (minimum) at the feed. In electrical terms, this means the element has minimum reactance, generating the maximum current for minimum voltage. This is the ideal situation, because it produces the maximum output for the minimum input, producing the highest possible efficiency. Contrary to an ideal (lossless) series-resonant circuit, a finite resistance remains (corresponding to the relatively small voltage at the feed-point) due to the antenna's radiation resistance as well as any actual electrical losses. For instance, if a transmitter delivers 100 W into an antenna having an efficiency of 80%, then the antenna will radiate 80 W as radio waves and produce 20 W of heat. In order to radiate 100 W of power, one would need to use a transmitter capable of supplying 125 W to the antenna. Note that antenna efficiency is a separate issue from impedance matching, which may also reduce the amount of power radiated using a given transmitter. If an SWR meter reads 150 W of incident power and 50 W of reflected power, that means that 100 W have actually been absorbed by the antenna (ignoring transmission line losses). How much of that power has actually been radiated cannot be directly determined through electrical measurements at (or before) the antenna terminals, but would require (for instance) careful measurement of field strength. Fortunately the loss resistance of antenna conductors such as aluminum rods can be calculated and the efficiency of an antenna using such materials predicted. It is a fundamental property of antennas that the electrical characteristics of an antenna described in the next section, such as gain, radiation pattern, impedance, bandwidth, resonant frequency and polarization, are the same whether the antenna is transmitting or receiving. For example, the ""receiving pattern"" (sensitivity as a function of direction) of an antenna when used for reception is identical to the radiation pattern of the antenna when it is driven and functions as a radiator. This is a consequence of the reciprocity theorem of electromagnetics. Therefore, in discussions of antenna properties no distinction is usually made between receiving and transmitting terminology, and the antenna can be viewed as either transmitting or receiving, whichever is more convenient. The radio signal's electrical component induces a voltage in the conductor. This causes an electrical current to begin flowing in the direction of the signal's instantaneous field. When the resulting current reaches the end of the conductor, it reflects, which is equivalent to a 180 degree change in phase. If the conductor is 1⁄4 of a wavelength long, current from the feed point will undergo 90 degree phase change by the time it reaches the end of the conductor, reflect through 180 degrees, and then another 90 degrees as it travels back. That means it has undergone a total 360 degree phase change, returning it to the original signal. The current in the element thus adds to the current being created from the source at that instant. This process creates a standing wave in the conductor, with the maximum current at the feed. The words antenna (plural: antennas in US English, although both ""antennas"" and ""antennae"" are used in International English) and aerial are used interchangeably. Occasionally the term ""aerial"" is used to mean a wire antenna. However, note the important international technical journal, the IEEE Transactions on Antennas and Propagation. In the United Kingdom and other areas where British English is used, the term aerial is sometimes used although 'antenna' has been universal in professional use for many years. The ordinary half-wave dipole is probably the most widely used antenna design. This consists of two 1⁄4-wavelength elements arranged end-to-end, and lying along essentially the same axis (or collinear), each feeding one side of a two-conductor transmission wire. The physical arrangement of the two elements places them 180 degrees out of phase, which means that at any given instant one of the elements is driving current into the transmission line while the other is pulling it out. The monopole antenna is essentially one half of the half-wave dipole, a single 1⁄4-wavelength element with the other side connected to ground or an equivalent ground plane (or counterpoise). Monopoles, which are one-half the size of a dipole, are common for long-wavelength radio signals where a dipole would be impractically large. Another common design is the folded dipole, which is essentially two dipoles placed side-by-side and connected at their ends to make a single one-wavelength antenna. Polarization is the sum of the E-plane orientations over time projected onto an imaginary plane perpendicular to the direction of motion of the radio wave. In the most general case, polarization is elliptical, meaning that the polarization of the radio waves varies over time. Two special cases are linear polarization (the ellipse collapses into a line) as we have discussed above, and circular polarization (in which the two axes of the ellipse are equal). In linear polarization the electric field of the radio wave oscillates back and forth along one direction; this can be affected by the mounting of the antenna but usually the desired direction is either horizontal or vertical polarization. In circular polarization, the electric field (and magnetic field) of the radio wave rotates at the radio frequency circularly around the axis of propagation. Circular or elliptically polarized radio waves are designated as right-handed or left-handed using the ""thumb in the direction of the propagation"" rule. Note that for circular polarization, optical researchers use the opposite right hand rule from the one used by radio engineers. The most widely used class of antenna, a dipole antenna consists of two symmetrical radiators such as metal rods or wires, with one side of the balanced feedline from the transmitter or receiver attached to each. A horizontal dipole radiates in two lobes perpendicular to the antenna's axis. A half-wave dipole the most common type, has two collinear elements each a quarter wavelength long and a gain of 2.15 dBi. Used individually as low gain antennas, dipoles are also used as driven elements in many more complicated higher gain types of antennas. However loss resistance will generally affect the feedpoint impedance, adding to its resistive (real) component. That resistance will consist of the sum of the radiation resistance Rr and the loss resistance Rloss. If an rms current I is delivered to the terminals of an antenna, then a power of I2Rr will be radiated and a power of I2Rloss will be lost as heat. Therefore, the efficiency of an antenna is equal to Rr / (Rr + Rloss). Of course only the total resistance Rr + Rloss can be directly measured. Reflections generally affect polarization. For radio waves, one important reflector is the ionosphere which can change the wave's polarization. Thus for signals received following reflection by the ionosphere (a skywave), a consistent polarization cannot be expected. For line-of-sight communications or ground wave propagation, horizontally or vertically polarized transmissions generally remain in about the same polarization state at the receiving location. Matching the receiving antenna's polarization to that of the transmitter can make a very substantial difference in received signal strength. Antennas are required by any radio receiver or transmitter to couple its electrical connection to the electromagnetic field. Radio waves are electromagnetic waves which carry signals through the air (or through space) at the speed of light with almost no transmission loss. Radio transmitters and receivers are used to convey signals (information) in systems including broadcast (audio) radio, television, mobile telephones, Wi-Fi (WLAN) data networks, trunk lines and point-to-point communications links (telephone, data networks), satellite links, many remote controlled devices such as garage door openers, and wireless remote sensors, among many others. Radio waves are also used directly for measurements in technologies including radar, GPS, and radio astronomy. In each and every case, the transmitters and receivers involved require antennas, although these are sometimes hidden (such as the antenna inside an AM radio or inside a laptop computer equipped with Wi-Fi). The radiation pattern and even the driving point impedance of an antenna can be influenced by the dielectric constant and especially conductivity of nearby objects. For a terrestrial antenna, the ground is usually one such object of importance. The antenna's height above the ground, as well as the electrical properties (permittivity and conductivity) of the ground, can then be important. Also, in the particular case of a monopole antenna, the ground (or an artificial ground plane) serves as the return connection for the antenna current thus having an additional effect, particularly on the impedance seen by the feed line. The radiation of many antennas shows a pattern of maxima or ""lobes"" at various angles, separated by ""nulls"", angles where the radiation falls to zero. This is because the radio waves emitted by different parts of the antenna typically interfere, causing maxima at angles where the radio waves arrive at distant points in phase, and zero radiation at other angles where the radio waves arrive out of phase. In a directional antenna designed to project radio waves in a particular direction, the lobe in that direction is designed larger than the others and is called the ""main lobe"". The other lobes usually represent unwanted radiation and are called ""sidelobes"". The axis through the main lobe is called the ""principal axis"" or ""boresight axis"". This is a consequence of Lorentz reciprocity. For an antenna element  not connected to anything (open circuited) one can write . But for an element  which is short circuited, a current is generated across that short but no voltage is allowed, so the corresponding . This is the case, for instance, with the so-called parasitic elements of a Yagi-Uda antenna where the solid rod can be viewed as a dipole antenna shorted across its feedpoint. Parasitic elements are unpowered elements that absorb and reradiate RF energy according to the induced current calculated using such a system of equations. Antenna tuning generally refers to cancellation of any reactance seen at the antenna terminals, leaving only a resistive impedance which might or might not be exactly the desired impedance (that of the transmission line). Although an antenna may be designed to have a purely resistive feedpoint impedance (such as a dipole 97% of a half wavelength long) this might not be exactly true at the frequency that it is eventually used at. In some cases the physical length of the antenna can be ""trimmed"" to obtain a pure resistance. On the other hand, the addition of a series inductance or parallel capacitance can be used to cancel a residual capacitative or inductive reactance, respectively. An electromagnetic wave refractor in some aperture antennas is a component which due to its shape and position functions to selectively delay or advance portions of the electromagnetic wavefront passing through it. The refractor alters the spatial characteristics of the wave on one side relative to the other side. It can, for instance, bring the wave to a focus or alter the wave front in other ways, generally in order to maximize the directivity of the antenna system. This is the radio equivalent of an optical lens. The bandwidth characteristics of a resonant antenna element can be characterized according to its Q, just as one uses to characterize the sharpness of an L-C resonant circuit. However it is often assumed that there is an advantage in an antenna having a high Q. After all, Q is short for ""quality factor"" and a low Q typically signifies excessive loss (due to unwanted resistance) in a resonant L-C circuit. However this understanding does not apply to resonant antennas where the resistance involved is the radiation resistance, a desired quantity which removes energy from the resonant element in order to radiate it (the purpose of an antenna, after all!). The Q is a measure of the ratio of reactance to resistance, so with a fixed radiation resistance (an element's radiation resistance is almost independent of its diameter) a greater reactance off-resonance corresponds to the poorer bandwidth of a very thin conductor. The Q of such a narrowband antenna can be as high as 15. On the other hand, a thick element presents less reactance at an off-resonant frequency, and consequently a Q as low as 5. These two antennas will perform equivalently at the resonant frequency, but the second antenna will perform over a bandwidth 3 times as wide as the ""hi-Q"" antenna consisting of a thin conductor. The end result is that the resonant antenna will efficiently feed a signal into the transmission line only when the source signal's frequency is close to that of the design frequency of the antenna, or one of the resonant multiples. This makes resonant antenna designs inherently narrowband, and they are most commonly used with a single target signal. They are particularly common on radar systems, where the same antenna is used for both broadcast and reception, or for radio and television broadcasts, where the antenna is working with a single frequency. They are less commonly used for reception where multiple channels are present, in which case additional modifications are used to increase the bandwidth, or entirely different antenna designs are used. Both the vertical and dipole antennas are simple in construction and relatively inexpensive. The dipole antenna, which is the basis for most antenna designs, is a balanced component, with equal but opposite voltages and currents applied at its two terminals through a balanced transmission line (or to a coaxial transmission line through a so-called balun). The vertical antenna, on the other hand, is a monopole antenna. It is typically connected to the inner conductor of a coaxial transmission line (or a matching network); the shield of the transmission line is connected to ground. In this way, the ground (or any large conductive surface) plays the role of the second conductor of a dipole, thereby forming a complete circuit. Since monopole antennas rely on a conductive ground, a so-called grounding structure may be employed to provide a better ground contact to the earth or which itself acts as a ground plane to perform that function regardless of (or in absence of) an actual contact with the earth. According to reciprocity, the efficiency of an antenna used as a receiving antenna is identical to the efficiency as defined above. The power that an antenna will deliver to a receiver (with a proper impedance match) is reduced by the same amount. In some receiving applications, the very inefficient antennas may have little impact on performance. At low frequencies, for example, atmospheric or man-made noise can mask antenna inefficiency. For example, CCIR Rep. 258-3 indicates man-made noise in a residential setting at 40 MHz is about 28 dB above the thermal noise floor. Consequently, an antenna with a 20 dB loss (due to inefficiency) would have little impact on system noise performance. The loss within the antenna will affect the intended signal and the noise/interference identically, leading to no reduction in signal to noise ratio (SNR). Recall that a current will reflect when there are changes in the electrical properties of the material. In order to efficiently send the signal into the transmission line, it is important that the transmission line has the same impedance as the elements, otherwise some of the signal will be reflected back into the antenna. This leads to the concept of impedance matching, the design of the overall system of antenna and transmission line so the impedance is as close as possible, thereby reducing these losses. Impedance matching between antennas and transmission lines is commonly handled through the use of a balun, although other solutions are also used in certain roles. An important measure of this basic concept is the standing wave ratio, which measures the magnitude of the reflected signal. The definition of antenna gain or power gain already includes the effect of the antenna's efficiency. Therefore, if one is trying to radiate a signal toward a receiver using a transmitter of a given power, one need only compare the gain of various antennas rather than considering the efficiency as well. This is likewise true for a receiving antenna at very high (especially microwave) frequencies, where the point is to receive a signal which is strong compared to the receiver's noise temperature. However, in the case of a directional antenna used for receiving signals with the intention of rejecting interference from different directions, one is no longer concerned with the antenna efficiency, as discussed above. In this case, rather than quoting the antenna gain, one would be more concerned with the directive gain which does not include the effect of antenna (in)efficiency. The directive gain of an antenna can be computed from the published gain divided by the antenna's efficiency. Maximum power transfer requires matching the impedance of an antenna system (as seen looking into the transmission line) to the complex conjugate of the impedance of the receiver or transmitter. In the case of a transmitter, however, the desired matching impedance might not correspond to the dynamic output impedance of the transmitter as analyzed as a source impedance but rather the design value (typically 50 ohms) required for efficient and safe operation of the transmitting circuitry. The intended impedance is normally resistive but a transmitter (and some receivers) may have additional adjustments to cancel a certain amount of reactance in order to ""tweak"" the match. When a transmission line is used in between the antenna and the transmitter (or receiver) one generally would like an antenna system whose impedance is resistive and near the characteristic impedance of that transmission line in order to minimize the standing wave ratio (SWR) and the increase in transmission line losses it entails, in addition to supplying a good match at the transmitter or receiver itself. For instance, a phased array consists of two or more simple antennas which are connected together through an electrical network. This often involves a number of parallel dipole antennas with a certain spacing. Depending on the relative phase introduced by the network, the same combination of dipole antennas can operate as a ""broadside array"" (directional normal to a line connecting the elements) or as an ""end-fire array"" (directional along the line connecting the elements). Antenna arrays may employ any basic (omnidirectional or weakly directional) antenna type, such as dipole, loop or slot antennas. These elements are often identical. For horizontal propagation between transmitting and receiving antennas situated near the ground reasonably far from each other, the distances traveled by tne direct and reflected rays are nearly the same. There is almost no relative phase shift. If the emission is polarized vertically, the two fields (direct and reflected) add and there is maximum of received signal. If the signal is polarized horizontally, the two signals subtract and the received signal is largely cancelled. The vertical plane radiation patterns are shown in the image at right. With vertical polarization there is always a maximum for θ=0, horizontal propagation (left pattern). For horizontal polarization, there is cancellation at that angle. Note that the above formulae and these plots assume the ground as a perfect conductor. These plots of the radiation pattern correspond to a distance between the antenna and its image of 2.5λ. As the antenna height is increased, the number of lobes increases as well. The radiation pattern of an antenna is a plot of the relative field strength of the radio waves emitted by the antenna at different angles. It is typically represented by a three-dimensional graph, or polar plots of the horizontal and vertical cross sections. The pattern of an ideal isotropic antenna, which radiates equally in all directions, would look like a sphere. Many nondirectional antennas, such as monopoles and dipoles, emit equal power in all horizontal directions, with the power dropping off at higher and lower angles; this is called an omnidirectional pattern and when plotted looks like a torus or donut. Consider a half-wave dipole designed to work with signals 1 m wavelength, meaning the antenna would be approximately 50 cm across. If the element has a length-to-diameter ratio of 1000, it will have an inherent resistance of about 63 ohms. Using the appropriate transmission wire or balun, we match that resistance to ensure minimum signal loss. Feeding that antenna with a current of 1 ampere will require 63 volts of RF, and the antenna will radiate 63 watts (ignoring losses) of radio frequency power. Now consider the case when the antenna is fed a signal with a wavelength of 1.25 m; in this case the reflected current would arrive at the feed out-of-phase with the signal, causing the net current to drop while the voltage remains the same. Electrically this appears to be a very high impedance. The antenna and transmission line no longer have the same impedance, and the signal will be reflected back into the antenna, reducing output. This could be addressed by changing the matching system between the antenna and transmission line, but that solution only works well at the new design frequency. When an electromagnetic wave strikes a plane surface such as the ground, part of the wave is transmitted into the ground and part of it is reflected, according to the Fresnel coefficients. If the ground is a very good conductor then almost all of the wave is reflected (180° out of phase), whereas a ground modeled as a (lossy) dielectric can absorb a large amount of the wave's power. The power remaining in the reflected wave, and the phase shift upon reflection, strongly depend on the wave's angle of incidence and polarization. The dielectric constant and conductivity (or simply the complex dielectric constant) is dependent on the soil type and is a function of frequency. It is also possible to use multiple active elements and combine them together with transmission lines to produce a similar system where the phases add up to reinforce the output. The antenna array and very similar reflective array antenna consist of multiple elements, often half-wave dipoles, spaced out on a plane and wired together with transmission lines with specific phase lengths to produce a single in-phase signal at the output. The log-periodic antenna is a more complex design that uses multiple in-line elements similar in appearance to the Yagi-Uda but using transmission lines between the elements to produce the output. The actual antenna which is transmitting the original wave then also may receive a strong signal from its own image from the ground. This will induce an additional current in the antenna element, changing the current at the feedpoint for a given feedpoint voltage. Thus the antenna's impedance, given by the ratio of feedpoint voltage to current, is altered due to the antenna's proximity to the ground. This can be quite a significant effect when the antenna is within a wavelength or two of the ground. But as the antenna height is increased, the reduced power of the reflected wave (due to the inverse square law) allows the antenna to approach its asymptotic feedpoint impedance given by theory. At lower heights, the effect on the antenna's impedance is very sensitive to the exact distance from the ground, as this affects the phase of the reflected wave relative to the currents in the antenna. Changing the antenna's height by a quarter wavelength, then changes the phase of the reflection by 180°, with a completely different effect on the antenna's impedance. The origin of the word antenna relative to wireless apparatus is attributed to Italian radio pioneer Guglielmo Marconi. In the summer of 1895, Marconi began testing his wireless system outdoors on his father's estate near Bologna and soon began to experiment with long wire ""aerials"". Marconi discovered that by raising the ""aerial"" wire above the ground and connecting the other side of his transmitter to ground, the transmission range was increased. Soon he was able to transmit signals over a hill, a distance of approximately 2.4 kilometres (1.5 mi). In Italian a tent pole is known as l'antenna centrale, and the pole with the wire was simply called l'antenna. Until then wireless radiating transmitting and receiving elements were known simply as aerials or terminals. Antennas more complex than the dipole or vertical designs are usually intended to increase the directivity and consequently the gain of the antenna. This can be accomplished in many different ways leading to a plethora of antenna designs. The vast majority of designs are fed with a balanced line (unlike a monopole antenna) and are based on the dipole antenna with additional components (or elements) which increase its directionality. Antenna ""gain"" in this instance describes the concentration of radiated power into a particular solid angle of space, as opposed to the spherically uniform radiation of the ideal radiator. The increased power in the desired direction is at the expense of that in the undesired directions. Power is conserved, and there is no net power increase over that delivered from the power source (the transmitter.) Polarization is predictable from an antenna's geometry, although in some cases it is not at all obvious (such as for the quad antenna). An antenna's linear polarization is generally along the direction (as viewed from the receiving location) of the antenna's currents when such a direction can be defined. For instance, a vertical whip antenna or Wi-Fi antenna vertically oriented will transmit and receive in the vertical polarization. Antennas with horizontal elements, such as most rooftop TV antennas in the United States, are horizontally polarized (broadcast TV in the U.S. usually uses horizontal polarization). Even when the antenna system has a vertical orientation, such as an array of horizontal dipole antennas, the polarization is in the horizontal direction corresponding to the current flow. The polarization of a commercial antenna is an essential specification. A Yagi-Uda array uses passive elements to greatly increase gain. It is built along a support boom that is pointed toward the signal, and thus sees no induced signal and does not contribute to the antenna's operation. The end closer to the source is referred to as the front. Near the rear is a single active element, typically a half-wave dipole or folded dipole. Passive elements are arranged in front (directors) and behind (reflectors) the active element along the boom. The Yagi has the inherent quality that it becomes increasingly directional, and thus has higher gain, as the number of elements increases. However, this also makes it increasingly sensitive to changes in frequency; if the signal frequency changes, not only does the active element receive less energy directly, but all of the passive elements adding to that signal also decrease their output as well and their signals no longer reach the active element in-phase. So an additional problem beyond canceling the unwanted reactance is of matching the remaining resistive impedance to the characteristic impedance of the transmission line. In principle this can always be done with a transformer, however the turns ratio of a transformer is not adjustable. A general matching network with at least two adjustments can be made to correct both components of impedance. Matching networks using discrete inductors and capacitors will have losses associated with those components, and will have power restrictions when used for transmitting. Avoiding these difficulties, commercial antennas are generally designed with fixed matching elements or feeding strategies to get an approximate match to standard coax, such as 50 or 75 Ohms. Antennas based on the dipole (rather than vertical antennas) should include a balun in between the transmission line and antenna element, which may be integrated into any such matching network. An antenna transmits and receives radio waves with a particular polarization which can be reoriented by tilting the axis of the antenna in many (but not all) cases. The physical size of an antenna is often a practical issue, particularly at lower frequencies (longer wavelengths). Highly directional antennas need to be significantly larger than the wavelength. Resonant antennas usually use a linear conductor (or element), or pair of such elements, each of which is about a quarter of the wavelength in length (an odd multiple of quarter wavelengths will also be resonant). Antennas that are required to be small compared to the wavelength sacrifice efficiency and cannot be very directional. Fortunately at higher frequencies (UHF, microwaves) trading off performance to obtain a smaller physical size is usually not required. The phase of reflection of electromagnetic waves depends on the polarization of the incident wave. Given the larger refractive index of the ground (typically n=2) compared to air (n=1), the phase of horizontally polarized radiation is reversed upon reflection (a phase shift of  radians or 180°). On the other hand, the vertical component of the wave's electric field is reflected at grazing angles of incidence approximately in phase. These phase shifts apply as well to a ground modelled as a good electrical conductor. The effective area or effective aperture of a receiving antenna expresses the portion of the power of a passing electromagnetic wave which it delivers to its terminals, expressed in terms of an equivalent area. For instance, if a radio wave passing a given location has a flux of 1 pW / m2 (10−12 watts per square meter) and an antenna has an effective area of 12 m2, then the antenna would deliver 12 pW of RF power to the receiver (30 microvolts rms at 75 ohms). Since the receiving antenna is not equally sensitive to signals received from all directions, the effective area is a function of the direction to the source. For example, at 30 MHz (10 m wavelength) a true resonant 1⁄4-wavelength monopole would be almost 2.5 meters long, and using an antenna only 1.5 meters tall would require the addition of a loading coil. Then it may be said that the coil has lengthened the antenna to achieve an electrical length of 2.5 meters. However, the resulting resistive impedance achieved will be quite a bit lower than the impedance of a resonant monopole, likely requiring further impedance matching. In addition to a lower radiation resistance, the reactance becomes higher as the antenna size is reduced, and the resonant circuit formed by the antenna and the tuning coil has a Q factor that rises and eventually causes the bandwidth of the antenna to be inadequate for the signal being transmitted. This is the major factor that sets the size of antennas at 1 MHz and lower frequencies. On the other hand, classical (analog) television transmissions are usually horizontally polarized, because in urban areas buildings can reflect the electromagnetic waves and create ghost images due to multipath propagation. Using horizontal polarization, ghosting is reduced because the amount of reflection of electromagnetic waves in the p polarization (horizontal polarization off the side of a building) is generally less than s (vertical, in this case) polarization. Vertically polarized analog television has nevertheless been used in some rural areas. In digital terrestrial television such reflections are less problematic, due to robustness of binary transmissions and error correction. Loop antennas consist of a loop or coil of wire. Loops with circumference of a wavelength or larger act similarly to dipole antennas. However loops small in comparison to a wavelength act differently. They interact with the magnetic field of the radio wave instead of the electric field as other antennas do, and so are relatively insensitive to nearby electrical noise. However they have low radiation resistance, and so are inefficient for transmitting. They are used as receiving antennas at low frequencies, and also as direction finding antennas. Current circulating in one antenna generally induces a voltage across the feedpoint of nearby antennas or antenna elements. The mathematics presented below are useful in analyzing the electrical behaviour of antenna arrays, where the properties of the individual array elements (such as half wave dipoles) are already known. If those elements were widely separated and driven in a certain amplitude and phase, then each would act independently as that element is known to. However, because of the mutual interaction between their electric and magnetic fields due to proximity, the currents in each element are not simply a function of the applied voltage (according to its driving point impedance), but depend on the currents in the other nearby elements. Note that this now is a near field phenomenon which could not be properly accounted for using the Friis transmission equation for instance. Instead, it is often desired to have an antenna whose impedance does not vary so greatly over a certain bandwidth. It turns out that the amount of reactance seen at the terminals of a resonant antenna when the frequency is shifted, say, by 5%, depends very much on the diameter of the conductor used. A long thin wire used as a half-wave dipole (or quarter wave monopole) will have a reactance significantly greater than the resistive impedance it has at resonance, leading to a poor match and generally unacceptable performance. Making the element using a tube of a diameter perhaps 1/50 of its length, however, results in a reactance at this altered frequency which is not so great, and a much less serious mismatch which will only modestly damage the antenna's net performance. Thus rather thick tubes are typically used for the solid elements of such antennas, including Yagi-Uda arrays. The polarization of an antenna refers to the orientation of the electric field (E-plane) of the radio wave with respect to the Earth's surface and is determined by the physical structure of the antenna and by its orientation; note that this designation is totally distinct from the antenna's directionality. Thus, a simple straight wire antenna will have one polarization when mounted vertically, and a different polarization when mounted horizontally. As a transverse wave, the magnetic field of a radio wave is at right angles to that of the electric field, but by convention, talk of an antenna's ""polarization"" is understood to refer to the direction of the electric field. One example of omnidirectional antennas is the very common vertical antenna or whip antenna consisting of a metal rod (often, but not always, a quarter of a wavelength long). A dipole antenna is similar but consists of two such conductors extending in opposite directions, with a total length that is often, but not always, a half of a wavelength long. Dipoles are typically oriented horizontally in which case they are weakly directional: signals are reasonably well radiated toward or received from all directions with the exception of the direction along the conductor itself; this region is called the antenna blind cone or null. Gain is a parameter which measures the degree of directivity of the antenna's radiation pattern. A high-gain antenna will radiate most of its power in a particular direction, while a low-gain antenna will radiate over a wider angle. The antenna gain, or power gain of an antenna is defined as the ratio of the intensity (power per unit surface area)  radiated by the antenna in the direction of its maximum output, at an arbitrary distance, divided by the intensity  radiated at the same distance by a hypothetical isotropic antenna which radiates equal power in all directions. This dimensionless ratio is usually expressed logarithmically in decibels, these units are called ""decibels-isotropic"" (dBi) This is fortunate, since antennas at lower frequencies which are not rather large (a good fraction of a wavelength in size) are inevitably inefficient (due to the small radiation resistance Rr of small antennas). Most AM broadcast radios (except for car radios) take advantage of this principle by including a small loop antenna for reception which has an extremely poor efficiency. Using such an inefficient antenna at this low frequency (530–1650 kHz) thus has little effect on the receiver's net performance, but simply requires greater amplification by the receiver's electronics. Contrast this tiny component to the massive and very tall towers used at AM broadcast stations for transmitting at the very same frequency, where every percentage point of reduced antenna efficiency entails a substantial cost. Rather than just using a thick tube, there are similar techniques used to the same effect such as replacing thin wire elements with cages to simulate a thicker element. This widens the bandwidth of the resonance. On the other hand, amateur radio antennas need to operate over several bands which are widely separated from each other. This can often be accomplished simply by connecting resonant elements for the different bands in parallel. Most of the transmitter's power will flow into the resonant element while the others present a high (reactive) impedance and draw little current from the same voltage. A popular solution uses so-called traps consisting of parallel resonant circuits which are strategically placed in breaks along each antenna element. When used at one particular frequency band the trap presents a very high impedance (parallel resonance) effectively truncating the element at that length, making it a proper resonant antenna. At a lower frequency the trap allows the full length of the element to be employed, albeit with a shifted resonant frequency due to the inclusion of the trap's net reactance at that lower frequency. The standing wave forms with this desired pattern at the design frequency, f0, and antennas are normally designed to be this size. However, feeding that element with 3f0 (whose wavelength is 1⁄3 that of f0) will also lead to a standing wave pattern. Thus, an antenna element is also resonant when its length is 3⁄4 of a wavelength. This is true for all odd multiples of 1⁄4 wavelength. This allows some flexibility of design in terms of antenna lengths and feed points. Antennas used in such a fashion are known to be harmonically operated. Monopole antennas consist of a single radiating element such as a metal rod, often mounted over a conducting surface, a ground plane. One side of the feedline from the receiver or transmitter is connected to the rod, and the other side to the ground plane, which may be the Earth. The most common form is the quarter-wave monopole which is one-quarter of a wavelength long and has a gain of 5.12 dBi when mounted over a ground plane. Monopoles have an omnidirectional radiation pattern, so they are used for broad coverage of an area, and have vertical polarization. The ground waves used for broadcasting at low frequencies must be vertically polarized, so large vertical monopole antennas are used for broadcasting in the MF, LF, and VLF bands. Small monopoles are used as nondirectional antennas on portable radios in the HF, VHF, and UHF bands. A necessary condition for the aforementioned reciprocity property is that the materials in the antenna and transmission medium are linear and reciprocal. Reciprocal (or bilateral) means that the material has the same response to an electric current or magnetic field in one direction, as it has to the field or current in the opposite direction. Most materials used in antennas meet these conditions, but some microwave antennas use high-tech components such as isolators and circulators, made of nonreciprocal materials such as ferrite. These can be used to give the antenna a different behavior on receiving than it has on transmitting, which can be useful in applications like radar. Reflection of the original signal also occurs when it hits an extended conductive surface, in a fashion similar to a mirror. This effect can also be used to increase signal through the use of a reflector, normally placed behind the active element and spaced so the reflected signal reaches the element in-phase. Generally the reflector will remain highly reflective even if it is not solid; gaps less than 1⁄10 generally have little effect on the outcome. For this reason, reflectors often take the form of wire meshes or rows of passive elements, which makes them lighter and less subject to wind. The parabolic reflector is perhaps the best known example of a reflector-based antenna, which has an effective area far greater than the active element alone. The net quality of a ground reflection depends on the topography of the surface. When the irregularities of the surface are much smaller than the wavelength, we are in the regime of specular reflection, and the receiver sees both the real antenna and an image of the antenna under the ground due to reflection. But if the ground has irregularities not small compared to the wavelength, reflections will not be coherent but shifted by random phases. With shorter wavelengths (higher frequencies), this is generally the case. The difference in the above factors for the case of θ=0 is the reason that most broadcasting (transmissions intended for the public) uses vertical polarization. For receivers near the ground, horizontally polarized transmissions suffer cancellation. For best reception the receiving antennas for these signals are likewise vertically polarized. In some applications where the receiving antenna must work in any position, as in mobile phones, the base station antennas use mixed polarization, such as linear polarization at an angle (with both vertical and horizontal components) or circular polarization. In some cases this is done in a more extreme manner, not simply to cancel a small amount of residual reactance, but to resonate an antenna whose resonance frequency is quite different from the intended frequency of operation. For instance, a ""whip antenna"" can be made significantly shorter than 1/4 wavelength long, for practical reasons, and then resonated using a so-called loading coil. This physically large inductor at the base of the antenna has an inductive reactance which is the opposite of the capacitative reactance that such a vertical antenna has at the desired operating frequency. The result is a pure resistance seen at feedpoint of the loading coil; unfortunately that resistance is somewhat lower than would be desired to match commercial coax.[citation needed] The majority of antenna designs are based on the resonance principle. This relies on the behaviour of moving electrons, which reflect off surfaces where the dielectric constant changes, in a fashion similar to the way light reflects when optical properties change. In these designs, the reflective surface is created by the end of a conductor, normally a thin metal wire or rod, which in the simplest case has a feed point at one end where it is connected to a transmission line. The conductor, or element, is aligned with the electrical field of the desired signal, normally meaning it is perpendicular to the line from the antenna to the source (or receiver in the case of a broadcast antenna). Efficiency of a transmitting antenna is the ratio of power actually radiated (in all directions) to the power absorbed by the antenna terminals. The power supplied to the antenna terminals which is not radiated is converted into heat. This is usually through loss resistance in the antenna's conductors, but can also be due to dielectric or magnetic core losses in antennas (or antenna systems) using such components. Such loss effectively robs power from the transmitter, requiring a stronger transmitter in order to transmit a signal of a given strength. At low frequencies (such as AM broadcast), arrays of vertical towers are used to achieve directionality  and they will occupy large areas of land. For reception, a long Beverage antenna can have significant directivity. For non directional portable use, a short vertical antenna or small loop antenna works well, with the main design challenge being that of impedance matching. With a vertical antenna a loading coil at the base of the antenna may be employed to cancel the reactive component of impedance; small loop antennas are tuned with parallel capacitors for this purpose. Unlike the above antennas, traveling wave antennas are nonresonant so they have inherently broad bandwidth. They are typically wire antennas multiple wavelengths long, through which the voltage and current waves travel in one direction, instead of bouncing back and forth to form standing waves as in resonant antennas. They have linear polarization (except for the helical antenna). Unidirectional traveling wave antennas are terminated by a resistor at one end equal to the antenna's characteristic resistance, to absorb the waves from one direction. This makes them inefficient as transmitting antennas. However a log-periodic dipole array consists of a number of dipole elements of different lengths in order to obtain a somewhat directional antenna having an extremely wide bandwidth: these are frequently used for television reception in fringe areas. The dipole antennas composing it are all considered ""active elements"" since they are all electrically connected together (and to the transmission line). On the other hand, a superficially similar dipole array, the Yagi-Uda Antenna (or simply ""Yagi""), has only one dipole element with an electrical connection; the other so-called parasitic elements interact with the electromagnetic field in order to realize a fairly directional antenna but one which is limited to a rather narrow bandwidth. The Yagi antenna has similar looking parasitic dipole elements but which act differently due to their somewhat different lengths. There may be a number of so-called ""directors"" in front of the active element in the direction of propagation, and usually a single (but possibly more) ""reflector"" on the opposite side of the active element. The amount of signal received from a distant transmission source is essentially geometric in nature due to the inverse square law, and this leads to the concept of effective area. This measures the performance of an antenna by comparing the amount of power it generates to the amount of power in the original signal, measured in terms of the signal's power density in Watts per square metre. A half-wave dipole has an effective area of 0.13 2. If more performance is needed, one cannot simply make the antenna larger. Although this would intercept more energy from the signal, due to the considerations above, it would decrease the output significantly due to it moving away from the resonant length. In roles where higher performance is needed, designers often use multiple elements combined together. Due to reciprocity (discussed above) the gain of an antenna used for transmitting must be proportional to its effective area when used for receiving. Consider an antenna with no loss, that is, one whose electrical efficiency is 100%. It can be shown that its effective area averaged over all directions must be equal to λ2/4π, the wavelength squared divided by 4π. Gain is defined such that the average gain over all directions for an antenna with 100% electrical efficiency is equal to 1. Therefore, the effective area Aeff in terms of the gain G in a given direction is given by: High-gain antennas have the advantage of longer range and better signal quality, but must be aimed carefully at the other antenna. An example of a high-gain antenna is a parabolic dish such as a satellite television antenna. Low-gain antennas have shorter range, but the orientation of the antenna is relatively unimportant. An example of a low-gain antenna is the whip antenna found on portable radios and cordless phones. Antenna gain should not be confused with amplifier gain, a separate parameter measuring the increase in signal power due to an amplifying device. An antenna lead-in is the transmission line (or feed line) which connects the antenna to a transmitter or receiver. The antenna feed may refer to all components connecting the antenna to the transmitter or receiver, such as an impedance matching network in addition to the transmission line. In a so-called aperture antenna, such as a horn or parabolic dish, the ""feed"" may also refer to a basic antenna inside the entire system (normally at the focus of the parabolic dish or at the throat of a horn) which could be considered the one active element in that antenna system. A microwave antenna may also be fed directly from a waveguide in place of a (conductive) transmission line. Antennas are characterized by a number of performance measures which a user would be concerned with in selecting or designing an antenna for a particular application. Chief among these relate to the directional characteristics (as depicted in the antenna's radiation pattern) and the resulting gain. Even in omnidirectional (or weakly directional) antennas, the gain can often be increased by concentrating more of its power in the horizontal directions, sacrificing power radiated toward the sky and ground. The antenna's power gain (or simply ""gain"") also takes into account the antenna's efficiency, and is often the primary figure of merit. Returning to the basic concept of current flows in a conductor, consider what happens if a half-wave dipole is not connected to a feed point, but instead shorted out. Electrically this forms a single 1⁄2-wavelength element. But the overall current pattern is the same; the current will be zero at the two ends, and reach a maximum in the center. Thus signals near the design frequency will continue to create a standing wave pattern. Any varying electrical current, like the standing wave in the element, will radiate a signal. In this case, aside from resistive losses in the element, the rebroadcast signal will be significantly similar to the original signal in both magnitude and shape. If this element is placed so its signal reaches the main dipole in-phase, it will reinforce the original signal, and increase the current in the dipole. Elements used in this way are known as passive elements. Although a resonant antenna has a purely resistive feed-point impedance at a particular frequency, many (if not most) applications require using an antenna over a range of frequencies. An antenna's bandwidth specifies the range of frequencies over which its performance does not suffer due to a poor impedance match. Also in the case of a Yagi-Uda array, the use of the antenna very far away from its design frequency reduces the antenna's directivity, thus reducing the usable bandwidth regardless of impedance matching. As an electro-magnetic wave travels through the different parts of the antenna system (radio, feed line, antenna, free space) it may encounter differences in impedance (E/H, V/I, etc.). At each interface, depending on the impedance match, some fraction of the wave's energy will reflect back to the source, forming a standing wave in the feed line. The ratio of maximum power to minimum power in the wave can be measured and is called the standing wave ratio (SWR). A SWR of 1:1 is ideal. A SWR of 1.5:1 is considered to be marginally acceptable in low power applications where power loss is more critical, although an SWR as high as 6:1 may still be usable with the right equipment. Minimizing impedance differences at each interface (impedance matching) will reduce SWR and maximize power transfer through each part of the antenna system. Another extreme case of impedance matching occurs when using a small loop antenna (usually, but not always, for receiving) at a relatively low frequency where it appears almost as a pure inductor. Resonating such an inductor with a capacitor at the frequency of operation not only cancels the reactance but greatly magnifies the very small radiation resistance of such a loop.[citation needed] This is implemented in most AM broadcast receivers, with a small ferrite loop antenna resonated by a capacitor which is varied along with the receiver tuning in order to maintain resonance over the AM broadcast band Resonant antennas are expected to be used around a particular resonant frequency; an antenna must therefore be built or ordered to match the frequency range of the intended application. A particular antenna design will present a particular feedpoint impedance. While this may affect the choice of an antenna, an antenna's impedance can also be adapted to the desired impedance level of a system using a matching network while maintaining the other characteristics (except for a possible loss of efficiency). Typically an antenna consists of an arrangement of metallic conductors (elements), electrically connected (often through a transmission line) to the receiver or transmitter. An oscillating current of electrons forced through the antenna by a transmitter will create an oscillating magnetic field around the antenna elements, while the charge of the electrons also creates an oscillating electric field along the elements. These time-varying fields radiate away from the antenna into space as a moving transverse electromagnetic field wave. Conversely, during reception, the oscillating electric and magnetic fields of an incoming radio wave exert force on the electrons in the antenna elements, causing them to move back and forth, creating oscillating currents in the antenna. It is best for the receiving antenna to match the polarization of the transmitted wave for optimum reception. Intermediate matchings will lose some signal strength, but not as much as a complete mismatch. A circularly polarized antenna can be used to equally well match vertical or horizontal linear polarizations. Transmission from a circularly polarized antenna received by a linearly polarized antenna (or vice versa) entails a 3 dB reduction in signal-to-noise ratio as the received power has thereby been cut in half."
Alsace,"The region, as part of Lorraine, was part of the Holy Roman Empire, and then was gradually annexed by France in the 17th century, and formalized as one of the provinces of France. The Calvinist manufacturing republic of Mulhouse, known as Stadtrepublik Mülhausen, became a part of Alsace after a vote by its citizens on 4 January 1798. Alsace is frequently mentioned with and as part of Lorraine and the former duchy of Lorraine, since it was a vital part of the duchy, and later because German possession as the imperial province (Alsace-Lorraine, 1871–1918) was contested in the 19th and 20th centuries; France and Germany exchanged control of parts of Lorraine (including Alsace) four times in 75 years. Alsace is one of the most conservative régions of France. It is one of just two régions in metropolitan France where the conservative right won the 2004 région elections and thus controls the Alsace Regional Council. Conservative leader Nicolas Sarkozy got his best score in Alsace (over 65%) in the second round of the French presidential elections of 2007. The president of the Regional Council is Philippe Richert, a member of the Union for a Popular Movement, elected in the 2010 regional election. The frequently changing status of the région throughout history has left its mark on modern day politics in terms of a particular interest in national identity issues. Alsace is also one of the most pro-EU regions of France. It was one of the few French regions that voted 'yes' to the European Constitution in 2005. During a reannexation by Germany (1940–1945), High German was reinstated as the language of education. The population was forced to speak German and 'French' family names were Germanized. Following the Second World War, the 1927 regulation was not reinstated and the teaching of German in primary schools was suspended by a provisional rectorial decree, which was supposed to enable French to regain lost ground. The teaching of German became a major issue, however, as early as 1946. Following World War II, the French government pursued, in line with its traditional language policy, a campaign to suppress the use of German as part of a wider Francization campaign. Following the Protestant Reformation, promoted by local reformer Martin Bucer, the principle of cuius regio, eius religio led to a certain amount of religious diversity in the highlands of northern Alsace. Landowners, who as ""local lords"" had the right to decide which religion was allowed on their land, were eager to entice populations from the more attractive lowlands to settle and develop their property. Many accepted without discrimination Catholics, Lutherans, Calvinists, Jews and Anabaptists. Multiconfessional villages appeared, particularly in the region of Alsace bossue. Alsace became one of the French regions boasting a thriving Jewish community, and the only region with a noticeable Anabaptist population. The schism of the Amish under the lead of Jacob Amman from the Mennonites occurred in 1693 in Sainte-Marie-aux-Mines. The strongly Catholic Louis XIV tried in vain to drive them from Alsace. When Napoleon imposed military conscription without religious exception, most emigrated to the American continent. ""Alsatia"", the Latin form of Alsace's name, has long ago entered the English language with the specialized meaning of ""a lawless place"" or ""a place under no jurisdiction"" - since Alsace was conceived by English people to be such. It was used into the 20th century as a term for a ramshackle marketplace, ""protected by ancient custom and the independence of their patrons"". As of 2007, the word is still in use among the English and Australian judiciaries with the meaning of a place where the law cannot reach: ""In setting up the Serious Organised Crime Agency, the state has set out to create an Alsatia - a region of executive action free of judicial oversight,"" Lord Justice Sedley in UMBS v SOCA 2007. Most of the Alsatian population is Roman Catholic, but, largely because of the region's German heritage, a significant Protestant community also exists: today, the EPCAAL (a Lutheran church) is France's second largest Protestant church, also forming an administrative union (UEPAL) with the much smaller Calvinist EPRAL. Unlike the rest of France, the Local law in Alsace-Moselle still provides for to the Napoleonic Concordat of 1801 and the organic articles, which provides public subsidies to the Roman Catholic, Lutheran, and Calvinist churches, as well as to Jewish synagogues; religion classes in one of these faiths is compulsory in public schools. This divergence in policy from the French majority is due to the region having been part of Imperial Germany when the 1905 law separating the French church and state was instituted (for a more comprehensive history, see: Alsace-Lorraine). Controversy erupts periodically on the appropriateness of this legal disposition, as well as on the exclusion of other religions from this arrangement. The year 1789 brought the French Revolution and with it the first division of Alsace into the départements of Haut- and Bas-Rhin. Alsatians played an active role in the French Revolution. On 21 July 1789, after receiving news of the Storming of the Bastille in Paris, a crowd of people stormed the Strasbourg city hall, forcing the city administrators to flee and putting symbolically an end to the feudal system in Alsace. In 1792, Rouget de Lisle composed in Strasbourg the Revolutionary marching song ""La Marseillaise"" (as Marching song for the Army of the Rhine), which later became the anthem of France. ""La Marseillaise"" was played for the first time in April of that year in front of the mayor of Strasbourg Philippe-Frédéric de Dietrich. Some of the most famous generals of the French Revolution also came from Alsace, notably Kellermann, the victor of Valmy, Kléber, who led the armies of the French Republic in Vendée and Westermann, who also fought in the Vendée. The constitution of the Fifth Republic states that French alone is the official language of the Republic. However, Alsatian, along with other regional languages, are recognized by the French government in the official list of languages of France. A 1999 INSEE survey counted 548,000 adult speakers of Alsatian in France, making it the second most-spoken regional language in the country (after Occitan). Like all regional languages in France, however, the transmission of Alsatian is on the decline. While 39% of the adult population of Alsace speaks Alsatian, only one in four children speaks it, and only one in ten children uses it regularly. It was not until 9 June 1982, with the Circulaire sur la langue et la culture régionales en Alsace (Memorandum on regional language and culture in Alsace) issued by the Vice-Chancellor of the Académie Pierre Deyon, that the teaching of German in primary schools in Alsace really began to be given more official status. The Ministerial Memorandum of 21 June 1982, known as the Circulaire Savary, introduced financial support, over three years, for the teaching of regional languages in schools and universities. This memorandum was, however, implemented in a fairly lax manner. At the same time, some Alsatians were in opposition to the Jacobins and sympathetic to the invading forces of Austria and Prussia who sought to crush the nascent revolutionary republic. Many of the residents of the Sundgau made ""pilgrimages"" to places like Mariastein Abbey, near Basel, in Switzerland, for baptisms and weddings. When the French Revolutionary Army of the Rhine was victorious, tens of thousands fled east before it. When they were later permitted to return (in some cases not until 1799), it was often to find that their lands and homes had been confiscated. These conditions led to emigration by hundreds of families to newly vacant lands in the Russian Empire in 1803–4 and again in 1808. A poignant retelling of this event based on what Goethe had personally witnessed can be found in his long poem Hermann and Dorothea. By the time of the Protestant Reformation in the 16th century, Strasbourg was a prosperous community, and its inhabitants accepted Protestantism in 1523. Martin Bucer was a prominent Protestant reformer in the region. His efforts were countered by the Roman Catholic Habsburgs who tried to eradicate heresy in Upper Alsace. As a result, Alsace was transformed into a mosaic of Catholic and Protestant territories. On the other hand, Mömpelgard (Montbéliard) to the southwest of Alsace, belonging to the Counts of Württemberg since 1397, remained a Protestant enclave in France until 1793. In 1469, following the Treaty of St. Omer, Upper Alsace was sold by Archduke Sigismund of Austria to Charles the Bold, Duke of Burgundy. Although Charles was the nominal landlord, taxes were paid to Frederick III, Holy Roman Emperor. The latter was able to use this tax and a dynastic marriage to his advantage to gain back full control of Upper Alsace (apart from the free towns, but including Belfort) in 1477 when it became part of the demesne of the Habsburg family, who were also rulers of the empire. The town of Mulhouse joined the Swiss Confederation in 1515, where it was to remain until 1798. From the annexation of Alsace by France in the 17th century and the language policy of the French Revolution up to 1870, knowledge of French in Alsace increased considerably. With the education reforms of the 19th century, the middle classes began to speak and write French well. The French language never really managed, however, to win over the masses, the vast majority of whom continued to speak their German dialects and write in German (which we would now call ""standard German"").[citation needed] Holy Roman Empire central power had begun to decline following years of imperial adventures in Italian lands, often ceding hegemony in Western Europe to France, which had long since centralized power. France began an aggressive policy of expanding eastward, first to the rivers Rhône and Meuse, and when those borders were reached, aiming for the Rhine. In 1299, the French proposed a marriage alliance between Philip IV of France's sister Blanche and Albert I of Germany's son Rudolf, with Alsace to be the dowry; however, the deal never came off. In 1307, the town of Belfort was first chartered by the Counts of Montbéliard. During the next century, France was to be militarily shattered by the Hundred Years' War, which prevented for a time any further tendencies in this direction. After the conclusion of the war, France was again free to pursue its desire to reach the Rhine and in 1444 a French army appeared in Lorraine and Alsace. It took up winter quarters, demanded the submission of Metz and Strasbourg and launched an attack on Basel. During the First World War, to avoid ground fights between brothers, many Alsatians served as sailors in the Kaiserliche Marine and took part in the Naval mutinies that led to the abdication of the Kaiser in November 1918, which left Alsace-Lorraine without a nominal head of state. The sailors returned home and tried to found a republic. While Jacques Peirotes, at this time deputy at the Landrat Elsass-Lothringen and just elected mayor of Strasbourg, proclaimed the forfeiture of the German Empire and the advent of the French Republic, a self-proclaimed government of Alsace-Lorraine declared independence as the ""Republic of Alsace-Lorraine"". French troops entered Alsace less than two weeks later to quash the worker strikes and remove the newly established Soviets and revolutionaries from power. At the arrival of the French soldiers, many Alsatians and local Prussian/German administrators and bureaucrats cheered the re-establishment of order (which can be seen and is described in detail in the reference video below). Although U.S. President Woodrow Wilson had insisted that the région was self-ruling by legal status, as its constitution had stated it was bound to the sole authority of the Kaiser and not to the German state, France tolerated no plebiscite, as granted by the League of Nations to some eastern German territories at this time, because Alsatians were considered by the French public as fellow Frenchmen liberated from German rule. Germany ceded the region to France under the Treaty of Versailles. Alsace-Lorraine was occupied by Germany in 1940 during the Second World War. Although Germany never formally annexed Alsace-Lorraine, it was incorporated into the Greater German Reich, which had been restructured into Reichsgaue. Alsace was merged with Baden, and Lorraine with the Saarland, to become part of a planned Westmark. During the war, 130,000 young men from Alsace and Lorraine were inducted into the German army against their will (malgré-nous) and in some cases, the Waffen SS. Some of the latter were involved in war crimes such as the Oradour-sur-Glane massacre. Most of them perished on the eastern front. The few that could escape fled to Switzerland or joined the resistance. In July 1944, 1500 malgré-nous were released from Soviet captivity and sent to Algiers, where they joined the Free French Forces. At present, plans are being considered for building a new dual carriageway west of Strasbourg, which would reduce the buildup of traffic in that area by picking up north and southbound vehicles and getting rid of the buildup outside Strasbourg. The line plans to link up the interchange of Hœrdt to the north of Strasbourg, with Innenheim in the southwest. The opening is envisaged at the end of 2011, with an average usage of 41,000 vehicles a day. Estimates of the French Works Commissioner however, raised some doubts over the interest of such a project, since it would pick up only about 10% of the traffic of the A35 at Strasbourg. Paradoxically, this reversed the situation of the 1950s. At that time, the French trunk road left of the Rhine not been built, so that traffic would cross into Germany to use the Karlsruhe-Basel Autobahn. The gastronomic symbol of the région is undoubtedly the Choucroute, a local variety of Sauerkraut. The word Sauerkraut in Alsatian has the form sûrkrût, same as in other southwestern German dialects, and means ""sour cabbage"" as its Standard German equivalent. This word was included into the French language as choucroute. To make it, the cabbage is finely shredded, layered with salt and juniper and left to ferment in wooden barrels. Sauerkraut can be served with poultry, pork, sausage or even fish. Traditionally it is served with Strasbourg sausage or frankfurters, bacon, smoked pork or smoked Morteau or Montbéliard sausages, or a selection of other pork products. Served alongside are often roasted or steamed potatoes or dumplings. France started the Franco-Prussian War (1870–71), and was defeated by the Kingdom of Prussia and other German states. The end of the war led to the unification of Germany. Otto von Bismarck annexed Alsace and northern Lorraine to the new German Empire in 1871; unlike other members states of the German federation, which had governments of their own, the new Imperial territory of Alsace-Lorraine was under the sole authority of the Kaiser, administered directly by the imperial government in Berlin. Between 100,000 and 130,000 Alsatians (of a total population of about a million and a half) chose to remain French citizens and leave Reichsland Elsaß-Lothringen, many of them resettling in French Algeria as Pieds-Noirs. Only in 1911 was Alsace-Lorraine granted some measure of autonomy, which was manifested also in a flag and an anthem (Elsässisches Fahnenlied). In 1913, however, the Saverne Affair (French: Incident de Saverne) showed the limits of this new tolerance of the Alsatian identity. As in much of Europe, the prosperity of Alsace came to an end in the 14th century by a series of harsh winters, bad harvests, and the Black Death. These hardships were blamed on Jews, leading to the pogroms of 1336 and 1339. In 1349, Jews of Alsace were accused of poisoning the wells with plague, leading to the massacre of thousands of Jews during the Strasbourg pogrom. Jews were subsequently forbidden to settle in the town. An additional natural disaster was the Rhine rift earthquake of 1356, one of Europe's worst which made ruins of Basel. Prosperity returned to Alsace under Habsburg administration during the Renaissance. There is controversy around the recognition of the Alsacian flag. The authentic historical flag is the Rot-un-Wiss ; Red and White are commonly found on the coat of arms of Alsacian cities (Strasbourg, Mulhouse, Sélestat...) and of many Swiss cites, especially in Basel's region. The German region Hesse uses a flag similar to the Rot-un-Wiss. As it underlines the Germanic roots of the region, it was replaced in 1949 by a new ""Union jack-like"" flag representing the union of the two déprtements. It has, however, no real historical relevance. It has been since replaced again by a slightly different one, also representing the two départements. With the purpose of ""Frenchizing"" the region, the Rot-un-Wiss has not been recognized by Paris. Some overzealous statesmen have called it a Nazi invention - while its origins date back to the XIth century and the Red and White banner of Gérard de Lorraine (aka. d'Alsace). The Rot-un-Wiss flag is still known as the real historical emblem of the region by most of the population and the departments' parliaments and has been widely used during protests against the creation of a new ""super-region"" gathering Champagne-Ardennes, Lorraine and Alsace, namely on Colmar's statue of liberty. With the decline of the Roman Empire, Alsace became the territory of the Germanic Alemanni. The Alemanni were agricultural people, and their Germanic language formed the basis of modern-day dialects spoken along the Upper Rhine (Alsatian, Alemannian, Swabian, Swiss). Clovis and the Franks defeated the Alemanni during the 5th century AD, culminating with the Battle of Tolbiac, and Alsace became part of the Kingdom of Austrasia. Under Clovis' Merovingian successors the inhabitants were Christianized. Alsace remained under Frankish control until the Frankish realm, following the Oaths of Strasbourg of 842, was formally dissolved in 843 at the Treaty of Verdun; the grandsons of Charlemagne divided the realm into three parts. Alsace formed part of the Middle Francia, which was ruled by the youngest grandson Lothar I. Lothar died early in 855 and his realm was divided into three parts. The part known as Lotharingia, or Lorraine, was given to Lothar's son. The rest was shared between Lothar's brothers Charles the Bald (ruler of the West Frankish realm) and Louis the German (ruler of the East Frankish realm). The Kingdom of Lotharingia was short-lived, however, becoming the stem duchy of Lorraine in Eastern Francia after the Treaty of Ribemont in 880. Alsace was united with the other Alemanni east of the Rhine into the stem duchy of Swabia. At about this time the surrounding areas experienced recurring fragmentation and reincorporations among a number of feudal secular and ecclesiastical lordships, a common process in the Holy Roman Empire. Alsace experienced great prosperity during the 12th and 13th centuries under Hohenstaufen emperors. Frederick I set up Alsace as a province (a procuratio, not a provincia) to be ruled by ministeriales, a non-noble class of civil servants. The idea was that such men would be more tractable and less likely to alienate the fief from the crown out of their own greed. The province had a single provincial court (Landgericht) and a central administration with its seat at Hagenau. Frederick II designated the Bishop of Strasbourg to administer Alsace, but the authority of the bishop was challenged by Count Rudolf of Habsburg, who received his rights from Frederick II's son Conrad IV. Strasbourg began to grow to become the most populous and commercially important town in the region. In 1262, after a long struggle with the ruling bishops, its citizens gained the status of free imperial city. A stop on the Paris-Vienna-Orient trade route, as well as a port on the Rhine route linking southern Germany and Switzerland to the Netherlands, England and Scandinavia, it became the political and economic center of the region. Cities such as Colmar and Hagenau also began to grow in economic importance and gained a kind of autonomy within the ""Decapole"" or ""Dekapolis"", a federation of ten free towns. The population grew rapidly, from 800,000 in 1814 to 914,000 in 1830 and 1,067,000 in 1846. The combination of economic and demographic factors led to hunger, housing shortages and a lack of work for young people. Thus, it is not surprising that people left Alsace, not only for Paris – where the Alsatian community grew in numbers, with famous members such as Baron Haussmann – but also for more distant places like Russia and the Austrian Empire, to take advantage of the new opportunities offered there: Austria had conquered lands in Eastern Europe from the Ottoman Empire and offered generous terms to colonists as a way of consolidating its hold on the new territories. Many Alsatians also began to sail to the United States, settling in many areas from 1820 to 1850. In 1843 and 1844, sailing ships bringing immigrant families from Alsace arrived at the port of New York. Some settled in Illinois, many to farm or to seek success in commercial ventures: for example, the sailing ships Sully (in May 1843) and Iowa (in June 1844) brought families who set up homes in northern Illinois and northern Indiana. Some Alsatian immigrants were noted for their roles in 19th-century American economic development. Others ventured to Canada to settle in southwestern Ontario, notably Waterloo County. Both Alsatian and Standard German were for a time banned from public life (including street and city names, official administration, and educational system). Though the ban has long been lifted and street signs today are often bilingual, Alsace-Lorraine is today very French in language and culture. Few young people speak Alsatian today, although there do still exist one or two enclaves in the Sundgau region where some older inhabitants cannot speak French, and where Alsatian is still used as the mother tongue. A related Alemannic German survives on the opposite bank of the Rhine, in Baden, and especially in Switzerland. However, while French is the major language of the region, the Alsatian dialect of French is heavily influenced by German and other languages such a Yiddish in phonology and vocabulary. This situation prevailed until 1639, when most of Alsace was conquered by France so as to keep it out of the hands of the Spanish Habsburgs, who wanted a clear road to their valuable and rebellious possessions in the Spanish Netherlands. Beset by enemies and seeking to gain a free hand in Hungary, the Habsburgs sold their Sundgau territory (mostly in Upper Alsace) to France in 1646, which had occupied it, for the sum of 1.2 million Thalers. When hostilities were concluded in 1648 with the Treaty of Westphalia, most of Alsace was recognized as part of France, although some towns remained independent. The treaty stipulations regarding Alsace were complex; although the French king gained sovereignty, existing rights and customs of the inhabitants were largely preserved. France continued to maintain its customs border along the Vosges mountains where it had been, leaving Alsace more economically oriented to neighbouring German-speaking lands. The German language remained in use in local administration, in schools, and at the (Lutheran) University of Strasbourg, which continued to draw students from other German-speaking lands. The 1685 Edict of Fontainebleau, by which the French king ordered the suppression of French Protestantism, was not applied in Alsace. France did endeavour to promote Catholicism; Strasbourg Cathedral, for example, which had been Lutheran from 1524 to 1681, was returned to the Catholic Church. However, compared to the rest of France, Alsace enjoyed a climate of religious tolerance. By 1790, the Jewish population of Alsace was approximately 22,500, about 3% of the provincial population. They were highly segregated and subject to long-standing anti-Jewish regulations. They maintained their own customs, Yiddish language, and historic traditions within the tightly-knit ghettos; they adhered to Talmudic law enforced by their rabbis. Jews were barred from most cities and instead lived in villages. They concentrated in trade, services, and especially in money lending. They financed about a third of the mortgages in Alsace. Official tolerance grew during the French Revolution, with full emancipation in 1791. However, local antisemitism also increased and Napoleon turned hostile in 1806, imposing a one-year moratorium on all debts owed to Jews.[citation needed] In the 1830-1870 era most Jews moved to the cities, where they integrated and acculturated, as antisemitism sharply declined. By 1831, the state began paying salaries to official rabbis, and in 1846 a special legal oath for Jews was discontinued. Antisemitic local riots occasionally occurred, especially during the Revolution of 1848. Merger of Alsace into Germany in 1871-1918 lessened antisemitic violence."
Buddhism,"China is the country with the largest population of Buddhists, approximately 244 million or 18.2% of its total population.[web 1] They are mostly followers of Chinese schools of Mahayana, making this the largest body of Buddhist traditions. Mahayana, also practiced in broader East Asia, is followed by over half of world Buddhists.[web 1] Theravadin Buddhists believe that personal effort is required to realize rebirth. Monks follow the vinaya: meditating, teaching and serving their lay communities. Laypersons can perform good actions, producing merit. This period marks the first known spread of Buddhism beyond India. According to the edicts of Aśoka, emissaries were sent to various countries west of India to spread Buddhism (Dharma), particularly in eastern provinces of the neighboring Seleucid Empire, and even farther to Hellenistic kingdoms of the Mediterranean. It is a matter of disagreement among scholars whether or not these emissaries were accompanied by Buddhist missionaries. Much of the material in the Canon is not specifically ""Theravadin"", but is instead the collection of teachings that this school preserved from the early, non-sectarian body of teachings. According to Peter Harvey, it contains material at odds with later Theravadin orthodoxy. He states: ""The Theravadins, then, may have added texts to the Canon for some time, but they do not appear to have tampered with what they already had from an earlier period."" Pre-sectarian Buddhism is the earliest phase of Buddhism, recognized by nearly all scholars. Its main scriptures are the Vinaya Pitaka and the four principal Nikayas or Agamas. Certain basic teachings appear in many places throughout the early texts, so most scholars conclude that Gautama Buddha must have taught something similar to the Three marks of existence, the Five Aggregates, dependent origination, karma and rebirth, the Four Noble Truths, the Noble Eightfold Path, and nirvana. Some scholars disagree, and have proposed many other theories. The precepts are not formulated as imperatives, but as training rules that laypeople undertake voluntarily to facilitate practice. In Buddhist thought, the cultivation of dana and ethical conduct themselves refine consciousness to such a level that rebirth in one of the lower heavens is likely, even if there is no further Buddhist practice. There is nothing improper or un-Buddhist about limiting one's aims to this level of attainment. Meditation was an aspect of the practice of the yogis in the centuries preceding the Buddha. The Buddha built upon the yogis' concern with introspection and developed their meditative techniques, but rejected their theories of liberation. In Buddhism, mindfulness and clear awareness are to be developed at all times; in pre-Buddhist yogic practices there is no such injunction. A yogi in the Brahmanical tradition is not to practice while defecating, for example, while a Buddhist monastic should do so. According to a demographic analysis reported by Peter Harvey (2013): Mahayana has 360 million adherents; Theravada has 150 million adherents; and Vajrayana has 18,2 million adherents. Seven million additional Buddhists are found outside of Asia. The complete list of ten precepts may be observed by laypeople for short periods. For the complete list, the seventh precept is partitioned into two, and a tenth added: Not all traditions of Buddhism share the same philosophical outlook, or treat the same concepts as central. Each tradition, however, does have its own core concepts, and some comparisons can be drawn between them. For example, according to one Buddhist ecumenical organization,[web 23] several concepts common to both major Buddhist branches: In Theravada doctrine, a person may awaken from the ""sleep of ignorance"" by directly realizing the true nature of reality; such people are called arahants and occasionally buddhas. After numerous lifetimes of spiritual striving, they have reached the end of the cycle of rebirth, no longer reincarnating as human, animal, ghost, or other being. The commentaries to the Pali Canon classify these awakened beings into three types: Bodhi and nirvana carry the same meaning, that of being freed from craving, hate, and delusion. In attaining bodhi, the arahant has overcome these obstacles. As a further distinction, the extinction of only hatred and greed (in the sensory context) with some residue of delusion, is called anagami. Theravāda is primarily practiced today in Sri Lanka, Burma, Laos, Thailand, Cambodia as well as small portions of China, Vietnam, Malaysia and Bangladesh. It has a growing presence in the west. According to the scriptures, Gautama Buddha presented himself as a model. The Dharma offers a refuge by providing guidelines for the alleviation of suffering and the attainment of Nirvana. The Sangha is considered to provide a refuge by preserving the authentic teachings of the Buddha and providing further examples that the truth of the Buddha's teachings is attainable. The teachings on the Four Noble Truths are regarded as central to the teachings of Buddhism, and are said to provide a conceptual framework for Buddhist thought. These four truths explain the nature of dukkha (suffering, anxiety, unsatisfactoriness), its causes, and how it can be overcome. The four truths are:[note 4] This view is supported by a study of the region where these notions originated. Buddhism arose in Greater Magadha, which stretched from Sravasti, the capital of Kosala in the north-west, to Rajagrha in the south east. This land, to the east of aryavarta, the land of the Aryas, was recognized as non-Vedic. Other Vedic texts reveal a dislike of the people of Magadha, in all probability because the Magadhas at this time were not Brahmanised.[page needed] It was not until the 2nd or 3rd centuries BCE that the eastward spread of Brahmanism into Greater Magadha became significant. Ideas that developed in Greater Magadha prior to this were not subject to Vedic influence. These include rebirth and karmic retribution that appear in a number of movements in Greater Magadha, including Buddhism. These movements inherited notions of rebirth and karmic retribution from an earlier culture[page needed] Nirvana (Sanskrit; Pali: ""Nibbāna"") means ""cessation"", ""extinction"" (of craving and ignorance and therefore suffering and the cycle of involuntary rebirths (saṃsāra)), ""extinguished"", ""quieted"", ""calmed""; it is also known as ""Awakening"" or ""Enlightenment"" in the West. The term for anybody who has achieved nirvana, including the Buddha, is arahant. Bruce Matthews notes that there is no cohesive presentation of karma in the Sutta Pitaka, which may mean that the doctrine was incidental to the main perspective of early Buddhist soteriology. Schmithausen is a notable scholar who has questioned whether karma already played a role in the theory of rebirth of earliest Buddhism.[page needed][note 32] According to Vetter, ""the Buddha at first sought ""the deathless"" (amata/amrta), which is concerned with the here and now. According to Vetter, only after this realization did he become acquainted with the doctrine of rebirth."" Bronkhorst disagrees, and concludes that the Buddha ""introduced a concept of karma that differed considerably from the commonly held views of his time."" According to Bronkhorst, not physical and mental activities as such were seen as responsible for rebirth, but intentions and desire. Theravada (""Doctrine of the Elders"", or ""Ancient Doctrine"") is the oldest surviving Buddhist school. It is relatively conservative, and generally closest to early Buddhism. The name Theravāda comes from the ancestral Sthāvirīya, one of the early Buddhist schools, from which the Theravadins claim descent. After unsuccessfully trying to modify the Vinaya, a small group of ""elderly members"", i.e. sthaviras, broke away from the majority Mahāsāṃghika during the Second Buddhist council, giving rise to the Sthavira sect. Sinhalese Buddhist reformers in the late nineteenth and early twentieth centuries portrayed the Pali Canon as the original version of scripture. They also emphasized Theravada being rational and scientific. According to Buddhist traditions a Buddha is a fully awakened being who has completely purified his mind of the three poisons of desire, aversion and ignorance. A Buddha is no longer bound by Samsara and has ended the suffering which unawakened people experience in life. In addition, Mahayana Buddhists believe there are innumerable other Buddhas in other universes. A Theravada commentary says that Buddhas arise one at a time in this world element, and not at all in others. The understandings of this matter reflect widely differing interpretations of basic terms, such as ""world realm"", between the various schools of Buddhism. At the same time, these movements were influenced by, and in some respects continued, philosophical thought within the Vedic tradition as reflected e.g. in the Upanishads. These movements included, besides Buddhism, various skeptics (such as Sanjaya Belatthiputta), atomists (such as Pakudha Kaccayana), materialists (such as Ajita Kesakambali), antinomians (such as Purana Kassapa); the most important ones in the 5th century BCE were the Ajivikas, who emphasized the rule of fate, the Lokayata (materialists), the Ajnanas (agnostics) and the Jains, who stressed that the soul must be freed from matter. Many of these new movements shared the same conceptual vocabulary—atman (""Self""), buddha (""awakened one""), dhamma (""rule"" or ""law""), karma (""action""), nirvana (""extinguishing""), samsara (""eternal recurrence"") and yoga (""spiritual practice"").[note 24] The shramanas rejected the Veda, and the authority of the brahmans, who claimed they possessed revealed truths not knowable by any ordinary human means. Moreover, they declared that the entire Brahmanical system was fraudulent: a conspiracy of the brahmans to enrich themselves by charging exorbitant fees to perform bogus rites and give useless advice. According to Vetter, the description of the Buddhist path may initially have been as simple as the term ""the middle way"". In time, this short description was elaborated, resulting in the description of the eightfold path. Buddhists do not consider Siddhartha Gautama to have been the only Buddha. The Pali Canon refers to many previous ones (see List of the 28 Buddhas), while the Mahayana tradition additionally has many Buddhas of celestial, rather than historical, origin (see Amitabha or Vairocana as examples, for lists of many thousands Buddha names see Taishō Shinshū Daizōkyō numbers 439–448). A common Theravada and Mahayana Buddhist belief is that the next Buddha will be one named Maitreya (Pali: Metteyya). By the late Middle Ages, Buddhism had become virtually extinct in India, although it continued to exist in surrounding countries. It is now again gaining strength worldwide. China and India are now starting to fund Buddhist shrines in various Asian countries as they compete for influence in the region.[web 20] A core problem in the study of early Buddhism is the relation between dhyana and insight. Schmithausen, in his often-cited article On some Aspects of Descriptions or Theories of 'Liberating Insight' and 'Enlightenment' in Early Buddhism notes that the mention of the four noble truths as constituting ""liberating insight"", which is attained after mastering the Rupa Jhanas, is a later addition to texts such as Majjhima Nikaya 36.[page needed] In Buddhism, Karma (from Sanskrit: ""action, work"") is the force that drives saṃsāra—the cycle of suffering and rebirth for each being. Good, skillful deeds (Pali: ""kusala"") and bad, unskillful (Pāli: ""akusala"") actions produce ""seeds"" in the mind that come to fruition either in this life or in a subsequent rebirth. The avoidance of unwholesome actions and the cultivation of positive actions is called sīla. Karma specifically refers to those actions of body, speech or mind that spring from mental intent (cetanā), and bring about a consequence or phala ""fruit"" or vipāka ""result"". A famous saying by the 8th-century Indian Buddhist scholar-saint Shantideva, which the 14th Dalai Lama often cites as his favourite verse, summarizes the Bodhisattva's intention (Bodhicitta) as follows: ""For as long as space endures, and for as long as living beings remain, until then may I too abide to dispel the misery of the world.""[citation needed] In Theravada Buddhism, the ultimate goal is the attainment of the sublime state of Nirvana, achieved by practicing the Noble Eightfold Path (also known as the Middle Way), thus escaping what is seen as a cycle of suffering and rebirth. Mahayana Buddhism instead aspires to Buddhahood via the bodhisattva path, a state wherein one remains in this cycle to help other beings reach awakening. Tibetan Buddhism aspires to Buddhahood or rainbow body. Mahayana Buddhism received significant theoretical grounding from Nagarjuna (perhaps c. 150–250 CE), arguably the most influential scholar within the Mahayana tradition. Nagarjuna's primary contribution to Buddhist philosophy was the systematic exposition of the concept of śūnyatā, or ""emptiness"", widely attested in the Prajñāpāramitā sutras that emerged in his era. The concept of emptiness brings together other key Buddhist doctrines, particularly anatta and dependent origination, to refute the metaphysics of Sarvastivada and Sautrantika (extinct non-Mahayana schools). For Nagarjuna, it is not merely sentient beings that are empty of ātman; all phenomena (dharmas) are without any svabhava (literally ""own-nature"" or ""self-nature""), and thus without any underlying essence; they are ""empty"" of being independent; thus the heterodox theories of svabhava circulating at the time were refuted on the basis of the doctrines of early Buddhism. Nagarjuna's school of thought is known as the Mādhyamaka. Some of the writings attributed to Nagarjuna made explicit references to Mahayana texts, but his philosophy was argued within the parameters set out by the agamas. He may have arrived at his positions from a desire to achieve a consistent exegesis of the Buddha's doctrine as recorded in the Canon. In the eyes of Nagarjuna the Buddha was not merely a forerunner, but the very founder of the Mādhyamaka system. During the period of Late Mahayana Buddhism, four major types of thought developed: Madhyamaka, Yogacara, Tathagatagarbha, and Buddhist Logic as the last and most recent. In India, the two main philosophical schools of the Mahayana were the Madhyamaka and the later Yogacara. According to Dan Lusthaus, Madhyamaka and Yogacara have a great deal in common, and the commonality stems from early Buddhism. There were no great Indian teachers associated with tathagatagarbha thought. According to Mahayana tradition, the Mahayana sutras were transmitted in secret, came from other Buddhas or Bodhisattvas, or were preserved in non-human worlds because human beings at the time could not understand them: The gradual spread of Buddhism into adjacent areas meant that it came into contact with new ethnical groups. During this period Buddhism was exposed to a variety of influences, from Persian and Greek civilization, to changing trends in non-Buddhist Indian religions—themselves influenced by Buddhism. Striking examples of this syncretistic development can be seen in the emergence of Greek-speaking Buddhist monarchs in the Indo-Greek Kingdom, and in the development of the Greco-Buddhist art of Gandhāra. A Greek king, Menander, has even been immortalized in the Buddhist canon. The Theravada school spread south from India in the 3rd century BCE, to Sri Lanka and Thailand and Burma and later also Indonesia. The Dharmagupta school spread (also in 3rd century BCE) north to Kashmir, Gandhara and Bactria (Afghanistan). In the 2nd century CE, Mahayana Sutras spread to China, and then to Korea and Japan, and were translated into Chinese. During the Indian period of Esoteric Buddhism (from the 8th century onwards), Buddhism spread from India to Tibet and Mongolia. Samatha meditation starts from being mindful of an object or idea, which is expanded to one's body, mind and entire surroundings, leading to a state of total concentration and tranquility (jhāna). There are many variations in the style of meditation, from sitting cross-legged or kneeling to chanting or walking. The most common method of meditation is to concentrate on one's breath (anapanasati), because this practice can lead to both samatha and vipassana'. The doctrine of pratītyasamutpāda, (Sanskrit; Pali: paticcasamuppāda; Tibetan Wylie: rten cing 'brel bar 'byung ba; Chinese: 緣起) is an important part of Buddhist metaphysics. It states that phenomena arise together in a mutually interdependent web of cause and effect. It is variously rendered into English as ""dependent origination"", ""conditioned genesis"", ""dependent relationship"", ""dependent co-arising"", ""interdependent arising"", or ""contingency"". An important guiding principle of Buddhist practice is the Middle Way (or Middle Path), which is said to have been discovered by Gautama Buddha prior to his enlightenment. The Middle Way has several definitions: While there is no convincing evidence for meditation in pre-Buddhist early Brahminic texts, Wynne argues that formless meditation originated in the Brahminic or Shramanic tradition, based on strong parallels between Upanishadic cosmological statements and the meditative goals of the two teachers of the Buddha as recorded in the early Buddhist texts. He mentions less likely possibilities as well. Having argued that the cosmological statements in the Upanishads also reflect a contemplative tradition, he argues that the Nasadiya Sukta contains evidence for a contemplative tradition, even as early as the late Rig Vedic period. In Buddhist practice, it is said that while samatha meditation can calm the mind, only vipassanā meditation can reveal how the mind was disturbed to start with, which is what leads to insight knowledge (jñāna; Pāli ñāṇa) and understanding (prajñā Pāli paññā), and thus can lead to nirvāṇa (Pāli nibbāna). When one is in jhana, all defilements are suppressed temporarily. Only understanding (prajñā or vipassana) eradicates the defilements completely. Jhanas are also states that Arahants abide in order to rest. The root schism was between the Sthaviras and the Mahāsāṅghikas. The fortunate survival of accounts from both sides of the dispute reveals disparate traditions. The Sthavira group offers two quite distinct reasons for the schism. The Dipavamsa of the Theravāda says that the losing party in the Second Council dispute broke away in protest and formed the Mahasanghika. This contradicts the Mahasanghikas' own vinaya, which shows them as on the same, winning side. The Mahāsāṅghikas argued that the Sthaviras were trying to expand the vinaya and may also have challenged what they perceived were excessive claims or inhumanly high criteria for arhatship. Both parties, therefore, appealed to tradition. The idea of the decline and gradual disappearance of the teaching has been influential in East Asian Buddhism. Pure Land Buddhism holds that it has declined to the point where few are capable of following the path, so it may be best to rely on the power of Amitābha. In the second half of the 20th Century a modern movement in Nichiren Buddhism: Soka Gakkai (Value Creation Society) emerged in Japan and spread further to other countries. Soka Gakkai International (SGI) is a lay Buddhist movement linking more than 12 million people around the world, and is currently described as ""the most diverse"" and ""the largest lay Buddhist movement in the world"".[web 21] According to Jan Nattier, the term Mahāyāna ""Great Vehicle"" was originally even an honorary synonym for Bodhisattvayāna ""Bodhisattva Vehicle."" The Aṣṭasāhasrikā Prajñāpāramitā Sūtra, an early and important Mahayana text, contains a simple and brief definition for the term bodhisattva: ""Because he has enlightenment as his aim, a bodhisattva-mahāsattva is so called."" According to Johnson and Grim (2013), Buddhism has grown from a total of 138 million adherents in 1910, of which 137 million were in Asia, to 495 million in 2010, of which 487 million are in Asia. According to them, there was a fast annual growth of Buddhism in Pakistan, Saudi Arabia, Lebanon and several Western European countries (1910–2010). More recently (2000–2010), the countries with highest growth rates are Qatar, the United Arab Emirates, Iran and some African countries. This narrative draws on the Nidānakathā of the Jataka tales of the Theravada, which is ascribed to Buddhaghoṣa in the 5th century CE. Earlier biographies such as the Buddhacarita, the Lokottaravādin Mahāvastu, and the Sarvāstivādin Lalitavistara Sūtra, give different accounts. Scholars are hesitant to make unqualified claims about the historical facts of the Buddha's life. Most accept that he lived, taught and founded a monastic order, but do not consistently accept all of the details contained in his biographies. Suffering (Pāli: दुक्ख dukkha; Sanskrit दुःख duḥkha) is also a central concept in Buddhism. The word roughly corresponds to a number of terms in English including suffering, pain, unsatisfactoriness, sorrow, affliction, anxiety, dissatisfaction, discomfort, anguish, stress, misery, and frustration. Although the term is often translated as ""suffering"", its philosophical meaning is more analogous to ""disquietude"" as in the condition of being disturbed. As such, ""suffering"" is too narrow a translation with ""negative emotional connotations""[web 9] that can give the impression that the Buddhist view is pessimistic, but Buddhism seeks to be neither pessimistic nor optimistic, but realistic. In English-language Buddhist literature translated from Pāli, ""dukkha"" is often left untranslated, so as to encompass its full range of meaning.[note 8] The Mahayana sutras often claim to articulate the Buddha's deeper, more advanced doctrines, reserved for those who follow the bodhisattva path. That path is explained as being built upon the motivation to liberate all living beings from unhappiness. Hence the name Mahāyāna (lit., the Great Vehicle). Buddhism may have spread only slowly in India until the time of the Mauryan emperor Ashoka, who was a public supporter of the religion. The support of Aśoka and his descendants led to the construction of more stūpas (Buddhist religious memorials) and to efforts to spread Buddhism throughout the enlarged Maurya empire and even into neighboring lands—particularly to the Iranian-speaking regions of Afghanistan and Central Asia, beyond the Mauryas' northwest border, and to the island of Sri Lanka south of India. These two missions, in opposite directions, would ultimately lead, in the first case to the spread of Buddhism into China, and in the second case, to the emergence of Theravāda Buddhism and its spread from Sri Lanka to the coastal lands of Southeast Asia. Mahayana Buddhism encourages everyone to become bodhisattvas and to take the bodhisattva vow, where the practitioner promises to work for the complete enlightenment of all beings by practicing the six pāramitās. According to Mahayana teachings, these perfections are: dāna, śīla, kṣanti, vīrya, dhyāna, and prajñā. A particular criticism of the Buddha was Vedic animal sacrifice.[web 18] He also mocked the Vedic ""hymn of the cosmic man"". However, the Buddha was not anti-Vedic, and declared that the Veda in its true form was declared by ""Kashyapa"" to certain rishis, who by severe penances had acquired the power to see by divine eyes. He names the Vedic rishis, and declared that the original Veda of the rishis[note 25] was altered by a few Brahmins who introduced animal sacrifices. The Buddha says that it was on this alteration of the true Veda that he refused to pay respect to the Vedas of his time. However, he did not denounce the union with Brahman,[note 26] or the idea of the self uniting with the Self. At the same time, the traditional Hindu itself gradually underwent profound changes, transforming it into what is recognized as early Hinduism. The earliest Mahāyāna sūtras to include the very first versions of the Prajñāpāramitā genre, along with texts concerning Akṣobhya Buddha, which were probably written down in the 1st century BCE in the south of India. Guang Xing states, ""Several scholars have suggested that the Prajñāpāramitā probably developed among the Mahāsāṃghikas in southern India, in the Āndhra country, on the Kṛṣṇa River."" A.K. Warder believes that ""the Mahāyāna originated in the south of India and almost certainly in the Āndhra country."" Information of the oldest teachings may be obtained by analysis of the oldest texts. One method to obtain information on the oldest core of Buddhism is to compare the oldest extant versions of the Theravadin Pali Canon and other texts.[note 27] The reliability of these sources, and the possibility to draw out a core of oldest teachings, is a matter of dispute.[page needed][page needed][page needed][page needed] According to Vetter, inconsistencies remain, and other methods must be applied to resolve those inconsistencies.[note 28] Bodhisattva means ""enlightenment being"", and generally refers to one who is on the path to buddhahood. Traditionally, a bodhisattva is anyone who, motivated by great compassion, has generated bodhicitta, which is a spontaneous wish to attain Buddhahood for the benefit of all sentient beings. Theravada Buddhism primarily uses the term in relation to Gautama Buddha's previous existences, but has traditionally acknowledged and respected the bodhisattva path as well.[web 17] There is no evidence that Mahāyāna ever referred to a separate formal school or sect of Buddhism, but rather that it existed as a certain set of ideals, and later doctrines, for bodhisattvas. Initially it was known as Bodhisattvayāna (the ""Vehicle of the Bodhisattvas""). Paul Williams has also noted that the Mahāyāna never had nor ever attempted to have a separate Vinaya or ordination lineage from the early schools of Buddhism, and therefore each bhikṣu or bhikṣuṇī adhering to the Mahāyāna formally belonged to an early school. This continues today with the Dharmaguptaka ordination lineage in East Asia, and the Mūlasarvāstivāda ordination lineage in Tibetan Buddhism. Therefore Mahāyāna was never a separate rival sect of the early schools. From Chinese monks visiting India, we now know that both Mahāyāna and non-Mahāyāna monks in India often lived in the same monasteries side by side. Śīla (Sanskrit) or sīla (Pāli) is usually translated into English as ""virtuous behavior"", ""morality"", ""moral discipline"", ""ethics"" or ""precept"". It is an action committed through the body, speech, or mind, and involves an intentional effort. It is one of the three practices (sīla, samādhi, and paññā) and the second pāramitā. It refers to moral purity of thought, word, and deed. The four conditions of śīla are chastity, calmness, quiet, and extinguishment. The first truth explains the nature of dukkha. Dukkha is commonly translated as ""suffering"", ""anxiety"", ""unsatisfactoriness"", ""unease"", etc., and it is said to have the following three aspects: According to East Asian and Tibetan Buddhism, there is an intermediate state (Tibetan ""bardo"") between one life and the next. The orthodox Theravada position rejects this; however there are passages in the Samyutta Nikaya of the Pali Canon that seem to lend support to the idea that the Buddha taught of an intermediate stage between one life and the next.[page needed] Bodhi (Pāli and Sanskrit, in devanagari: बॊधि) is a term applied to the experience of Awakening of arahants. Bodhi literally means ""awakening"", but it is more commonly translated into English as ""enlightenment"". In Early Buddhism, bodhi carried a meaning synonymous to nirvana, using only some different metaphors to describe the experience, which implies the extinction of raga (greed, craving),[web 12] dosa (hate, aversion)[web 13] and moha (delusion).[web 14] In the later school of Mahayana Buddhism, the status of nirvana was downgraded in some scriptures, coming to refer only to the extinction of greed and hate, implying that delusion was still present in one who attained nirvana, and that one needed to attain bodhi to eradicate delusion: The term parinirvana is also encountered in Buddhism, and this generally refers to the complete nirvana attained by the arahant at the moment of death, when the physical body expires. Regarding the monastic rules, the Buddha constantly reminds his hearers that it is the spirit that counts. On the other hand, the rules themselves are designed to assure a satisfying life, and provide a perfect springboard for the higher attainments. Monastics are instructed by the Buddha to live as ""islands unto themselves"". In this sense, living life as the vinaya prescribes it is, as one scholar puts it: ""more than merely a means to an end: it is very nearly the end in itself."" Rebirth refers to a process whereby beings go through a succession of lifetimes as one of many possible forms of sentient life, each running from conception to death. The doctrine of anattā (Sanskrit anātman) rejects the concepts of a permanent self or an unchanging, eternal soul, as it is called in Hinduism and Christianity. According to Buddhism there ultimately is no such thing as a self independent from the rest of the universe. Buddhists also refer to themselves as the believers of the anatta doctrine—Nairatmyavadin or Anattavadin. Rebirth in subsequent existences must be understood as the continuation of a dynamic, ever-changing process of pratītyasamutpāda (""dependent arising"") determined by the laws of cause and effect (karma) rather than that of one being, reincarnating from one existence to the next. According to the scriptures, soon after the parinirvāṇa (from Sanskrit: ""highest extinguishment"") of Gautama Buddha, the first Buddhist council was held. As with any ancient Indian tradition, transmission of teaching was done orally. The primary purpose of the assembly was to collectively recite the teachings to ensure that no errors occurred in oral transmission. In the first council, Ānanda, a cousin of the Buddha and his personal attendant, was called upon to recite the discourses (sūtras, Pāli suttas) of the Buddha, and, according to some sources, the abhidhamma. Upāli, another disciple, recited the monastic rules (vinaya). Most scholars regard the traditional accounts of the council as greatly exaggerated if not entirely fictitious.[note 36]Richard Gombrich noted Sariputta led communal recitations of the Buddha's teaching for preservation in the Buddha's lifetime in Sangiti Sutta (Digha Nikaya #33), and something similar to the First Council must have taken place to compose Buddhist scriptures. Buddhism is practiced by an estimated 488 million,[web 1] 495 million, or 535 million people as of the 2010s, representing 7% to 8% of the world's total population. According to the scriptures, soon after the death of the Buddha, the first Buddhist council was held; a monk named Mahākāśyapa (Pāli: Mahākassapa) presided. The goal of the council was to record the Buddha's teachings. Upāli recited the vinaya. Ānanda, the Buddha's personal attendant, was called upon to recite the dhamma. These became the basis of the Tripitaka. However, this record was initially transmitted orally in form of chanting, and was committed to text in the last century BCE. Both the sūtras and the vinaya of every Buddhist school contain a wide variety of elements including discourses on the Dharma, commentaries on other teachings, cosmological and cosmogonical texts, stories of the Gautama Buddha's previous lives, and various other subjects. Vinaya is the specific moral code for monks and nuns. It includes the Patimokkha, a set of 227 rules for monks in the Theravadin recension. The precise content of the vinayapitaka (scriptures on Vinaya) differs slightly according to different schools, and different schools or subschools set different standards for the degree of adherence to Vinaya. Novice-monks use the ten precepts, which are the basic precepts for monastics. Buddhism traditionally incorporates states of meditative absorption (Pali: jhāna; Skt: dhyāna). The most ancient sustained expression of yogic ideas is found in the early sermons of the Buddha. One key innovative teaching of the Buddha was that meditative absorption must be combined with liberating cognition. The difference between the Buddha's teaching and the yoga presented in early Brahminic texts is striking. Meditative states alone are not an end, for according to the Buddha, even the highest meditative state is not liberating. Instead of attaining a complete cessation of thought, some sort of mental activity must take place: a liberating cognition, based on the practice of mindful awareness. The Noble Eightfold Path—the fourth of the Buddha's Noble Truths—consists of a set of eight interconnected factors or conditions, that when developed together, lead to the cessation of dukkha. These eight factors are: Right View (or Right Understanding), Right Intention (or Right Thought), Right Speech, Right Action, Right Livelihood, Right Effort, Right Mindfulness, and Right Concentration. Buddhist schools vary on the exact nature of the path to liberation, the importance and canonicity of various teachings and scriptures, and especially their respective practices. Buddhism denies a creator deity and posits that mundane deities such as Mahabrahma are misperceived to be a creator. The foundations of Buddhist tradition and practice are the Three Jewels: the Buddha, the Dharma (the teachings), and the Sangha (the community). Taking ""refuge in the triple gem"" has traditionally been a declaration and commitment to being on the Buddhist path, and in general distinguishes a Buddhist from a non-Buddhist. Other practices are Ten Meritorious Deeds including, giving charity to reduce the greediness; following ethical precepts; renouncing conventional living and becoming a monastic; the development of mindfulness and practice of meditation; cultivation of higher wisdom and discernment; study of scriptures; devotional practices; ceremonies; and in the Mahayana tradition, invocation of buddhas and bodhisattvas. Buddhism /ˈbudɪzəm/ is a nontheistic religion[note 1] or philosophy (Sanskrit: धर्म dharma; Pali: धम्म dhamma) that encompasses a variety of traditions, beliefs and spiritual practices largely based on teachings attributed to Gautama Buddha, commonly known as the Buddha (""the awakened one""). According to Buddhist tradition, the Buddha lived and taught in the eastern part of the Indian subcontinent, present-day Nepal sometime between the 6th and 4th centuries BCE.[note 1] He is recognized by Buddhists as an awakened or enlightened teacher who shared his insights to help sentient beings end their suffering through the elimination of ignorance and craving. Buddhists believe that this is accomplished through the direct understanding and perception of dependent origination and the Four Noble Truths. Zen Buddhism is divided into two main schools: Rinzai (臨済宗) and Sōtō (曹洞宗), the former greatly favouring the use in meditation on the koan (公案, a meditative riddle or puzzle) as a device for spiritual break-through, and the latter (while certainly employing koans) focusing more on shikantaza or ""just sitting"".[note 13] Not-self (Pāli: anatta; Sanskrit: anātman) is the third mark of existence. Upon careful examination, one finds that no phenomenon is really ""I"" or ""mine""; these concepts are in fact constructed by the mind. In the Nikayas anatta is not meant as a metaphysical assertion, but as an approach for gaining release from suffering. In fact, the Buddha rejected both of the metaphysical assertions ""I have a Self"" and ""I have no Self"" as ontological views that bind one to suffering.[note 9] When asked if the self was identical with the body, the Buddha refused to answer. By analyzing the constantly changing physical and mental constituents (skandhas) of a person or object, the practitioner comes to the conclusion that neither the respective parts nor the person as a whole comprise a self. Some scholars[note 44] use other schemes. Buddhists themselves have a variety of other schemes. Hinayana (literally ""lesser vehicle"") is used by Mahayana followers to name the family of early philosophical schools and traditions from which contemporary Theravada emerged, but as this term is rooted in the Mahayana viewpoint and can be considered derogatory, a variety of other terms are increasingly used instead, including Śrāvakayāna, Nikaya Buddhism, early Buddhist schools, sectarian Buddhism, conservative Buddhism, mainstream Buddhism and non-Mahayana Buddhism. Once the meditator achieves a strong and powerful concentration (jhāna, Sanskrit ध्यान dhyāna), his mind is ready to penetrate and gain insight (vipassanā) into the ultimate nature of reality, eventually obtaining release from all suffering. The cultivation of mindfulness is essential to mental concentration, which is needed to achieve insight. Following (or leading up to) the schisms, each Saṅgha started to accumulate an Abhidharma, a detailed scholastic reworking of doctrinal material appearing in the Suttas, according to schematic classifications. These Abhidharma texts do not contain systematic philosophical treatises, but summaries or numerical lists. Scholars generally date these texts to around the 3rd century BCE, 100 to 200 years after the death of the Buddha. Therefore the seven Abhidharma works are generally claimed not to represent the words of the Buddha himself, but those of disciples and great scholars.[note 38] Every school had its own version of the Abhidharma, with different theories and different texts. The different Abhidharmas of the various schools did not agree with each other. Scholars disagree on whether the Mahasanghika school had an Abhidhamma Pitaka or not.[note 38] The Mahayana sutras are a very broad genre of Buddhist scriptures that the Mahayana Buddhist tradition holds are original teachings of the Buddha. Some adherents of Mahayana accept both the early teachings (including in this the Sarvastivada Abhidharma, which was criticized by Nagarjuna and is in fact opposed to early Buddhist thought) and the Mahayana sutras as authentic teachings of Gautama Buddha, and claim they were designed for different types of persons and different levels of spiritual understanding. Śīla is the foundation of Samādhi/Bhāvana (Meditative cultivation) or mind cultivation. Keeping the precepts promotes not only the peace of mind of the cultivator, which is internal, but also peace in the community, which is external. According to the Law of Karma, keeping the precepts is meritorious and it acts as causes that would bring about peaceful and happy effects. Keeping these precepts keeps the cultivator from rebirth in the four woeful realms of existence. The above are further subdivided into 31 planes of existence.[web 4] Rebirths in some of the higher heavens, known as the Śuddhāvāsa Worlds or Pure Abodes, can be attained only by skilled Buddhist practitioners known as anāgāmis (non-returners). Rebirths in the Ārūpyadhātu (formless realms) can be attained by only those who can meditate on the arūpajhānas, the highest object of meditation. Approximately six hundred Mahayana sutras have survived in Sanskrit or in Chinese or Tibetan translations. In addition, East Asian Buddhism recognizes some sutras regarded by scholars as of Chinese rather than Indian origin. In the Mahayana, the Buddha tends not to be viewed as merely human, but as the earthly projection of a beginningless and endless, omnipresent being (see Dharmakaya) beyond the range and reach of thought. Moreover, in certain Mahayana sutras, the Buddha, Dharma and Sangha are viewed essentially as One: all three are seen as the eternal Buddha himself. Though based upon Mahayana, Tibeto-Mongolian Buddhism is one of the schools that practice Vajrayana or ""Diamond Vehicle"" (also referred to as Mantrayāna, Tantrayāna, Tantric Buddhism, or esoteric Buddhism). It accepts all the basic concepts of Mahāyāna, but also includes a vast array of spiritual and physical techniques designed to enhance Buddhist practice. Tantric Buddhism is largely concerned with ritual and meditative practices. One component of the Vajrayāna is harnessing psycho-physical energy through ritual, visualization, physical exercises, and meditation as a means of developing the mind. Using these techniques, it is claimed that a practitioner can achieve Buddhahood in one lifetime, or even as little as three years. In the Tibetan tradition, these practices can include sexual yoga, though only for some very advanced practitioners. Gautama was now determined to complete his spiritual quest. At the age of 35, he famously sat in meditation under a Ficus religiosa tree now called the Bodhi Tree in the town of Bodh Gaya and vowed not to rise before achieving enlightenment. After many days, he finally destroyed the fetters of his mind, thereby liberating himself from the cycle of suffering and rebirth, and arose as a fully enlightened being (Skt. samyaksaṃbuddha). Soon thereafter, he attracted a band of followers and instituted a monastic order. Now, as the Buddha, he spent the rest of his life teaching the path of awakening he had discovered, traveling throughout the northeastern part of the Indian subcontinent, and died at the age of 80 (483 BCE) in Kushinagar, India. The south branch of the original fig tree available only in Anuradhapura Sri Lanka is known as Jaya Sri Maha Bodhi. Dwight Goddard collected a sample of Buddhist scriptures, with the emphasis on Zen, along with other classics of Eastern philosophy, such as the Tao Te Ching, into his 'Buddhist Bible' in the 1920s. More recently, Dr. Babasaheb Ambedkar attempted to create a single, combined document of Buddhist principles in ""The Buddha and His Dhamma"". Other such efforts have persisted to present day, but currently there is no single text that represents all Buddhist traditions. The three marks of existence may reflect Upanishadic or other influences. K.R. Norman supposes that the these terms were already in use at the Buddha's time, and were familiair to his hearers. There are differences of opinion on the question of whether or not Buddhism should be considered a religion. Many sources commonly refer to Buddhism as a religion. For example: Within Buddhism, samsara is defined as the continual repetitive cycle of birth and death that arises from ordinary beings' grasping and fixating on a self and experiences. Specifically, samsara refers to the process of cycling through one rebirth after another within the six realms of existence,[note 2] where each realm can be understood as physical realm or a psychological state characterized by a particular type of suffering. Samsara arises out of avidya (ignorance) and is characterized by dukkha (suffering, anxiety, dissatisfaction). In the Buddhist view, liberation from samsara is possible by following the Buddhist path. The Pāli Tipitaka is the only early Tipitaka (Sanskrit: Tripiṭaka) to survive intact in its original language, but a number of early schools had their own recensions of the Tipitaka featuring much of the same material. We have portions of the Tipitakas of the Sārvāstivāda, Dharmaguptaka, Sammitya, Mahāsaṅghika, Kāśyapīya, and Mahīśāsaka schools, most of which survive in Chinese translation only. According to some sources, some early schools of Buddhism had five or seven pitakas. 
The method of self-exertion or ""self-power""—without reliance on an external force or being—stands in contrast to another major form of Buddhism, Pure Land, which is characterized by utmost trust in the salvific ""other-power"" of Amitabha Buddha. Pure Land Buddhism is a very widespread and perhaps the most faith-orientated manifestation of Buddhism and centres upon the conviction that faith in Amitabha Buddha and the chanting of homage to his name liberates one at death into the Blissful (安樂), Pure Land (淨土) of Amitabha Buddha. This Buddhic realm is variously construed as a foretaste of Nirvana, or as essentially Nirvana itself. The great vow of Amitabha Buddha to rescue all beings from samsaric suffering is viewed within Pure Land Buddhism as universally efficacious, if only one has faith in the power of that vow or chants his name. Buddhists believe Gautama Buddha was the first to achieve enlightenment in this Buddha era and is therefore credited with the establishment of Buddhism. A Buddha era is the stretch of history during which people remember and practice the teachings of the earliest known Buddha. This Buddha era will end when all the knowledge, evidence and teachings of Gautama Buddha have vanished. This belief therefore maintains that many Buddha eras have started and ended throughout the course of human existence.[web 15][web 16] The Gautama Buddha, therefore, is the Buddha of this era, who taught directly or indirectly to all other Buddhas in it (see types of Buddhas). In Theravāda Buddhism, the cause of human existence and suffering is identified as craving, which carries with it the various defilements. These various defilements are traditionally summed up as greed, hatred and delusion. These are believed deeply rooted afflictions of the mind that create suffering and stress. To be free from suffering and stress, these defilements must be permanently uprooted through internal investigation, analyzing, experiencing, and understanding of the true nature of those defilements by using jhāna, a technique of the Noble Eightfold Path. It then leads the meditator to realize the Four Noble Truths, Enlightenment and Nibbāna. Nibbāna is the ultimate goal of Theravadins. According to both Bronkhorst and Anderson, the four truths became a substitution for prajna, or ""liberating insight"", in the suttas in those texts where ""liberating insight"" was preceded by the four jhanas. According to Bronkhorst, the four truths may not have been formulated in earliest Buddhism, and did not serve in earliest Buddhism as a description of ""liberating insight"". Gotama's teachings may have been personal, ""adjusted to the need of each person."" The best-known application of the concept of pratītyasamutpāda is the scheme of Twelve Nidānas (from Pāli ""nidāna"" meaning ""cause, foundation, source or origin""), which explain the continuation of the cycle of suffering and rebirth (saṃsāra) in detail.[note 10] Śīla refers to overall principles of ethical behavior. There are several levels of sīla, which correspond to ""basic morality"" (five precepts), ""basic morality with asceticism"" (eight precepts), ""novice monkhood"" (ten precepts) and ""monkhood"" (Vinaya or Patimokkha). Lay people generally undertake to live by the five precepts, which are common to all Buddhist schools. If they wish, they can choose to undertake the eight precepts, which add basic asceticism. Buddhism provides many opportunities for comparative study with a diverse range of subjects. For example, Buddhism's emphasis on the Middle way not only provides a unique guideline for ethics but has also allowed Buddhism to peacefully coexist with various differing beliefs, customs and institutions in countries where it has resided throughout its history. Also, its moral and spiritual parallels with other systems of thought—for example, with various tenets of Christianity—have been subjects of close study. In addition, the Buddhist concept of dependent origination has been compared to modern scientific thought, as well as Western metaphysics. Over the years, various attempts have been made to synthesize a single Buddhist text that can encompass all of the major principles of Buddhism. In the Theravada tradition, condensed 'study texts' were created that combined popular or influential scriptures into single volumes that could be studied by novice monks. Later in Sri Lanka, the Dhammapada was championed as a unifying scripture. The Sthaviras gave rise to several schools, one of which was the Theravāda school. Originally, these schisms were caused by disputes over vinaya, and monks following different schools of thought seem to have lived happily together in the same monasteries, but eventually, by about 100 CE if not earlier, schisms were being caused by doctrinal disagreements too. Although ""Nibbāna"" (Sanskrit: Nirvāna) is the common term for the desired goal of this practice, many other terms can be found throughout the Nikayas, which are not specified.[note 35] Zen Buddhism (禅), pronounced Chán in Chinese, seon in Korean or zen in Japanese (derived from the Sanskrit term dhyāna, meaning ""meditation"") is a form of Buddhism that became popular in China, Korea and Japan and that lays special emphasis on meditation.[note 12] Zen places less emphasis on scriptures than some other forms of Buddhism and prefers to focus on direct spiritual breakthroughs to truth. Much of the early extant evidence for the origins of Mahāyāna comes from early Chinese translations of Mahāyāna texts. These Mahāyāna teachings were first propagated into China by Lokakṣema, the first translator of Mahāyāna sūtras into Chinese during the 2nd century CE.[note 39] Some scholars have traditionally considered the earliest Mahāyāna sūtras to include the very first versions of the Prajñāpāramitā series, along with texts concerning Akṣobhya Buddha, which were probably composed in the 1st century BCE in the south of India.[note 40] Generally, scholars conclude that the Mahayana scriptures were composed from the 1st century CE onwards: ""Large numbers of Mahayana sutras were being composed in the period between the beginning of the common era and the fifth century"", five centuries after the historical Gautama Buddha. Some of these had their roots in other scriptures composed in the 1st century BCE. It was not until after the 5th century CE that the Mahayana sutras started to influence the behavior of mainstream Buddhists in India: ""But outside of texts, at least in India, at exactly the same period, very different—in fact seemingly older—ideas and aspirations appear to be motivating actual behavior, and old and established Hinnayana groups appear to be the only ones that are patronized and supported."" These texts were apparently not universally accepted among Indian Buddhists when they appeared; the pejorative label Hinayana was applied by Mahayana supporters to those who rejected the Mahayana sutras. The concept of liberation (nirvāṇa)—the goal of the Buddhist path—is closely related to overcoming ignorance (avidyā), a fundamental misunderstanding or mis-perception of the nature of reality. In awakening to the true nature of the self and all phenomena one develops dispassion for the objects of clinging, and is liberated from suffering (dukkha) and the cycle of incessant rebirths (saṃsāra). To this end, the Buddha recommended viewing things as characterized by the three marks of existence. The Buddhist texts are probably the earliest describing meditation techniques. They describe meditative practices and states that existed before the Buddha as well as those first developed within Buddhism. Two Upanishads written after the rise of Buddhism do contain full-fledged descriptions of yoga as a means to liberation. The evidence of the early texts suggests that Siddhārtha Gautama was born in a community that was on the periphery, both geographically and culturally, of the northeastern Indian subcontinent in the fifth century BCE. It was either a small republic, in which case his father was an elected chieftain, or an oligarchy, in which case his father was an oligarch. Therefore, according to Mahayana Buddhism, the arahant has attained only nirvana, thus still being subject to delusion, while the bodhisattva not only achieves nirvana but full liberation from delusion as well. He thus attains bodhi and becomes a buddha. In Theravada Buddhism, bodhi and nirvana carry the same meaning as in the early texts, that of being freed from greed, hate and delusion. According to the Mahāsaccakasutta,[note 33] from the fourth jhana the Buddha gained bodhi. Yet, it is not clear what he was awakened to.[page needed] ""Liberating insight"" is a later addition to this text, and reflects a later development and understanding in early Buddhism.[page needed][page needed] The mentioning of the four truths as constituting ""liberating insight"" introduces a logical problem, since the four truths depict a linear path of practice, the knowledge of which is in itself not depicted as being liberating.[note 34] Formal membership varies between communities, but basic lay adherence is often defined in terms of a traditional formula in which the practitioner takes refuge in The Three Jewels: the Buddha, the Dharma (the teachings of the Buddha), and the Sangha (the Buddhist community). At the present time, the teachings of all three branches of Buddhism have spread throughout the world, and Buddhist texts are increasingly translated into local languages. While in the West Buddhism is often seen as exotic and progressive, in the East it is regarded as familiar and traditional. Buddhists in Asia are frequently well organized and well funded. In countries such as Cambodia and Bhutan, it is recognized as the state religion and receives government support. Modern influences increasingly lead to new forms of Buddhism that significantly depart from traditional beliefs and practices. Sarvastivada teachings—which were criticized by Nāgārjuna—were reformulated by scholars such as Vasubandhu and Asanga and were adapted into the Yogacara school. While the Mādhyamaka school held that asserting the existence or non-existence of any ultimately real thing was inappropriate, some exponents of Yogacara asserted that the mind and only the mind is ultimately real (a doctrine known as cittamatra). Not all Yogacarins asserted that mind was truly existent; Vasubandhu and Asanga in particular did not.[web 11] These two schools of thought, in opposition or synthesis, form the basis of subsequent Mahayana metaphysics in the Indo-Tibetan tradition. According to this narrative, shortly after the birth of young prince Gautama, an astrologer named Asita visited the young prince's father, Suddhodana, and prophesied that Siddhartha would either become a great king or renounce the material world to become a holy man, depending on whether he saw what life was like outside the palace walls. According to Tilmann Vetter, the core of earliest Buddhism is the practice of dhyāna. Bronkhorst agrees that dhyana was a Buddhist invention, whereas Norman notes that ""the Buddha's way to release [...] was by means of meditative practices."" Discriminating insight into transiency as a separate path to liberation was a later development. In the language of the Noble Eightfold Path, samyaksamādhi is ""right concentration"". The primary means of cultivating samādhi is meditation. Upon development of samādhi, one's mind becomes purified of defilement, calm, tranquil, and luminous. Traditionally, the first step in most Buddhist schools requires taking refuge in the Three Jewels (Sanskrit: tri-ratna, Pāli: ti-ratana)[web 19] as the foundation of one's religious practice. The practice of taking refuge on behalf of young or even unborn children is mentioned in the Majjhima Nikaya, recognized by most scholars as an early text (cf. Infant baptism). Tibetan Buddhism sometimes adds a fourth refuge, in the lama. In Mahayana, the person who chooses the bodhisattva path makes a vow or pledge, considered the ultimate expression of compassion. In Mahayana, too, the Three Jewels are perceived as possessed of an eternal and unchanging essence and as having an irreversible effect: ""The Three Jewels have the quality of excellence. Just as real jewels never change their faculty and goodness, whether praised or reviled, so are the Three Jewels (Refuges), because they have an eternal and immutable essence. These Three Jewels bring a fruition that is changeless, for once one has reached Buddhahood, there is no possibility of falling back to suffering. Various classes of Vajrayana literature developed as a result of royal courts sponsoring both Buddhism and Saivism. The Mañjusrimulakalpa, which later came to classified under Kriyatantra, states that mantras taught in the Saiva, Garuda and Vaisnava tantras will be effective if applied by Buddhists since they were all taught originally by Manjushri. The Guhyasiddhi of Padmavajra, a work associated with the Guhyasamaja tradition, prescribes acting as a Saiva guru and initiating members into Saiva Siddhanta scriptures and mandalas. The Samvara tantra texts adopted the pitha list from the Saiva text Tantrasadbhava, introducing a copying error where a deity was mistaken for a place. According to author Michael Carrithers, while there are good reasons to doubt the traditional account, ""the outline of the life must be true: birth, maturity, renunciation, search, awakening and liberation, teaching, death."" In writing her biography of the Buddha, Karen Armstrong noted, ""It is obviously difficult, therefore, to write a biography of the Buddha that meets modern criteria, because we have very little information that can be considered historically sound... [but] we can be reasonably confident Siddhatta Gotama did indeed exist and that his disciples preserved the memory of his life and teachings as well as they could.""[dubious – discuss] Scholar Isabelle Onians asserts that although ""the Mahāyāna ... very occasionally referred contemptuously to earlier Buddhism as the Hinayāna, the Inferior Way,"" ""the preponderance of this name in the secondary literature is far out of proportion to occurrences in the Indian texts."" She notes that the term Śrāvakayāna was ""the more politically correct and much more usual"" term used by Mahāyānists. Jonathan Silk has argued that the term ""Hinayana"" was used to refer to whomever one wanted to criticize on any given occasion, and did not refer to any definite grouping of Buddhists. Only the Theravada school does not include the Mahayana scriptures in its canon. As the modern Theravada school is descended from a branch of Buddhism that diverged and established itself in Sri Lanka prior to the emergence of the Mahayana texts, debate exists as to whether the Theravada were historically included in the hinayana designation; in the modern era, this label is seen as derogatory, and is generally avoided. While he searched for enlightenment, Gautama combined the yoga practice of his teacher Kalama with what later became known as ""the immeasurables"".[dubious – discuss] Gautama thus invented a new kind of human, one without egotism.[dubious – discuss] What Thich Nhat Hanh calls the ""Four Immeasurable Minds"" of love, compassion, joy, and equanimity[full citation needed] are also known as brahmaviharas, divine abodes, or simply as four immeasurables.[web 5] Pema Chödrön calls them the ""four limitless ones"". Of the four, mettā or loving-kindness meditation is perhaps the best known.[web 5] The Four Immeasurables are taught as a form of meditation that cultivates ""wholesome attitudes towards all sentient beings.""[web 6][web 7] The Silk Road transmission of Buddhism to China is most commonly thought to have started in the late 2nd or the 1st century CE, though the literary sources are all open to question.[note 41] The first documented translation efforts by foreign Buddhist monks in China were in the 2nd century CE, probably as a consequence of the expansion of the Kushan Empire into the Chinese territory of the Tarim Basin. The Pāli Tipitaka, which means ""three baskets"", refers to the Vinaya Pitaka, the Sutta Pitaka, and the Abhidhamma Pitaka. The Vinaya Pitaka contains disciplinary rules for the Buddhist monks and nuns, as well as explanations of why and how these rules were instituted, supporting material, and doctrinal clarification. The Sutta Pitaka contains discourses ascribed to Gautama Buddha. The Abhidhamma Pitaka contains material often described as systematic expositions of the Gautama Buddha's teachings. Mahayana Buddhism flourished in India from the 5th century CE onwards, during the dynasty of the Guptas. Mahāyāna centres of learning were established, the most important one being the Nālandā University in north-eastern India. The Twelve Nidānas describe a causal connection between the subsequent characteristics or conditions of cyclic existence, each one giving rise to the next: In Theravada Buddhism there can be no divine salvation or forgiveness for one's karma, since it is a purely impersonal process that is a part of the makeup of the universe.[citation needed] In Mahayana Buddhism, the texts of certain Mahayana sutras (such as the Lotus Sutra, the Aṅgulimālīya Sūtra and the Mahāyāna Mahāparinirvāṇa Sūtra) claim that the recitation or merely the hearing of their texts can expunge great swathes of negative karma. Some forms of Buddhism (for example, Vajrayana) regard the recitation of mantras as a means for cutting off of previous negative karma. The Japanese Pure Land teacher Genshin taught that Amitābha has the power to destroy the karma that would otherwise bind one in saṃsāra. Anthony Barber and Sree Padma note that ""historians of Buddhist thought have been aware for quite some time that such pivotally important Mahayana Buddhist thinkers as Nāgārjuna, Dignaga, Candrakīrti, Āryadeva, and Bhavaviveka, among many others, formulated their theories while living in Buddhist communities in Āndhra."" They note that the ancient Buddhist sites in the lower Kṛṣṇa Valley, including Amaravati, Nāgārjunakoṇḍā and Jaggayyapeṭa ""can be traced to at least the third century BCE, if not earlier."" Akira Hirakawa notes the ""evidence suggests that many Early Mahayana scriptures originated in South India."" Religious knowledge or ""vision"" was indicated as a result of practice both within and outside of the Buddhist fold. According to the Samaññaphala Sutta, this sort of vision arose for the Buddhist adept as a result of the perfection of ""meditation"" coupled with the perfection of ""discipline"" (Pali sīla; Skt. śīla). Some of the Buddha's meditative techniques were shared with other traditions of his day, but the idea that ethics are causally related to the attainment of ""transcendent wisdom"" (Pali paññā; Skt. prajñā) was original.[web 18] The Buddha's death is seen as an illusion, he is living on in other planes of existence, and monks are therefore permitted to offer ""new truths"" based on his input. Mahayana also differs from Theravada in its concept of śūnyatā (that ultimately nothing has existence), and in its belief in bodhisattvas (enlightened people who vow to continue being reborn until all beings can be enlightened). Historically, the roots of Buddhism lie in the religious thought of ancient India during the second half of the first millennium BCE. That was a period of social and religious turmoil, as there was significant discontent with the sacrifices and rituals of Vedic Brahmanism.[note 15] It was challenged by numerous new ascetic religious and philosophical groups and teachings that broke with the Brahmanic tradition and rejected the authority of the Vedas and the Brahmans.[note 16] These groups, whose members were known as shramanas, were a continuation of a non-Vedic strand of Indian thought distinct from Indo-Aryan Brahmanism.[note 17] Scholars have reasons to believe that ideas such as samsara, karma (in the sense of the influence of morality on rebirth), and moksha originated in the shramanas, and were later adopted by Brahmin orthodoxy.[note 18][note 19][note 20][note 21][note 22][note 23] Śuddhodana was determined to see his son become a king, so he prevented him from leaving the palace grounds. But at age 29, despite his father's efforts, Gautama ventured beyond the palace several times. In a series of encounters—known in Buddhist literature as the four sights—he learned of the suffering of ordinary people, encountering an old man, a sick man, a corpse and, finally, an ascetic holy man, apparently content and at peace with the world. These experiences prompted Gautama to abandon royal life and take up a spiritual quest. Buddhist meditation is fundamentally concerned with two themes: transforming the mind and using it to explore itself and other phenomena. According to Theravada Buddhism the Buddha taught two types of meditation, samatha meditation (Sanskrit: śamatha) and vipassanā meditation (Sanskrit: vipaśyanā). In Chinese Buddhism, these exist (translated chih kuan), but Chán (Zen) meditation is more popular. According to Peter Harvey, whenever Buddhism has been healthy, not only monks, nuns, and married lamas, but also more committed lay people have practiced meditation. According to Routledge's Encyclopedia of Buddhism, in contrast, throughout most of Buddhist history before modern times, serious meditation by lay people has been unusual. The evidence of the early texts suggests that at the time of the Buddha, many male and female lay practitioners did practice meditation, some even to the point of proficiency in all eight jhānas (see the next section regarding these).[note 11] Buddhist scriptures and other texts exist in great variety. Different schools of Buddhism place varying levels of value on learning the various texts. Some schools venerate certain texts as religious objects in themselves, while others take a more scholastic approach. Buddhist scriptures are mainly written in Pāli, Tibetan, Mongolian, and Chinese. Some texts still exist in Sanskrit and Buddhist Hybrid Sanskrit. The second truth is that the origin of dukkha can be known. Within the context of the four noble truths, the origin of dukkha is commonly explained as craving (Pali: tanha) conditioned by ignorance (Pali: avijja). On a deeper level, the root cause of dukkha is identified as ignorance (Pali: avijja) of the true nature of things. The third noble truth is that the complete cessation of dukkha is possible, and the fourth noble truth identifies a path to this cessation.[note 7] Devotion is an important part of the practice of most Buddhists. Devotional practices include bowing, offerings, pilgrimage, and chanting. In Pure Land Buddhism, devotion to the Buddha Amitabha is the main practice. In Nichiren Buddhism, devotion to the Lotus Sutra is the main practice. Mahayana schools recognize all or part of the Mahayana Sutras. Some of these sutras became for Mahayanists a manifestation of the Buddha himself, and faith in and veneration of those texts are stated in some sutras (e.g. the Lotus Sutra and the Mahaparinirvana Sutra) to lay the foundations for the later attainment of Buddhahood itself. Unlike many religions, Buddhism has no single central text that is universally referred to by all traditions. However, some scholars have referred to the Vinaya Pitaka and the first four Nikayas of the Sutta Pitaka as the common core of all Buddhist traditions.[page needed] This could be considered misleading, as Mahāyāna considers these merely a preliminary, and not a core, teaching. The Tibetan Buddhists have not even translated most of the āgamas (though theoretically they recognize them) and they play no part in the religious life of either clergy or laity in China and Japan. Other scholars say there is no universally accepted common core. The size and complexity of the Buddhist canons have been seen by some (including Buddhist social reformer Babasaheb Ambedkar) as presenting barriers to the wider understanding of Buddhist philosophy. Besides emptiness, Mahayana schools often place emphasis on the notions of perfected spiritual insight (prajñāpāramitā) and Buddha-nature (tathāgatagarbha). There are conflicting interpretations of the tathāgatagarbha in Mahāyāna thought. The idea may be traced to Abhidharma, and ultimately to statements of the Buddha in the Nikāyas. In Tibetan Buddhism, according to the Sakya school, tathāgatagarbha is the inseparability of the clarity and emptiness of one's mind. In Nyingma, tathāgatagarbha also generally refers to inseparability of the clarity and emptiness of one's mind. According to the Gelug school, it is the potential for sentient beings to awaken since they are empty (i.e. dependently originated). According to the Jonang school, it refers to the innate qualities of the mind that expresses themselves as omniscience etc. when adventitious obscurations are removed. The ""Tathāgatagarbha Sutras"" are a collection of Mahayana sutras that present a unique model of Buddha-nature. Even though this collection was generally ignored in India, East Asian Buddhism provides some significance to these texts. Buddhist scholars have produced a number of intellectual theories, philosophies and world view concepts (see, for example, Abhidharma, Buddhist philosophy and Reality in Buddhism). Some schools of Buddhism discourage doctrinal study, and some regard it as essential practice. Zen Buddhist teaching is often full of paradox, in order to loosen the grip of the ego and to facilitate the penetration into the realm of the True Self or Formless Self, which is equated with the Buddha himself.[note 14] According to Zen master Kosho Uchiyama, when thoughts and fixation on the little ""I"" are transcended, an Awakening to a universal, non-dual Self occurs: ""When we let go of thoughts and wake up to the reality of life that is working beyond them, we discover the Self that is living universal non-dual life (before the separation into two) that pervades all living creatures and all existence."" Thinking and thought must therefore not be allowed to confine and bind one. According to most scholars, at some period after the Second Council the Sangha began to break into separate factions.[note 37] The various accounts differ as to when the actual schisms occurred. According to the Dipavamsa of the Pāli tradition, they started immediately after the Second Council, the Puggalavada tradition places it in 137 AN, the Sarvastivada tradition of Vasumitra says it was in the time of Ashoka and the Mahasanghika tradition places it much later, nearly 100 BCE. The history of Indian Buddhism may be divided into five periods: Early Buddhism (occasionally called Pre-sectarian Buddhism), Nikaya Buddhism or Sectarian Buddhism: The period of the Early Buddhist schools, Early Mahayana Buddhism, Later Mahayana Buddhism, and Esoteric Buddhism (also called Vajrayana Buddhism). A number of modern movements or tendencies in Buddhism emerged during the second half of the 20th Century, including the Dalit Buddhist movement (also sometimes called 'neo-Buddhism'), Engaged Buddhism, and the further development of various Western Buddhist traditions. Gautama first went to study with famous religious teachers of the day, and mastered the meditative attainments they taught. But he found that they did not provide a permanent end to suffering, so he continued his quest. He next attempted an extreme asceticism, which was a religious pursuit common among the śramaṇas, a religious culture distinct from the Vedic one. Gautama underwent prolonged fasting, breath-holding, and exposure to pain. He almost starved himself to death in the process. He realized that he had taken this kind of practice to its limit, and had not put an end to suffering. So in a pivotal moment he accepted milk and rice from a village girl and changed his approach. He devoted himself to anapanasati meditation, through which he discovered what Buddhists call the Middle Way (Skt. madhyamā-pratipad): a path of moderation between the extremes of self-indulgence and self-mortification.[web 2][web 3] Initially, prajñā is attained at a conceptual level by means of listening to sermons (dharma talks), reading, studying, and sometimes reciting Buddhist texts and engaging in discourse. Once the conceptual understanding is attained, it is applied to daily life so that each Buddhist can verify the truth of the Buddha's teaching at a practical level. Notably, one could in theory attain Nirvana at any point of practice, whether deep in meditation, listening to a sermon, conducting the business of one's daily life, or any other activity. Ajahn Sucitto describes the path as ""a mandala of interconnected factors that support and moderate each other."" The eight factors of the path are not to be understood as stages, in which each stage is completed before moving on to the next. Rather, they are understood as eight significant dimensions of one's behaviour—mental, spoken, and bodily—that operate in dependence on one another; taken together, they define a complete path, or way of living. Impermanence (Pāli: anicca) expresses the Buddhist notion that all compounded or conditioned phenomena (all things and experiences) are inconstant, unsteady, and impermanent. Everything we can experience through our senses is made up of parts, and its existence is dependent on external conditions. Everything is in constant flux, and so conditions and the thing itself are constantly changing. Things are constantly coming into being, and ceasing to be. Since nothing lasts, there is no inherent or fixed nature to any object or experience. According to the doctrine of impermanence, life embodies this flux in the aging process, the cycle of rebirth (saṃsāra), and in any experience of loss. The doctrine asserts that because things are impermanent, attachment to them is futile and leads to suffering (dukkha). Sentient beings always suffer throughout saṃsāra until they free themselves from this suffering (dukkha) by attaining Nirvana. Then the absence of the first Nidāna—ignorance—leads to the absence of the others. Two major extant branches of Buddhism are generally recognized by scholars: Theravada (""The School of the Elders"") and Mahayana (""The Great Vehicle""). Vajrayana, a body of teachings attributed to Indian siddhas, may be viewed as a third branch or merely a part of Mahayana. Theravada has a widespread following in Sri Lanka and Southeast Asia. Mahayana which includes the traditions of Pure Land, Zen, Nichiren Buddhism, Shingon, and Tiantai (Tendai) is found throughout East Asia. Tibetan Buddhism, which preserves the Vajrayana teachings of eighth century India, is practiced in regions surrounding the Himalayas, Mongolia and Kalmykia. Buddhists number between an estimated 488 million[web 1] and 535 million, making it one of the world's major religions. In the eight precepts, the third precept on sexual misconduct is made more strict, and becomes a precept of celibacy. The three additional precepts are: Several scholars have suggested that the Prajñāpāramitā sūtras, which are among the earliest Mahāyāna sūtras, developed among the Mahāsāṃghika along the Kṛṣṇa River in the Āndhra region of South India. Native Mahayana Buddhism is practiced today in China, Japan, Korea, Singapore, parts of Russia and most of Vietnam (also commonly referred to as ""Eastern Buddhism""). The Buddhism practiced in Tibet, the Himalayan regions, and Mongolia is also Mahayana in origin, but is discussed below under the heading of Vajrayana (also commonly referred to as ""Northern Buddhism""). There are a variety of strands in Eastern Buddhism, of which ""the Pure Land school of Mahayana is the most widely practised today."". In most of this area however, they are fused into a single unified form of Buddhism. In Japan in particular, they form separate denominations with the five major ones being: Nichiren, peculiar to Japan; Pure Land; Shingon, a form of Vajrayana; Tendai, and Zen. In Korea, nearly all Buddhists belong to the Chogye school, which is officially Son (Zen), but with substantial elements from other traditions. In Eastern Buddhism, there is also a distinctive Vinaya and ethics contained within the Mahayana Brahmajala Sutra (not to be confused with the Pali text of that name) for Bodhisattvas, where, for example, the eating of meat is frowned upon and vegetarianism is actively encouraged (see vegetarianism in Buddhism). In Japan, this has almost completely displaced the monastic vinaya, and allows clergy to marry. Prajñā (Sanskrit) or paññā (Pāli) means wisdom that is based on a realization of dependent origination, The Four Noble Truths and the three marks of existence. Prajñā is the wisdom that is able to extinguish afflictions and bring about bodhi. It is spoken of as the principal means of attaining nirvāṇa, through its revelation of the true nature of all things as dukkha (unsatisfactoriness), anicca (impermanence) and anatta (not-self). Prajñā is also listed as the sixth of the six pāramitās of the Mahayana."
Virgil,"According to the commentators, Virgil received his first education when he was five years old and he later went to Cremona, Milan, and finally Rome to study rhetoric, medicine, and astronomy, which he soon abandoned for philosophy. From Virgil's admiring references to the neoteric writers Pollio and Cinna, it has been inferred that he was, for a time, associated with Catullus' neoteric circle. However schoolmates considered Virgil extremely shy and reserved, according to Servius, and he was nicknamed ""Parthenias"" or ""maiden"" because of his social aloofness. Virgil seems to have suffered bad health throughout his life and in some ways lived the life of an invalid. According to the Catalepton, while in the Epicurean school of Siro the Epicurean at Naples, he began to write poetry. A group of small works attributed to the youthful Virgil by the commentators survive collected under the title Appendix Vergiliana, but are largely considered spurious by scholars. One, the Catalepton, consists of fourteen short poems, some of which may be Virgil's, and another, a short narrative poem titled the Culex (""The Gnat""), was attributed to Virgil as early as the 1st century AD. Virgil is traditionally ranked as one of Rome's greatest poets. His Aeneid has been considered the national epic of ancient Rome from the time of its composition to the present day. Modeled after Homer's Iliad and Odyssey, the Aeneid follows the Trojan refugee Aeneas as he struggles to fulfill his destiny and arrive on the shores of Italy—in Roman mythology the founding act of Rome. Virgil's work has had wide and deep influence on Western literature, most notably Dante's Divine Comedy, in which Virgil appears as Dante's guide through hell and purgatory. In the Late Empire and Middle Ages Vergilius was spelled Virgilius. Two explanations are commonly given for this alteration. One deduces a false etymology associated with the word virgo (""maiden"" in Latin) due to Virgil's excessive, ""maiden""-like modesty. Alternatively, some argue that Vergilius was altered to Virgilius by analogy with the Latin virga (""wand"") due to the magical or prophetic powers attributed to Virgil in the Middle Ages (this explanation is found in only a handful of manuscripts, however, and was probably not widespread). In Norman schools (following the French practice), the habit was to anglicize Latin names by dropping their Latin endings, hence Virgil. In the 19th century, some German-trained classicists in the United States suggested modification to Vergil, as it is closer to his original name, and is also the traditional German spelling.[citation needed] Modern usage permits both, though the Oxford guide to style recommends Vergilius to avoid confusion with the 8th-century grammarian Virgilius Maro Grammaticus. Some post-Renaissance writers liked to affect the sobriquet ""The Swan of Mantua"".[citation needed] Even as the Western Roman empire collapsed, literate men acknowledged that Virgil was a master poet. Gregory of Tours read Virgil, whom he quotes in several places, along with some other Latin poets, though he cautions that ""we ought not to relate their lying fables, lest we fall under sentence of eternal death."" According to the tradition, Virgil traveled to Greece in about 19 BC to revise the Aeneid. After meeting Augustus in Athens and deciding to return home, Virgil caught a fever while visiting a town near Megara. After crossing to Italy by ship, weakened with disease, Virgil died in Brundisium harbor on September 21, 19 BC. Augustus ordered Virgil's literary executors, Lucius Varius Rufus and Plotius Tucca, to disregard Virgil's own wish that the poem be burned, instead ordering it published with as few editorial changes as possible. As a result, the text of the Aeneid that exists may contain faults which Virgil was planning to correct before publication. However, the only obvious imperfections are a few lines of verse that are metrically unfinished (i.e. not a complete line of dactylic hexameter). Some scholars have argued that Virgil deliberately left these metrically incomplete lines for dramatic effect. Other alleged imperfections are subject to scholarly debate. In the Middle Ages, Virgil's reputation was such that it inspired legends associating him with magic and prophecy. From at least the 3rd century, Christian thinkers interpreted Eclogues 4, which describes the birth of a boy ushering in a golden age, as a prediction of Jesus' birth. As such, Virgil came to be seen on a similar level as the Hebrew prophets of the Bible as one who had heralded Christianity. Dante made Virgil his guide in Hell and the greater part of Purgatory in The Divine Comedy. Dante also mentions Virgil in De vulgari eloquentia, along with Ovid, Lucan and Statius, as one of the four regulati poetae (ii, vi, 7). The Aeneid is widely considered Virgil's finest work and one of the most important poems in the history of western literature. Virgil worked on the Aeneid during the last eleven years of his life (29–19 BC), commissioned, according to Propertius, by Augustus. The epic poem consists of 12 books in dactylic hexameter verse which describe the journey of Aeneas, a warrior fleeing the sack of Troy, to Italy, his battle with the Italian prince Turnus, and the foundation of a city from which Rome would emerge. The Aeneid's first six books describe the journey of Aeneas from Troy to Rome. Virgil made use of several models in the composition of his epic; Homer, the preeminent author of classical epic, is everywhere present, but Virgil also makes special use of the Latin poet Ennius and the Hellenistic poet Apollonius of Rhodes among the various other writers to which he alludes. Although the Aeneid casts itself firmly into the epic mode, it often seeks to expand the genre by including elements of other genres such as tragedy and aetiological poetry. Ancient commentators noted that Virgil seems to divide the Aeneid into two sections based on the poetry of Homer; the first six books were viewed as employing the Odyssey as a model while the last six were connected to the Iliad. The Aeneid appears to have been a great success. Virgil is said to have recited Books 2, 4, and 6 to Augustus; and Book 6 apparently caused Augustus' sister Octavia to faint. Although the truth of this claim is subject to scholarly scepticism, it has served as a basis for later art, such as Jean-Baptiste Wicar's Virgil Reading the Aeneid. The structure known as ""Virgil's tomb"" is found at the entrance of an ancient Roman tunnel (also known as ""grotta vecchia"") in Piedigrotta, a district two miles from the centre of Naples, near the Mergellina harbor, on the road heading north along the coast to Pozzuoli. While Virgil was already the object of literary admiration and veneration before his death, in the Middle Ages his name became associated with miraculous powers, and for a couple of centuries his tomb was the destination of pilgrimages and veneration. Sometime after the publication of the Eclogues (probably before 37 BC), Virgil became part of the circle of Maecenas, Octavian's capable agent d'affaires who sought to counter sympathy for Antony among the leading families by rallying Roman literary figures to Octavian's side. Virgil came to know many of the other leading literary figures of the time, including Horace, in whose poetry he is often mentioned, and Varius Rufus, who later helped finish the Aeneid. Publius Vergilius Maro (Classical Latin: [ˈpuː.blɪ.ʊs wɛrˈɡɪ.lɪ.ʊs ˈma.roː]; October 15, 70 BC – September 21, 19 BC), usually called Virgil or Vergil /ˈvɜːrdʒᵻl/ in English, was an ancient Roman poet of the Augustan period. He is known for three major works of Latin literature, the Eclogues (or Bucolics), the Georgics, and the epic Aeneid. A number of minor poems, collected in the Appendix Vergiliana, are sometimes attributed to him. Book 1 (at the head of the Odyssean section) opens with a storm which Juno, Aeneas' enemy throughout the poem, stirs up against the fleet. The storm drives the hero to the coast of Carthage, which historically was Rome's deadliest foe. The queen, Dido, welcomes the ancestor of the Romans, and under the influence of the gods falls deeply in love with him. At a banquet in Book 2, Aeneas tells the story of the sack of Troy, the death of his wife, and his escape, to the enthralled Carthaginians, while in Book 3 he recounts to them his wanderings over the Mediterranean in search of a suitable new home. Jupiter in Book 4 recalls the lingering Aeneas to his duty to found a new city, and he slips away from Carthage, leaving Dido to commit suicide, cursing Aeneas and calling down revenge in a symbolic anticipation of the fierce wars between Carthage and Rome. In Book 5, Aeneas' father Anchises dies and funeral games are celebrated for him. On reaching Cumae, in Italy in Book 6, Aeneas consults the Cumaean Sibyl, who conducts him through the Underworld where Aeneas meets the dead Anchises who reveals Rome's destiny to his son. The tradition holds that Virgil was born in the village of Andes, near Mantua in Cisalpine Gaul. Analysis of his name has led to beliefs that he descended from earlier Roman colonists. Modern speculation ultimately is not supported by narrative evidence either from his own writings or his later biographers. Macrobius says that Virgil's father was of a humble background; however, scholars generally believe that Virgil was from an equestrian landowning family which could afford to give him an education. He attended schools in Cremona, Mediolanum, Rome and Naples. After considering briefly a career in rhetoric and law, the young Virgil turned his talents to poetry. Virgil's biographical tradition is thought to depend on a lost biography by Varius, Virgil's editor, which was incorporated into the biography by Suetonius and the commentaries of Servius and Donatus, the two great commentators on Virgil's poetry. Although the commentaries no doubt record much factual information about Virgil, some of their evidence can be shown to rely on inferences made from his poetry and allegorizing; thus, Virgil's biographical tradition remains problematic. The legend of Virgil in his Basket arose in the Middle Ages, and is often seen in art and mentioned in literature as part of the Power of Women literary topos, demonstrating the disruptive force of female attractiveness on men. In this story Virgil became enamoured of a beautiful woman, sometimes described as the emperor's daughter or mistress and called Lucretia. She played him along and agreed to an assignation at her house, which he was to sneak into at night by climbing into a large basket let down from a window. When he did so he was only hoisted halfway up the wall and then left him trapped there into the next day, exposed to public ridicule. The story paralleled that of Phyllis riding Aristotle. Among other artists depicting the scene, Lucas van Leyden made a woodcut and later an engraving. Book 7 (beginning the Iliadic half) opens with an address to the muse and recounts Aeneas' arrival in Italy and betrothal to Lavinia, daughter of King Latinus. Lavinia had already been promised to Turnus, the king of the Rutulians, who is roused to war by the Fury Allecto, and Amata Lavinia's mother. In Book 8, Aeneas allies with King Evander, who occupies the future site of Rome, and is given new armor and a shield depicting Roman history. Book 9 records an assault by Nisus and Euryalus on the Rutulians, Book 10, the death of Evander's young son Pallas, and 11 the death of the Volscian warrior princess Camilla and the decision to settle the war with a duel between Aeneas and Turnus. The Aeneid ends in Book 12 with the taking of Latinus' city, the death of Amata, and Aeneas' defeat and killing of Turnus, whose pleas for mercy are spurned. The final book ends with the image of Turnus' soul lamenting as it flees to the underworld. The works of Virgil almost from the moment of their publication revolutionized Latin poetry. The Eclogues, Georgics, and above all the Aeneid became standard texts in school curricula with which all educated Romans were familiar. Poets following Virgil often refer intertextually to his works to generate meaning in their own poetry. The Augustan poet Ovid parodies the opening lines of the Aeneid in Amores 1.1.1–2, and his summary of the Aeneas story in Book 14 of the Metamorphoses, the so-called ""mini-Aeneid"", has been viewed as a particularly important example of post-Virgilian response to the epic genre. Lucan's epic, the Bellum Civile has been considered an anti-Virgilian epic, disposing with the divine mechanism, treating historical events, and diverging drastically from Virgilian epic practice. The Flavian poet Statius in his 12-book epic Thebaid engages closely with the poetry of Virgil; in his epilogue he advises his poem not to ""rival the divine Aeneid, but follow afar and ever venerate its footsteps."" In Silius Italicus, Virgil finds one of his most ardent admirers. With almost every line of his epic Punica Silius references Virgil. Indeed, Silius is known to have bought Virgil's tomb and worshipped the poet. Partially as a result of his so-called ""Messianic"" Fourth Eclogue—widely interpreted later to have predicted the birth of Jesus Christ—Virgil was in later antiquity imputed to have the magical abilities of a seer; the Sortes Vergilianae, the process of using Virgil's poetry as a tool of divination, is found in the time of Hadrian, and continued into the Middle Ages. In a similar vein Macrobius in the Saturnalia credits the work of Virgil as the embodiment of human knowledge and experience, mirroring the Greek conception of Homer. Virgil also found commentators in antiquity. Servius, a commentator of the 4th century AD, based his work on the commentary of Donatus. Servius' commentary provides us with a great deal of information about Virgil's life, sources, and references; however, many modern scholars find the variable quality of his work and the often simplistic interpretations frustrating. Possibly as early as the second century AD, Virgil's works were seen as having magical properties and were used for divination. In what became known as the Sortes Vergilianae (Virgilian Lots), passages would be selected at random and interpreted to answer questions. In the 12th century, starting around Naples but eventually spreading widely throughout Europe, a tradition developed in which Virgil was regarded as a great magician. Legends about Virgil and his magical powers remained popular for over two hundred years, arguably becoming as prominent as his writings themselves. Virgil's legacy in medieval Wales was such that the Welsh version of his name, Fferyllt or Pheryllt, became a generic term for magic-worker, and survives in the modern Welsh word for pharmacist, fferyllydd. The Georgics' tone wavers between optimism and pessimism, sparking critical debate on the poet's intentions, but the work lays the foundations for later didactic poetry. Virgil and Maecenas are said to have taken turns reading the Georgics to Octavian upon his return from defeating Antony and Cleopatra at the Battle of Actium in 31 BC. Critics of the Aeneid focus on a variety of issues. The tone of the poem as a whole is a particular matter of debate; some see the poem as ultimately pessimistic and politically subversive to the Augustan regime, while others view it as a celebration of the new imperial dynasty. Virgil makes use of the symbolism of the Augustan regime, and some scholars see strong associations between Augustus and Aeneas, the one as founder and the other as re-founder of Rome. A strong teleology, or drive towards a climax, has been detected in the poem. The Aeneid is full of prophecies about the future of Rome, the deeds of Augustus, his ancestors, and famous Romans, and the Carthaginian Wars; the shield of Aeneas even depicts Augustus' victory at Actium against Mark Antony and Cleopatra VII in 31 BC. A further focus of study is the character of Aeneas. As the protagonist of the poem, Aeneas seems to waver constantly between his emotions and commitment to his prophetic duty to found Rome; critics note the breakdown of Aeneas' emotional control in the last sections of the poem where the ""pious"" and ""righteous"" Aeneas mercilessly slaughters Turnus. At Maecenas' insistence (according to the tradition) Virgil spent the ensuing years (perhaps 37–29 BC) on the long didactic hexameter poem called the Georgics (from Greek, ""On Working the Earth"") which he dedicated to Maecenas. The ostensible theme of the Georgics is instruction in the methods of running a farm. In handling this theme, Virgil follows in the didactic (""how to"") tradition of the Greek poet Hesiod's Works and Days and several works of the later Hellenistic poets. The four books of the Georgics focus respectively on raising crops and trees (1 and 2), livestock and horses (3), and beekeeping and the qualities of bees (4). Well-known passages include the beloved Laus Italiae of Book 2, the prologue description of the temple in Book 3, and the description of the plague at the end of Book 3. Book 4 concludes with a long mythological narrative, in the form of an epyllion which describes vividly the discovery of beekeeping by Aristaeus and the story of Orpheus' journey to the underworld. Ancient scholars, such as Servius, conjectured that the Aristaeus episode replaced, at the emperor's request, a long section in praise of Virgil's friend, the poet Gallus, who was disgraced by Augustus, and who committed suicide in 26 BC. The biographical tradition asserts that Virgil began the hexameter Eclogues (or Bucolics) in 42 BC and it is thought that the collection was published around 39–38 BC, although this is controversial. The Eclogues (from the Greek for ""selections"") are a group of ten poems roughly modeled on the bucolic hexameter poetry (""pastoral poetry"") of the Hellenistic poet Theocritus. After his victory in the Battle of Philippi in 42 BC, fought against the army led by the assassins of Julius Caesar, Octavian tried to pay off his veterans with land expropriated from towns in northern Italy, supposedly including, according to the tradition, an estate near Mantua belonging to Virgil. The loss of his family farm and the attempt through poetic petitions to regain his property have traditionally been seen as Virgil's motives in the composition of the Eclogues. This is now thought to be an unsupported inference from interpretations of the Eclogues. In Eclogues 1 and 9, Virgil indeed dramatizes the contrasting feelings caused by the brutality of the land expropriations through pastoral idiom, but offers no indisputable evidence of the supposed biographic incident. While some readers have identified the poet himself with various characters and their vicissitudes, whether gratitude by an old rustic to a new god (Ecl. 1), frustrated love by a rustic singer for a distant boy (his master's pet, Ecl. 2), or a master singer's claim to have composed several eclogues (Ecl. 5), modern scholars largely reject such efforts to garner biographical details from works of fiction, preferring to interpret an author's characters and themes as illustrations of contemporary life and thought. The ten Eclogues present traditional pastoral themes with a fresh perspective. Eclogues 1 and 9 address the land confiscations and their effects on the Italian countryside. 2 and 3 are pastoral and erotic, discussing both homosexual love (Ecl. 2) and attraction toward people of any gender (Ecl. 3). Eclogue 4, addressed to Asinius Pollio, the so-called ""Messianic Eclogue"" uses the imagery of the golden age in connection with the birth of a child (who the child was meant to be has been subject to debate). 5 and 8 describe the myth of Daphnis in a song contest, 6, the cosmic and mythological song of Silenus; 7, a heated poetic contest, and 10 the sufferings of the contemporary elegiac poet Cornelius Gallus. Virgil is credited[by whom?] in the Eclogues with establishing Arcadia as a poetic ideal that still resonates in Western literature and visual arts and setting the stage for the development of Latin pastoral by Calpurnius Siculus, Nemesianus, and later writers."
Classical_music,"Many of the instruments used to perform medieval music still exist, but in different forms. Medieval instruments included the wood flute (which in the 21st century is made of metal), the recorder and plucked string instruments like the lute. As well, early versions of the organ, fiddle (or vielle), and trombone (called the sackbut) existed. Medieval instruments in Europe had most commonly been used singly, often self accompanied with a drone note, or occasionally in parts. From at least as early as the 13th century through the 15th century there was a division of instruments into haut (loud, shrill, outdoor instruments) and bas (quieter, more intimate instruments). Given the wide range of styles in classical music, from Medieval plainchant sung by monks to Classical and Romantic symphonies for orchestra from the 1700s and 1800s to avant-garde atonal compositions for solo piano from the 1900s, it is difficult to list characteristics that can be attributed to all works of that type. However, there are characteristics that classical music contains that few or no other genres of music contain, such as the use of a printed score and the performance of very complex instrumental works (e.g., the fugue). As well, although the symphony did not exist through the entire classical music period, from the mid-1700s to the 2000s the symphony ensemble—and the works written for it—have become a defining feature of classical music. The Classical era, from about 1750 to 1820, established many of the norms of composition, presentation, and style, and was also when the piano became the predominant keyboard instrument. The basic forces required for an orchestra became somewhat standardized (although they would grow as the potential of a wider array of instruments was developed in the following centuries). Chamber music grew to include ensembles with as many as 8 to 10 performers for serenades. Opera continued to develop, with regional styles in Italy, France, and German-speaking lands. The opera buffa, a form of comic opera, rose in popularity. The symphony came into its own as a musical form, and the concerto was developed as a vehicle for displays of virtuoso playing skill. Orchestras no longer required a harpsichord (which had been part of the traditional continuo in the Baroque style), and were often led by the lead violinist (now called the concertmaster). The dates are generalizations, since the periods and eras overlap and the categories are somewhat arbitrary, to the point that some authorities reverse terminologies and refer to a common practice ""era"" comprising baroque, classical, and romantic ""periods"". For example, the use of counterpoint and fugue, which is considered characteristic of the Baroque era (or period), was continued by Haydn, who is classified as typical of the Classical era. Beethoven, who is often described as a founder of the Romantic era, and Brahms, who is classified as Romantic, also used counterpoint and fugue, but other characteristics of their music define their era. Works of classical repertoire often exhibit complexity in their use of orchestration, counterpoint, harmony, musical development, rhythm, phrasing, texture, and form. Whereas most popular styles are usually written in song forms, classical music is noted for its development of highly sophisticated musical forms, like the concerto, symphony, sonata, and opera. Wind instruments became more refined in the Classical era. While double reeded instruments like the oboe and bassoon became somewhat standardized in the Baroque, the clarinet family of single reeds was not widely used until Mozart expanded its role in orchestral, chamber, and concerto settings. The ""standard complement"" of double winds and brass in the orchestra from the first half of the 19th century is generally attributed to Beethoven. The exceptions to this are his Symphony No. 4, Violin Concerto, and Piano Concerto No. 4, which each specify a single flute. The composer's instrumentation usually included paired flutes, oboes, clarinets, bassoons, horns and trumpets. Beethoven carefully calculated the expansion of this particular timbral ""palette"" in Symphonies 3, 5, 6, and 9 for an innovative effect. The third horn in the ""Eroica"" Symphony arrives to provide not only some harmonic flexibility, but also the effect of ""choral"" brass in the Trio. Piccolo, contrabassoon, and trombones add to the triumphal finale of his Symphony No. 5. A piccolo and a pair of trombones help deliver ""storm"" and ""sunshine"" in the Sixth. The Ninth asks for a second pair of horns, for reasons similar to the ""Eroica"" (four horns has since become standard); Beethoven's use of piccolo, contrabassoon, trombones, and untuned percussion—plus chorus and vocal soloists—in his finale, are his earliest suggestion that the timbral boundaries of symphony should be expanded. For several decades after he died, symphonic instrumentation was faithful to Beethoven's well-established model, with few exceptions. European cultural ideas and institutions began to follow colonial expansion into other parts of the world. There was also a rise, especially toward the end of the era, of nationalism in music (echoing, in some cases, political sentiments of the time), as composers such as Edvard Grieg, Nikolai Rimsky-Korsakov, and Antonín Dvořák echoed traditional music of their homelands in their compositions. Saxophones appear in some scores from the late 19th century onwards. While appearing only as featured solo instruments in some works, for example Maurice Ravel's orchestration of Modest Mussorgsky's Pictures at an Exhibition and Sergei Rachmaninoff's Symphonic Dances, the saxophone is included in other works, such as Ravel's Boléro, Sergei Prokofiev's Romeo and Juliet Suites 1 and 2 and many other works as a member of the orchestral ensemble. The euphonium is featured in a few late Romantic and 20th-century works, usually playing parts marked ""tenor tuba"", including Gustav Holst's The Planets, and Richard Strauss's Ein Heldenleben. The theories surrounding equal temperament began to be put in wider practice, especially as it enabled a wider range of chromatic possibilities in hard-to-tune keyboard instruments. Although Bach did not use equal temperament, as a modern piano is generally tuned, changes in the temperaments from the meantone system, common at the time, to various temperaments that made modulation between all keys musically acceptable, made possible Bach's Well-Tempered Clavier. Almost all of the composers who are described in music textbooks on classical music and whose works are widely performed as part of the standard concert repertoire are male composers, even though there has been a large number of women composers throughout the classical music period. Musicologist Marcia Citron has asked ""[w]hy is music composed by women so marginal to the standard 'classical' repertoire?"" Citron ""examines the practices and attitudes that have led to the exclusion of women composers from the received 'canon' of performed musical works."" She argues that in the 1800s, women composers typically wrote art songs for performance in small recitals rather than symphonies intended for performance with an orchestra in a large hall, with the latter works being seen as the most important genre for composers; since women composers did not write many symphonies, they were deemed to be not notable as composers. In the ""...Concise Oxford History of Music, Clara Shumann  [sic] is one of the only  [sic] female composers mentioned."" Abbey Philips states that ""[d]uring the 20th century the women who were composing/playing gained far less attention than their male counterparts."" Many instruments originated during the Renaissance; others were variations of, or improvements upon, instruments that had existed previously. Some have survived to the present day; others have disappeared, only to be recreated in order to perform music of the period on authentic instruments. As in the modern day, instruments may be classified as brass, strings, percussion, and woodwind. During the earlier medieval period, the vocal music from the liturgical genre, predominantly Gregorian chant, was monophonic, using a single, unaccompanied vocal melody line. Polyphonic vocal genres, which used multiple independent vocal melodies, began to develop during the high medieval era, becoming prevalent by the later 13th and early 14th century. The prefix neo is used to describe a 20th-century or contemporary composition written in the style of an earlier era, such as Classical or Romantic. Stravinsky's Pulcinella, for example, is a neoclassical composition because it is stylistically similar to works of the Classical era. In the Romantic era, the modern piano, with a more powerful, sustained tone and a wider range took over from the more delicate-sounding fortepiano. In the orchestra, the existing Classical instruments and sections were retained (string section, woodwinds, brass and percussion), but these sections were typically expanded to make a fuller, bigger sound. For example, while a Baroque orchestra may have had two double bass players, a Romantic orchestra could have as many as ten. ""As music grew more expressive, the standard orchestral palette just wasn't rich enough for many Romantic composers.""  New woodwind instruments were added, such as the contrabassoon, bass clarinet and piccolo and new percussion instruments were added, including xylophones, drums, celestes (a bell-like keyboard instrument), large orchestral harps, bells, and triangles  and even wind machines for sound effects. Baroque music is characterized by the use of complex tonal counterpoint and the use of a basso continuo, a continuous bass line. Music became more complex in comparison with the songs of earlier periods. The beginnings of the sonata form took shape in the canzona, as did a more formalized notion of theme and variations. The tonalities of major and minor as means for managing dissonance and chromaticism in music took full shape. Classical music is art music produced or rooted in the traditions of Western music, including both liturgical (religious) and secular music. While a similar term is also used to refer to the period from 1750 to 1820 (the Classical period), this article is about the broad span of time from roughly the 11th century to the present day, which includes the Classical period and various other periods. The central norms of this tradition became codified between 1550 and 1900, which is known as the common practice period. The major time divisions of classical music are as follows: the early music period, which includes the Medieval (500–1400) and the Renaissance (1400–1600) eras; the Common practice period, which includes the Baroque (1600–1750), Classical (1750–1820), and Romantic eras (1804–1910); and the 20th century (1901–2000) which includes the modern (1890–1930) that overlaps from the late 19th-century, the high modern (mid 20th-century), and contemporary or postmodern (1975–2015) eras.[citation needed] The instruments currently used in most classical music were largely invented before the mid-19th century (often much earlier) and codified in the 18th and 19th centuries. They consist of the instruments found in an orchestra or in a concert band, together with several other solo instruments (such as the piano, harpsichord, and organ). The symphony orchestra is the most widely known medium for classical music and includes members of the string, woodwind, brass, and percussion families of instruments. The concert band consists of members of the woodwind, brass, and percussion families. It generally has a larger variety and amount of woodwind and brass instruments than the orchestra but does not have a string section. However, many concert bands use a double bass. The vocal practices changed a great deal over the classical period, from the single line monophonic Gregorian chant done by monks in the Medieval period to the complex, polyphonic choral works of the Renaissance and subsequent periods, which used multiple independent vocal melodies at the same time. Shawn Vancour argues that the commercialization of classical music in the early 20th century served to harm the music industry through inadequate representation. The Classical era stringed instruments were the four instruments which form the string section of the orchestra: the violin, viola, cello and contrabass. Woodwinds included the basset clarinet, basset horn, clarinette d'amour, the Classical clarinet, the chalumeau, the flute, oboe and bassoon. Keyboard instruments included the clavichord and the fortepiano. While the harpsichord was still used in basso continuo accompaniment in the 1750s and 1760s, it fell out of use in the end of the century. Brass instruments included the buccin, the ophicleide (a serpent replacement which was the precursor of tuba) and the natural horn. Electric instruments such as the electric guitar, the electric bass and the ondes Martenot appear occasionally in the classical music of the 20th and 21st centuries. Both classical and popular musicians have experimented in recent decades with electronic instruments such as the synthesizer, electric and digital techniques such as the use of sampled or computer-generated sounds, and instruments from other cultures such as the gamelan. Some critics express the opinion that it is only from the mid-19th century, and especially in the 20th century, that the score began to hold such a high significance. Previously, improvisation (in preludes, cadenzas and ornaments), rhythmic flexibility (e.g., tempo rubato), improvisatory deviation from the score and oral tradition of playing was integral to the style. Yet in the 20th century, this oral tradition and passing on of stylistic features within classical music disappeared. Instead, musicians tend to use just the score to play music. Yet, even with the score providing the key elements of the music, there is considerable controversy about how to perform the works. Some of this controversy relates to the fact that this score-centric approach has led to performances that emphasize metrically strict block-rhythms (just as the music is notated in the score). Some ""popular"" genre musicians have had significant classical training, such as Billy Joel, Elton John, the Van Halen brothers, Randy Rhoads and Ritchie Blackmore. Moreover, formal training is not unique to the classical genre. Many rock and pop musicians have completed degrees in commercial music programs such as those offered by the Berklee College of Music and many jazz musicians have completed degrees in music from universities with jazz programs, such as the Manhattan School of Music and McGill University. While equal temperament became gradually accepted as the dominant musical temperament during the 18th century, different historical temperaments are often used for music from earlier periods. For instance, music of the English Renaissance is often performed in meantone temperament. Performers who have studied classical music extensively are said to be ""classically trained"". This training may be from private lessons from instrument or voice teachers or from completion of a formal program offered by a Conservatory, college or university, such as a B.mus. or M.mus. degree (which includes individual lessons from professors). In classical music, ""...extensive formal music education and training, often to postgraduate [Master's degree] level"" is required. In the 19th century, musical institutions emerged from the control of wealthy patrons, as composers and musicians could construct lives independent of the nobility. Increasing interest in music by the growing middle classes throughout western Europe spurred the creation of organizations for the teaching, performance, and preservation of music. The piano, which achieved its modern construction in this era (in part due to industrial advances in metallurgy) became widely popular with the middle class, whose demands for the instrument spurred a large number of piano builders. Many symphony orchestras date their founding to this era. Some musicians and composers were the stars of the day; some, like Franz Liszt and Niccolò Paganini, fulfilled both roles. The primacy of the composer's written score has also led, today, to a relatively minor role played by improvisation in classical music, in sharp contrast to the practice of musicians who lived during the baroque, classical and romantic era. Improvisation in classical music performance was common during both the Baroque and early romantic eras, yet lessened strongly during the second half of the 19th and in the 20th centuries. During the classical era, Mozart and Beethoven often improvised the cadenzas to their piano concertos (and thereby encouraged others to do so), but they also provided written cadenzas for use by other soloists. In opera, the practice of singing strictly by the score, i.e. come scritto, was famously propagated by soprano Maria Callas, who called this practice 'straitjacketing' and implied that it allows the intention of the composer to be understood better, especially during studying the music for the first time. The family of instruments used, especially in orchestras, grew. A wider array of percussion instruments began to appear. Brass instruments took on larger roles, as the introduction of rotary valves made it possible for them to play a wider range of notes. The size of the orchestra (typically around 40 in the Classical era) grew to be over 100. Gustav Mahler's 1906 Symphony No. 8, for example, has been performed with over 150 instrumentalists and choirs of over 400. Classical music has often incorporated elements or material from popular music of the composer's time. Examples include occasional music such as Brahms' use of student drinking songs in his Academic Festival Overture, genres exemplified by Kurt Weill's The Threepenny Opera, and the influence of jazz on early- and mid-20th-century composers including Maurice Ravel, exemplified by the movement entitled ""Blues"" in his sonata for violin and piano. Certain postmodern, minimalist and postminimalist classical composers acknowledge a debt to popular music. Performance of classical music repertoire requires a proficiency in sight-reading and ensemble playing, harmonic principles, strong ear training (to correct and adjust pitches by ear), knowledge of performance practice (e.g., Baroque ornamentation), and a familiarity with the style/musical idiom expected for a given composer or musical work (e.g., a Brahms symphony or a Mozart concerto). Burgh (2006), suggests that the roots of Western classical music ultimately lie in ancient Egyptian art music via cheironomy and the ancient Egyptian orchestra, which dates to 2695 BC. This was followed by early Christian liturgical music, which itself dates back to the Ancient Greeks[citation needed]. The development of individual tones and scales was made by ancient Greeks such as Aristoxenus and Pythagoras. Pythagoras created a tuning system and helped to codify musical notation. Ancient Greek instruments such as the aulos (a reed instrument) and the lyre (a stringed instrument similar to a small harp) eventually led to the modern-day instruments of a classical orchestra. The antecedent to the early period was the era of ancient music before the fall of the Roman Empire (476 AD). Very little music survives from this time, most of it from ancient Greece. Numerous examples show influence in the opposite direction, including popular songs based on classical music, the use to which Pachelbel's Canon has been put since the 1970s, and the musical crossover phenomenon, where classical musicians have achieved success in the popular music arena. In heavy metal, a number of lead guitarists (playing electric guitar) modeled their playing styles on Baroque or Classical era instrumental music, including Ritchie Blackmore and Randy Rhoads. The term ""classical music"" did not appear until the early 19th century, in an attempt to distinctly canonize the period from Johann Sebastian Bach to Beethoven as a golden age. The earliest reference to ""classical music"" recorded by the Oxford English Dictionary is from about 1836. Longer works are often divided into self-contained pieces, called movements, often with contrasting characters or moods. For instance, symphonies written during the Classical period are usually divided into four movements: (1) an opening Allegro in sonata form, (2) a slow movement, (3) a minuet or scherzo, and (4) a final Allegro. These movements can then be further broken down into a hierarchy of smaller units: first sections, then periods, and finally phrases. The Wagner tuba, a modified member of the horn family, appears in Richard Wagner's cycle Der Ring des Nibelungen and several other works by Strauss, Béla Bartók, and others; it has a prominent role in Anton Bruckner's Symphony No. 7 in E Major. Cornets appear in Pyotr Ilyich Tchaikovsky's ballet Swan Lake, Claude Debussy's La Mer, and several orchestral works by Hector Berlioz. Unless these instruments are played by members doubling on another instrument (for example, a trombone player changing to euphonium for a certain passage), orchestras will use freelance musicians to augment their regular rosters. The Renaissance era was from 1400 to 1600. It was characterized by greater use of instrumentation, multiple interweaving melodic lines, and the use of the first bass instruments. Social dancing became more widespread, so musical forms appropriate to accompanying dance began to standardize. During the Baroque era, keyboard music played on the harpsichord and pipe organ became increasingly popular, and the violin family of stringed instruments took the form generally seen today. Opera as a staged musical drama began to differentiate itself from earlier musical and dramatic forms, and vocal forms like the cantata and oratorio became more common. Vocalists began adding embellishments to melodies. Instrumental ensembles began to distinguish and standardize by size, giving rise to the early orchestra for larger ensembles, with chamber music being written for smaller groups of instruments where parts are played by individual (instead of massed) instruments. The concerto as a vehicle for solo performance accompanied by an orchestra became widespread, although the relationship between soloist and orchestra was relatively simple. Several works from the Golden Age of Animation matched the action to classical music. Notable examples are Walt Disney's Fantasia, Tom and Jerry's Johann Mouse, and Warner Bros.' Rabbit of Seville and What's Opera, Doc?. Classical musicians continued to use many of instruments from the Baroque era, such as the cello, contrabass, recorder, trombone, timpani, fortepiano and organ. While some Baroque instruments fell into disuse (e.g., the theorbo and rackett), many Baroque instruments were changed into the versions that are still in use today, such as the Baroque violin (which became the violin), the Baroque oboe (which became the oboe) and the Baroque trumpet, which transitioned to the regular valved trumpet. Composers of classical music have often made use of folk music (music created by musicians who are commonly not classically trained, often from a purely oral tradition). Some composers, like Dvořák and Smetana, have used folk themes to impart a nationalist flavor to their work, while others like Bartók have used specific themes lifted whole from their folk-music origins. In 2013, an article in Mother Jones stated that while ""[m]any prestigious orchestras have significant female membership—women outnumber men in the New York Philharmonic's violin section—and several renowned ensembles, including the National Symphony Orchestra, the Detroit Symphony, and the Minnesota Symphony, are led by women violinists"", the double bass, brass, and percussion sections of major orchestras ""...are still predominantly male."" A 2014 BBC article stated that the ""...introduction of 'blind' auditions, where a prospective instrumentalist performs behind a screen so that the judging panel can exercise no gender or racial prejudice, has seen the gender balance of traditionally male-dominated symphony orchestras gradually shift."" Improvisation once played an important role in classical music. A remnant of this improvisatory tradition in classical music can be heard in the cadenza, a passage found mostly in concertos and solo works, designed to allow skilled performers to exhibit their virtuoso skills on the instrument. Traditionally this was improvised by the performer; however, it is often written for (or occasionally by) the performer beforehand. Improvisation is also an important aspect in authentic performances of operas of Baroque era and of bel canto (especially operas of Vincenzo Bellini), and is best exemplified by the da capo aria, a form by which famous singers typically perform variations of the thematic matter of the aria in the recapitulation section ('B section' / the 'da capo' part). An example is Beverly Sills' complex, albeit pre-written, variation of Da tempeste il legno infranto from Händel's Giulio Cesare. Although Classical music in the 2000s has lost most of its tradition for musical improvisation, from the Baroque era to the Romantic era, there are examples of performers who could improvise in the style of their era. In the Baroque era, organ performers would improvise preludes, keyboard performers playing harpsichord would improvise chords from the figured bass symbols beneath the bass notes of the basso continuo part and both vocal and instrumental performers would improvise musical ornaments. J.S. Bach was particularly noted for his complex improvisations. During the Classical era, the composer-performer Mozart was noted for his ability to improvise melodies in different styles. During the Classical era, some virtuoso soloists would improvise the cadenza sections of a concerto. During the Romantic era, Beethoven would improvise at the piano. For more information, see Improvisation. During the 1990s, several research papers and popular books wrote on what came to be called the ""Mozart effect"": an observed temporary, small elevation of scores on certain tests as a result of listening to Mozart's works. The approach has been popularized in a book by Don Campbell, and is based on an experiment published in Nature suggesting that listening to Mozart temporarily boosted students' IQ by 8 to 9 points. This popularized version of the theory was expressed succinctly by the New York Times music columnist Alex Ross: ""researchers... have determined that listening to Mozart actually makes you smarter."" Promoters marketed CDs claimed to induce the effect. Florida passed a law requiring toddlers in state-run schools to listen to classical music every day, and in 1998 the governor of Georgia budgeted $105,000 per year to provide every child born in Georgia with a tape or CD of classical music. One of the co-authors of the original studies of the Mozart effect commented ""I don't think it can hurt. I'm all for exposing children to wonderful cultural experiences. But I do think the money could be better spent on music education programs."" It is in this time that the notation of music on a staff and other elements of musical notation began to take shape. This invention made possible the separation of the composition of a piece of music from its transmission; without written music, transmission was oral, and subject to change every time it was transmitted. With a musical score, a work of music could be performed without the composer's presence. The invention of the movable-type printing press in the 15th century had far-reaching consequences on the preservation and transmission of music. Its written transmission, along with the veneration bestowed on certain classical works, has led to the expectation that performers will play a work in a way that realizes in detail the original intentions of the composer. During the 19th century the details that composers put in their scores generally increased. Yet the opposite trend—admiration of performers for new ""interpretations"" of the composer's work—can be seen, and it is not unknown for a composer to praise a performer for achieving a better realization of the original intent than the composer was able to imagine. Thus, classical performers often achieve high reputations for their musicianship, even if they do not compose themselves. Generally however, it is the composers who are remembered more than the performers. The common practice period is when many of the ideas that make up western classical music took shape, standardized, or were codified. It began with the Baroque era, running from roughly 1600 to the middle of the 18th century. The Classical era followed, ending roughly around 1820. The Romantic era ran through the 19th century, ending about 1910. European art music is largely distinguished from many other non-European and popular musical forms by its system of staff notation, in use since about the 16th century. Western staff notation is used by composers to prescribe to the performer the pitches (e.g., melodies, basslines and/or chords), tempo, meter and rhythms for a piece of music. This leaves less room for practices such as improvisation and ad libitum ornamentation, which are frequently heard in non-European art music and in popular music  styles such as jazz and blues. Another difference is that whereas most popular styles lend themselves to the song form, classical music has been noted for its development of highly sophisticated forms of instrumental music such as the concerto, symphony, sonata, and mixed vocal and instrumental styles such as opera which, since they are written down, can attain a high level of complexity. In 1997, the Vienna Philharmonic was ""facing protests during a [US] tour"" by the National Organization for Women and the International Alliance for Women in Music. Finally, ""after being held up to increasing ridicule even in socially conservative Austria, members of the orchestra gathered [on 28 February 1997] in an extraordinary meeting on the eve of their departure and agreed to admit a woman, Anna Lelkes, as harpist."" As of 2013, the orchestra has six female members; one of them, violinist Albena Danailova became one of the orchestra's concertmasters in 2008, the first woman to hold that position. In 2012, women still made up just 6% of the orchestra's membership. VPO president Clemens Hellsberg said the VPO now uses completely screened blind auditions. Encompassing a wide variety of post-Romantic styles composed through the year 2000, 20th century classical music includes late romantic, modern, high-modern, and postmodern styles of composition. Modernism (1890–1930) marked an era when many composers rejected certain values of the common practice period, such as traditional tonality, melody, instrumentation, and structure. The high-modern era saw the emergence of neo-classical and serial music. A few authorities have claimed high-modernism as the beginning of postmodern music from about 1930. Others have more or less equated postmodern music with the ""contemporary music"" composed from the late 20th century through to the early 21st century. The key characteristic of classical music that distinguishes it from popular music and folk music is that the repertoire tends to be written down in musical notation, creating a musical part or score. This score typically determines details of rhythm, pitch, and, where two or more musicians (whether singers or instrumentalists) are involved, how the various parts are coordinated. The written quality of the music has enabled a high level of complexity within them: J.S. Bach's fugues, for instance, achieve a remarkable marriage of boldly distinctive melodic lines weaving in counterpoint yet creating a coherent harmonic logic that would be impossible in the heat of live improvisation. The use of written notation also preserves a record of the works and enables Classical musicians to perform music from many centuries ago. Musical notation enables 2000s-era performers to sing a choral work from the 1300s Renaissance era or a 1700s Baroque concerto with many of the features of the music (the melodies, lyrics, forms, and rhythms) being reproduced. The term ""classical music"" has two meanings: the broader meaning includes all Western art music from the Medieval era to today, and the specific meaning refers to the music from the 1750s to the early 1830s—the era of Mozart and Haydn. This section is about the more specific meaning. Typical stringed instruments of the early period include the harp, lute, vielle, and psaltery, while wind instruments included the flute family (including recorder), shawm (an early member of the oboe family), trumpet, and the bagpipes. Simple pipe organs existed, but were largely confined to churches, although there were portable varieties. Later in the period, early versions of keyboard instruments like the clavichord and harpsichord began to appear. Stringed instruments such as the viol had emerged by the 16th century, as had a wider variety of brass and reed instruments. Printing enabled the standardization of descriptions and specifications of instruments, as well as instruction in their use. In 1996–1997, a research study was conducted on a large population of middle age students in the Cherry Creek School District in Denver, Colorado, USA. The study showed that students who actively listen to classical music before studying had higher academic scores. The research further indicated that students who listened to the music prior to an examination also had positively elevated achievement scores. Students who listened to rock-and-roll or country had moderately lower scores. The study further indicated that students who used classical during the course of study had a significant leap in their academic performance; whereas, those who listened to other types of music had significantly lowered academic scores. The research was conducted over several schools within the Cherry Creek School District and was conducted through University of Colorado. This study is reflective of several recent studies (i.e. Mike Manthei and Steve N. Kelly of the University of Nebraska at Omaha; Donald A. Hodges and Debra S. O'Connell of the University of North Carolina at Greensboro; etc.) and others who had significant results through the discourse of their work. Similarly, movies and television often revert to standard, clichéd snatches of classical music to convey refinement or opulence: some of the most-often heard pieces in this category include Bach´s Cello Suite No. 1, Mozart's Eine kleine Nachtmusik, Vivaldi's Four Seasons, Mussorgsky's Night on Bald Mountain (as orchestrated by Rimsky-Korsakov), and Rossini's William Tell Overture. That said, the score does not provide complete and exact instructions on how to perform a historical work. Even if the tempo is written with an Italian instruction (e.g., Allegro), we do not know exactly how fast the piece should be played. As well, in the Baroque era, many works that were designed for basso continuo accompaniment do not specify which instruments should play the accompaniment or exactly how the chordal instrument (harpsichord, lute, etc.) should play the chords, which are not notated in the part (only a figured bass symbol beneath the bass part is used to guide the chord-playing performer). The performer and/or the conductor have a range of options for musical expression and interpretation of a scored piece, including the phrasing of melodies, the time taken during fermatas (held notes) or pauses, and the use (or choice not to use) of effects such as vibrato or glissando (these effects are possible on various stringed, brass and woodwind instruments and with the human voice). Many instruments today associated with popular music filled important roles in early classical music, such as bagpipes, vihuelas, hurdy-gurdies, and some woodwind instruments. On the other hand, instruments such as the acoustic guitar, once associated mainly with popular music, gained prominence in classical music in the 19th and 20th centuries. The major time divisions of classical music up to 1900 are the early music period, which includes Medieval (500–1400) and Renaissance (1400–1600) eras, and the Common practice period, which includes the Baroque (1600–1750), Classical (1750–1830) and Romantic (1804–1910) eras. Since 1900, classical periods have been reckoned more by calendar century than by particular stylistic movements that have become fragmented and difficult to define. The 20th century calendar period (1901–2000) includes most of the early modern musical era (1890–1930), the entire high modern (mid 20th-century), and the first 25 years of the contemporary or postmodern musical era (1975–current). The 21st century has so far been characterized by a continuation of the contemporary/postmodern musical era. Brass instruments in the Renaissance were traditionally played by professionals who were members of Guilds and they included the slide trumpet, the wooden cornet, the valveless trumpet and the sackbut. Stringed instruments included the viol, the harp-like lyre, the hurdy-gurdy, the cittern and the lute. Keyboard instruments with strings included the harpsichord and the virginal. Percussion instruments include the triangle, the Jew's harp, the tambourine, the bells, the rumble-pot, and various kinds of drums. Woodwind instruments included the double reed shawm, the reed pipe, the bagpipe, the transverse flute and the recorder. The written score, however, does not usually contain explicit instructions as to how to interpret the piece in terms of production or performance, apart from directions for dynamics, tempo and expression (to a certain extent). This is left to the discretion of the performers, who are guided by their personal experience and musical education, their knowledge of the work's idiom, their personal artistic tastes, and the accumulated body of historic performance practices. Baroque instruments included some instruments from the earlier periods (e.g., the hurdy-gurdy and recorder) and a number of new instruments (e.g, the cello, contrabass and fortepiano). Some instruments from previous eras fell into disuse, such as the shawm and the wooden cornet. The key Baroque instruments for strings included the violin, viol, viola, viola d'amore, cello, contrabass, lute, theorbo (which often played the basso continuo parts), mandolin, cittern, Baroque guitar, harp and hurdy-gurdy. Woodwinds included the Baroque flute, Baroque oboe, rackett, recorder and the bassoon. Brass instruments included the cornett, natural horn, Baroque trumpet, serpent and the trombone. Keyboard instruments included the clavichord, tangent piano, the fortepiano (an early version of the piano), the harpsichord and the pipe organ. Percussion instruments included the timpani, snare drum, tambourine and the castanets. Historically, major professional orchestras have been mostly or entirely composed of male musicians. Some of the earliest cases of women being hired in professional orchestras was in the position of harpist. The Vienna Philharmonic, for example, did not accept women to permanent membership until 1997, far later than the other orchestras ranked among the world's top five by Gramophone in 2008. The last major orchestra to appoint a woman to a permanent position was the Berlin Philharmonic. As late as February 1996, the Vienna Philharmonic's principal flute, Dieter Flury, told Westdeutscher Rundfunk that accepting women would be ""gambling with the emotional unity (emotionelle Geschlossenheit) that this organism currently has"". In April 1996, the orchestra's press secretary wrote that ""compensating for the expected leaves of absence"" of maternity leave would be a problem. The modernist views hold that classical music is considered primarily a written musical tradition, preserved in music notation, as opposed to being transmitted orally, by rote, or by recordings of particular performances.[citation needed] While there are differences between particular performances of a classical work, a piece of classical music is generally held to transcend any interpretation of it. The use of musical notation is an effective method for transmitting classical music, since the written music contains the technical instructions for performing the work. The music of the Romantic era, from roughly the first decade of the 19th century to the early 20th century, was characterized by increased attention to an extended melodic line, as well as expressive and emotional elements, paralleling romanticism in other art forms. Musical forms began to break from the Classical era forms (even as those were being codified), with free-form pieces like nocturnes, fantasias, and preludes being written where accepted ideas about the exposition and development of themes were ignored or minimized. The music became more chromatic, dissonant, and tonally colorful, with tensions (with respect to accepted norms of the older forms) about key signatures increasing. The art song (or Lied) came to maturity in this era, as did the epic scales of grand opera, ultimately transcended by Richard Wagner's Ring cycle. One major difference between Baroque music and the classical era that followed it is that the types of instruments used in ensembles were much less standardized. Whereas a classical era string quartet consists almost exclusively of two violins, a viola and a cello, a Baroque group accompanying a soloist or opera could include one of several different types of keyboard instruments (e.g., pipe organ, harpsichord, or clavichord), additional stringed chordal instruments (e.g., a lute) and an unspecified number of bass instruments performing the basso continuo bassline, including bowed strings, woodwinds and brass instruments (e.g., a cello, contrabass, viol, bassoon, serpent, etc.). Certain staples of classical music are often used commercially (either in advertising or in movie soundtracks). In television commercials, several passages have become clichéd, particularly the opening of Richard Strauss' Also sprach Zarathustra (made famous in the film 2001: A Space Odyssey) and the opening section ""O Fortuna"" of Carl Orff's Carmina Burana, often used in the horror genre; other examples include the Dies Irae from the Verdi Requiem, Edvard Grieg's In the Hall of the Mountain King from Peer Gynt, the opening bars of Beethoven's Symphony No. 5, Wagner's Ride of the Valkyries from Die Walküre, Rimsky-Korsakov's Flight of the Bumblebee, and excerpts of Aaron Copland's Rodeo. The Medieval period includes music from after the fall of Rome to about 1400. Monophonic chant, also called plainsong or Gregorian chant, was the dominant form until about 1100. Polyphonic (multi-voiced) music developed from monophonic chant throughout the late Middle Ages and into the Renaissance, including the more complex voicings of motets. Vocal music in the Renaissance is noted for the flourishing of an increasingly elaborate polyphonic style. The principal liturgical forms which endured throughout the entire Renaissance period were masses and motets, with some other developments towards the end, especially as composers of sacred music began to adopt secular forms (such as the madrigal) for their own designs. Towards the end of the period, the early dramatic precursors of opera such as monody, the madrigal comedy, and the intermedio are seen."
Arena_Football_League,"The league's 2016 schedule, announced on the league's website on December 10, 2015, shows an eight-team league playing a 16-game regular season over 18 weeks, with two bye weeks for each team, one on a rotational basis and the other a ""universal bye"" for all teams during the Independence Day weekend, the first weekend in July. All teams will qualify for the postseason, meaning that the regular season will serve only to establish seeding. Although the Drive moved to Massachusetts for the 1994 season, the AFL had a number of other teams which it considered ""dynasties"", including the Tampa Bay Storm (the only team that has existed in some form for all twenty-eight contested seasons), their arch-rival the Orlando Predators, the now-defunct San Jose SaberCats of the present decade, and their rivals the Arizona Rattlers. Following the suspension of the AFL's 2009 season, league officials and owners of af2 (which had played its season as scheduled) began discussing the future of arena football and the two leagues. With its 50.1 percent ownership of af2, the AFL's bankruptcy and dissolution prompted the dissolution of af2 as well. That league was formally considered disbanded on September 8, 2009, when no owner committed his or her team to the league's eleventh season by that deadline. For legal reasons, af2 league officials and owners agreed to form a new legal entity, Arena Football 1 (AF1), with former AFL teams the Arizona Rattlers and Orlando Predators joining the former af2. On November 12, the league announced the defending champion San Jose SaberCats would be ceasing operations due to ""reasons unrelated to League operations"". A statement from the league indicated that the AFL is working to secure new, long-term owners for the franchise. This leaves the AFL with eight teams for 2016. On July 20, 2009, Sports Business Journal reported that the AFL owed approximately $14 million to its creditors and were considering filing for Chapter 11 bankruptcy protection. In early August 2009, numerous media outlets began reporting that the AFL was folding permanently and would file for Chapter 7 bankruptcy. The league released a statement on August 4 to the effect that while the league was not folding, it was suspending league operations indefinitely. Despite this, several of the league's creditors filed papers to force a Chapter 7 liquidation if the league did not do so voluntarily. This request was granted on August 7, though converted to a Chapter 11 reorganization on August 26. Jim Foster, a promotions manager with the National Football League, conceived of indoor football while watching an indoor soccer match at Madison Square Garden in 1981. While at the game, he wrote his idea on a 9x12 envelope, with sketches of the field and notes on gameplay. He presented the idea to a few friends at the NFL offices, where he received praise and encouragement for his concept. After solidifying the rules and a business plan, and supplemented with sketches by a professional artist, Foster presented his idea to various television networks. He reached an agreement with NBC for a ""test game"". The NFL Network ceased airing Arena Football League games partway through the 2012 season as a result of ongoing labor problems within the league. Briefly, the games were broadcast on a tape delay to prevent the embarrassment that would result should the players stage a work stoppage immediately prior to a scheduled broadcast. (In at least once incidence this actually happened, resulting in a non-competitive game being played with replacement players, and further such incidents were threatened.) Once the labor issues were resolved, the NFL Network resumed the practice of broadcasting a live Friday night game. NFL Network dropped the league at the end of the 2012 season. The AFL currently runs as under the single-entity model, with the league owning the rights to the teams, players, and coaches. The single-entity model was adopted in 2010 when the league emerged from bankruptcy. Prior to that, the league followed the franchise model more common in North American professional sports leagues; each team essentially operated as its own business and the league itself was a separate entity which in exchange for franchise fees paid by the team owners provided rules, officials, scheduling and the other elements of organizational structure. A pool of money is allotted to teams to aid in travel costs. For the 2011 season, the Philadelphia Soul, Kansas City Brigade, San Jose SaberCats, New Orleans VooDoo, and the Georgia Force returned to the AFL after having last played in 2008. However, the Grand Rapids Rampage, Colorado Crush, Columbus Destroyers, Los Angeles Avengers, and the New York Dragons did not return. The league added one expansion team, the Pittsburgh Power. Former Pittsburgh Steelers wide receiver Lynn Swann was one of the team's owners. It was the first time the AFL returned to Pittsburgh since the Pittsburgh Gladiators were an original franchise in 1987 before becoming the Tampa Bay Storm. The Brigade changed its name to the Command, becoming the Kansas City Command. Even though they were returning teams, the Bossier–Shreveport Battle Wings moved to New Orleans as the Voodoo, the identity formerly owned by New Orleans Saints owner Tom Benson. The Alabama Vipers moved to Duluth, Georgia to become the new Georgia Force (the earlier franchise of that name being a continuation of the first Nashville Kats franchise). On October 25, 2010 the Oklahoma City Yard Dawgz did not return. The Milwaukee Iron also changed names to the Milwaukee Mustangs, the name of Milwaukee's original AFL team that had existed from 1994 to 2001. In 2010, the first year of the reconstituted league following bankruptcy, the overall attendance average decreased to 8,135, with only one team (Tampa Bay) exceeding 13,000 per game. On Saturday, July 23, 1989, much of America learned of the AFL for an unintended reason, when the Pittsburgh Gladiators' head coach, Joe Haering, made football history by punching commissioner Jim Foster during a game with the Chicago Bruisers. The national media ran with the story, including a photo in USA Today. The game was played between the two teams in Sacramento's Arco Arena, as part of the AFL's 'Barnstorming America' tour. Foster had walked onto the field of play to mediate an altercation between the two teams when Haering, a former NFL assistant, punched him in the jaw. Haering was suspended without pay. The Arena Football League (AFL) is the highest level of professional indoor American football in the United States. It was founded in 1987 by Jim Foster, making it the third longest-running professional football league in North America, after the Canadian Football League and the National Football League. It is played indoors on a 68-yard field (about half the distance of an NFL field), resulting in a faster-paced and higher-scoring game. The sport was invented in the early 1980s and patented by Foster, a former executive of the United States Football League and the National Football League. The year 2000 brought heightened interest in the AFL. Then-St. Louis Rams quarterback Kurt Warner, who was MVP of Super Bowl XXXIV, was first noticed because he played quarterback for the AFL's Iowa Barnstormers. While many sports commentators and fans continued to ridicule the league, Warner's story gave the league positive exposure, and it brought the league a new television deal with TNN, which, unlike ESPN, televised regular season games live. While it was not financially lucrative, it helped set the stage for what the league would become in the new millennium. Also, the year also brought a spin-off league, the AF2, intended to be a developmental league, comparable to the National Football League's NFL Europe. There was a lot of expansion in the 2000s. Expansion teams included the Austin Wranglers, Carolina Cobras, Los Angeles Avengers, Chicago Rush, Detroit Fury, Dallas Desperados, Colorado Crush, New Orleans VooDoo, Philadelphia Soul, Nashville Kats, Kansas City Brigade, New York Dragons and Utah Blaze. Some of these teams, including the Crush, Desperados, Kats, and VooDoo, were owned by the same group which owned the NFL teams in their host cities. The NFL purchased, but never exercised, an option to buy a major interest the AFL. Of all of these teams, only the Soul still compete in the AFL as of now. In 2012, the AFL celebrated its silver anniversary for its 25th season of operations. The season kicked off on March 9, 2012. The Tulsa Talons moved to San Antonio, Texas and Jeffrey Vinik became owner of the Tampa Bay Storm. The Dallas Vigilantes were left off the schedule for the 2012 season with no announcement from the management, raising speculations that either the team had suspended operations for the season or was ceasing operations altogether. (Apparently the latter was the case as the organization did not field a team for the 2013 season or any subsequent one either.) Like the National Football League, the AFL postponed the free agency period to October 31 due to Hurricane Sandy. In October 2008, Tom Benson announced that the New Orleans VooDoo were ceasing operations and folding ""based on circumstances currently affecting the league and the team"". Shortly thereafter, an article in Sports Business Journal announced that the AFL had a tentative agreement to sell a $100 million stake in the league to Platinum Equity; in exchange, Platinum Equity would create a centralized, single-entity business model that would streamline league and team operations and allow the league to be more profitable. Benson's move to shut down the VooDoo came during the Platinum Equity conference call, leading to speculation that he had folded because of the deal. The 2016 regular season consists of an 18-week schedule during which each team plays 16 games and two bye weeks. Each team plays two or three games against the teams within its own conference, and two games (home/road) against each team interconference-wise. The 2015 season started during the last week of March and ran weekly into late August. At the end of the regular season, all teams from each conference (the conference winner and three wild card teams) play in the AFL playoffs, an eight-team single-elimination tournament that culminates with the championship game, known as the ArenaBowl. From 1987 to 2004, 2010 and 2011 and again starting in 2014, the game was played at the site of the higher seeded team. From 2005 to 2008, the games were at neutral sites, Las Vegas and New Orleans. In 2012, the league championship returned to a neutral site and ArenaBowl XXV was held at the New Orleans Arena; ArenaBowl XXVI was held in Orlando. The 2016 season will begin April 1, 2016. In 2014, the league announced the granting of a new franchise to former Mötley Crüe frontman Vince Neil, previously part-owner of the Jacksonville Sharks. That franchise, the Las Vegas Outlaws, were originally to play in Las Vegas at the MGM Grand Garden Arena in 2015, but instead played their home games at the Thomas & Mack Center, previous home to the Las Vegas Sting and Las Vegas Gladiators. After 20 years as a familiar name to the league, an AFL mainstay, the Iowa Barnstormers, departed the league to join the Indoor Football League. The San Antonio Talons folded on October 13, 2014, after the league (which owned the team) failed to find a new owner. On November 16, 2014, despite a successful season record-wise, the Pittsburgh Power became the second team to cease operations after the 2014 season. This resulted from poor attendance. It was later announced by the league that the Power would go dormant for 2015 and were looking for new ownership. On February 17, 2010, AF1 announced it would use the ""Arena Football League"" name. The league announced plans for the upcoming season and details of its contract with NFL Network to broadcast AFL games in 2010. AF1 teams were given the option of restoring historical names to their teams. In addition to the historical teams, the league added two new expansion franchises, the Dallas Vigilantes and the Jacksonville Sharks. Beginning with the 2003 season, the AFL made a deal with NBC to televise league games, which was renewed for another two years in 2005. In conjunction with this, the league moved the beginning of the season from May to February (the week after the NFL's Super Bowl) and scheduled most of its games on Sunday instead of Friday or Saturday as it had in the past. In 2006, because of the XX Winter Olympic Games, the Stanley Cup playoffs and the Daytona 500, NBC scaled back from weekly coverage to scattered coverage during the regular season, but committed to a full playoff schedule ending with the 20th ArenaBowl. NBC and the Arena Football League officially severed ties on June 30, 2006, having failed to reach a new broadcast deal. Las Vegas owner Jim Ferraro stated during a radio interview that the reason why a deal failed is because ESPN refused to show highlights or even mention a product being broadcast on NBC. The first game in Arena Football League history was played on June 19, 1987, between the Gladiators and Commandos at Pittsburgh Civic Arena in front of 12,117 fans. The game was deliberately not televised so that it could be analyzed and any follies and failures would not be subject to national public scrutiny. Following the inaugural game, tweaks and adjustments were made, and the first season continued. The Dynamite and Bruisers played in the first-ever televised AFL game the next night, on June 20, 1987, at the Rosemont Horizon in suburban Chicago on ESPN with Bob Rathbun and Lee Corso calling the play. The broadcast showed a short clip of the Commandos-Gladiators game. Each team played six games, two against each other team. The top two teams, Denver and Pittsburgh, then competed in the first-ever AFL championship game, ArenaBowl I. Plans for arena football were put on hold in 1982 as the United States Football League was launched. Foster left the NFL to accept a position in the USFL. He eventually became executive vice-president with the Chicago Blitz, where he returned to his concept of arena football. In 1983, he began organizing the test game in his spare time from his job with the Blitz. By 1985, the USFL had ceased football operations and he began devoting all his time to arena football, and on April 27, 1986, his concept was realized when the test game was played. In 1993, the league staged its first All-Star Game in Des Moines, Iowa, the future home of the long-running Iowa Barnstormers, as a fundraiser for flood victims in the area. The National Conference defeated the American Conference 64–40 in front of a crowd of 7,189. The second Allstar game was in Oct. 2013, with two games, the first in Honolulu, Hawai'i, the second being in Beijing, China. Average attendance for AFL games were around 10,000–11,000 per game in the 1990s, though during the recession connected to the dot-com bubble and the September 11, 2001 attacks average attendance dropped below 10,000 for several years. From the start of the 2004 season until the final season of the original league in 2008, average attendance was above 12,000, with 12,392 in 2007. Eleven of the seventeen teams in operation in 2007 had average attendance figures over 13,000. In 2008, the overall attendance average increased to 12,957, with eight teams exceeding 13,000 per game. From 2000 to 2009, the AFL had its own developmental league, the af2. The AFL played 22 seasons from 1987 to 2008; internal issues caused the league to cancel its 2009 season, though the af2 did play. Later that year both the AFL and af2 were dissolved and reorganized as a new corporation comprising teams from both leagues, and the AFL returned in 2010. The Arena Football League has its headquarters in Chicago, Illinois. Rumors began swirling with regards to bringing the AFL back to Austin and San Antonio, Texas. Both cities have hosted franchises in the past (Austin Wranglers, San Antonio Force and San Antonio Talons), but an AFL spokesman, BJ Pickard, was quoted as saying, ""News to me."" Announcements have yet to be made on any sort of expansion plans. An expected ""big announcement"" on Friday, October 30 at a San Antonio Spurs game never came to fruition. It was announced on December 12, 2012, that the AFL reached a partnership agreement with NET10 Wireless to be the first non-motorsports-related professional sports league in the United States to have a title sponsor, renaming it the NET10 Wireless Arena Football League. The redesigned website showed the new logo which incorporated the current AFL logo with the one from NET10 Wireless. The title sponsorship agreement ended in 2014 after a two-year partnership. Following the successes of his trial-run games, Foster moved ahead with his idea for arena football. He founded the Arena Football League with four teams: the Pittsburgh Gladiators, Denver Dynamite, Washington Commandos, and Chicago Bruisers. Foster appointed legendary Darrel ""Mouse"" Davis, godfather of the ""run and shoot"" and modern pro offenses, as executive director of football operations. Davis hired the original coaches and was the architect of the league's original wide-open offensive playbooks. For the 2013 season, the league's new national broadcast partner was the CBS Sports Network. CBSSN would air 19 regular season games and two playoff games. CBS would also air the ArenaBowl, marking the first time since 2008 that the league's finale aired on network television. Regular season CBSSN broadcast games are usually on Saturday nights. As the games are being shown live, the start times are not uniform as with most football broadcast packages, but vary with the time zone in which the home team is located. This means that the AFL may appear either prior to or following the CBSSN's featured Major League Lacrosse game. AFL Global and Ganlan Media were created in 2012 by businessman Martin E. Judge, founder and owner of the Judge Group. The company, called AFL Global, LLC, looks to introduce and launch professional Arena Football teams and franchises in various locations throughout the world (like NFL Europe). After their successful trip to China to help promote the game, they formally announced plans to further develop AFL China by the fall of 2014 by starting a comprehensive training program in May 2013 with exhibition games planned for the cities of Beijing and Guangzhou in October. This is the first time professional football of any kind will be played in China with the support of the Chinese government and the CRFA (Chinese Rugby Football Association). Key persons involved include founder and CEO. Martin E. Judge, partner Ron Jaworski, CAFL CEO Gary Morris and president David Niu. Ganlan Media has since dropped this name and will carry the league's name as its corporate identity. In 2001, Jeff Foley published War on the Floor: An Average Guy Plays in the Arena Football League and Lives to Write About It. The book details a journalist's two preseasons (1999 and 2000) as an offensive specialist/writer with the now-defunct Albany Firebirds. The 5-foot-6 (170 cm), self-described ""unathletic writer"" played in three preseason games and had one catch for −2 yards. One of the league's early success stories was the Detroit Drive. A primary team for some of the AFL's most highly regarded players, including George LaFrance and Gary and Alvin Rettig, as well as being a second career chance for quarterback Art Schlichter, the Drive regularly played before sold out crowds at Joe Louis Arena, and went to the ArenaBowl every year of their existence (1988–1993). The AFL's first dynasty came to an end when their owner, Mike Ilitch (who also owned Little Caesars Pizza and the Detroit Red Wings) bought the Detroit Tigers baseball team and sold the AFL team. For its 2015 season, the league consisted of 12 teams, all from the United States; however, upon the completion of the regular season, the league announced that the two teams it had assumed operation of during the season would cease all operations effective immediately; a regular season game slated between the two had previously been canceled and declared a tie. Subsequently, one of the remaining teams, the Spokane Shock, severed its ties with the league to join the competing IFL. The AFL is divided into two conferences  –  the American Conference and National Conference. Starting 2016, each conference will have only four teams as the champion San Jose SaberCats announced in November 2015 that they were ceasing activity for ""reasons not associated with League operations."" On September 30, 1987, Foster filed an application with the United States Patent and Trademark Office to patent his invented sport. The patent application covered the rules of the game, specifically detailing the goalposts and rebound netting and their impact on gameplay. Foster's application was granted on March 27, 1990. The patent expired on September 30, 2007. High hopes for the AFL waned when interim commissioner Ed Policy announced his resignation, citing the obsolescence of his position in the reformatted league. Two weeks later, the Los Angeles Avengers announced that they were formally folding the franchise. One month later, the league missed the deadline to formally ratify the new collective bargaining agreement and announced that it was eliminating health insurance for the players. Progress on the return stalled, and no announcements were made regarding the future of the league. From the league's inception through ArenaBowl XVIII, the championship game was played at the home of the highest-seeded remaining team. The AFL then switched to a neutral-site championship, with ArenaBowls XIX and XX in Las Vegas. New Orleans Arena, home of the New Orleans VooDoo, served as the site of ArenaBowl XXI on July 29, 2007. This was the first professional sports championship to be staged in the city since Hurricane Katrina struck in August 2005. The San Jose SaberCats earned their third championship in six years by defeating the Columbus Destroyers 55–33. ArenaBowl XXI in New Orleans was deemed a success, and the city was chosen to host ArenaBowl XXII, in which the Philadelphia Soul defeated the defending champion San Jose Sabercats. In 2010, the location returned to being decided by which of the two participating teams was seeded higher. In ArenaBowl XXIII, the Spokane Shock defeated the Tampa Bay Storm at their home arena, Spokane Arena, in Spokane, Washington. In ArenaBowl XXIV, the Jacksonville Sharks, coming off of a victory in their conference final game four nights earlier, traveled to US Airways Center in Phoenix and defeated the Arizona Rattlers 73–70. ArenaBowl XXV returned to a neutral site and was once again played in New Orleans, where the Rattlers returned and defeated the Philadelphia Soul. Since 2014 the ArenaBowl has been played at the higher-seeded team. Because of the sudden loss of the New Orleans franchise, the league announced in October that the beginning of the free agency period would be delayed in order to accommodate a dispersal draft. Dates were eventually announced as December 2 for the dispersal draft and December 4 for free agency, but shortly before the draft the league issued a press release announcing the draft had been postponed one day to December 3. Shortly thereafter, another press release announced that the draft would be held on December 9 and free agency would commence on December 11. However, the draft still never took place, and instead another press release was issued stating that both the draft and free agency had been postponed indefinitely. Rumors began circulating that the league was in trouble and on the verge of folding, but owners denied those claims. It was soon revealed the players' union had agreed to cut the salary cap for the 2009 season to prevent a total cessation of operations. However, the announced Platinum Equity investment never materialized. The 2014 scheduled beginning proved to be too ambitious for the group; its official website now cites an anticipated beginning of professional play in 2016 and shows photos from a six-team collegiate tournament staged in early November, 2015 On December 19, 2006, ESPN announced the purchase of a minority stake in the AFL. This deal included television rights for the ESPN family of networks. ESPN would televise a minimum of 17 regular season games, most on Monday nights, and nine playoff games, including ArenaBowl XXI on ABC. The deal resulted in added exposure on ESPN's SportsCenter. However, after the original AFL filed for bankruptcy, this arrangement did not carry over to the new AFL, which is a separate legal entity. Starting in 2014, ESPN returned to the AFL as broadcast partners, with weekly games being shown on CBS Sports Network, ESPN, ESPN2, ESPNEWS along with all games being broadcast on ESPN3 for free live on WatchESPN. ArenaBowl XXVII was also broadcast on ESPN. Most teams also have a local TV station broadcast their games locally and all games are available on local radio. The AFL also had a regional-cable deal with FSN, where FSN regional affiliates in AFL markets carried local team games. In some areas, such as with the Arizona Rattlers, Fox Sports affiliates still carry the games. The test game was played in Rockford, Illinois, at the Rockford MetroCentre. Sponsors were secured, and players and coaches from local colleges were recruited to volunteer to play for the teams, the Chicago Politicians and Rockford Metros, with the guarantee of a tryout should the league take off. Interest was high enough following the initial test game that Foster decided to put on a second, ""showcase"", game. The second game was held on February 26, 1987 at the Rosemont Horizon in Chicago with a budget of $20,000, up from $4,000 in the test game. Foster also invited ESPN to send a film crew to the game; a highlights package aired on SportsCenter. In 2013, the league expanded with the addition of two new franchises to play in 2014, the Los Angeles Kiss (owned by Gene Simmons and Paul Stanley of the legendary rock band Kiss) and the Portland Thunder. Jerry Kurz also stepped down as commissioner of the AFL as he was promoted to be the AFL's first president. Former Foxwoods CEO Scott Butera was hired as his successor as commissioner. The first video game based on the AFL was Arena Football for the C-64 released in 1988. On May 18, 2000, Kurt Warner's Arena Football Unleashed was released by Midway Games for the PlayStation game console. On February 7, 2006 EA Sports released Arena Football for the PlayStation 2 and Xbox. EA Sports released another AFL video game, titled Arena Football: Road to Glory, on February 21, 2007, for the PlayStation 2. On January 6, 2016, the league took over ""ownership and operational control"" of the Portland Thunder from its previous owners. The AFL stated this move was made after months of trying work out an arrangement ""to provide financial and operational support."" On February 3, 2016, it was announced that the franchise will start from scratch and no longer be called the ""Thunder"" as the name and trademarks belong to former franchise owner Terry Emmert (similar to the Jerry Jones move with the Desperados). AFL commissioner Scott Butera announced that a new identity will be announced at a later date. From the 1987 season until the late 1990s, the most exposure the league would receive was on ESPN, which aired tape-delayed games, often well after midnight, and often edited to match the alloted time slot. The league received its first taste of wide exposure in 1998, when Arena Bowl XII was televised nationally as part of ABC's old Wide World of Sports. The practice of playing one or two preseason exhibition games by each team before the start of the regular season was discontinued when the NBC contract was initiated, and the regular season was extended from 14 games, the length that it had been since 1996, to 16 from 2001 to 2010, and since 2016. From 2011 to 2015, the regular season league expanded to 18 games, with each team having two bye weeks and the option of two preseason games. In August 2012, the AFL announced a new project into China, known as the China American Football League. The CAFL project is headed up by ESPN NFL analyst and Philadelphia Soul majority owner president Ron Jaworski. The plans were to establish a six-team league that would play a 10-week schedule that was slated to start in October 2014. The AFL coaches and trainers will travel to China to help teach the rules of the sport to squads made up of Chinese and American players with the goal of starting an official Chinese arena league. Ganlan Media International were given exclusive rights to the new Chinese league. On August 9, 2015, ESPN reported that the New Orleans VooDoo and Las Vegas Outlaws had ceased operations, effective immediately, a claim which was subsequently validated on the AFL website. On September 1, 2015, the Spokane Shock officially left the AFL and joined the IFL under the new name Spokane Empire, becoming the fifth active AFL/af2 franchise to leave for the IFL since bankruptcy (Iowa Barnstormers, Tri-Cities Fever, Green Bay Blizzard and Arkansas Twisters—now the Texas Revolution—left previously). While some teams have enjoyed considerable on-field and even financial success, many teams in the history of the league have enjoyed little success either on or off of the field of play. There are a number of franchises which existed in the form of a number of largely-unrelated teams under numerous management groups until they folded (an example is the New York CityHawks whose owners transferred the team from New York to Hartford to become the New England Sea Wolves after two seasons, then after another two seasons were sold and became the Toronto Phantoms, who lasted another two seasons until folding). There are a number of reasons why these teams failed, including financially weak ownership groups, lack of deep financial support from some owners otherwise capable of providing it, lack of media exposure, and the host city's evident lack of interest in its team or the sport as a whole. Although the Arenafootball2 league played its tenth season in 2009, a conference call in December 2008 resulted in enough votes from owners and cooperation from the AFLPA for the AFL to suspend the entire 2009 season in order to create ""a long-term plan to improve its economic model"". In doing so, the AFL became the second sports league to cancel an entire season, after the National Hockey League cancelled the 2004-05 season because of a lockout. The AFL also became the third sports league to lose its postseason (the first being Major League Baseball, which lost its postseason in 1994 because of a strike). Efforts to reformat the league's business model were placed under the leadership of Columbus Destroyers owner Jim Renacci and interim commissioner Policy. After 12 years as commissioner of the AFL, David Baker retired unexpectedly on July 25, 2008, just two days before ArenaBowl XXII; deputy commissioner Ed Policy was named interim commissioner until Baker's replacement was found. Baker explained, ""When I took over as commissioner, I thought it would be for one year. It turned into 12. But now it's time."" After its return in 2010, the AFL had its national television deal with the NFL Network for a weekly Friday night game. All AFL games not on the NFL Network could be seen for free online, provided by Ustream. All assets of the Arena Football League were put up for auction. On November 11, 2009, the new league announced its intention to purchase the entire assets of the former AFL; the assets included the team names and logos of all but one of the former AFL and af2 teams. The lone exception was that of the Dallas Desperados; Desperados owner Jerry Jones had purposely designed the Desperados' properties around those of the Dallas Cowboys, making the two inseparable. The auction occurred on November 25, 2009. The assets were awarded to Arena Football 1 on December 7, 2009, with a winning bid of $6.1 million. In 2003, the season expanded to 16 games. There were also several rule changes in this period. In 2005, players were no longer allowed to run out of bounds. The only way for a player to go out of bounds presently is if he is tackled into or deliberately contacts the side boards. This was also the first year the ArenaBowl was played at a neutral site. In 2007, free substitution was allowed, ending the ""iron man"" era of one-platoon football. And in 2008, the ""jack"" linebacker was allowed to go sideboard to sideboard without being penalized for ""illegal defense""."
Circadian_rhythm,"What drove circadian rhythms to evolve has been an enigmatic question. Previous hypotheses emphasized that photosensitive proteins and circadian rhythms may have originated together in the earliest cells, with the purpose of protecting replicating DNA from high levels of damaging ultraviolet radiation during the daytime. As a result, replication was relegated to the dark. However, evidence for this is lacking, since the simplest organisms with a circadian rhythm, the cyanobacteria, do the opposite of this - they divide more in the daytime. Recent studies instead highlight the importance of co-evolution of redox proteins with circadian oscillators in all three kingdoms of life following the Great Oxidation Event approximately 2.3 billion years ago. The current view is that circadian changes in environmental oxygen levels and the production of reactive oxygen species (ROS) in the presence of daylight are likely to have driven a need to evolve circadian rhythms to preempt, and therefore counteract, damaging redox reactions on a daily basis. More-or-less independent circadian rhythms are found in many organs and cells in the body outside the suprachiasmatic nuclei (SCN), the ""master clock"". These clocks, called peripheral oscillators, are found in the adrenal gland,[citation needed] oesophagus, lungs, liver, pancreas, spleen, thymus, and skin.[citation needed] Though oscillators in the skin respond to light, a systemic influence has not been proven. There is also some evidence that the olfactory bulb and prostate may experience oscillations when cultured, suggesting that these structures may also be weak oscillators.[citation needed] Due to the work nature of airline pilots, who often cross several timezones and regions of sunlight and darkness in one day, and spend many hours awake both day and night, they are often unable to maintain sleep patterns that correspond to the natural human circadian rhythm; this situation can easily lead to fatigue. The NTSB cites this as contributing to many accidents[unreliable medical source?]  and has conducted several research studies in order to find methods of combating fatigue in pilots. Mutations or deletions of clock gene in mice have demonstrated the importance of body clocks to ensure the proper timing of cellular/metabolic events; clock-mutant mice are hyperphagic and obese, and have altered glucose metabolism. In mice, deletion of the Rev-ErbA alpha clock gene facilitates diet-induced obesity and changes the balance between glucose and lipid utilization predisposing to diabetes. However, it is not clear whether there is a strong association between clock gene polymorphisms in humans and the susceptibility to develop the metabolic syndrome. The rhythm is linked to the light–dark cycle. Animals, including humans, kept in total darkness for extended periods eventually function with a free-running rhythm. Their sleep cycle is pushed back or forward each ""day"", depending on whether their ""day"", their endogenous period, is shorter or longer than 24 hours. The environmental cues that reset the rhythms each day are called zeitgebers (from the German, ""time-givers""). Totally blind subterranean mammals (e.g., blind mole rat Spalax sp.) are able to maintain their endogenous clocks in the apparent absence of external stimuli. Although they lack image-forming eyes, their photoreceptors (which detect light) are still functional; they do surface periodically as well.[page needed] Early research into circadian rhythms suggested that most people preferred a day closer to 25 hours when isolated from external stimuli like daylight and timekeeping. However, this research was faulty because it failed to shield the participants from artificial light. Although subjects were shielded from time cues (like clocks) and daylight, the researchers were not aware of the phase-delaying effects of indoor electric lights.[dubious – discuss] The subjects were allowed to turn on light when they were awake and to turn it off when they wanted to sleep. Electric light in the evening delayed their circadian phase.[citation needed] A more stringent study conducted in 1999 by Harvard University estimated the natural human rhythm to be closer to 24 hours, 11 minutes: much closer to the solar day but still not perfectly in sync. For temperature studies, subjects must remain awake but calm and semi-reclined in near darkness while their rectal temperatures are taken continuously. Though variation is great among normal chronotypes, the average human adult's temperature reaches its minimum at about 05:00 (5 a.m.), about two hours before habitual wake time. Baehr et al. found that, in young adults, the daily body temperature minimum occurred at about 04:00 (4 a.m.) for morning types but at about 06:00 (6 a.m.) for evening types. This minimum occurred at approximately the middle of the eight hour sleep period for morning types, but closer to waking in evening types. A defect in the human homologue of the Drosophila ""period"" gene was identified as a cause of the sleep disorder FASPS (Familial advanced sleep phase syndrome), underscoring the conserved nature of the molecular circadian clock through evolution. Many more genetic components of the biological clock are now known. Their interactions result in an interlocked feedback loop of gene products resulting in periodic fluctuations that the cells of the body interpret as a specific time of the day.[citation needed] Shift-work or chronic jet-lag have profound consequences on circadian and metabolic events in the body. Animals that are forced to eat during their resting period show increased body mass and altered expression of clock and metabolic genes.[medical citation needed] In humans, shift-work that favors irregular eating times is associated with altered insulin sensitivity and higher body mass. Shift-work also leads to increased metabolic risks for cardio-metabolic syndrome, hypertension, inflammation. It is now known that the molecular circadian clock can function within a single cell; i.e., it is cell-autonomous. This was shown by Gene Block in isolated mollusk BRNs.[clarification needed] At the same time, different cells may communicate with each other resulting in a synchronised output of electrical signaling. These may interface with endocrine glands of the brain to result in periodic release of hormones. The receptors for these hormones may be located far across the body and synchronise the peripheral clocks of various organs. Thus, the information of the time of the day as relayed by the eyes travels to the clock in the brain, and, through that, clocks in the rest of the body may be synchronised. This is how the timing of, for example, sleep/wake, body temperature, thirst, and appetite are coordinately controlled by the biological clock.[citation needed] Plant circadian rhythms tell the plant what season it is and when to flower for the best chance of attracting pollinators. Behaviors showing rhythms include leaf movement, growth, germination, stomatal/gas exchange, enzyme activity, photosynthetic activity, and fragrance emission, among others. Circadian rhythms occur as a plant entrains to synchronize with the light cycle of its surrounding environment. These rhythms are endogenously generated and self-sustaining and are relatively constant over a range of ambient temperatures. Important features include two interacting transcription-translation feedback loops: proteins containing PAS domains, which facilitate protein-protein interactions; and several photoreceptors that fine-tune the clock to different light conditions. Anticipation of changes in the environment allows appropriate changes in a plant's physiological state, conferring an adaptive advantage. A better understanding of plant circadian rhythms has applications in agriculture, such as helping farmers stagger crop harvests to extend crop availability and securing against massive losses due to weather. The primary circadian ""clock"" in mammals is located in the suprachiasmatic nucleus (or nuclei) (SCN), a pair of distinct groups of cells located in the hypothalamus. Destruction of the SCN results in the complete absence of a regular sleep–wake rhythm. The SCN receives information about illumination through the eyes. The retina of the eye contains ""classical"" photoreceptors (""rods"" and ""cones""), which are used for conventional vision. But the retina also contains specialized ganglion cells that are directly photosensitive, and project directly to the SCN, where they help in the entrainment (synchronization) of this master circadian clock. The simplest known circadian clock is that of the prokaryotic cyanobacteria. Recent research has demonstrated that the circadian clock of Synechococcus elongatus can be reconstituted in vitro with just the three proteins (KaiA, KaiB, KaiC) of their central oscillator. This clock has been shown to sustain a 22-hour rhythm over several days upon the addition of ATP. Previous explanations of the prokaryotic circadian timekeeper were dependent upon a DNA transcription/translation feedback mechanism.[citation needed] The earliest recorded account of a circadian process dates from the 4th century B.C.E., when Androsthenes, a ship captain serving under Alexander the Great, described diurnal leaf movements of the tamarind tree. The observation of a circadian or diurnal process in humans is mentioned in Chinese medical texts dated to around the 13th century, including the Noon and Midnight Manual and the Mnemonic Rhyme to Aid in the Selection of Acu-points According to the Diurnal Cycle, the Day of the Month and the Season of the Year. Light is the signal by which plants synchronize their internal clocks to their environment and is sensed by a wide variety of photoreceptors. Red and blue light are absorbed through several phytochromes and cryptochromes. One phytochrome, phyA, is the main phytochrome in seedlings grown in the dark but rapidly degrades in light to produce Cry1. Phytochromes B–E are more stable with phyB, the main phytochrome in seedlings grown in the light. The cryptochrome (cry) gene is also a light-sensitive component of the circadian clock and is thought to be involved both as a photoreceptor and as part of the clock's endogenous pacemaker mechanism. Cryptochromes 1–2 (involved in blue–UVA) help to maintain the period length in the clock through a whole range of light conditions. The central oscillator generates a self-sustaining rhythm and is driven by two interacting feedback loops that are active at different times of day. The morning loop consists of CCA1 (Circadian and Clock-Associated 1) and LHY (Late Elongated Hypocotyl), which encode closely related MYB transcription factors that regulate circadian rhythms in Arabidopsis, as well as PRR 7 and 9 (Pseudo-Response Regulators.) The evening loop consists of GI (Gigantea) and ELF4, both involved in regulation of flowering time genes. When CCA1 and LHY are overexpressed (under constant light or dark conditions), plants become arrhythmic, and mRNA signals reduce, contributing to a negative feedback loop. Gene expression of CCA1 and LHY oscillates and peaks in the early morning, whereas TOC1 gene expression oscillates and peaks in the early evening. While it was previously hypothesised that these three genes model a negative feedback loop in which over-expressed CCA1 and LHY repress TOC1 and over-expressed TOC1 is a positive regulator of CCA1 and LHY, it was shown in 2012 by Andrew Millar and others that TOC1 in fact serves as a repressor not only of CCA1, LHY, and PRR7 and 9 in the morning loop but also of GI and ELF4 in the evening loop. This finding and further computational modeling of TOC1 gene functions and interactions suggest a reframing of the plant circadian clock as a triple negative-component repressilator model rather than the positive/negative-element feedback loop characterizing the clock in mammals. In 1896, Patrick and Gilbert observed that during a prolonged period of sleep deprivation, sleepiness increases and decreases with a period of approximately 24 hours. In 1918, J.S. Szymanski showed that animals are capable of maintaining 24-hour activity patterns in the absence of external cues such as light and changes in temperature. In the early 20th century, circadian rhythms were noticed in the rhythmic feeding times of bees. Extensive experiments were done by Auguste Forel, Ingeborg Beling, and Oskar Wahl to see whether this rhythm was due to an endogenous clock.[citation needed] Ron Konopka and Seymour Benzer isolated the first clock mutant in Drosophila in the early 1970s and mapped the ""period"" gene, the first discovered genetic determinant of behavioral rhythmicity. Joseph Takahashi discovered the first mammalian circadian clock mutation (clockΔ19) using mice in 1994. However, recent studies show that deletion of clock does not lead to a behavioral phenotype (the animals still have normal circadian rhythms), which questions its importance in rhythm generation. Studies by Nathaniel Kleitman in 1938 and by Derk-Jan Dijk and Charles Czeisler in the 1990s put human subjects on enforced 28-hour sleep–wake cycles, in constant dim light and with other time cues suppressed, for over a month. Because normal people cannot entrain to a 28-hour day in dim light if at all,[citation needed] this is referred to as a forced desynchrony protocol. Sleep and wake episodes are uncoupled from the endogenous circadian period of about 24.18 hours and researchers are allowed to assess the effects of circadian phase on aspects of sleep and wakefulness including sleep latency and other functions.[page needed] Norwegian researchers at the University of Tromsø have shown that some Arctic animals (ptarmigan, reindeer) show circadian rhythms only in the parts of the year that have daily sunrises and sunsets. In one study of reindeer, animals at 70 degrees North showed circadian rhythms in the autumn, winter and spring, but not in the summer. Reindeer on Svalbard at 78 degrees North showed such rhythms only in autumn and spring. The researchers suspect that other Arctic animals as well may not show circadian rhythms in the constant light of summer and the constant dark of winter. Studies conducted on both animals and humans show major bidirectional relationships between the circadian system and abusive drugs. It is indicated that these abusive drugs affect the central circadian pacemaker. Individuals suffering from substance abuse display disrupted rhythms. These disrupted rhythms can increase the risk for substance abuse and relapse. It is possible that genetic and/or environmental disturbances to the normal sleep and wake cycle can increase the susceptibility to addiction. Circadian rhythms allow organisms to anticipate and prepare for precise and regular environmental changes. They thus enable organisms to best capitalize on environmental resources (e.g. light and food) compared to those that cannot predict such availability. It has therefore been suggested that circadian rhythms put organisms at a selective advantage in evolutionary terms. However, rhythmicity appears to be as important in regulating and coordinating internal metabolic processes, as in coordinating with the environment. This is suggested by the maintenance (heritability) of circadian rhythms in fruit flies after several hundred generations in constant laboratory conditions, as well as in creatures in constant darkness in the wild, and by the experimental elimination of behavioral, but not physiological, circadian rhythms in quail. Melatonin is absent from the system or undetectably low during daytime. Its onset in dim light, dim-light melatonin onset (DLMO), at roughly 21:00 (9 p.m.) can be measured in the blood or the saliva. Its major metabolite can also be measured in morning urine. Both DLMO and the midpoint (in time) of the presence of the hormone in the blood or saliva have been used as circadian markers. However, newer research indicates that the melatonin offset may be the more reliable marker. Benloucif et al. found that melatonin phase markers were more stable and more highly correlated with the timing of sleep than the core temperature minimum. They found that both sleep offset and melatonin offset are more strongly correlated with phase markers than the onset of sleep. In addition, the declining phase of the melatonin levels is more reliable and stable than the termination of melatonin synthesis."
