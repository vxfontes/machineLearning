title,contextos
Westminster_Abbey,"The chapter house has an original mid-13th-century tiled pavement. A door within the vestibule dates from around 1050 and is believed to be the oldest in England.[citation needed] The exterior includes flying buttresses added in the 14th century and a leaded tent-lantern roof on an iron frame designed by Scott. The Chapter house was originally used in the 13th century by Benedictine monks for daily meetings. It later became a meeting place of the King's Great Council and the Commons, predecessors of Parliament. Westminster School and Westminster Abbey Choir School are also in the precincts of the abbey. It was natural for the learned and literate monks to be entrusted with education, and Benedictine monks were required by the Pope to maintain a charity school in 1179. The Choir School educates and trains the choirboys who sing for services in the Abbey. Since the coronations in 1066 of both King Harold and William the Conqueror, coronations of English and British monarchs were held in the abbey. In 1216, Henry III was unable to be crowned in London when he first came to the throne, because the French prince Louis had taken control of the city, and so the king was crowned in Gloucester Cathedral. This coronation was deemed by the Pope to be improper, and a further coronation was held in the abbey on 17 May 1220. The Archbishop of Canterbury is the traditional cleric in the coronation ceremony.[citation needed] Later wax effigies include a likeness of Horatio, Viscount Nelson, wearing some of his own clothes and another of Prime Minister William Pitt, Earl of Chatham, modelled by the American-born sculptor Patience Wright.[citation needed] During recent conservation of Elizabeth I's effigy, a unique corset dating from 1603 was found on the figure and is now displayed separately.[citation needed] The chapter house and Pyx Chamber at Westminster Abbey are in the guardianship of English Heritage, but under the care and management of the Dean and Chapter of Westminster. English Heritage have funded a major programme of work on the chapter house, comprising repairs to the roof, gutters, stonework on the elevations and flying buttresses as well as repairs to the lead light. Since 1066, when Harold Godwinson and William the Conqueror were crowned, the coronations of English and British monarchs have been held there. There have been at least 16 royal weddings at the abbey since 1100. Two were of reigning monarchs (Henry I and Richard II), although, before 1919, there had been none for some 500 years. In addition to the Dean and canons, there are at present two full-time minor canons, one is precentor, and the other is sacrist. The office of Priest Vicar was created in the 1970s for those who assist the minor canons. Together with the clergy and Receiver General and Chapter Clerk, various lay officers constitute the college, including the Organist and Master of the Choristers, the Registrar, the Auditor, the Legal Secretary, the Surveyor of the Fabric, the Head Master of the choir school, the Keeper of the Muniments and the Clerk of the Works, as well as 12 lay vicars, 10 choristers and the High Steward and High Bailiff. In June 2009 the first major building work at the abbey for 250 years was proposed. A corona—a crown-like architectural feature—was intended to be built around the lantern over the central crossing, replacing an existing pyramidal structure dating from the 1950s. This was part of a wider £23m development of the abbey expected to be completed in 2013. On 4 August 2010 the Dean and Chapter announced that, ""[a]fter a considerable amount of preliminary and exploratory work"", efforts toward the construction of a corona would not be continued. In 2012, architects Panter Hudspith completed refurbishment of the 14th-century food-store originally used by the abbey's monks, converting it into a restaurant with English Oak furniture by Covent Garden-based furniture makers Luke Hughes and Company. A project that is proceeding is the creation of The Queen's Diamond Jubilee Galleries in the medieval triforium of the abbey. The aim is to create a new display area for the abbey's treasures in the galleries high up around the abbey's nave. To this end a new Gothic access tower with lift has been designed by the abbey architect and Surveyor of the Fabric, Ptolemy Dean. It is planned that the new galleries will open in 2018. The abbey became the coronation site of Norman kings. None were buried there until Henry III, intensely devoted to the cult of the Confessor, rebuilt the abbey in Anglo-French Gothic style as a shrine to venerate King Edward the Confessor and as a suitably regal setting for Henry's own tomb, under the highest Gothic nave in England. The Confessor's shrine subsequently played a great part in his canonisation. The work continued between 1245 and 1517 and was largely finished by the architect Henry Yevele in the reign of Richard II. Henry III also commissioned unique Cosmati pavement in front of the High Altar (the pavement has recently undergone a major cleaning and conservation programme and was re-dedicated by the Dean at a service on 21 May 2010). Henry VII added a Perpendicular style chapel dedicated to the Blessed Virgin Mary in 1503 (known as the Henry VII Chapel or the ""Lady Chapel""). Much of the stone came from Caen, in France (Caen stone), the Isle of Portland (Portland stone) and the Loire Valley region of France (tuffeau limestone).[citation needed] Until the 19th century, Westminster was the third seat of learning in England, after Oxford and Cambridge. It was here that the first third of the King James Bible Old Testament and the last half of the New Testament were translated. The New English Bible was also put together here in the 20th century. Westminster suffered minor damage during the Blitz on 15 November 1940. On Saturday September 6, 1997 the formal, though not ""state"" Funeral of Diana, Princess of Wales, was held. It was a royal ceremonial funeral including royal pageantry and Anglican funeral liturgy. A Second Public service was held on Sunday at the demand of the people. The burial occurred privately later the same day. Diana's former husband, sons, mother, siblings, a close friend, and a clergyman were present. Diana's body was clothed in a black long-sleeved dress designed by Catherine Walker, which she had chosen some weeks before. A set of rosary beads was placed in her hands, a gift she had received from Mother Teresa. Her grave is on the grounds of her family estate, Althorp, on a private island.[citation needed] In the 1990s two icons by the Russian icon painter Sergei Fyodorov were hung in the abbey. On 6 September 1997 the funeral of Diana, Princess of Wales, was held at the Abbey. On 17 September 2010 Pope Benedict XVI became the first pope to set foot in the abbey. According to a tradition first reported by Sulcard in about 1080, a church was founded at the site (then known as Thorn Ey (Thorn Island)) in the 7th century, at the time of Mellitus, a Bishop of London. Construction of the present church began in 1245, on the orders of King Henry III. The organ was built by Harrison & Harrison in 1937, then with four manuals and 84 speaking stops, and was used for the first time at the coronation of King George VI. Some pipework from the previous Hill organ of 1848 was revoiced and incorporated in the new scheme. The two organ cases, designed in the late 19th century by John Loughborough Pearson, were re-instated and coloured in 1959. In 1982 and 1987, Harrison and Harrison enlarged the organ under the direction of the then abbey organist Simon Preston to include an additional Lower Choir Organ and a Bombarde Organ: the current instrument now has five manuals and 109 speaking stops. In 2006, the console of the organ was refurbished by Harrison and Harrison, and space was prepared for two additional 16 ft stops on the Lower Choir Organ and the Bombarde Organ. One part of the instrument, the Celestial Organ, is currently not connected or playable. The abbot and monks, in proximity to the royal Palace of Westminster, the seat of government from the later 12th century, became a powerful force in the centuries after the Norman Conquest. The abbot often was employed on royal service and in due course took his place in the House of Lords as of right. Released from the burdens of spiritual leadership, which passed to the reformed Cluniac movement after the mid-10th century, and occupied with the administration of great landed properties, some of which lay far from Westminster, ""the Benedictines achieved a remarkable degree of identification with the secular life of their times, and particularly with upper-class life"", Barbara Harvey concludes, to the extent that her depiction of daily life provides a wider view of the concerns of the English gentry in the High and Late Middle Ages.[citation needed] The exhibits include a collection of royal and other funeral effigies (funeral saddle, helm and shield of Henry V), together with other treasures, including some panels of mediaeval glass, 12th-century sculpture fragments, Mary II's coronation chair and replicas of the coronation regalia, and historic effigies of Edward III, Henry VII and his queen, Elizabeth of York, Charles II, William III, Mary II and Queen Anne. A narthex (a portico or entrance hall) for the west front was designed by Sir Edwin Lutyens in the mid-20th century but was not built. Images of the abbey prior to the construction of the towers are scarce, though the abbey's official website states that the building was without towers following Yevele's renovation, with just the lower segments beneath the roof level of the Nave completed. King Edward's Chair (or St Edward's Chair), the throne on which English and British sovereigns have been seated at the moment of coronation, is housed within the abbey and has been used at every coronation since 1308. From 1301 to 1996 (except for a short time in 1950 when it was temporarily stolen by Scottish nationalists), the chair also housed the Stone of Scone upon which the kings of Scots are crowned. Although the Stone is now kept in Scotland, in Edinburgh Castle, at future coronations it is intended that the Stone will be returned to St Edward's Chair for use during the coronation ceremony.[citation needed] Westminster Abbey is a collegiate church governed by the Dean and Chapter of Westminster, as established by Royal charter of Queen Elizabeth I in 1560, which created it as the Collegiate Church of St Peter Westminster and a Royal Peculiar under the personal jurisdiction of the Sovereign. The members of the Chapter are the Dean and four canons residentiary, assisted by the Receiver General and Chapter Clerk. One of the canons is also Rector of St Margaret's Church, Westminster, and often holds also the post of Chaplain to the Speaker of the House of Commons. It suffered damage during the turbulent 1640s, when it was attacked by Puritan iconoclasts, but was again protected by its close ties to the state during the Commonwealth period. Oliver Cromwell was given an elaborate funeral there in 1658, only to be disinterred in January 1661 and posthumously hanged from a gibbet at Tyburn. A recent addition to the exhibition is the late 13th-century Westminster Retable, England's oldest altarpiece, which was most probably designed for the high altar of the abbey. Although it has been damaged in past centuries, the panel has been expertly cleaned and conserved. The Pyx Chamber formed the undercroft of the monks' dormitory. It dates to the late 11th century and was used as a monastic and royal treasury. The outer walls and circular piers are of 11th-century date, several of the capitals were enriched in the 12th century and the stone altar added in the 13th century. The term pyx refers to the boxwood chest in which coins were held and presented to a jury during the Trial of the Pyx, in which newly minted coins were presented to ensure they conformed to the required standards. In addition there are two service bells, cast by Robert Mot, in 1585 and 1598 respectively, a Sanctus bell cast in 1738 by Richard Phelps and Thomas Lester and two unused bells—one cast about 1320, by the successor to R de Wymbish, and a second cast in 1742, by Thomas Lester. The two service bells and the 1320 bell, along with a fourth small silver ""dish bell"", kept in the refectory, have been noted as being of historical importance by the Church Buildings Council of the Church of England. The proximity of the Palace of Westminster did not extend to providing monks or abbots with high royal connections; in social origin the Benedictines of Westminster were as modest as most of the order. The abbot remained Lord of the Manor of Westminster as a town of two to three thousand persons grew around it: as a consumer and employer on a grand scale the monastery helped fuel the town economy, and relations with the town remained unusually cordial, but no enfranchising charter was issued during the Middle Ages. The abbey built shops and dwellings on the west side, encroaching upon the sanctuary.[citation needed] In 1535, the abbey's annual income of £2400–2800[citation needed] (£1,310,000 to £1,530,000 as of 2016), during the assessment attendant on the Dissolution of the Monasteries rendered it second in wealth only to Glastonbury Abbey. From the Middle Ages, aristocrats were buried inside chapels, while monks and other people associated with the abbey were buried in the cloisters and other areas. One of these was Geoffrey Chaucer, who was buried here as he had apartments in the abbey where he was employed as master of the King's Works. Other poets, writers and musicians were buried or memorialised around Chaucer in what became known as Poets' Corner. Abbey musicians such as Henry Purcell were also buried in their place of work.[citation needed] Henry III rebuilt the abbey in honour of a royal saint, Edward the Confessor, whose relics were placed in a shrine in the sanctuary. Henry III himself was interred nearby, as were many of the Plantagenet kings of England, their wives and other relatives. Until the death of George II of Great Britain in 1760, most kings and queens were buried in the abbey, some notable exceptions being Henry VI, Edward IV, Henry VIII and Charles I who are buried in St George's Chapel at Windsor Castle. Other exceptions include Richard III, now buried at Leicester Cathedral, and the de facto queen Lady Jane Grey, buried in the chapel of St Peter ad Vincula in the Tower of London. Most monarchs and royals who died after 1760 are buried either in St George's Chapel or at Frogmore to the east of Windsor Castle.[citation needed] Inner and outer vestibules lead to the octagonal chapter house, which is of exceptional architectural purity. It is built in a Geometrical Gothic style with an octagonal crypt below. A pier of eight shafts carries the vaulted ceiling. To the sides are blind arcading, remains of 14th-century paintings and numerous stone benches above which are innovatory large 4-light quatre-foiled windows. These are virtually contemporary with the Sainte-Chapelle, Paris. Westminster Abbey, formally titled the Collegiate Church of St Peter at Westminster, is a large, mainly Gothic abbey church in the City of Westminster, London, located just to the west of the Palace of Westminster. It is one of the most notable religious buildings in the United Kingdom and has been the traditional place of coronation and burial site for English and, later, British monarchs. Between 1540 and 1556 the abbey had the status of a cathedral. Since 1560, however, the building is no longer an abbey nor a cathedral, having instead the status of a Church of England ""Royal Peculiar""—a church responsible directly to the sovereign. The building itself is the original abbey church. The only extant depiction of Edward's abbey, together with the adjacent Palace of Westminster, is in the Bayeux Tapestry. Some of the lower parts of the monastic dormitory, an extension of the South Transept, survive in the Norman undercroft of the Great School, including a door said to come from the previous Saxon abbey. Increased endowments supported a community increased from a dozen monks in Dunstan's original foundation, up to a maximum about eighty monks, although there was also a large community of lay brothers who supported the monastery's extensive property and activities. The Westminster Abbey Museum is located in the 11th-century vaulted undercroft beneath the former monks' dormitory in Westminster Abbey. This is one of the oldest areas of the abbey, dating back almost to the foundation of the church by Edward the Confessor in 1065. This space has been used as a museum since 1908. The first reports of the abbey are based on a late tradition claiming that a young fisherman called Aldrich on the River Thames saw a vision of Saint Peter near the site. This seems to be quoted to justify the gifts of salmon from Thames fishermen that the abbey received in later years. In the present era, the Fishmonger's Company still gives a salmon every year. The proven origins are that in the 960s or early 970s, Saint Dunstan, assisted by King Edgar, installed a community of Benedictine monks here. At the east end of the Lady Chapel is a memorial chapel to the airmen of the RAF who were killed in the Second World War. It incorporates a memorial window to the Battle of Britain, which replaces an earlier Tudor stained glass window destroyed in the war. Subsequently, it became one of Britain's most significant honours to be buried or commemorated in the abbey. The practice of burying national figures in the abbey began under Oliver Cromwell with the burial of Admiral Robert Blake in 1657. The practice spread to include generals, admirals, politicians, doctors and scientists such as Isaac Newton, buried on 4 April 1727, and Charles Darwin, buried 26 April 1882. Another was William Wilberforce who led the movement to abolish slavery in the United Kingdom and the Plantations, buried on 3 August 1833. Wilberforce was buried in the north transept, close to his friend, the former Prime Minister, William Pitt.[citation needed] The abbey was restored to the Benedictines under the Catholic Mary I of England, but they were again ejected under Elizabeth I in 1559. In 1560, Elizabeth re-established Westminster as a ""Royal Peculiar"" – a church of the Church of England responsible directly to the Sovereign, rather than to a diocesan bishop – and made it the Collegiate Church of St Peter (that is, a non-cathedral church with an attached chapter of canons, headed by a dean.) The last of Mary's abbots was made the first dean. During the early 20th century it became increasingly common to bury cremated remains rather than coffins in the abbey. In 1905 the actor Sir Henry Irving was cremated and his ashes buried in Westminster Abbey, thereby becoming the first person ever to be cremated prior to interment at the abbey. The majority of interments at the Abbey are of cremated remains, but some burials still take place - Frances Challen, wife of the Rev Sebastian Charles, Canon of Westminster, was buried alongside her husband in the south choir aisle in 2014. Members of the Percy Family have a family vault, The Northumberland Vault, in St Nicholas's chapel within the abbey. In the floor, just inside the great west door, in the centre of the nave, is the tomb of The Unknown Warrior, an unidentified British soldier killed on a European battlefield during the First World War. He was buried in the abbey on 11 November 1920. This grave is the only one in the abbey on which it is forbidden to walk.[citation needed] The chapter house was built concurrently with the east parts of the abbey under Henry III, between about 1245 and 1253. It was restored by Sir George Gilbert Scott in 1872. The entrance is approached from the east cloister walk and includes a double doorway with a large tympanum above. The abbey's two western towers were built between 1722 and 1745 by Nicholas Hawksmoor, constructed from Portland stone to an early example of a Gothic Revival design. Purbeck marble was used for the walls and the floors of Westminster Abbey, even though the various tombstones are made of different types of marble. Further rebuilding and restoration occurred in the 19th century under Sir George Gilbert Scott. Between 1042 and 1052 King Edward the Confessor began rebuilding St Peter's Abbey to provide himself with a royal burial church. It was the first church in England built in the Romanesque style. The building was not completed until around 1090 but was consecrated on 28 December 1065, only a week before Edward's death on 5 January 1066. A week later he was buried in the church, and nine years later his wife Edith was buried alongside him. His successor, Harold II, was probably crowned in the abbey, although the first documented coronation is that of William the Conqueror later the same year. The bells at the abbey were overhauled in 1971. The ring is now made up of ten bells, hung for change ringing, cast in 1971, by the Whitechapel Bell Foundry, tuned to the notes: F#, E, D, C#, B, A, G, F#, E and D. The Tenor bell in D (588.5 Hz) has a weight of 30 cwt, 1 qtr, 15 lb (3403 lb or 1544 kg). Henry VIII assumed direct royal control in 1539 and granted the abbey the status of a cathedral by charter in 1540, simultaneously issuing letters patent establishing the Diocese of Westminster. By granting the abbey cathedral status Henry VIII gained an excuse to spare it from the destruction or dissolution which he inflicted on most English abbeys during this period. Westminster diocese was dissolved in 1550, but the abbey was recognised (in 1552, retroactively to 1550) as a second cathedral of the Diocese of London until 1556. The already-old expression ""robbing Peter to pay Paul"" may have been given a new lease of life when money meant for the abbey, which is dedicated to Saint Peter, was diverted to the treasury of St Paul's Cathedral."
Armenians,"Following the breakup of the Russian Empire in the aftermath of World War I for a brief period, from 1918 to 1920, Armenia was an independent republic. In late 1920, the communists came to power following an invasion of Armenia by the Red Army, and in 1922, Armenia became part of the Transcaucasian SFSR of the Soviet Union, later forming the Armenian Soviet Socialist Republic (1936 to September 21, 1991). In 1991, Armenia declared independence from the USSR and established the second Republic of Armenia. Small Armenian trading and religious communities have existed outside of Armenia for centuries. For example, a community has existed for over a millennium in the Holy Land, and one of the four quarters of the walled Old City of Jerusalem has been called the Armenian Quarter. An Armenian Catholic monastic community of 35 founded in 1717 exists on an island near Venice, Italy. There are also remnants of formerly populous communities in India, Myanmar, Thailand, Belgium, Portugal, Italy, Poland, Austria, Hungary, Bulgaria, Romania, Serbia, Ethiopia, Sudan and Egypt.[citation needed] Eric P. Hamp in his 2012 Indo-European family tree, groups the Armenian language along with Greek and Ancient Macedonian (""Helleno-Macedonian"") in the Pontic Indo-European (also called Helleno-Armenian) subgroup. In Hamp's view the homeland of this subgroup is the northeast coast of the Black Sea and its hinterlands. He assumes that they migrated from there southeast through the Caucasus with the Armenians remaining after Batumi while the pre-Greeks proceeded westwards along the southern coast of the Black Sea. The first geographical entity that was called Armenia by neighboring peoples (such as by Hecataeus of Miletus and on the Achaemenid Behistun Inscription) was established in the late 6th century BC under the Orontid dynasty within the Achaemenid Persian Empire as part of the latters' territories, and which later became a kingdom. At its zenith (95–65 BC), the state extended from the Caucasus all the way to what is now central Turkey, Lebanon, and northern Iran. The imperial reign of Tigranes the Great is thus the span of time during which Armenia itself conquered areas populated by other peoples. The Armenian Highland lies in the highlands surrounding Mount Ararat, the highest peak of the region. In the Bronze Age, several states flourished in the area of Greater Armenia, including the Hittite Empire (at the height of its power), Mitanni (South-Western historical Armenia), and Hayasa-Azzi (1600–1200 BC). Soon after Hayasa-Azzi were Arme-Shupria (1300s–1190 BC), the Nairi (1400–1000 BC) and the Kingdom of Urartu (860–590 BC), who successively established their sovereignty over the Armenian Highland. Each of the aforementioned nations and tribes participated in the ethnogenesis of the Armenian people. Under Ashurbanipal (669–627 BC), the Assyrian empire reached the Caucasus Mountains (modern Armenia, Georgia and Azerbaijan). The first Armenian churches were built between the 4th and 7th century, beginning when Armenia converted to Christianity, and ending with the Arab invasion of Armenia. The early churches were mostly simple basilicas, but some with side apses. By the fifth century the typical cupola cone in the center had become widely used. By the seventh century, centrally planned churches had been built and a more complicated niched buttress and radiating Hrip'simé style had formed. By the time of the Arab invasion, most of what we now know as classical Armenian architecture had formed. Armenians have had a presence in the Armenian Highland for over four thousand years, since the time when Hayk, the legendary patriarch and founder of the first Armenian nation, led them to victory over Bel of Babylon. Today, with a population of 3.5 million, they not only constitute an overwhelming majority in Armenia, but also in the disputed region of Nagorno-Karabakh. Armenians in the diaspora informally refer to them as Hayastantsis (Հայաստանցի), meaning those that are from Armenia (that is, those born and raised in Armenia). They, as well as the Armenians of Iran and Russia speak the Eastern dialect of the Armenian language. The country itself is secular as a result of Soviet domination, but most of its citizens identify themselves as Apostolic Armenian Christian. While the Armenian Apostolic Church remains the most prominent church in the Armenian community throughout the world, Armenians (especially in the diaspora) subscribe to any number of other Christian denominations. These include the Armenian Catholic Church (which follows its own liturgy but recognizes the Roman Catholic Pope), the Armenian Evangelical Church, which started as a reformation in the Mother church but later broke away, and the Armenian Brotherhood Church, which was born in the Armenian Evangelical Church, but later broke apart from it. There are other numerous Armenian churches belonging to Protestant denominations of all kinds. During Soviet rule, Armenian athletes rose to prominence winning plenty of medals and helping the USSR win the medal standings at the Olympics on numerous occasions. The first medal won by an Armenian in modern Olympic history was by Hrant Shahinyan, who won two golds and two silvers in gymnastics at the 1952 Summer Olympics in Helsinki. In football, their most successful team was Yerevan's FC Ararat, which had claimed most of the Soviet championships in the 70s and had also gone to post victories against professional clubs like FC Bayern Munich in the Euro cup. Armenian literature dates back to 400 AD, when Mesrop Mashtots first invented the Armenian alphabet. This period of time is often viewed as the Golden Age of Armenian literature. Early Armenian literature was written by the ""father of Armenian history"", Moses of Chorene, who authored The History of Armenia. The book covers the time-frame from the formation of the Armenian people to the fifth century AD. The nineteenth century beheld a great literary movement that was to give rise to modern Armenian literature. This period of time, during which Armenian culture flourished, is known as the Revival period (Zartonki sherchan). The Revivalist authors of Constantinople and Tiflis, almost identical to the Romanticists of Europe, were interested in encouraging Armenian nationalism. Most of them adopted the newly created Eastern or Western variants of the Armenian language depending on the targeted audience, and preferred them over classical Armenian (grabar). This period ended after the Hamidian massacres, when Armenians experienced turbulent times. As Armenian history of the 1920s and of the Genocide came to be more openly discussed, writers like Paruyr Sevak, Gevork Emin, Silva Kaputikyan and Hovhannes Shiraz began a new era of literature. Instruments like the duduk, the dhol, the zurna and the kanun are commonly found in Armenian folk music. Artists such as Sayat Nova are famous due to their influence in the development of Armenian folk music. One of the oldest types of Armenian music is the Armenian chant which is the most common kind of religious music in Armenia. Many of these chants are ancient in origin, extending to pre-Christian times, while others are relatively modern, including several composed by Saint Mesrop Mashtots, the inventor of the Armenian alphabet. Whilst under Soviet rule, Armenian classical music composer Aram Khatchaturian became internationally well known for his music, for various ballets and the Sabre Dance from his composition for the ballet Gayane. Art historian Hravard Hakobyan notes that ""Artsakh carpets occupy a special place in the history of Armenian carpet-making."" Common themes and patterns found on Armenian carpets were the depiction of dragons and eagles. They were diverse in style, rich in color and ornamental motifs, and were even separated in categories depending on what sort of animals were depicted on them, such as artsvagorgs (eagle-carpets), vishapagorgs (dragon-carpets) and otsagorgs (serpent-carpets). The rug mentioned in the Kaptavan inscriptions is composed of three arches, ""covered with vegatative ornaments"", and bears an artistic resemblance to the illuminated manuscripts produced in Artsakh. Armenians enjoy many different native and foreign foods. Arguably the favorite food is khorovats an Armenian-styled barbecue. Lavash is a very popular Armenian flat bread, and Armenian paklava is a popular dessert made from filo dough. Other famous Armenian foods include the kabob (a skewer of marinated roasted meat and vegetables), various dolmas (minced lamb, or beef meat and rice wrapped in grape leaves, cabbage leaves, or stuffed into hollowed vegetables), and pilaf, a rice dish. Also, ghapama, a rice-stuffed pumpkin dish, and many different salads are popular in Armenian culture. Fruits play a large part in the Armenian diet. Apricots (Prunus armeniaca, also known as Armenian Plum) have been grown in Armenia for centuries and have a reputation for having an especially good flavor. Peaches are popular as well, as are grapes, figs, pomegranates, and melons. Preserves are made from many fruits, including cornelian cherries, young walnuts, sea buckthorn, mulberries, sour cherries, and many others. Historically, the name Armenian has come to internationally designate this group of people. It was first used by neighbouring countries of ancient Armenia. The earliest attestations of the exonym Armenia date around the 6th century BC. In his trilingual Behistun Inscription dated to 517 BC, Darius I the Great of Persia refers to Urashtu (in Babylonian) as Armina (in Old Persian; Armina (    ) and Harminuya (in Elamite). In Greek, Αρμένιοι ""Armenians"" is attested from about the same time, perhaps the earliest reference being a fragment attributed to Hecataeus of Miletus (476 BC). Xenophon, a Greek general serving in some of the Persian expeditions, describes many aspects of Armenian village life and hospitality in around 401 BC. He relates that the people spoke a language that to his ear sounded like the language of the Persians. From the early 16th century, both Western Armenia and Eastern Armenia fell under Iranian Safavid rule. Owing to the century long Turco-Iranian geo-political rivalry that would last in Western Asia, significant parts of the region were frequently fought over between the two rivalling empires. From the mid 16th century with the Peace of Amasya, and decisively from the first half of the 17th century with the Treaty of Zuhab until the first half of the 19th century, Eastern Armenia was ruled by the successive Iranian Safavid, Afsharid and Qajar empires, while Western Armenia remained under Ottoman rule. In the late 1820s, the parts of historic Armenia under Iranian control centering on Yerevan and Lake Sevan (all of Eastern Armenia) were incorporated into the Russian Empire following Iran's forced ceding of the territories after its loss in the Russo-Persian War (1826-1828) and the outcoming Treaty of Turkmenchay. Western Armenia however, remained in Ottoman hands. Governments of Republic of Turkey since that time have consistently rejected charges of genocide, typically arguing either that those Armenians who died were simply in the way of a war or that killings of Armenians were justified by their individual or collective support for the enemies of the Ottoman Empire. Passage of legislation in various foreign countries condemning the persecution of the Armenians as genocide has often provoked diplomatic conflict. (See Recognition of the Armenian Genocide) The Arsacid Kingdom of Armenia, itself a branch of the Arsacid dynasty of Parthia, was the first state to adopt Christianity as its religion (it had formerly been adherent to Armenian paganism, which was influenced by Zoroastrianism, while later on adopting a few elements regarding identification of its pantheon with Greco-Roman deities). in the early years of the 4th century, likely AD 301, partly in defiance of the Sassanids it seems. In the late Parthian period, Armenia was a predominantly Zoroastrian-adhering land, but by the Christianisation, previously predominant Zoroastrianism and paganism in Armenia gradually declined. Later on, in order to further strengthen Armenian national identity, Mesrop Mashtots invented the Armenian alphabet, in 405 AD. This event ushered the Golden Age of Armenia, during which many foreign books and manuscripts were translated to Armenian by Mesrop's pupils. Armenia lost its sovereignty again in 428 AD to the rivalling Byzantine and Sassanid Persian empires, until the Muslim conquest of Persia overran also the regions in which Armenians lived. The Armenian Genocide caused widespread emigration that led to the settlement of Armenians in various countries in the world. Armenians kept to their traditions and certain diasporans rose to fame with their music. In the post-Genocide Armenian community of the United States, the so-called ""kef"" style Armenian dance music, using Armenian and Middle Eastern folk instruments (often electrified/amplified) and some western instruments, was popular. This style preserved the folk songs and dances of Western Armenia, and many artists also played the contemporary popular songs of Turkey and other Middle Eastern countries from which the Armenians emigrated. Richard Hagopian is perhaps the most famous artist of the traditional ""kef"" style and the Vosbikian Band was notable in the 40s and 50s for developing their own style of ""kef music"" heavily influenced by the popular American Big Band Jazz of the time. Later, stemming from the Middle Eastern Armenian diaspora and influenced by Continental European (especially French) pop music, the Armenian pop music genre grew to fame in the 60s and 70s with artists such as Adiss Harmandian and Harout Pamboukjian performing to the Armenian diaspora and Armenia. Also with artists such as Sirusho, performing pop music combined with Armenian folk music in today's entertainment industry. Other Armenian diasporans that rose to fame in classical or international music circles are world-renowned French-Armenian singer and composer Charles Aznavour, pianist Sahan Arzruni, prominent opera sopranos such as Hasmik Papian and more recently Isabel Bayrakdarian and Anna Kasyan. Certain Armenians settled to sing non-Armenian tunes such as the heavy metal band System of a Down (which nonetheless often incorporates traditional Armenian instrumentals and styling into their songs) or pop star Cher. Ruben Hakobyan (Ruben Sasuntsi) is a well recognized Armenian ethnographic and patriotic folk singer who has achieved widespread national recognition due to his devotion to Armenian folk music and exceptional talent. In the Armenian diaspora, Armenian revolutionary songs are popular with the youth.[citation needed] These songs encourage Armenian patriotism and are generally about Armenian history and national heroes. Armenians constitute the main population of Armenia and the de facto independent Nagorno-Karabakh Republic. There is a wide-ranging diaspora of around 5 million people of full or partial Armenian ancestry living outside of modern Armenia. The largest Armenian populations today exist in Russia, the United States, France, Georgia, Iran, Ukraine, Lebanon, and Syria. With the exceptions of Iran and the former Soviet states, the present-day Armenian diaspora was formed mainly as a result of the Armenian Genocide. The Armenians collective has, at times, constituted a Christian ""island"" in a mostly Muslim region. There is, however, a minority of ethnic Armenian Muslims, known as Hamshenis but many Armenians view them as a separate race, while the history of the Jews in Armenia dates back 2,000 years. The Armenian Kingdom of Cilicia had close ties to European Crusader States. Later on, the deteriorating situation in the region led the bishops of Armenia to elect a Catholicos in Etchmiadzin, the original seat of the Catholicosate. In 1441, a new Catholicos was elected in Etchmiadzin in the person of Kirakos Virapetsi, while Krikor Moussapegiants preserved his title as Catholicos of Cilicia. Therefore, since 1441, there have been two Catholicosates in the Armenian Church with equal rights and privileges, and with their respective jurisdictions. The primacy of honor of the Catholicosate of Etchmiadzin has always been recognized by the Catholicosate of Cilicia. In 885 AD the Armenians reestablished themselves as a sovereign kingdom under the leadership of Ashot I of the Bagratid Dynasty. A considerable portion of the Armenian nobility and peasantry fled the Byzantine occupation of Bagratid Armenia in 1045, and the subsequent invasion of the region by Seljuk Turks in 1064. They settled in large numbers in Cilicia, an Anatolian region where Armenians were already established as a minority since Roman times. In 1080, they founded an independent Armenian Principality then Kingdom of Cilicia, which became the focus of Armenian nationalism. The Armenians developed close social, cultural, military, and religious ties with nearby Crusader States, but eventually succumbed to Mamluk invasions. In the next few centuries, Djenghis Khan, Timurids, and the tribal Turkic federations of the Ak Koyunlu and the Kara Koyunlu ruled over the Armenians. Armenia established a Church that still exists independently of both the Catholic and the Eastern Orthodox churches, having become so in 451 AD as a result of its stance regarding the Council of Chalcedon. Today this church is known as the Armenian Apostolic Church, which is a part of the Oriental Orthodox communion, not to be confused with the Eastern Orthodox communion. During its later political eclipses, Armenia depended on the church to preserve and protect its unique identity. The original location of the Armenian Catholicosate is Echmiadzin. However, the continuous upheavals, which characterized the political scenes of Armenia, made the political power move to safer places. The Church center moved as well to different locations together with the political authority. Therefore, it eventually moved to Cilicia as the Holy See of Cilicia. From the 9th to 11th century, Armenian architecture underwent a revival under the patronage of the Bagratid Dynasty with a great deal of building done in the area of Lake Van, this included both traditional styles and new innovations. Ornately carved Armenian Khachkars were developed during this time. Many new cities and churches were built during this time, including a new capital at Lake Van and a new Cathedral on Akdamar Island to match. The Cathedral of Ani was also completed during this dynasty. It was during this time that the first major monasteries, such as Haghpat and Haritchavank were built. This period was ended by the Seljuk invasion. Within the diasporan Armenian community, there is an unofficial classification of the different kinds of Armenians. For example, Armenians who originate from Iran are referred to as Parskahay (Պարսկահայ), while Armenians from Lebanon are usually referred to as Lipananahay (Լիբանանահայ). Armenians of the Diaspora are the primary speakers of the Western dialect of the Armenian language. This dialect has considerable differences with Eastern Armenian, but speakers of either of the two variations can usually understand each other. Eastern Armenian in the diaspora is primarily spoken in Iran and European countries such as Ukraine, Russia, and Georgia (where they form a majority in the Samtskhe-Javakheti province). In diverse communities (such as in Canada and the U.S.) where many different kinds of Armenians live together, there is a tendency for the different groups to cluster together. Carpet-weaving is historically a major traditional profession for the majority of Armenian women, including many Armenian families. Prominent Karabakh carpet weavers there were men too. The oldest extant Armenian carpet from the region, referred to as Artsakh (see also Karabakh carpet) during the medieval era, is from the village of Banants (near Gandzak) and dates to the early 13th century. The first time that the Armenian word for carpet, gorg, was used in historical sources was in a 1242–1243 Armenian inscription on the wall of the Kaptavan Church in Artsakh."
Egypt,"The Suez Canal is an artificial sea-level waterway in Egypt considered the most important centre of the maritime transport in the Middle East, connecting the Mediterranean Sea and the Red Sea. Opened in November 1869 after 10 years of construction work, it allows ship transport between Europe and Asia without navigation around Africa. The northern terminus is Port Said and the southern terminus is Port Tawfiq at the city of Suez. Ismailia lies on its west bank, 3 km (1.9 mi) from the half-way point. With over 90 million inhabitants, Egypt is the most populous country in North Africa and the Arab World, the third-most populous in Africa (after Nigeria and Ethiopia), and the fifteenth-most populous in the world. The great majority of its people live near the banks of the Nile River, an area of about 40,000 square kilometres (15,000 sq mi), where the only arable land is found. The large regions of the Sahara desert, which constitute most of Egypt's territory, are sparsely inhabited. About half of Egypt's residents live in urban areas, with most spread across the densely populated centres of greater Cairo, Alexandria and other major cities in the Nile Delta. Egypt's economy depends mainly on agriculture, media, petroleum imports, natural gas, and tourism; there are also more than three million Egyptians working abroad, mainly in Saudi Arabia, the Persian Gulf and Europe. The completion of the Aswan High Dam in 1970 and the resultant Lake Nasser have altered the time-honored place of the Nile River in the agriculture and ecology of Egypt. A rapidly growing population, limited arable land, and dependence on the Nile all continue to overtax resources and stress the economy. Egypt actively practices capital punishment. Egypt's authorities do not release figures on death sentences and executions, despite repeated requests over the years by human rights organisations. The United Nations human rights office and various NGOs expressed ""deep alarm"" after an Egyptian Minya Criminal Court sentenced 529 people to death in a single hearing on 25 March 2014. Sentenced supporters of former President Mohamed Morsi will be executed for their alleged role in violence following his ousting in July 2013. The judgment was condemned as a violation of international law. By May 2014, approximately 16,000 people (and as high as more than 40,000 by one independent count), mostly Brotherhood members or supporters, have been imprisoned after the coup  after the Muslim Brotherhood was labelled as terrorist organisation by the post-coup interim Egyptian government. The permanent headquarters of the Arab League are located in Cairo and the body's secretary general has traditionally been Egyptian. This position is currently held by former foreign minister Nabil el-Araby. The Arab League briefly moved from Egypt to Tunis in 1978 to protest the Egypt-Israel Peace Treaty, but it later returned to Cairo in 1989. Gulf monarchies, including the United Arab Emirates and Saudi Arabia, have pledged billions of dollars to help Egypt overcome its economic difficulties since the July 2013 coup. Of the Christian minority in Egypt over 90% belong to the native Coptic Orthodox Church of Alexandria, an Oriental Orthodox Christian Church. Other native Egyptian Christians are adherents of the Coptic Catholic Church, the Evangelical Church of Egypt and various other Protestant denominations. Non-native Christian communities are largely found in the urban regions of Cairo and Alexandria, such as the Syro-Lebanese, who belong to Greek Catholic, Greek Orthodox, and Maronite Catholic denominations. The New Kingdom c. 1550–1070 BC began with the Eighteenth Dynasty, marking the rise of Egypt as an international power that expanded during its greatest extension to an empire as far south as Tombos in Nubia, and included parts of the Levant in the east. This period is noted for some of the most well known Pharaohs, including Hatshepsut, Thutmose III, Akhenaten and his wife Nefertiti, Tutankhamun and Ramesses II. The first historically attested expression of monotheism came during this period as Atenism. Frequent contacts with other nations brought new ideas to the New Kingdom. The country was later invaded and conquered by Libyans, Nubians and Assyrians, but native Egyptians eventually drove them out and regained control of their country. Although Egypt was a majority Christian country before the 7th Century, after Islam arrived, the country was slowly Islamified to become a majority Muslim country. Egypt emerged as a centre of politics and culture in the Muslim world. Under Anwar Sadat, Islam became the official state religion and Sharia the main source of law. It is estimated that 15 million Egyptians follow Native Sufi orders, with the Sufi leadership asserting that the numbers are much greater as many Egyptian Sufis are not officially registered with a Sufi order. Miṣr (IPA: [mi̠sˤr] or Egyptian Arabic pronunciation: [mesˤɾ]; Arabic: مِصر‎) is the Classical Quranic Arabic and modern official name of Egypt, while Maṣr (IPA: [mɑsˤɾ]; Egyptian Arabic: مَصر) is the local pronunciation in Egyptian Arabic. The name is of Semitic origin, directly cognate with other Semitic words for Egypt such as the Hebrew מִצְרַיִם (Mitzráyim). The oldest attestation of this name for Egypt is the Akkadian 𒆳 𒈪 𒄑 𒊒 KURmi-iṣ-ru miṣru, related to miṣru/miṣirru/miṣaru, meaning ""border"" or ""frontier"". At the time of the fall of the Egyptian monarchy in the early 1950s, less than half a million Egyptians were considered upper class and rich, four million middle class and 17 million lower class and poor. Fewer than half of all primary-school-age children attended school, most of them being boys. Nasser's policies changed this. Land reform and distribution, the dramatic growth in university education, and government support to national industries greatly improved social mobility and flattened the social curve. From academic year 1953-54 through 1965-66, overall public school enrolments more than doubled. Millions of previously poor Egyptians, through education and jobs in the public sector, joined the middle class. Doctors, engineers, teachers, lawyers, journalists, constituted the bulk of the swelling middle class in Egypt under Nasser. During the 1960s, the Egyptian economy went from sluggish to the verge of collapse, the society became less free, and Nasser's appeal waned considerably. Following the 1973 war and the subsequent peace treaty, Egypt became the first Arab nation to establish diplomatic relations with Israel. Despite that, Israel is still widely considered as a hostile state by the majority of Egyptians. Egypt has played a historical role as a mediator in resolving various disputes in the Middle East, most notably its handling of the Israeli-Palestinian conflict and the peace process. Egypt's ceasefire and truce brokering efforts in Gaza have hardly been challenged following Israel's evacuation of its settlements from the strip in 2005, despite increasing animosity towards the Hamas government in Gaza following the ouster of Mohamed Morsi, and despite recent attempts by countries like Turkey and Qatar to take over this role. As a result of modernisation efforts over the years, Egypt's healthcare system has made great strides forward. Access to healthcare in both urban and rural areas greatly improved and immunisation programs are now able to cover 98% of the population. Life expectancy increased from 44.8 years during the 1960s to 72.12 years in 2009. There was a noticeable decline of the infant mortality rate (during the 1970s to the 1980s the infant mortality rate was 101-132/1000 live births, in 2000 the rate was 50-60/1000, and in 2008 it was 28-30/1000). Coptic Christians face discrimination at multiple levels of the government, ranging from disproportionate representation in government ministries to laws that limit their ability to build or repair churches. Intolerance of Bahá'ís and non-orthodox Muslim sects, such as Sufis, Shi'a and Ahmadis, also remains a problem. When the government moved to computerise identification cards, members of religious minorities, such as Bahá'ís, could not obtain identification documents. An Egyptian court ruled in early 2008 that members of other faiths may obtain identity cards without listing their faiths, and without becoming officially recognised. Drinking water supply and sanitation in Egypt is characterised by both achievements and challenges. Among the achievements are an increase of piped water supply between 1990 and 2010 from 89% to 100% in urban areas and from 39% to 93% in rural areas despite rapid population growth, the elimination of open defecation in rural areas during the same period, and in general a relatively high level of investment in infrastructure. Access to an improved water source in Egypt is now practically universal with a rate of 99%. About one half of the population is connected to sanitary sewers. Egypt's most prominent multinational companies are the Orascom Group and Raya Contact Center. The information technology (IT) sector has expanded rapidly in the past few years, with many start-ups selling outsourcing services to North America and Europe, operating with companies such as Microsoft, Oracle and other major corporations, as well as many small and medium size enterprises. Some of these companies are the Xceed Contact Center, Raya, E Group Connections and C3. The IT sector has been stimulated by new Egyptian entrepreneurs with government encouragement.[citation needed] The canal is 193.30 km (120.11 mi) long, 24 m (79 ft) deep and 205 metres (673 ft) wide as of 2010. It consists of the northern access channel of 22 km (14 mi), the canal itself of 162.25 km (100.82 mi) and the southern access channel of 9 km (5.6 mi). The canal is a single lane with passing places in the ""Ballah By-Pass"" and the Great Bitter Lake. It contains no locks; seawater flows freely through the canal. In general, the canal north of the Bitter Lakes flows north in winter and south in summer. The current south of the lakes changes with the tide at Suez. Ethnic Egyptians are by far the largest ethnic group in the country, constituting 91% of the total population. Ethnic minorities include the Abazas, Turks, Greeks, Bedouin Arab tribes living in the eastern deserts and the Sinai Peninsula, the Berber-speaking Siwis (Amazigh) of the Siwa Oasis, and the Nubian communities clustered along the Nile. There are also tribal Beja communities concentrated in the south-eastern-most corner of the country, and a number of Dom clans mostly in the Nile Delta and Faiyum who are progressively becoming assimilated as urbanisation increases. In mid May 1967, the Soviet Union issued warnings to Nasser of an impending Israeli attack on Syria. Although the chief of staff Mohamed Fawzi verified them as ""baseless"", Nasser took three successive steps that made the war virtually inevitable: On 14 May he deployed his troops in Sinai near the border with Israel, on 19 May he expelled the UN peacekeepers stationed in the Sinai Peninsula border with Israel, and on 23 May he closed the Straits of Tiran to Israeli shipping. On 26 May Nasser declared, ""The battle will be a general one and our basic objective will be to destroy Israel"". The Byzantines were able to regain control of the country after a brief Sasanian Persian invasion early in the 7th century amidst the Byzantine–Sasanian War of 602–628 during which they established a new short-lived province for ten years known as Sasanian Egypt, until 639–42, when Egypt was invaded and conquered by the Islamic Empire by the Muslim Arabs. When they defeated the Byzantine Armies in Egypt, the Arabs brought Sunni Islam to the country. Early in this period, Egyptians began to blend their new faith with indigenous beliefs and practices, leading to various Sufi orders that have flourished to this day. These earlier rites had survived the period of Coptic Christianity. On 18 January 2014, the interim government instituted a new constitution following a referendum in which 98.1% of voters were supportive. Participation was low with only 38.6% of registered voters participating although this was higher than the 33% who voted in a referendum during Morsi's tenure. On 26 March 2014 Abdel Fattah el-Sisi the head of the Egyptian Armed Forces, who at this time was in control of the country, resigned from the military, announcing he would stand as a candidate in the 2014 presidential election. The poll, held between 26 and 28 May 2014, resulted in a landslide victory for el-Sisi. Sisi was sworn into office as President of Egypt on 8 June 2014. The Muslim Brotherhood and some liberal and secular activist groups boycotted the vote. Even though the military-backed authorities extended voting to a third day, the 46% turnout was lower than the 52% turnout in the 2012 election. Egypt also hosts an unknown number of refugees and asylum seekers, estimated to be between 500,000 and 3 million. There are some 70,000 Palestinian refugees, and about 150,000 recently arrived Iraqi refugees, but the number of the largest group, the Sudanese, is contested.[nb 1] The once-vibrant and ancient Greek and Jewish communities in Egypt have almost disappeared, with only a small number remaining in the country, but many Egyptian Jews visit on religious or other occasions and tourism. Several important Jewish archaeological and historical sites are found in Cairo, Alexandria and other cities. Egyptian literature traces its beginnings to ancient Egypt and is some of the earliest known literature. Indeed, the Egyptians were the first culture to develop literature as we know it today, that is, the book. It is an important cultural element in the life of Egypt. Egyptian novelists and poets were among the first to experiment with modern styles of Arabic literature, and the forms they developed have been widely imitated throughout the Middle East. The first modern Egyptian novel Zaynab by Muhammad Husayn Haykal was published in 1913 in the Egyptian vernacular. Egyptian novelist Naguib Mahfouz was the first Arabic-language writer to win the Nobel Prize in Literature. Egyptian women writers include Nawal El Saadawi, well known for her feminist activism, and Alifa Rifaat who also writes about women and tradition. In 1975, Sadat shifted Nasser's economic policies and sought to use his popularity to reduce government regulations and encourage foreign investment through his program of Infitah. Through this policy, incentives such as reduced taxes and import tariffs attracted some investors, but investments were mainly directed at low risk and profitable ventures like tourism and construction, abandoning Egypt's infant industries. Even though Sadat's policy was intended to modernise Egypt and assist the middle class, it mainly benefited the higher class, and, because of the elimination of subsidies on basic foodstuffs, led to the 1977 Egyptian Bread Riots. In the 1980s, 1990s, and 2000s, terrorist attacks in Egypt became numerous and severe, and began to target Christian Copts, foreign tourists and government officials. In the 1990s an Islamist group, Al-Gama'a al-Islamiyya, engaged in an extended campaign of violence, from the murders and attempted murders of prominent writers and intellectuals, to the repeated targeting of tourists and foreigners. Serious damage was done to the largest sector of Egypt's economy—tourism—and in turn to the government, but it also devastated the livelihoods of many of the people on whom the group depended for support. Economic conditions have started to improve considerably, after a period of stagnation, due to the adoption of more liberal economic policies by the government as well as increased revenues from tourism and a booming stock market. In its annual report, the International Monetary Fund (IMF) has rated Egypt as one of the top countries in the world undertaking economic reforms. Some major economic reforms undertaken by the government since 2003 include a dramatic slashing of customs and tariffs. A new taxation law implemented in 2005 decreased corporate taxes from 40% to the current 20%, resulting in a stated 100% increase in tax revenue by the year 2006. Egypt has a wide range of beaches situated on the Mediterranean and the Red Sea that extend to over 3,000 km. The Red Sea has serene waters, coloured coral reefs, rare fish and beautiful mountains. The Akba Gulf beaches also provide facilities for practising sea sports. Safaga tops the Red Sea zone with its beautiful location on the Suez Gulf. Last but not least, Sharm el-Sheikh (or City of Peace), Hurghada, Luxor (known as world's greatest open-air museum/ or City of the ⅓ of world monuments), Dahab, Ras Sidr, Marsa Alam, Safaga and the northern coast of the Mediterranean are major tourist's destinations of the recreational tourism. On 18 January 2014, the interim government successfully institutionalised a more secular constitution. The president is elected to a four-year term and may serve 2 terms. The parliament may impeach the president. Under the constitution, there is a guarantee of gender equality and absolute freedom of thought. The military retains the ability to appoint the national Minister of Defence for the next 8 years. Under the constitution, political parties may not be based on ""religion, race, gender or geography"". Muhammad Ali Pasha evolved the military from one that convened under the tradition of the corvée to a great modernised army. He introduced conscription of the male peasantry in 19th century Egypt, and took a novel approach to create his great army, strengthening it with numbers and in skill. Education and training of the new soldiers was not an option; the new concepts were furthermore enforced by isolation. The men were held in barracks to avoid distraction of their growth as a military unit to be reckoned with. The resentment for the military way of life eventually faded from the men and a new ideology took hold, one of nationalism and pride. It was with the help of this newly reborn martial unit that Muhammad Ali imposed his rule over Egypt. The Ptolemaic Kingdom was a powerful Hellenistic state, extending from southern Syria in the east, to Cyrene to the west, and south to the frontier with Nubia. Alexandria became the capital city and a centre of Greek culture and trade. To gain recognition by the native Egyptian populace, they named themselves as the successors to the Pharaohs. The later Ptolemies took on Egyptian traditions, had themselves portrayed on public monuments in Egyptian style and dress, and participated in Egyptian religious life. The official language of the Republic is Modern Standard Arabic. Arabic was adopted by the Egyptians after the Arab invasion of Egypt. The spoken languages are: Egyptian Arabic (68%), Sa'idi Arabic (29%), Eastern Egyptian Bedawi Arabic (1.6%), Sudanese Arabic (0.6%), Domari (0.3%), Nobiin (0.3%), Beja (0.1%), Siwi and others. Additionally, Greek, Armenian and Italian are the main languages of immigrants. In Alexandria in the 19th century there was a large community of Italian Egyptians and Italian was the ""lingua franca"" of the city. The United States provides Egypt with annual military assistance, which in 2015 amounted to US$1.3 billion. In 1989, Egypt was designated as a major non-NATO ally of the United States. Nevertheless, ties between the two countries have partially soured since the July 2013 military coup that deposed Islamist president Mohamed Morsi, with the Obama administration condemning Egypt's violent crackdown on the Muslim Brotherhood and its supporters, and cancelling future military exercises involving the two countries. There have been recent attempts, however, to normalise relations between the two, with both governments frequently calling for mutual support in the fight against regional and international terrorism. In 1970, President Nasser died and was succeeded by Anwar Sadat. Sadat switched Egypt's Cold War allegiance from the Soviet Union to the United States, expelling Soviet advisors in 1972. He launched the Infitah economic reform policy, while clamping down on religious and secular opposition. In 1973, Egypt, along with Syria, launched the October War, a surprise attack to regain part of the Sinai territory Israel had captured 6 years earlier. it presented Sadat with a victory that allowed him to regain the Sinai later in return for peace with Israel. Football is the most popular national sport of Egypt. The Cairo Derby is one of the fiercest derbies in Africa, and the BBC picked it as one of the 7 toughest derbies in the world. Al Ahly is the most successful club of the 20th century in the African continent according to CAF, closely followed by their rivals Zamalek SC. Al Ahly was named in 2000 by the Confederation of African Football as the ""African Club of the Century"". With twenty titles, Al Ahly is currently the world's most successful club in terms of international trophies, surpassing Italy's A.C. Milan and Argentina's Boca Juniors, both having eighteen. Modern Egypt is considered to be a regional and middle power, with significant cultural, political, and military influence in North Africa, the Middle East and the Muslim world. Its economy is one of the largest and most diversified in the Middle East, with sectors such as tourism, agriculture, industry and services at almost equal production levels. In 2011, longtime President Hosni Mubarak stepped down amid mass protests. Later elections saw the rise of the Muslim Brotherhood, which was ousted by the army a year later amid mass protests. Although one of the main obstacles still facing the Egyptian economy is the limited trickle down of wealth to the average population, many Egyptians criticise their government for higher prices of basic goods while their standards of living or purchasing power remains relatively stagnant. Corruption is often cited by Egyptians as the main impediment to further economic growth. The government promised major reconstruction of the country's infrastructure, using money paid for the newly acquired third mobile license ($3 billion) by Etisalat in 2006. In the Corruption Perceptions Index 2013, Egypt was ranked 114 out of 177. Egypt has a developed energy market based on coal, oil, natural gas, and hydro power. Substantial coal deposits in the northeast Sinai are mined at the rate of about 600,000 tonnes (590,000 long tons; 660,000 short tons) per year. Oil and gas are produced in the western desert regions, the Gulf of Suez, and the Nile Delta. Egypt has huge reserves of gas, estimated at 2,180 cubic kilometres (520 cu mi), and LNG up to 2012 exported to many countries. In 2013, the Egyptian General Petroleum Co (EGPC) said the country will cut exports of natural gas and tell major industries to slow output this summer to avoid an energy crisis and stave off political unrest, Reuters has reported. Egypt is counting on top liquid natural gas (LNG) exporter Qatar to obtain additional gas volumes in summer, while encouraging factories to plan their annual maintenance for those months of peak demand, said EGPC chairman, Tarek El Barkatawy. Egypt produces its own energy, but has been a net oil importer since 2008 and is rapidly becoming a net importer of natural gas. Egypt has one of the longest histories of any modern country, arising in the tenth millennium BC as one of the world's first nation states. Considered a cradle of civilisation, Ancient Egypt experienced some of the earliest developments of writing, agriculture, urbanisation, organised religion and central government. Iconic monuments such as the Giza Necropolis and its Great Sphinx, as well the ruins of Memphis, Thebes, Karnak, and the Valley of the Kings, reflect this legacy and remain a significant focus of archaeological study and popular interest worldwide. Egypt's rich cultural heritage is an integral part of its national identity, having endured, and at times assimilated, various foreign influences, including Greek, Persian, Roman, Arab, Ottoman, and European. Although Christianised in the first century of the Common Era, it was subsequently Islamised due to the Islamic conquests of the seventh century. By about 6000 BC, a Neolithic culture rooted in the Nile Valley. During the Neolithic era, several predynastic cultures developed independently in Upper and Lower Egypt. The Badarian culture and the successor Naqada series are generally regarded as precursors to dynastic Egypt. The earliest known Lower Egyptian site, Merimda, predates the Badarian by about seven hundred years. Contemporaneous Lower Egyptian communities coexisted with their southern counterparts for more than two thousand years, remaining culturally distinct, but maintaining frequent contact through trade. The earliest known evidence of Egyptian hieroglyphic inscriptions appeared during the predynastic period on Naqada III pottery vessels, dated to about 3200 BC. Most of Egypt's rain falls in the winter months. South of Cairo, rainfall averages only around 2 to 5 mm (0.1 to 0.2 in) per year and at intervals of many years. On a very thin strip of the northern coast the rainfall can be as high as 410 mm (16.1 in), mostly between October and March. Snow falls on Sinai's mountains and some of the north coastal cities such as Damietta, Baltim, Sidi Barrany, etc. and rarely in Alexandria. A very small amount of snow fell on Cairo on 13 December 2013, the first time Cairo received snowfall in many decades. Frost is also known in mid-Sinai and mid-Egypt. Egypt is the driest and the sunniest country in the world, and most of its land surface is desert. The First Intermediate Period ushered in a time of political upheaval for about 150 years. Stronger Nile floods and stabilisation of government, however, brought back renewed prosperity for the country in the Middle Kingdom c. 2040 BC, reaching a peak during the reign of Pharaoh Amenemhat III. A second period of disunity heralded the arrival of the first foreign ruling dynasty in Egypt, that of the Semitic Hyksos. The Hyksos invaders took over much of Lower Egypt around 1650 BC and founded a new capital at Avaris. They were driven out by an Upper Egyptian force led by Ahmose I, who founded the Eighteenth Dynasty and relocated the capital from Memphis to Thebes. Foreign direct investment (FDI) in Egypt increased considerably before the removal of Hosni Mubarak, exceeding $6 billion in 2006, due to economic liberalisation and privatisation measures taken by minister of investment Mahmoud Mohieddin.[citation needed] Since the fall of Hosni Mubarak in 2011, Egypt has experienced a drastic fall in both foreign investment and tourism revenues, followed by a 60% drop in foreign exchange reserves, a 3% drop in growth, and a rapid devaluation of the Egyptian pound. Egyptian music is a rich mixture of indigenous, Mediterranean, African and Western elements. It has been an integral part of Egyptian culture since antiquity. The ancient Egyptians credited one of their gods Hathor with the invention of music, which Osiris in turn used as part of his effort to civilise the world. Egyptians used music instruments since then. Contemporary Egyptian music traces its beginnings to the creative work of people such as Abdu El Hamouli, Almaz and Mahmoud Osman, who influenced the later work of Sayed Darwish, Umm Kulthum, Mohammed Abdel Wahab and Abdel Halim Hafez whose age is considered the golden age of music in Egypt and the whole Middle East and North-Africa. Prominent contemporary Egyptian pop singers include Amr Diab and Mohamed Mounir. The Egyptian military has dozens of factories manufacturing weapons as well as consumer goods. The Armed Forces' inventory includes equipment from different countries around the world. Equipment from the former Soviet Union is being progressively replaced by more modern US, French, and British equipment, a significant portion of which is built under license in Egypt, such as the M1 Abrams tank.[citation needed] Relations with Russia have improved significantly following Mohamed Morsi's removal and both countries have worked since then to strengthen military and trade ties among other aspects of bilateral co-operation. Relations with China have also improved considerably. In 2014, Egypt and China have established a bilateral ""comprehensive strategic partnership"". The Pew Forum on Religion & Public Life ranks Egypt as the fifth worst country in the world for religious freedom. The United States Commission on International Religious Freedom, a bipartisan independent agency of the US government, has placed Egypt on its watch list of countries that require close monitoring due to the nature and extent of violations of religious freedom engaged in or tolerated by the government. According to a 2010 Pew Global Attitudes survey, 84% of Egyptians polled supported the death penalty for those who leave Islam; 77% supported whippings and cutting off of hands for theft and robbery; and 82% support stoning a person who commits adultery. The plan stated that the following numbers of species of different groups had been recorded from Egypt: algae (1483 species), animals (about 15,000 species of which more than 10,000 were insects), fungi (more than 627 species), monera (319 species), plants (2426 species), protozoans (371 species). For some major groups, for example lichen-forming fungi and nematode worms, the number was not known. Apart from small and well-studied groups like amphibians, birds, fish, mammals and reptiles, the many of those numbers are likely to increase as further species are recorded from Egypt. For the fungi, including lichen-forming species, for example, subsequent work has shown that over 2200 species have been recorded from Egypt, and the final figure of all fungi actually occurring in the country is expected to be much higher. The Suez Canal, built in partnership with the French, was completed in 1869. Its construction led to enormous debt to European banks, and caused popular discontent because of the onerous taxation it required. In 1875 Ismail was forced to sell Egypt's share in the canal to the British Government. Within three years this led to the imposition of British and French controllers who sat in the Egyptian cabinet, and, ""with the financial power of the bondholders behind them, were the real power in the Government."" Partly because of low sanitation coverage about 17,000 children die each year because of diarrhoea. Another challenge is low cost recovery due to water tariffs that are among the lowest in the world. This in turn requires government subsidies even for operating costs, a situation that has been aggravated by salary increases without tariff increases after the Arab Spring. Poor operation of facilities, such as water and wastewater treatment plants, as well as limited government accountability and transparency, are also issues. The last ruler from the Ptolemaic line was Cleopatra VII, who committed suicide following the burial of her lover Mark Antony who had died in her arms (from a self-inflicted stab wound), after Octavian had captured Alexandria and her mercenary forces had fled. The Ptolemies faced rebellions of native Egyptians often caused by an unwanted regime and were involved in foreign and civil wars that led to the decline of the kingdom and its annexation by Rome. Nevertheless, Hellenistic culture continued to thrive in Egypt well after the Muslim conquest. Egypt has hosted several international competitions. the last one was 2009 FIFA U-20 World Cup which took place between 24 September - 16 October 2009. On Friday 19 September of the year 2014, Guinness World Records has announced that Egyptian scuba diver Ahmed Gabr is the new title holder for deepest salt water scuba dive, at 332.35 metres. Ahmed set a new world record Friday when he reached a depth of more than 1,000 feet. The 14-hour feat took Gabr 1,066 feet down into the abyss near the Egyptian town of Dahab in ther Red Sea, where he works as a diving instructor. The House of Representatives, whose members are elected to serve five-year terms, specialises in legislation. Elections were last held between November 2011 and January 2012 which was later dissolved. The next parliamentary election will be held within 6 months of the constitution's ratification on 18 January 2014. Originally, the parliament was to be formed before the president was elected, but interim president Adly Mansour pushed the date. The Egyptian presidential election, 2014, took place on 26–28 May 2014. Official figures showed a turnout of 25,578,233 or 47.5%, with Abdel Fattah el-Sisi winning with 23.78 million votes, or 96.91% compared to 757,511 (3.09%) for Hamdeen Sabahi. After Morsi was ousted by the military, the judiciary system aligned itself with the new government, actively suopporting the repression of Muslim Brotherhood members. This resulted in a sharp increase in mass death sentences that arose criticism from the US president Barack Obama and the General Secretary of the UN, Ban Ki Moon. In April 2013, one judge of the Minya governatorate of Upper Egypt, sentenced 1,212 people to death. In December 2014 the judge Mohammed Nagi Shahata, notorious for his fierceness in passing on death sentences, condemened to the capital penalty 188 members of the Muslim Brotherhood, for assaulting a police station. Various Egyptian and international human rights organisations have already pointed out the lack of fair trials, that often last only a few minutes and do not take into consideration the procedural standards of fair trials. In 1958, Egypt and Syria formed a sovereign union known as the United Arab Republic. The union was short-lived, ending in 1961 when Syria seceded, thus ending the union. During most of its existence, the United Arab Republic was also in a loose confederation with North Yemen (or the Mutawakkilite Kingdom of Yemen), known as the United Arab States. In 1959, the All-Palestine Government of the Gaza Strip, an Egyptian client state, was absorbed into the United Arab Republic under the pretext of Arab union, and was never restored. The Egyptians were one of the first major civilisations to codify design elements in art and architecture. Egyptian blue, also known as calcium copper silicate is a pigment used by Egyptians for thousands of years. It is considered to be the first synthetic pigment. The wall paintings done in the service of the Pharaohs followed a rigid code of visual rules and meanings. Egyptian civilisation is renowned for its colossal pyramids, temples and monumental tombs. Well-known examples are the Pyramid of Djoser designed by ancient architect and engineer Imhotep, the Sphinx, and the temple of Abu Simbel. Modern and contemporary Egyptian art can be as diverse as any works in the world art scene, from the vernacular architecture of Hassan Fathy and Ramses Wissa Wassef, to Mahmoud Mokhtar's sculptures, to the distinctive Coptic iconography of Isaac Fanous. The Cairo Opera House serves as the main performing arts venue in the Egyptian capital. The new government drafted and implemented a constitution in 1923 based on a parliamentary system. Saad Zaghlul was popularly elected as Prime Minister of Egypt in 1924. In 1936, the Anglo-Egyptian Treaty was concluded. Continued instability due to remaining British influence and increasing political involvement by the king led to the dissolution of the parliament in a military coup d'état known as the 1952 Revolution. The Free Officers Movement forced King Farouk to abdicate in support of his son Fuad. British military presence in Egypt lasted until 1954. During Mubarak's reign, the political scene was dominated by the National Democratic Party, which was created by Sadat in 1978. It passed the 1993 Syndicates Law, 1995 Press Law, and 1999 Nongovernmental Associations Law which hampered freedoms of association and expression by imposing new regulations and draconian penalties on violations.[citation needed] As a result, by the late 1990s parliamentary politics had become virtually irrelevant and alternative avenues for political expression were curtailed as well. Egypt (i/ˈiːdʒɪpt/; Arabic: مِصر‎ Miṣr, Egyptian Arabic: مَصر Maṣr, Coptic: Ⲭⲏⲙⲓ Khemi), officially the Arab Republic of Egypt, is a transcontinental country spanning the northeast corner of Africa and southwest corner of Asia, via a land bridge formed by the Sinai Peninsula. It is the world's only contiguous Eurafrasian nation. Most of Egypt's territory of 1,010,408 square kilometres (390,000 sq mi) lies within the Nile Valley. Egypt is a Mediterranean country. It is bordered by the Gaza Strip and Israel to the northeast, the Gulf of Aqaba to the east, the Red Sea to the east and south, Sudan to the south and Libya to the west. Egypt was producing 691,000 bbl/d of oil and 2,141.05 Tcf of natural gas (in 2013), which makes Egypt as the largest oil producer not member of the Organization of the Petroleum Exporting Countries (OPEC) and the second-largest dry natural gas producer in Africa. In 2013, Egypt was the largest consumer of oil and natural gas in Africa, as more than 20% of total oil consumption and more than 40% of total dry natural gas consumption in Africa. Also, Egypt possesses the largest oil refinery capacity in Africa 726,000 bbl/d (in 2012). Egypt is currently planning to build its first nuclear power plant in El Dabaa city, northern Egypt. Some consider koshari (a mixture of rice, lentils, and macaroni) to be the national dish. Fried onions can be also added to koshari. In addition, ful medames (mashed fava beans) is one of the most popular dishes. Fava bean is also used in making falafel (also known as ""ta'meyya""), which may have originated in Egypt and spread to other parts of the Middle East. Garlic fried with coriander is added to mulukhiyya, a popular green soup made from finely chopped jute leaves, sometimes with chicken or rabbit. Constitutional changes voted on 19 March 2007 prohibited parties from using religion as a basis for political activity, allowed the drafting of a new anti-terrorism law, authorised broad police powers of arrest and surveillance, and gave the president power to dissolve parliament and end judicial election monitoring. In 2009, Dr. Ali El Deen Hilal Dessouki, Media Secretary of the National Democratic Party (NDP), described Egypt as a ""pharaonic"" political system, and democracy as a ""long-term goal"". Dessouki also stated that ""the real center of power in Egypt is the military"". In 525 BC, the powerful Achaemenid Persians, led by Cambyses II, began their conquest of Egypt, eventually capturing the pharaoh Psamtik III at the battle of Pelusium. Cambyses II then assumed the formal title of pharaoh, but ruled Egypt from his home of Susa in Persia (modern Iran), leaving Egypt under the control of a satrapy. The entire Twenty-seventh Dynasty of Egypt, from 525 BC to 402 BC, save for Petubastis III, was an entirely Persian ruled period, with the Achaemenid kings all being granted the title of pharaoh. A few temporarily successful revolts against the Persians marked the fifth century BC, but Egypt was never able to permanently overthrow the Persians. Egypt recognises only three religions: Islam, Christianity, and Judaism. Other faiths and minority Muslim sects practised by Egyptians, such as the small Bahá'í and Ahmadi community, are not recognised by the state and face persecution since they are labelled as far right groups that threaten Egypt's national security. Individuals, particularly Baha'is and atheists, wishing to include their religion (or lack thereof) on their mandatory state issued identification cards are denied this ability (see Egyptian identification card controversy), and are put in the position of either not obtaining required identification or lying about their faith. A 2008 court ruling allowed members of unrecognised faiths to obtain identification and leave the religion field blank. Egyptian cinema became a regional force with the coming of sound. In 1936, Studio Misr, financed by industrialist Talaat Harb, emerged as the leading Egyptian studio, a role the company retained for three decades. For over 100 years, more than 4000 films have been produced in Egypt, three quarters of the total Arab production.[citation needed] Egypt is considered the leading country in the field of cinema in the Middle East. Actors from all over the Arab World seek to appear in the Egyptian cinema for the sake of fame. The Cairo International Film Festival has been rated as one of 11 festivals with a top class rating worldwide by the International Federation of Film Producers' Associations. Egypt has one of the oldest civilisations in the world. It has been in contact with many other civilisations and nations and has been through so many eras, starting from prehistoric age to the modern age, passing through so many ages such as; Pharonic, Roman, Greek, Islamic and many other ages. Because of this wide variation of ages, the continuous contact with other nations and the big number of conflicts Egypt had been through, at least 60 museums may be found in Egypt, mainly covering a wide area of these ages and conflicts. Cairo University is ranked as 401-500 according to the Academic Ranking of World Universities (Shanghai Ranking) and 551-600 according to QS World University Rankings. American University in Cairo is ranked as 360 according to QS World University Rankings and Al-Azhar University, Alexandria University and Ain Shams University fall in the 701+ range. Egypt is currently opening new research institutes for the aim of modernising research in the nation, the most recent example of which is Zewail City of Science and Technology."
Capital_punishment_in_the_United_States,"In a five-to-four decision, the Supreme Court struck down the impositions of the death penalty in each of the consolidated cases as unconstitutional. The five justices in the majority did not produce a common opinion or rationale for their decision, however, and agreed only on a short statement announcing the result. The narrowest opinions, those of Byron White and Potter Stewart, expressed generalized concerns about the inconsistent application of the death penalty across a variety of cases but did not exclude the possibility of a constitutional death penalty law. Stewart and William O. Douglas worried explicitly about racial discrimination in enforcement of the death penalty. Thurgood Marshall and William J. Brennan, Jr. expressed the opinion that the death penalty was proscribed absolutely by the Eighth Amendment as ""cruel and unusual"" punishment. Other states with long histories of no death penalty include Wisconsin (the only state with only one execution), Rhode Island (although later reintroduced, it was unused and abolished again), Maine, North Dakota, Minnesota, West Virginia, Iowa, and Vermont. The District of Columbia has also abolished the death penalty; it was last used in 1957. Oregon abolished the death penalty through an overwhelming majority in a 1964 public referendum but reinstated it in a 1984 joint death penalty/life imprisonment referendum by an even higher margin after a similar 1978 referendum succeeded but was not implemented due to judicial rulings. In 2010, bills to abolish the death penalty in Kansas and in South Dakota (which had a de facto moratorium at the time) were rejected. Idaho ended its de facto moratorium, during which only one volunteer had been executed, on November 18, 2011 by executing Paul Ezra Rhoades; South Dakota executed Donald Moeller on October 30, 2012, ending a de facto moratorium during which only two volunteers had been executed. Of the 12 prisoners whom Nevada has executed since 1976, 11 waived their rights to appeal. Kentucky and Montana have executed two prisoners against their will (KY: 1997 and 1999, MT: 1995 and 1998) and one volunteer, respectively (KY: 2008, MT: 2006). Colorado (in 1997) and Wyoming (in 1992) have executed only one prisoner, respectively. Puerto Rico's constitution expressly forbids capital punishment, stating ""The death penalty shall not exist"", setting it apart from all U.S. states and territories other than Michigan, which also has a constitutional prohibition (eleven other states and the District of Columbia have abolished capital punishment through statutory law). However, capital punishment is still applicable to offenses committed in Puerto Rico, if they fall under the jurisdiction of the federal government, though federal death penalty prosecutions there have generated significant controversy. Other capital crimes include: the use of a weapon of mass destruction resulting in death, espionage, terrorism, certain violations of the Geneva Conventions that result in the death of one or more persons, and treason at the federal level; aggravated rape in Louisiana, Florida, and Oklahoma; extortionate kidnapping in Oklahoma; aggravated kidnapping in Georgia, Idaho, Kentucky and South Carolina; aircraft hijacking in Alabama and Mississippi; assault by an escaping capital felon in Colorado; armed robbery in Georgia; drug trafficking resulting in a person's death in Florida; train wrecking which leads to a person's death, and perjury which leads to a person's death in California, Colorado, Idaho and Nebraska. The death penalty is sought and applied more often in some jurisdictions, not only between states but within states. A 2004 Cornell University study showed that while 2.5 percent of murderers convicted nationwide were sentenced to the death penalty, in Nevada 6 percent were given the death penalty. Texas gave 2 percent of murderers a death sentence, less than the national average. Texas, however, executed 40 percent of those sentenced, which was about four times higher than the national average. California had executed only 1 percent of those sentenced. In May 2014, Oklahoma Director of Corrections, Robert Patton, recommended an indefinite hold on executions in the state after the botched execution of African-American Clayton Lockett. The prisoner had to be tasered to restrain him prior to the execution, and the lethal injection missed a vein in his groin, resulting in Lockett regaining consciousness, trying to get up, and to speak, before dying of a heart attack 43 minutes later, after the attempted execution had been called off. In 2015, the state approved nitrogen asphyxiation as a method of execution. Around 1890, a political movement developed in the United States to mandate private executions. Several states enacted laws which required executions to be conducted within a ""wall"" or ""enclosure"" to ""exclude public view."" For example, in 1919, the Missouri legislature adopted a statute (L.1919, p. 781) which required, ""the sentence of death should be executed within the county jail, if convenient, and otherwise within an enclosure near the jail."" The Missouri law permitted the local sheriff to distribute passes to individuals (usually local citizens) whom he believed should witness the hanging, but the sheriffs – for various reasons – sometimes denied passes to individuals who wanted to watch. Missouri executions conducted after 1919 were not ""public"" because they were conducted behind closed walls, and the general public was not permitted to attend. After the September 2011 execution of Troy Davis, believed by many to be innocent, Richard Dieter, the director of the Death Penalty Information Center, said this case was a clear wake-up call to politicians across the United States. He said: ""They weren't expecting such passion from people in opposition to the death penalty. There's a widely held perception that all Americans are united in favor of executions, but this message came across loud and clear that many people are not happy with it."" Brian Evans of Amnesty International, which led the campaign to spare Davis's life, said that there was a groundswell in America of people ""who are tired of a justice system that is inhumane and inflexible and allows executions where there is clear doubts about guilt"". He predicted the debate would now be conducted with renewed energy. The moratorium ended on January 17, 1977 with the shooting of Gary Gilmore by firing squad in Utah. The first use of the electric chair after the moratorium was the electrocution of John Spenkelink in Florida on May 25, 1979. The first use of the gas chamber after the moratorium was the gassing of Jesse Bishop in Nevada on October 22, 1979. The first use of the gallows after the moratorium was the hanging of Westley Allan Dodd in Washington on January 5, 1993. The first use of lethal injection was on December 7, 1982, when Charles Brooks, Jr., was executed in Texas. In October 2009, the American Law Institute voted to disavow the framework for capital punishment that it had created in 1962, as part of the Model Penal Code, ""in light of the current intractable institutional and structural obstacles to ensuring a minimally adequate system for administering capital punishment."" A study commissioned by the institute had said that experience had proved that the goal of individualized decisions about who should be executed and the goal of systemic fairness for minorities and others could not be reconciled. Four states in the modern era, Nebraska in 2008, New York and Kansas in 2004, and Massachusetts in 1984, had their statutes ruled unconstitutional by state courts. The death rows of New York and Massachusetts were disestablished, and attempts to restore the death penalty were unsuccessful. Kansas successfully appealed State v. Kleypas, the Kansas Supreme Court decision that declared the state's death penalty statute unconstitutional, to the United States Supreme Court. Nebraska's death penalty statute was rendered ineffective on February 8, 2008 when the required method, electrocution, was ruled unconstitutional by the Nebraska Supreme Court. In 2009, Nebraska enacted a bill that changed its method of execution to lethal injection. In total, 156 prisoners have been either acquitted, or received pardons or commutations on the basis of possible innocence, between 1973 to 2015. Death penalty opponents often argue that this statistic shows how perilously close states have come to undertaking wrongful executions; proponents point out that the statistic refers only to those exonerated in law, and that the truly innocent may be a smaller number. Statistics likely understate the actual problem of wrongful convictions because once an execution has occurred there is often insufficient motivation and finance to keep a case open, and it becomes unlikely at that point that the miscarriage of justice will ever be exposed. Electrocution was the preferred method of execution during the 20th century. Electric chairs have commonly been nicknamed Old Sparky; however, Alabama's electric chair became known as the ""Yellow Mama"" due to its unique color. Some, particularly in Florida, were noted for malfunctions, which caused discussion of their cruelty and resulted in a shift to lethal injection as the preferred method of execution. Although lethal injection dominates as a method of execution, some states allow prisoners on death row to choose the method used to execute them. As noted in the introduction to this article, the American public has maintained its position of support for capital punishment for murder. However, when given a choice between the death penalty and life imprisonment without parole, support has traditionally been significantly lower than polling which has only mentioned the death penalty as a punishment. In 2010, for instance, one poll showed 49 percent favoring the death penalty and 46 percent favoring life imprisonment while in another 61% said they preferred another punishment to the death penalty. The highest level of support for the death penalty recorded overall was 80 percent in 1994 (16 percent opposed), and the lowest recorded was 42 percent in 1966 (47 percent opposed). On the question of the death penalty vs. life without parole, the strongest preference for the death penalty was 61 percent in 1997 (29 percent favoring life), and the lowest preference for the death penalty was 47 percent in 2006 (48 percent favoring life). Present-day statutes from across the nation use the same words and phrases, requiring modern executions to take place within a wall or enclosure to exclude public view. Connecticut General Statute § 54–100 requires death sentences to be conducted in an ""enclosure"" which ""shall be so constructed as to exclude public view."" Kentucky Revised Statute 431.220 and Missouri Revised Statute § 546.730 contain substantially identical language. New Mexico's former death penalty, since repealed, see N.M. Stat. § 31-14-12, required executions be conducted in a ""room or place enclosed from public view."" Similarly, a dormant Massachusetts law, see Mass. Gen. Law ch. 279 § 60, required executions to take place ""within an enclosure or building."" North Carolina General Statute § 15-188 requires death sentences to be executed ""within the walls"" of the penitentiary, as do Oklahoma Statute Title 22 § 1015 and Montana Code § 46-19-103. Ohio Revised Code § 2949.22 requires that ""[t]he enclosure shall exclude public view."" Similarly, Tennessee Code § 40-23-116 requires ""an enclosure"" for ""strict seclusion and privacy."" United States Code Title 18 § 3596 and the Code of Federal Regulations 28 CFR 26.4 limit the witnesses permitted at federal executions. At times when a death sentence is affirmed on direct review, it is considered final. Yet, supplemental methods to attack the judgment, though less familiar than a typical appeal, do remain. These supplemental remedies are considered collateral review, that is, an avenue for upsetting judgments that have become otherwise final. Where the prisoner received his death sentence in a state-level trial, as is usually the case, the first step in collateral review is state collateral review. (If the case is a federal death penalty case, it proceeds immediately from direct review to federal habeas corpus.) Although all states have some type of collateral review, the process varies widely from state to state. Generally, the purpose of these collateral proceedings is to permit the prisoner to challenge his sentence on grounds that could not have been raised reasonably at trial or on direct review. Most often these are claims, such as ineffective assistance of counsel, which requires the court to consider new evidence outside the original trial record, something courts may not do in an ordinary appeal. State collateral review, though an important step in that it helps define the scope of subsequent review through federal habeas corpus, is rarely successful in and of itself. Only around 6 percent of death sentences are overturned on state collateral review. In 2010, the death sentences of 53 inmates were overturned as a result of legal appeals or high court reversals. Traditionally, Section 1983 was of limited use for a state prisoner under sentence of death because the Supreme Court has held that habeas corpus, not Section 1983, is the only vehicle by which a state prisoner can challenge his judgment of death. In the 2006 Hill v. McDonough case, however, the United States Supreme Court approved the use of Section 1983 as a vehicle for challenging a state's method of execution as cruel and unusual punishment in violation of the Eighth Amendment. The theory is that a prisoner bringing such a challenge is not attacking directly his judgment of death, but rather the means by which that the judgment will be carried out. Therefore, the Supreme Court held in the Hill case that a prisoner can use Section 1983 rather than habeas corpus to bring the lawsuit. Yet, as Clarence Hill's own case shows, lower federal courts have often refused to hear suits challenging methods of execution on the ground that the prisoner brought the claim too late and only for the purposes of delay. Further, the Court's decision in Baze v. Rees, upholding a lethal injection method used by many states, has drastically narrowed the opportunity for relief through Section 1983. The United States Supreme Court in Penry v. Lynaugh and the United States Court of Appeals for the Fifth Circuit in Bigby v. Dretke have been clear in their decisions that jury instructions in death penalty cases that do not ask about mitigating factors regarding the defendant's mental health violate the defendant's Eighth Amendment rights, saying that the jury is to be instructed to consider mitigating factors when answering unrelated questions. This ruling suggests that specific explanations to the jury are necessary to weigh mitigating factors. All of the executions which have taken place since the 1936 hanging of Bethea in Owensboro have been conducted within a wall or enclosure. For example, Fred Adams was legally hanged in Kennett, Missouri, on April 2, 1937, within a 10-foot (3 m) wooden stockade. Roscoe ""Red"" Jackson was hanged within a stockade in Galena, Missouri, on May 26, 1937. Two Kentucky hangings were conducted after Galena in which numerous persons were present within a wooden stockade, that of John ""Peter"" Montjoy in Covington, Kentucky on December 17, 1937, and that of Harold Van Venison in Covington on June 3, 1938. An estimated 400 witnesses were present for the hanging of Lee Simpson in Ryegate, Montana, on December 30, 1939. The execution of Timothy McVeigh on June 11, 2001 was witnessed by some 300 people, some by closed-circuit television. Pharmaceutical companies whose products are used in the three-drug cocktails for lethal injections are predominantly European, and they have strenuously objected to the use of their drugs for executions and taken steps to prevent their use. For example, Hospira, the sole American manufacturer of sodium thiopental, the critical anesthetic in the three-drug cocktail, announced in 2011 that it would no longer manufacture the drug for the American market, in part for ethical reasons and in part because its transfer of sodium thiopental manufacturing to Italy would subject it to the European Union's Torture Regulation, which forbids the use of any product manufactured within the Union for torture (as execution by lethal injection is considered by the Regulation). Since the drug manufacturers began taking these steps and the EU regulation ended the importation of drugs produced in Europe, the resulting shortage of execution drugs has led to or influenced decisions to impose moratoria in Arkansas, California, Kentucky, Louisiana, Mississippi, Montana, Nevada, North Carolina, and Tennessee. African Americans made up 41 percent of death row inmates while making up only 12.6 percent of the general population. (They have made up 34 percent of those actually executed since 1976.) However, that number is lower than that of prison inmates, which is 47 percent. According to the US Department of Justice, African Americans accounted for 52.5% of homicide offenders from 1980 to 2008, with whites 45.3% and Native Americans and Asians 2.2%. This means African Americans are less likely to be executed on a per capita basis. However, according to a 2003 Amnesty International report, blacks and whites were the victims of murder in almost equal numbers, yet 80 percent of the people executed since 1977 were convicted of murders involving white victims. 13.5% of death row inmates are of Hispanic or Latino descent, while they make up 17.4% of the general population. Since 1642 (in the 13 colonies, the United States under the Articles of Confederation, and the current United States) an estimated 364 juvenile offenders have been put to death by the states and the federal government. The earliest known execution of a prisoner for crimes committed as a juvenile was Thomas Graunger in 1642. Twenty-two of the executions occurred after 1976, in seven states. Due to the slow process of appeals, it was highly unusual for a condemned person to be under 18 at the time of execution. The youngest person to be executed in the 20th century was George Stinney, who was electrocuted in South Carolina at the age of 14 on June 16, 1944. The last execution of a juvenile may have been Leonard Shockley, who died in the Maryland gas chamber on April 10, 1959, at the age of 17. No one has been under age 19 at time of execution since at least 1964. Since the reinstatement of the death penalty in 1976, 22 people have been executed for crimes committed under the age of 18. Twenty-one were 17 at the time of the crime. The last person to be executed for a crime committed as a juvenile was Scott Hain on April 3, 2003 in Oklahoma. The last use of the firing squad between 1608 and the moratorium on judicial executions between 1967 and 1977 was when Utah shot James W. Rodgers on March 30, 1960. The last use of the gallows between 1608 and the moratorium was when Kansas hanged George York on June 22, 1965. The last use of the electric chair between the first electrocution on August 6, 1890 and the moratorium was when Oklahoma electrocuted James French on August 10, 1966. The last use of the gas chamber between the first gassing on February 8, 1924 and the moratorium was when Colorado gassed Luis Monge on June 2, 1967. Sixteen was held to be the minimum permissible age in the 1988 Supreme Court decision of Thompson v. Oklahoma. The Court, considering the case Roper v. Simmons in March 2005, found the execution of juvenile offenders unconstitutional by a 5–4 margin, effectively raising the minimum permissible age to 18. State laws have not been updated to conform with this decision. In the American legal system, unconstitutional laws do not need to be repealed; instead, they are held to be unenforceable. (See also List of juvenile offenders executed in the United States) Several states have never had capital punishment, the first being Michigan, which abolished it shortly after entering the Union. (However, the United States government executed Tony Chebatoris at the Federal Correctional Institution in Milan, Michigan in 1938.) Article 4, Section 46 of Michigan's fourth Constitution (ratified in 1963; effective in 1964) prohibits any law providing for the penalty of death. Attempts to change the provision have failed. In 2004, a constitutional amendment proposed to allow capital punishment in some circumstances failed to make it on the November ballot after a resolution failed in the legislature and a public initiative failed to gather enough signatures. Congress acted defiantly toward the Supreme Court by passing the Drug Kingpin Act of 1988 and the Federal Death Penalty Act of 1994 that made roughly fifty crimes punishable by death, including crimes that do not always involve the death of someone. Such non-death capital offenses include treason, espionage (spying for another country), and high-level drug trafficking. Since no one has yet been sentenced to death for such non-death capital offenses, the Supreme Court has not ruled on their constitutionality. James Liebman, a professor of law at Columbia Law School, stated in 1996 that his study found that when habeas corpus petitions in death penalty cases were traced from conviction to completion of the case that there was ""a 40 percent success rate in all capital cases from 1978 to 1995."" Similarly, a study by Ronald Tabak in a law review article puts the success rate in habeas corpus cases involving death row inmates even higher, finding that between ""1976 and 1991, approximately 47 percent of the habeas petitions filed by death row inmates were granted."" The different numbers are largely definitional, rather than substantive. Freedam's statistics looks at the percentage of all death penalty cases reversed, while the others look only at cases not reversed prior to habeas corpus review. Capital punishment was suspended in the United States from 1972 through 1976 primarily as a result of the Supreme Court's decision in Furman v. Georgia. The last pre-Furman execution was that of Luis Monge on June 2, 1967. In this case, the court found that the death penalty was being imposed in an unconstitutional manner, on the grounds of cruel and unusual punishment in violation of the Eighth Amendment to the United States Constitution. The Supreme Court has never ruled the death penalty to be per se unconstitutional. In 1976, contemporaneously with Woodson and Roberts, the Court decided Gregg v. Georgia and upheld a procedure in which the trial of capital crimes was bifurcated into guilt-innocence and sentencing phases. At the first proceeding, the jury decides the defendant's guilt; if the defendant is innocent or otherwise not convicted of first-degree murder, the death penalty will not be imposed. At the second hearing, the jury determines whether certain statutory aggravating factors exist, whether any mitigating factors exist, and, in many jurisdictions, weigh the aggravating and mitigating factors in assessing the ultimate penalty – either death or life in prison, either with or without parole. Under the Antiterrorism and Effective Death Penalty Act of 1996, a state prisoner is ordinarily only allowed one suit for habeas corpus in federal court. If the federal courts refuse to issue a writ of habeas corpus, an execution date may be set. In recent times, however, prisoners have postponed execution through a final round of federal litigation using the Civil Rights Act of 1871 — codified at 42 U.S.C. § 1983 — which allows people to bring lawsuits against state actors to protect their federal constitutional and statutory rights. The legal administration of the death penalty in the United States is complex. Typically, it involves four critical steps: (1) sentencing, (2) direct review, (3) state collateral review, and (4) federal habeas corpus. Recently, a narrow and final fifth level of process – (5) the Section 1983 challenge – has become increasingly important. (Clemency or pardon, through which the Governor or President of the jurisdiction can unilaterally reduce or abrogate a death sentence, is an executive rather than judicial process.) The number of new death sentences handed down peaked in 1995–1996 (309). There were 73 new death sentences handed down in 2014, the lowest number since 1973 (44). The method of execution of federal prisoners for offenses under the Violent Crime Control and Law Enforcement Act of 1994 is that of the state in which the conviction took place. If the state has no death penalty, the judge must choose a state with the death penalty for carrying out the execution. For offenses under the Drug Kingpin Act of 1988, the method of execution is lethal injection. The Federal Correctional Complex in Terre Haute, Indiana is currently the home of the only death chamber for federal death penalty recipients in the United States, where inmates are put to death by lethal injection. The complex has so far been the only location used for federal executions post-Gregg. Timothy McVeigh and Juan Garza were put to death in June 2001, and Louis Jones, Jr. was put to death on March 18, 2003. In the decades since Furman, new questions have emerged about whether or not prosecutorial arbitrariness has replaced sentencing arbitrariness. A study by Pepperdine University School of Law published in Temple Law Review, ""Unpredictable Doom and Lethal Injustice: An Argument for Greater Transparency in Death Penalty Decisions,"" surveyed the decision-making process among prosecutors in various states. The authors found that prosecutors' capital punishment filing decisions remain marked by local ""idiosyncrasies,"" suggesting they are not in keeping with the spirit of the Supreme Court's directive. This means that ""the very types of unfairness that the Supreme Court sought to eliminate"" may still ""infect capital cases."" Wide prosecutorial discretion remains because of overly broad criteria. California law, for example, has 22 ""special circumstances,"" making nearly all premeditated murders potential capital cases. The 32 death penalty states have varying numbers and types of ""death qualifiers"" – circumstances that allow for capital charges. The number varies from a high of 34 in California to 22 in Colorado and Delaware to 12 in Texas, Nebraska, Georgia and Montana. The study's authors call for reform of state procedures along the lines of reforms in the federal system, which the U.S. Department of Justice initiated with a 1995 protocol. Crimes subject to the death penalty vary by jurisdiction. All jurisdictions that use capital punishment designate the highest grade of murder a capital crime, although most jurisdictions require aggravating circumstances. Treason against the United States, as well as treason against the states of Arkansas, California, Georgia, Louisiana, Mississippi, and Missouri are capital offenses. In the 2010s, American jurisdictions have experienced a shortage of lethal injection drugs, due to anti-death penalty advocacy and low production volume. Hospira, the only U.S. manufacturer of sodium thiopental, stopped making the drug in 2011. The European Union has outlawed the export of any product that could be used in an execution; this has prevented executioners from using EU-manufactured anesthetics like propofol which are needed for general medical purposes. Another alternative, pentobarbital, is also only manufactured in the European Union, which has caused the Danish producer to restrict distribution to U.S. government customers. If a defendant is sentenced to death at the trial level, the case then goes into a direct review. The direct review process is a typical legal appeal. An appellate court examines the record of evidence presented in the trial court and the law that the lower court applied and decides whether the decision was legally sound or not. Direct review of a capital sentencing hearing will result in one of three outcomes. If the appellate court finds that no significant legal errors occurred in the capital sentencing hearing, the appellate court will affirm the judgment, or let the sentence stand. If the appellate court finds that significant legal errors did occur, then it will reverse the judgment, or nullify the sentence and order a new capital sentencing hearing. Lastly, if the appellate court finds that no reasonable juror could find the defendant eligible for the death penalty, a rarity, then it will order the defendant acquitted, or not guilty, of the crime for which he/she was given the death penalty, and order him sentenced to the next most severe punishment for which the offense is eligible. About 60 percent survive the process of direct review intact. Opponents argue that the death penalty is not an effective means of deterring crime, risks the execution of the innocent, is unnecessarily barbaric in nature, cheapens human life, and puts a government on the same base moral level as those criminals involved in murder. Furthermore, some opponents argue that the arbitrariness with which it is administered and the systemic influence of racial, socio-economic, geographic, and gender bias on determinations of desert make the current practice of capital punishment immoral and illegitimate. Executions resumed on January 17, 1977, when Gary Gilmore went before a firing squad in Utah. But the pace was quite slow due to the use of litigation tactics which involved filing repeated writs of habeas corpus, which succeeded for many in delaying their actual execution for many years. Although hundreds of individuals were sentenced to death in the United States during the 1970s and early 1980s, only ten people besides Gilmore (who had waived all of his appeal rights) were actually executed prior to 1984. After a death sentence is affirmed in state collateral review, the prisoner may file for federal habeas corpus, which is a unique type of lawsuit that can be brought in federal courts. Federal habeas corpus is a species of collateral review, and it is the only way that state prisoners may attack a death sentence in federal court (other than petitions for certiorari to the United States Supreme Court after both direct review and state collateral review). The scope of federal habeas corpus is governed by the Antiterrorism and Effective Death Penalty Act of 1996, which restricted significantly its previous scope. The purpose of federal habeas corpus is to ensure that state courts, through the process of direct review and state collateral review, have done at least a reasonable job in protecting the prisoner's federal constitutional rights. Prisoners may also use federal habeas corpus suits to bring forth new evidence that they are innocent of the crime, though to be a valid defense at this late stage in the process, evidence of innocence must be truly compelling. Various methods have been used in the history of the American colonies and the United States but only five methods are currently used. Historically, burning, crushing, breaking on wheel, and bludgeoning were used for a small number of executions, while hanging was the most common method. The last person burned at the stake was a black slave in South Carolina in August 1825. The last person to be hanged in chains was a murderer named John Marshall in West Virginia on April 4, 1913. Although beheading was a legal method in Utah from 1851 to 1888, it was never used. Previous post-Furman mass clemencies took place in 1986 in New Mexico, when Governor Toney Anaya commuted all death sentences because of his personal opposition to the death penalty. In 1991, outgoing Ohio Governor Dick Celeste commuted the sentences of eight prisoners, among them all four women on the state's death row. And during his two terms (1979–1987) as Florida's Governor, Bob Graham, although a strong death penalty supporter who had overseen the first post-Furman involuntary execution as well as 15 others, agreed to commute the sentences of six people on the grounds of ""possible innocence"" or ""disproportionality."" The largest single execution in United States history was the hanging of 38 American Indians convicted of murder and rape during the Dakota War of 1862. They were executed simultaneously on December 26, 1862, in Mankato, Minnesota. A single blow from an axe cut the rope that held the large four-sided platform, and the prisoners (except for one whose rope had broken and who had to be re-hanged) fell to their deaths. The second-largest mass execution was also a hanging: the execution of 13 African-American soldiers for taking part in the Houston Riot of 1917. The largest non-military mass execution occurred in one of the original thirteen colonies in 1723, when 26 convicted pirates were hanged in Newport, Rhode Island by order of the Admiralty Court. Within the context of the overall murder rate, the death penalty cannot be said to be widely or routinely used in the United States; in recent years the average has been about one execution for about every 700 murders committed, or 1 execution for about every 325 murder convictions. However, 32 of the 50 states still execute people. Among them, Alabama has the highest per capita rate of death sentences. This is due to judges overriding life imprisonment sentences and imposing the death penalty. No other states allow this. As of November 2008, there is only one person on death row facing capital punishment who has not been convicted of murder. Demarcus Sears remains under a death sentence in Georgia for the crime of ""kidnapping with bodily injury."" Sears was convicted in 1986 for the kidnapping and bodily injury of victim Gloria Ann Wilbur. Wilbur was kidnapped and beaten in Georgia, raped in Tennessee, and murdered in Kentucky. Sears was never charged with the murder of Wilbur in Kentucky, but was sentenced to death by a jury in Georgia for ""kidnapping with bodily injury."" In 1977, the Supreme Court's Coker v. Georgia decision barred the death penalty for rape of an adult woman, and implied that the death penalty was inappropriate for any offense against another person other than murder. Prior to the decision, the death penalty for rape of an adult had been gradually phased out in the United States, and at the time of the decision, the State of Georgia and the U.S. Federal government were the only two jurisdictions to still retain the death penalty for that offense. However, three states maintained the death penalty for child rape, as the Coker decision only imposed a ban on executions for the rape of an adult woman. In 2008, the Kennedy v. Louisiana decision barred the death penalty for child rape. The result of these two decisions means that the death penalty in the United States is largely restricted to cases where the defendant took the life of another human being. The current federal kidnapping statute, however, may be exempt because the death penalty applies if the victim dies in the perpetrator's custody, not necessarily by his hand, thus stipulating a resulting death, which was the wording of the objection. In addition, the Federal government retains the death penalty for non-murder offenses that are considered crimes against the state, including treason, espionage, and crimes under military jurisdiction. Possibly in part due to expedited federal habeas corpus procedures embodied in the Antiterrorism and Effective Death Penalty Act of 1996, the pace of executions picked up, reaching a peak of 98 in 1999 and then they declined gradually to 28 in 2015. Since the death penalty was reauthorized in 1976, 1,411 people have been executed, almost exclusively by the states, with most occurring after 1990. Texas has accounted for over one-third of modern executions (although only two death sentences were imposed in Texas during 2015, with the courts preferring to issue sentences of life without parole instead) and over four times as many as Oklahoma, the state with the second-highest number. California has the greatest number of prisoners on death row, has issued the highest number of death sentences but has held relatively few executions. In New Jersey and Illinois, all death row inmates had their sentences commuted to life in prison without parole when the death penalty repeal bills were signed into law. In Maryland, Governor Martin O'Malley commuted the state's four remaining death sentences to life in prison without parole in January 2015. While the bill repealing capital punishment in Connecticut was not retroactive, the Connecticut Supreme Court ruled in 2015 in State v. Santiago that the legislature's decision to prospectively abolish capital punishment rendered it an offense to ""evolving standards of decency,"" thus commuting the sentences of the 11 men remaining on death row to life in prison without parole. New Mexico may yet execute two condemned inmates sentenced prior to abolition, and Nebraska has ten death row inmates who may still be executed despite abolition."
"Charleston,_South_Carolina","The first settlers primarily came from England, its Caribbean colony of Barbados, and its Atlantic colony of Bermuda. Among these were free people of color, born in the West Indies of alliances and marriages between Africans and Englanders, when color lines were looser among the working class in the early colonial years, and some wealthy whites took black consorts or concubines. Charles Town attracted a mixture of ethnic and religious groups. French, Scottish, Irish, and Germans migrated to the developing seacoast town, representing numerous Protestant denominations. Because of the battles between English ""royalty"" and the Roman Catholic Church, practicing Catholics were not allowed to settle in South Carolina until after the American Revolution. Jews were allowed, and Sephardic Jews migrated to the city in such numbers that by the beginning of the 19th century, the city was home to the largest and wealthiest Jewish community in North America—a status it held until about 1830. By the mid-18th century, Charles Town had become a bustling trade center, the hub of the Atlantic trade for the southern colonies. Charles Towne was also the wealthiest and largest city south of Philadelphia, in part because of the lucrative slave trade. By 1770, it was the fourth-largest port in the colonies, after Boston, New York, and Philadelphia, with a population of 11,000—slightly more than half of them slaves. By 1708, the majority of the colony's population was slaves, and the future state would continue to be a majority of African descent until after the Great Migration of the early 20th century. Interstate 26 begins in downtown Charleston, with exits to the Septima Clark Expressway, the Arthur Ravenel, Jr. Bridge and Meeting Street. Heading northwest, it connects the city to North Charleston, the Charleston International Airport, Interstate 95, and Columbia. The Arthur Ravenel, Jr. Bridge and Septima Clark Expressway are part of U.S. Highway 17, which travels east-west through the cities of Charleston and Mount Pleasant. The Mark Clark Expressway, or Interstate 526, is the bypass around the city and begins and ends at U.S. Highway 17. U.S. Highway 52 is Meeting Street and its spur is East Bay Street, which becomes Morrison Drive after leaving the east side. This highway merges with King Street in the city's Neck area (industrial district). U.S. Highway 78 is King Street in the downtown area, eventually merging with Meeting Street. The early settlement was often subject to attack from sea and land, including periodic assaults from Spain and France (both of whom contested England's claims to the region), and pirates. These were combined with raids by Native Americans, who tried to protect themselves from so-called European ""settlers,"" who in turn wanted to expand the settlement. The heart of the city was fortified according to a 1704 plan by Governor Johnson. Except those fronting Cooper River, the walls were largely removed during the 1720s. The city also had a large class of free people of color. By 1860, 3,785 free people of color were in Charleston, nearly 18% of the city's black population, and 8% of the total population. Free people of color were far more likely to be of mixed racial background than slaves. Many were educated, practiced skilled crafts, and some even owned substantial property, including slaves. In 1790, they established the Brown Fellowship Society for mutual aid, initially as a burial society. It continued until 1945. The traditional Charleston accent has long been noted in the state and throughout the South. It is typically heard in wealthy white families who trace their families back generations in the city. It has ingliding or monophthongal long mid-vowels, raises ay and aw in certain environments, and is nonrhotic. Sylvester Primer of the College of Charleston wrote about aspects of the local dialect in his late 19th-century works: ""Charleston Provincialisms"" (1887)  and ""The Huguenot Element in Charleston's Provincialisms"", published in a German journal. He believed the accent was based on the English as it was spoken by the earliest settlers, therefore derived from Elizabethan England and preserved with modifications by Charleston speakers. The rapidly disappearing ""Charleston accent"" is still noted in the local pronunciation of the city's name. Some elderly (and usually upper-class) Charleston natives ignore the 'r' and elongate the first vowel, pronouncing the name as ""Chah-l-ston"". Some observers attribute these unique features of Charleston's speech to its early settlement by French Huguenots and Sephardic Jews (who were primarily English speakers from London), both of whom played influential roles in Charleston's early development and history.[citation needed] During this period, the Weapons Station was the Atlantic Fleet's loadout base for all nuclear ballistic missile submarines. Two SSBN ""Boomer"" squadrons and a submarine tender were homeported at the Weapons Station, while one SSN attack squadron, Submarine Squadron 4, and a submarine tender were homeported at the Naval Base. At the 1996 closure of the station's Polaris Missile Facility Atlantic (POMFLANT), over 2,500 nuclear warheads and their UGM-27 Polaris, UGM-73 Poseidon, and UGM-96 Trident I delivery missiles (SLBM) were stored and maintained, guarded by a U.S. Marine Corps security force company. In Charleston, the African American population increased as freedmen moved from rural areas to the major city: from 17,000 in 1860 to over 27,000 in 1880. Historian Eric Foner noted that blacks were glad to be relieved of the many regulations of slavery and to operate outside of white surveillance. Among other changes, most blacks quickly left the Southern Baptist Church, setting up their own black Baptist congregations or joining new African Methodist Episcopal Church and AME Zion churches, both independent black denominations first established in the North. Freedmen ""acquired dogs, guns, and liquor (all barred to them under slavery), and refused to yield the sidewalks to whites"". In 1832, South Carolina passed an ordinance of nullification, a procedure by which a state could, in effect, repeal a federal law; it was directed against the most recent tariff acts. Soon, federal soldiers were dispensed to Charleston's forts, and five United States Coast Guard cutters were detached to Charleston Harbor ""to take possession of any vessel arriving from a foreign port, and defend her against any attempt to dispossess the Customs Officers of her custody until all the requirements of law have been complied with."" This federal action became known as the Charleston incident. The state's politicians worked on a compromise law in Washington to gradually reduce the tariffs. Charleston has one official sister city, Spoleto, Umbria, Italy. The relationship between the two cities began when Pulitzer Prize-winning Italian composer Gian Carlo Menotti selected Charleston as the city to host the American version of Spoleto's annual Festival of Two Worlds. ""Looking for a city that would provide the charm of Spoleto, as well as its wealth of theaters, churches, and other performance spaces, they selected Charleston, South Carolina, as the ideal location. The historic city provided a perfect fit: intimate enough that the Festival would captivate the entire city, yet cosmopolitan enough to provide an enthusiastic audience and robust infrastructure."" The City of Charleston is served by the Charleston International Airport. It is located in the City of North Charleston and is about 12 miles (20 km) northwest of downtown Charleston. It is the busiest passenger airport in South Carolina (IATA: CHS, ICAO: KCHS). The airport shares runways with the adjacent Charleston Air Force Base. Charleston Executive Airport is a smaller airport located in the John's Island section of the city of Charleston and is used by noncommercial aircraft. Both airports are owned and operated by the Charleston County Aviation Authority. The community was established by several shiploads of settlers from Bermuda (which lies due east of South Carolina, although at 1,030 km or 640 mi, it is closest to Cape Hatteras, North Carolina), under the leadership of governor William Sayle, on the west bank of the Ashley River, a few miles northwest of the present-day city center. It was soon predicted by the Earl of Shaftesbury, one of the Lords Proprietors, to become a ""great port towne"", a destiny the city quickly fulfilled. In 1680, the settlement was moved east of the Ashley River to the peninsula between the Ashley and Cooper Rivers. Not only was this location more defensible, but it also offered access to a fine natural harbor. As many as five bands were on tour during the 1920s. The Jenkins Orphanage Band played in the inaugural parades of Presidents Theodore Roosevelt and William Taft and toured the USA and Europe. The band also played on Broadway for the play ""Porgy"" by DuBose and Dorothy Heyward, a stage version of their novel of the same title. The story was based in Charleston and featured the Gullah community. The Heywards insisted on hiring the real Jenkins Orphanage Band to portray themselves on stage. Only a few years later, DuBose Heyward collaborated with George and Ira Gershwin to turn his novel into the now famous opera, Porgy and Bess (so named so as to distinguish it from the play). George Gershwin and Heyward spent the summer of 1934 at Folly Beach outside of Charleston writing this ""folk opera"", as Gershwin called it. Porgy and Bess is considered the Great American Opera[citation needed] and is widely performed. The Jenkins Orphanage was established in 1891 by the Rev. Daniel J. Jenkins in Charleston. The orphanage accepted donations of musical instruments and Rev. Jenkins hired local Charleston musicians and Avery Institute Graduates to tutor the boys in music. As a result, Charleston musicians became proficient on a variety of instruments and were able to read music expertly. These traits set Jenkins musicians apart and helped land some of them positions in big bands with Duke Ellington and Count Basie. William ""Cat"" Anderson, Jabbo Smith, and Freddie Green are but a few of the alumni from the Jenkins Orphanage band who became professional musicians in some of the best bands of the day. Orphanages around the country began to develop brass bands in the wake of the Jenkins Orphanage Band's success. At the Colored Waif's Home Brass Band in New Orleans, for example, a young trumpeter named Louis Armstrong first began to draw attention. Charles Town was a hub of the deerskin trade, the basis of its early economy. Trade alliances with the Cherokee and Creek nations insured a steady supply of deer hides. Between 1699 and 1715, colonists exported an average of 54,000 deer skins annually to Europe through Charles Town. Between 1739 and 1761, the height of the deerskin trade era, an estimated 500,000 to 1,250,000 deer were slaughtered. During the same period, Charles Town records show an export of 5,239,350 pounds of deer skins. Deer skins were used in the production of men's fashionable and practical buckskin pantaloons, gloves, and book bindings. The City of Charleston Fire Department consists over 300 full-time firefighters. These firefighters operate out of 19 companies located throughout the city: 16 engine companies, two tower companies, and one ladder company. Training, Fire Marshall, Operations, and Administration are the divisions of the department. The department operates on a 24/48 schedule and had a Class 1 ISO rating until late 2008, when ISO officially lowered it to Class 3. Russell (Rusty) Thomas served as Fire Chief until June 2008, and was succeeded by Chief Thomas Carr in November 2008. On August 31, 1886, Charleston was nearly destroyed by an earthquake. The shock was estimated to have a moment magnitude of 7.0 and a maximum Mercalli intensity of X (Extreme). It was felt as far away as Boston to the north, Chicago and Milwaukee to the northwest, as far west as New Orleans, as far south as Cuba, and as far east as Bermuda. It damaged 2,000 buildings in Charleston and caused $6 million worth of damage ($133 million in 2006 dollars), at a time when all the city's buildings were valued around $24 million ($531 million in 2006 dollars). Charleston's oldest community theater group, the Footlight Players, has provided theatrical productions since 1931. A variety of performing arts venues includes the historic Dock Street Theatre. The annual Charleston Fashion Week held each spring in Marion Square brings in designers, journalists, and clients from across the nation. Charleston is known for its local seafood, which plays a key role in the city's renowned cuisine, comprising staple dishes such as gumbo, she-crab soup, fried oysters, Lowcountry boil, deviled crab cakes, red rice, and shrimp and grits. Rice is the staple in many dishes, reflecting the rice culture of the Low Country. The cuisine in Charleston is also strongly influenced by British and French elements. Public institutions of higher education in Charleston include the College of Charleston (the nation's 13th-oldest university), The Citadel, The Military College of South Carolina, and the Medical University of South Carolina. The city is also home to private universities, including the Charleston School of Law . Charleston is also home to the Roper Hospital School of Practical Nursing, and the city has a downtown satellite campus for the region's technical school, Trident Technical College. Charleston is also the location for the only college in the country that offers bachelor's degrees in the building arts, The American College of the Building Arts. The Art Institute of Charleston, located downtown on North Market Street, opened in 2007. The City of Charleston Police Department, with a total of 452 sworn officers, 137 civilians, and 27 reserve police officers, is South Carolina's largest police department. Their procedures on cracking down on drug use and gang violence in the city are used as models to other cities to do the same.[citation needed] According to the final 2005 FBI Crime Reports, Charleston crime level is worse than the national average in almost every major category. Greg Mullen, the former Deputy Chief of the Virginia Beach, Virginia Police Department, serves as the current Chief of the Charleston Police Department. The former Charleston police chief was Reuben Greenberg, who resigned August 12, 2005. Greenberg was credited with creating a polite police force that kept police brutality well in check, even as it developed a visible presence in community policing and a significant reduction in crime rates. On June 17, 2015, 21-year-old Dylann Roof entered the historic Emanuel African Methodist Episcopal Church during a Bible study and killed nine people. Senior pastor Clementa Pinckney, who also served as a state senator, was among those killed during the attack. The deceased also included congregation members Susie Jackson, 87; Rev. Daniel Simmons Sr., 74; Ethel Lance, 70; Myra Thompson, 59; Cynthia Hurd, 54; Rev. Depayne Middleton-Doctor, 49; Rev. Sharonda Coleman-Singleton, 45; and Tywanza Sanders, 26. The attack garnered national attention, and sparked a debate on historical racism, Confederate symbolism in Southern states, and gun violence. On July 10, 2015, the Confederate battle flag was removed from the South Carolina State House. A memorial service on the campus of the College of Charleston was attended by President Barack Obama, Michelle Obama, Vice President Joe Biden, Jill Biden, and Speaker of the House John Boehner. Charleston is the oldest and second-largest city in the U.S. state of South Carolina, the county seat of Charleston County, and the principal city in the Charleston–North Charleston–Summerville Metropolitan Statistical Area. The city lies just south of the geographical midpoint of South Carolina's coastline and is located on Charleston Harbor, an inlet of the Atlantic Ocean formed by the confluence of the Ashley and Cooper Rivers, or, as is locally expressed, ""where the Cooper and Ashley Rivers come together to form the Atlantic Ocean."" The traditional parish system persisted until the Reconstruction Era, when counties were imposed.[citation needed] Nevertheless, traditional parishes still exist in various capacities, mainly as public service districts. When the city of Charleston was formed, it was defined by the limits of the Parish of St. Philip and St. Michael, now also includes parts of St. James' Parish, St. George's Parish, St. Andrew's Parish, and St. John's Parish, although the last two are mostly still incorporated rural parishes. By 1820, Charleston's population had grown to 23,000, maintaining its black (and mostly slave) majority. When a massive slave revolt planned by Denmark Vesey, a free black, was revealed in May 1822, whites reacted with intense fear, as they were well aware of the violent retribution of slaves against whites during the Haitian Revolution. Soon after, Vesey was tried and executed, hanged in early July with five slaves. Another 28 slaves were later hanged. Later, the state legislature passed laws requiring individual legislative approval for manumission (the freeing of a slave) and regulating activities of free blacks and slaves. Founded in 1670 as Charles Town in honor of King Charles II of England, Charleston adopted its present name in 1783. It moved to its present location on Oyster Point in 1680 from a location on the west bank of the Ashley River known as Albemarle Point. By 1690, Charles Town was the fifth-largest city in North America, and it remained among the 10 largest cities in the United States through the 1840 census. With a 2010 census population of 120,083  (and a 2014 estimate of 130,113), current trends put Charleston as the fastest-growing municipality in South Carolina. The population of the Charleston metropolitan area, comprising Berkeley, Charleston, and Dorchester Counties, was counted by the 2014 estimate at 727,689 – the third-largest in the state – and the 78th-largest metropolitan statistical area in the United States. After Charles II of England (1630–1685) was restored to the English throne in 1660 following Oliver Cromwell's Protectorate, he granted the chartered Province of Carolina to eight of his loyal friends, known as the Lords Proprietors, on March 24, 1663. It took seven years before the group arranged for settlement expeditions. The first of these founded Charles Town, in 1670. Governance, settlement, and development were to follow a visionary plan known as the Grand Model prepared for the Lords Proprietors by John Locke. Colonial Lowcountry landowners experimented with cash crops ranging from tea to silkworms. African slaves brought knowledge of rice cultivation, which plantation owners cultivated and developed as a successful commodity crop by 1700. With the coerced help of African slaves from the Caribbean, Eliza Lucas, daughter of plantation owner George Lucas, learned how to raise and use indigo in the Lowcountry in 1747. Supported with subsidies from Britain, indigo was a leading export by 1750. Those and naval stores were exported in an extremely profitable shipping industry. Industries slowly brought the city and its inhabitants back to a renewed vitality and jobs attracted new residents. As the city's commerce improved, residents worked to restore or create community institutions. In 1865, the Avery Normal Institute was established by the American Missionary Association as the first free secondary school for Charleston's African American population. General William T. Sherman lent his support to the conversion of the United States Arsenal into the Porter Military Academy, an educational facility for former soldiers and boys left orphaned or destitute by the war. Porter Military Academy later joined with Gaud School and is now a university-preparatory school, Porter-Gaud School. Charleston annually hosts Spoleto Festival USA founded by Gian Carlo Menotti, a 17-day art festival featuring over 100 performances by individual artists in a variety of disciplines. The Spoleto Festival is internationally recognized as America's premier performing arts festival. The annual Piccolo Spoleto festival takes place at the same time and features local performers and artists, with hundreds of performances throughout the city. Other festivals and events include Historic Charleston Foundation's Festival of Houses and Gardens and Charleston Antiques Show, the Taste of Charleston, The Lowcountry Oyster Festival, the Cooper River Bridge Run, The Charleston Marathon, Southeastern Wildlife Exposition (SEWE), Charleston Food and Wine Festival, Charleston Fashion Week, the MOJA Arts Festival, and the Holiday Festival of Lights (at James Island County Park), and the Charleston International Film Festival. The Charleston-North Charleston-Summerville Metropolitan Statistical Area consists of three counties: Charleston, Berkeley, and Dorchester. As of the 2013 U.S. Census, the metropolitan statistical area had a total population of 712,239 people. North Charleston is the second-largest city in the Charleston-North Charleston-Summerville Metropolitan Statistical Area and ranks as the third-largest city in the state; Mount Pleasant and Summerville are the next-largest cities. These cities combined with other incorporated and unincorporated areas along with the city of Charleston form the Charleston-North Charleston Urban Area with a population of 548,404 as of 2010. The metropolitan statistical area also includes a separate and much smaller urban area within Berkeley County, Moncks Corner (with a 2000 population of 9,123). Africans were brought to Charles Town on the Middle Passage, first as ""servants"", then as slaves. Ethnic groups transported here included especially Wolof, Yoruba, Fulani, Igbo, Malinke, and other people of the Windward Coast. An estimated 40% of the total 400,000 Africans transported and sold as slaves into North America are estimated to have landed at Sullivan's Island, just off the port of Charles Town; it is described as a ""hellish Ellis Island of sorts .... Today nothing commemorates that ugly fact but a simple bench, established by the author Toni Morrison using private funds."" Charleston is the primary medical center for the eastern portion of the state. The city has several major hospitals located in the downtown area: Medical University of South Carolina Medical Center (MUSC), Ralph H. Johnson VA Medical Center, and Roper Hospital. MUSC is the state's first school of medicine, the largest medical university in the state, and the sixth-oldest continually operating school of medicine in the United States. The downtown medical district is experiencing rapid growth of biotechnology and medical research industries coupled with substantial expansions of all the major hospitals. Additionally, more expansions are planned or underway at another major hospital located in the West Ashley portion of the city: Bon Secours-St Francis Xavier Hospital. The Trident Regional Medical Center located in the City of North Charleston and East Cooper Regional Medical Center located in Mount Pleasant also serve the needs of residents of the city of Charleston. Charleston has a humid subtropical climate (Köppen climate classification Cfa), with mild winters, hot, humid summers, and significant rainfall all year long. Summer is the wettest season; almost half of the annual rainfall occurs from June to September in the form of thundershowers. Fall remains relatively warm through November. Winter is short and mild, and is characterized by occasional rain. Measurable snow (≥0.1 in or 0.25 cm) only occurs several times per decade at the most, with the last such event occurring December 26, 2010. However, 6.0 in (15 cm) fell at the airport on December 23, 1989, the largest single-day fall on record, contributing to a single-storm and seasonal record of 8.0 in (20 cm) snowfall. Violent incidents occurred throughout the Piedmont of the state as white insurgents struggled to maintain white supremacy in the face of social changes after the war and granting of citizenship to freedmen by federal constitutional amendments. After former Confederates were allowed to vote again, election campaigns from 1872 on were marked by violent intimidation of blacks and Republicans by white Democratic paramilitary groups, known as the Red Shirts. Violent incidents took place in Charleston on King Street in September 6 and in nearby Cainhoy on October 15, both in association with political meetings before the 1876 election. The Cainhoy incident was the only one statewide in which more whites were killed than blacks. The Red Shirts were instrumental in suppressing the black Republican vote in some areas in 1876 and narrowly electing Wade Hampton as governor, and taking back control of the state legislature. Another riot occurred in Charleston the day after the election, when a prominent Republican leader was mistakenly reported killed. Investment in the city continued. The William Enston Home, a planned community for the city's aged and infirm, was built in 1889. An elaborate public building, the United States Post Office and Courthouse, was completed by the federal government in 1896 in the heart of the city. The Democrat-dominated state legislature passed a new constitution in 1895 that disfranchised blacks, effectively excluding them entirely from the political process, a second-class status that was maintained for more than six decades in a state that was majority black until about 1930. The Roman Catholic Diocese of Charleston Office of Education also operates out of the city and oversees several K-8 parochial schools, such as Blessed Sacrament School, Christ Our King School, Charleston Catholic School, Nativity School, and Divine Redeemer School, all of which are ""feeder"" schools into Bishop England High School, a diocesan high school within the city. Bishop England, Porter-Gaud School, and Ashley Hall are the city's oldest and most prominent private schools, and are a significant part of Charleston history, dating back some 150 years. As Charles Town grew, so did the community's cultural and social opportunities, especially for the elite merchants and planters. The first theatre building in America was built in 1736 on the site of today's Dock Street Theatre. Benevolent societies were formed by different ethnic groups, from French Huguenots to free people of color to Germans to Jews. The Charles Towne Library Society was established in 1748 by well-born young men who wanted to share the financial cost to keep up with the scientific and philosophical issues of the day. This group also helped establish the College of Charles Towne in 1770, the oldest college in South Carolina. Until its transition to state ownership in 1970, this was the oldest municipally supported college in the United States. After the defeat of the Confederacy, federal forces remained in Charleston during the city's reconstruction. The war had shattered the prosperity of the antebellum city. Freed slaves were faced with poverty and discrimination, but a large community of free people of color had been well-established in the city before the war and became the leaders of the postwar Republican Party and its legislators. Men who had been free people of color before the war comprised 26% of those elected to state and federal office in South Carolina from 1868 to 1876. The highest temperature recorded within city limits was 104 °F (40 °C), on June 2, 1985, and June 24, 1944, and the lowest was 7 °F (−14 °C) on February 14, 1899, although at the airport, where official records are kept, the historical range is 105 °F (41 °C) on August 1, 1999 down to 6 °F (−14 °C) on January 21, 1985. Hurricanes are a major threat to the area during the summer and early fall, with several severe hurricanes hitting the area – most notably Hurricane Hugo on September 21, 1989 (a category 4 storm). Dewpoint in the summer ranges from 67.8 to 71.4 °F (20 to 22 °C). Clinton returned in 1780 with 14,000 soldiers. American General Benjamin Lincoln was trapped and surrendered his entire 5,400-man force after a long fight, and the Siege of Charles Towne was the greatest American defeat of the war. Several Americans who escaped the carnage joined other militias, including those of Francis Marion, the ""Swamp Fox""; and Andrew Pickens. The British retained control of the city until December 1782. After the British left, the city's name was officially changed to Charleston in 1783. Charleston is known for its unique culture, which blends traditional Southern U.S., English, French, and West African elements. The downtown peninsula has gained a reputation for its art, music, local cuisine, and fashion. Spoleto Festival USA, held annually in late spring, has become one of the world's major performing arts festivals. It was founded in 1977 by Pulitzer Prize-winning composer Gian Carlo Menotti, who sought to establish a counterpart to the Festival dei Due Mondi (the Festival of Two Worlds) in Spoleto, Italy. As it has on every aspect of Charleston culture, the Gullah community has had a tremendous influence on music in Charleston, especially when it comes to the early development of jazz music. In turn, the music of Charleston has had an influence on that of the rest of the country. The geechee dances that accompanied the music of the dock workers in Charleston followed a rhythm that inspired Eubie Blake's ""Charleston Rag"" and later James P. Johnson's ""The Charleston"", as well as the dance craze that defined a nation in the 1920s. ""Ballin' the Jack"", which was a popular dance in the years before ""The Charleston"", was written by native Charlestonian Chris Smith. On June 28, 1776, General Sir Henry Clinton along with 2,000 men and a naval squadron tried to seize Charles Towne, hoping for a simultaneous Loyalist uprising in South Carolina. When the fleet fired cannonballs, they failed to penetrate Fort Sullivan's unfinished, yet thick, palmetto-log walls. No local Loyalists attacked the town from the mainland side, as the British had hoped they would do. Col. Moultrie's men returned fire and inflicted heavy damage on several of the British ships. The British were forced to withdraw their forces, and the Americans renamed the defensive installation as Fort Moultrie in honor of its commander. Although the city lost the status of state capital to Columbia in 1786, Charleston became even more prosperous in the plantation-dominated economy of the post-Revolutionary years. The invention of the cotton gin in 1793 revolutionized the processing of this crop, making short-staple cotton profitable. It was more easily grown in the upland areas, and cotton quickly became South Carolina's major export commodity. The Piedmont region was developed into cotton plantations, to which the sea islands and Lowcountry were already devoted. Slaves were also the primary labor force within the city, working as domestics, artisans, market workers, and laborers. In 1875, blacks made up 57% of the city's population, and 73% of Charleston County. With leadership by members of the antebellum free black community, historian Melinda Meeks Hennessy described the community as ""unique"" in being able to defend themselves without provoking ""massive white retaliation"", as occurred in numerous other areas during Reconstruction. In the 1876 election cycle, two major riots between black Republicans and white Democrats occurred in the city, in September and the day after the election in November, as well as a violent incident in Cainhoy at an October joint discussion meeting. The Arthur Ravenel Jr. Bridge across the Cooper River opened on July 16, 2005, and was the second-longest cable-stayed bridge in the Americas at the time of its construction.[citation needed] The bridge links Mount Pleasant with downtown Charleston, and has eight lanes plus a 12-foot lane shared by pedestrians and bicycles. It replaced the Grace Memorial Bridge (built in 1929) and the Silas N. Pearman Bridge (built in 1966). They were considered two of the more dangerous bridges in America and were demolished after the Ravenel Bridge opened. According to the United States Census Bureau, the city has a total area of 127.5 square miles (330.2 km2), of which 109.0 square miles (282.2 km2) is land and 18.5 square miles (47.9 km2) is covered by water. The old city is located on a peninsula at the point where, as Charlestonians say, ""The Ashley and the Cooper Rivers come together to form the Atlantic Ocean."" The entire peninsula is very low, some is landfill material, and as such, frequently floods during heavy rains, storm surges, and unusually high tides. The city limits have expanded across the Ashley River from the peninsula, encompassing the majority of West Ashley as well as James Island and some of Johns Island. The city limits also have expanded across the Cooper River, encompassing Daniel Island and the Cainhoy area. North Charleston blocks any expansion up the peninsula, and Mount Pleasant occupies the land directly east of the Cooper River. By 1840, the Market Hall and Sheds, where fresh meat and produce were brought daily, became a hub of commercial activity. The slave trade also depended on the port of Charleston, where ships could be unloaded and the slaves bought and sold. The legal importation of African slaves had ended in 1808, although smuggling was significant. However, the domestic trade was booming. More than one million slaves were transported from the Upper South to the Deep South in the antebellum years, as cotton plantations were widely developed through what became known as the Black Belt. Many slaves were transported in the coastwise slave trade, with slave ships stopping at ports such as Charleston."
Department_store,"In Denmark you find three department store chains: Magasin (1868), Illum (1891), Salling (1906). Magasin is by far the largest with 6 stores all over the country, with the flagship store being Magasin du Nord on Kongens Nytorv in Copenhagen. Illums only store on Amagertorv in Copenhagen has the appearance of a department store with 20% run by Magasin, but has individual shop owners making it a shopping centre. But in people's mind it remains a department store. Salling has two stores in Jutland with one of these being the reason for the closure of a magasin store due to the competition. Since the opening policy in 1979, the Chinese department stores also develops swiftly along with the fast-growing economy. There are different department store groups dominating different regions. For example, INTIME department store has the biggest market presence in Zhejiang province, while Jinying department stores dominate Jiangsu Province. Besides, there are many other department store groups, such as Pacific, Parkson, Wangfujing，New World，etc., many of them are expanding quickly by listing in the financial market. All major cities have their distinctive local department stores, which anchored the downtown shopping district until the arrival of the malls in the 1960s. Washington, for example, after 1887 had Woodward & Lothrop and Garfinckel's starting in 1905. Garfield's went bankrupt in 1990, as did Woodward & Lothrop in 1994. Baltimore had four major department stores: Hutzler's was the prestige leader, followed by Hecht's, Hochschild's and Stewart's. They all operated branches in the suburbs, but all closed in the late twentieth century. By 2015, most locally owned department stores around the country had been consolidated into larger chains, or had closed down entirely. All the major British cities had flourishing department stores by the mid-or late nineteenth century. Increasingly, women became the major shoppers and middle-class households. Kendals (formerly Kendal Milne & Faulkner) in Manchester lays claim to being one of the first department stores and is still known to many of its customers as Kendal's, despite its 2005 name change to House of Fraser. The Manchester institution dates back to 1836 but had been trading as Watts Bazaar since 1796. At its zenith the store had buildings on both sides of Deansgate linked by a subterranean passage ""Kendals Arcade"" and an art nouveau tiled food hall. The store was especially known for its emphasis on quality and style over low prices giving it the nickname ""the Harrods of the North"", although this was due in part to Harrods acquiring the store in 1919. Other large Manchester stores included Paulden's (currently Debenhams) and Lewis's (now a Primark). A novelty shop called Au Bon Marché had been founded in Paris in 1838 to sell lace, ribbons, sheets, mattresses, buttons, umbrellas and other assorted goods. It originally had four departments, twelve employees, and a floor space of three hundred meters. The entrepreneur Aristide Boucicaut became a partner in 1852, and changed the marketing plan, instituting fixed prices and guarantees that allowed exchanges and refunds, advertising, and a much wider variety of merchandise. The annual income of the store increased from 500,000 francs in 1852 to five million in 1860. In 1869 he built much larger building at 24 rue de Sèvres on the Left Bank, and enlarged the store again in 1872, with help from the engineering firm of Gustave Eiffel, creator of the Eiffel Tower. The income rose from twenty million francs in 1870 to 72 million at the time of the Boucicaut's death in 1877. The floor space had increased from three hundred square meters in 1838 to fifty thousand, and the number of employees had increased from twelve in 1838 to 1788 in 1879. Boucicaut was famous for his marketing innovations; a reading room for husbands while their wives shopped; extensive newspaper advertising; entertainment for children; and six million catalogs sent out to customers. By 1880 half the employees were women; unmarried women employees lived in dormitories on the upper floors. John Lewis Newcastle (formerly Bainbridge) in Newcastle upon Tyne, is the world's oldest Department Store. It is still known to many of its customers as Bainbridge, despite the name change to 'John Lewis'. The Newcastle institution dates back to 1838 when Emerson Muschamp Bainbridge, aged 21, went into partnership with William Alder Dunn and opened a draper's and fashion in Market Street, Newcastle. In terms of retailing history, one of the most significant facts about the Newcastle Bainbridge shop, is that as early as 1849 weekly takings were recorded by department, making it the earliest of all department stores. This ledger survives and is kept in the John Lewis archives. John Lewis bought the Bainbridge store in 1952. The iconic department stores of New Zealand's three major centres are Smith & Caughey's (founded 1880), in New Zealand's most populous city, Auckland; Kirkcaldie & Stains (founded 1863) in the capital, Wellington; and Ballantynes (founded 1854) in New Zealand's second biggest city, Christchurch. These offer high-end and luxury items. Additionally, Arthur Barnett (1903) operates in Dunedin. H & J Smith is a small chain operating throughout Southland with a large flagship store in Invercargill. Farmers is a mid-range national chain of stores (originally a mail-order firm known as Laidlaw Leeds founded in 1909). Historical department stores include DIC. Discount chains include The Warehouse, Kmart Australia, and the now-defunct DEKA. Also, Kendals in Manchester can lay claim to being one of the oldest department stores in the UK. Beginning as a small shop owned by S. and J. Watts in 1796, its sold a variety of goods. Kendal Milne and Faulkner purchased the business in 1835. Expanding the space, rather than use it as a typical warehouse simply to showcase textiles, it became a vast bazaar. Serving Manchester's upmarket clientele for over 200 years, it was taken over by House of Fraser and recently rebranded as House of Fraser Manchester – although most Mancunians still refer to it as Kendals. The Kendal Milne signage still remains over the main entrance to the art deco building in the city's Deansgate. France's major upscale department stores are Galeries Lafayette and Le Printemps, which both have flagship stores on Boulevard Haussmann in Paris and branches around the country. The first department store in France, Le Bon Marché in Paris, was founded in 1852 and is now owned by the luxury goods conglomerate LVMH. La Samaritaine, another upscale department store also owned by LVMH, closed in 2005. Mid-range department stores chains also exist in France such as the BHV (Bazar de l'Hotel de Ville), part of the same group as Galeries Lafayette. Panama's first department stores such as Bazaar Francés, La Dalia and La Villa de Paris started as textile retailers at the turn of the nineteenth century. Later on in the twentieth century these eventually gave way to stores such as Felix B. Maduro, Sarah Panamá, Figali, Danté, Sears, Gran Morrison and smaller ones such as Bon Bini, Cocos, El Lider, Piccolo and Clubman among others. Of these only Felix B. Maduro (usually referred to as Felix by locals) and Danté remain strong. All the others have either folded or declined although Cocos has managed to secure a good position in the market. The design and function of department stores in Germany followed the lead of London, Paris and New York. Germany used to have a number of department stores; nowadays only a few of them remain. Next to some smaller, independent department stores these are Karstadt (in 2010 taken over by Nicolas Berggruen, also operating the KaDeWe in Berlin, the Alsterhaus in Hamburg and the Oberpollinger in Munich), GALERIA Kaufhof (part of the Metro AG). Others like Hertie, Wertheim and Horten AG were taken over by others and either fully integrated or later closed. From its origins in the fur trade, the Hudson's Bay Company is the oldest corporation in North America and was the largest department store operator in Canada until the mid-1980s, with locations across the country. It also previously owned Zellers, another major Canadian department store which ceased to exist in March 2013 after selling its lease holdings to Target Canada. Other department stores in Canada are: Canadian Tire, Sears Canada, Ogilvy, Les Ailes de la Mode, Giant Tiger, Co-op, Costco and Holt Renfrew. Grocery giant Superstores carry many non-grocery items akin to a department store. Woolco had 160 stores in Canada when operations ceased (Walmart bought out Woolco in 1994). Today low-price Walmart is by far the most dominant department store retailer in Canada with outlets throughout the country. Historically, department stores were a significant component in Canadian economic life, and chain stores such as Eaton's, Charles Ogilvy Limited, Freiman's, Spencer's, Simpsons, Morgan's, and Woodward's were staples in their respective communities. Department stores in Canada are similar in design and style to department stores in the United States. After World War II Hudson's realized that the limited parking space at its downtown skyscraper would increasingly be a problem for its customers. The solution in 1954 was to open the Northland Center in nearby Southfield, just beyond the city limits. It was the largest suburban shopping center in the world, and quickly became the main shopping destination for northern and western Detroit, and for much of the suburbs. By 1961 the downtown skyscraper accounted for only half of Hudson's sales; it closed in 1986. The Northland Center Hudson's, rebranded Macy's in 2006 following acquisition by Federated Department Stores, was closed along with the remaining stores in the center in March 2015 due to the mall's high storefront vacancy, decaying infrastructure, and financial mismanagement. The first department store in Spain was Almacenes el Siglo opened in October 1881 in Barcelona. Following the 2002 closure by the Australian group Partridges of their SEPU (Sociedad Española de Precios Unicos) department store chain, which was one of Spain's oldest, the market is now dominated by El Corte Inglés, founded in 1934 as a drapery store. El Corte Inglés stores tend to be vast buildings, selling a very broad range of products and the group also controls a number of other retail formats including supermarket chain 'Supercor' and hypermarket chain 'Hipercor'. Other competitors such as 'Simago' and 'Galerías Preciados' closed in the 1990s, however El Corte Inglés, faces major competition from French discount operators such as Carrefour and Auchan. The middle up segment is mainly occupied by Metro Department Store originated from Singapore and Sogo from Japan. 2007 saw the re-opening of Jakarta's Seibu, poised to be the largest and second most upscale department store in Indonesia after Harvey Nichols, which the latter closed in 2010 and yet plans to return. Other international department stores include Debenhams and Marks & Spencer. Galeries Lafayette also joins the Indonesian market in 2013 inside Pacific Place Mall. This department store is targeting middle up market with price range from affordable to luxury, poised to be the largest upscale department store. Galeries Lafayette, Debenhams, Harvey Nichols, Marks & Spencer, Seibu and Sogo are all operated by PT. Mitra Adiperkasa. Ireland developed a strong middle class, especially in the major cities, by the mid-nineteenth century. They were active patrons of department stores. Delany's New Mart was opened in 1853 in Dublin, Ireland. Unlike others, Delany's had not evolved gradually from a smaller shop on site. Thus it could claim to be the first purpose-built Department Store in the world. The word department store had not been invented at that time and thus it was called the ""Monster House"". The store was completely destroyed in the 1916 Easter Rising, but reopened in 1922. Socialism confronted consumerism in the chain State Department Stores (GUM), set up by Lenin in 1921 as a model retail enterprise. It operated stores throughout Russia and targeted consumers across class, gender, and ethnic lines. GUM was designed to advance the Bolsheviks' goals of eliminating private enterprise and rebuilding consumerism along socialist lines, as well as democratizing consumption for workers and peasants nationwide. GUM became a major propaganda purveyor, with advertising and promotional campaigns that taught Russians the goals of the regime and attempted to inculcate new attitudes and behavior. In trying to create a socialist consumer culture from scratch, GUM recast the functions and meanings of buying and selling, turning them into politically charged acts that could either contribute to or delay the march toward utopian communism. By the late 1920s, however, GUM's gandiose goals had proven unrealistic and largely alienated consumers, who instead learned a culture of complaint and entitlement. GUM's main function became one of distributing whatever the factories sent them, regardless of consumer demand or quality. In the 21st century the most famous department store in Russia is GUM in Moscow, followed by TsUM and the Petrovsky Passage. Other popular stores are Mega (shopping malls), Stockmann, and Marks & Spencer. Media Markt, M-video, Technosila, and White Wind (Beliy Veter) sell large number of electronic devices. In St. Petersburg The Passage has been popular since the 1840s. 1956 Soviet film Behind Store Window (За витриной универмага) on YouTube depicts operation of a Moscow department store in 1950's. In New York City in 1846, Alexander Turney Stewart established the ""Marble Palace"" on Broadway, between Chambers and Reade streets. He offered European retail merchandise at fixed prices on a variety of dry goods, and advertised a policy of providing ""free entrance"" to all potential customers. Though it was clad in white marble to look like a Renaissance palazzo, the building's cast iron construction permitted large plate glass windows that permitted major seasonal displays, especially in the Christmas shopping season. In 1862, Stewart built a new store on a full city block with eight floors and nineteen departments of dress goods and furnishing materials, carpets, glass and china, toys and sports equipment, ranged around a central glass-covered court. His innovations included buying from manufacturers for cash and in large quantities, keeping his markup small and prices low, truthful presentation of merchandise, the one-price policy (so there was no haggling), simple merchandise returns and cash refund policy, selling for cash and not credit, buyers who searched worldwide for quality merchandise, departmentalization, vertical and horizontal integration, volume sales, and free services for customers such as waiting rooms and free delivery of purchases. His innovations were quickly copied by other department stores. David Jones was started by David Jones, a Welsh merchant who met Hobart businessman Charles Appleton in London. Appleton established a store in Sydney in 1825 and Jones subsequently established a partnership with Appleton, moved to Australia in 1835, and the Sydney store became known as Appleton & Jones. When the partnership was dissolved in 1838, Jones moved his business to premises on the corner of George Street and Barrack Lane, Sydney. David Jones claims to be the oldest department store in the world still trading under its original name. Parkson enters by acquiring local brand Centro Department Store in 2011. Centro still operates for middle market while the 'Parkson' brand itself, positioned for middle-up segment, enters in 2014 by opening its first store in Medan, followed by its second store in Jakarta. Lotte, meanwhile, enters the market by inking partnership with Ciputra Group, creating what its called 'Lotte Shopping Avenue' inside the Ciputra World Jakarta complex, as well as acquiring Makro and rebranding it into Lotte Mart. Arnold, Constable was the first American department store. It was founded in 1825 by Aaron Arnold (1794?-1876), an emigrant from Great Britain, as a small dry goods store on Pine Street in New York City. In 1857 the store moved into a five-story white marble dry goods palace known as the Marble House. During the Civil War Arnold, Constable was one of the first stores to issue charge bills of credit to its customers each month instead of on a bi-annual basis. Recognized as an emporium for high-quality fashions, the store soon outgrew the Marble House and erected a cast-iron building on Broadway and Nineteenth Street in 1869; this “Palace of Trade” expanded over the years until it was necessary to move into a larger space in 1914. In 1925, Arnold, Constable merged with Stewart & Company and expanded into the suburbs, first with a 1937 store in New Rochelle, New York and later in Hempstead and Manhasset on Long Island, and in New Jersey. Financial problems led to bankruptcy in 1975. The first department stores Lane Crawford was opened in 1850 by Scots Thomas Ash Lane and Ninian Crawford on Des Voeux Road, Hong Kong Island. At the beginning, the store mainly catered visiting ships' crews as well as British Navy staff and their families. In 1900, the first ethnic-Chinese owned Sincere Department Store was opened by Ma Ying Piu, who returned from Australia and inspired by David Jones. In 1907, another former Hong Kong expatriate in Australia, the Kwok's family, returned to Hong Kong and founded Wing On. The Paris department store had its roots in the magasin de nouveautés, or novelty store; the first, the Tapis Rouge, was created in 1784. They flourished in the early 19th century, with La Belle Jardiniere (1824), Aux Trois Quartiers (1829), and Le Petit Saint Thomas (1830). Balzac described their functioning in his novel César Birotteau. In the 1840s, with the arrival of the railroads in Paris and the increased number of shoppers they brought, they grew in size, and began to have large plate glass display windows, fixed prices and price tags, and advertising in newspapers. Although there were a number of department stores in Australia for much of the 20th Century, including chains such as Grace Bros. and Waltons, many disappeared during the 1980s and 1990s. Today Myer and David Jones, located nationally, are practically the national department stores duopoly in Australia. When Russian-born migrant, Sidney Myer, came to Australia in 1899 he formed the Myer retail group with his brother, Elcon Myer. In 1900, they opened the first Myer department store, in Bendigo, Victoria. Since then, the Myer retail group has grown to be Australia's largest retailer. Both, Myer and David Jones, are up-market chains, offering a wide variety of products from mid-range names to luxury brands. Other retail chain stores such as Target (unrelated to the American chain of the same name), Venture (now defunct), Kmart and Big W, also located nationally, are considered to be Australia's discount department stores. Harris Scarfe, though only operating in four states and one territory, is a department store using both the large full-line and small discount department store formats. Most department stores in Australia have their own credit card companies, each having their own benefits while the discount department stores do not have their own credit card rights. Chain department stores grew rapidly after 1920, and provided competition for the downtown upscale department stores, as well as local department stores in small cities. J. C. Penney had four stores in 1908, 312 in 1920, and 1452 in 1930. Sears, Roebuck & Company, a giant mail-order house, opened its first eight retail stores in 1925, and operated 338 by 1930, and 595 by 1940. The chains reached a middle-class audience, that was more interested in value than in upscale fashions. Sears was a pioneer in creating department stores that catered to men as well as women, especially with lines of hardware and building materials. It deemphasized the latest fashions in favor of practicality and durability, and allowed customers to select goods without the aid of a clerk. Its stores were oriented to motorists – set apart from existing business districts amid residential areas occupied by their target audience; had ample, free, off-street parking; and communicated a clear corporate identity. In the 1930s, the company designed fully air-conditioned, ""windowless"" stores whose layout was driven wholly by merchandising concerns. George Dayton had founded his Dayton's Dry Goods store in Minneapolis in 1902 and the AMC cooperative in 1912. His descendants built Southdale Center in 1956, opened the Target discount store chain in 1962 and the B. Dalton Bookseller chain in 1966. Dayton's grew to 19 stores under the Dayton's name plus five other regional names acquired by Dayton-Hudson. The Dayton-Hudson Corporation closed the flagship J. L. Hudson Department Store in downtown Detroit in 1983, but expanded its other retail operations. It acquired Mervyn's in 1978, Marshall Field's in 1990, and renamed itself the Target Corporation in 2000. In 2002, Dayton's and Hudson's were consolidated into the Marshall Field's name. In 2005, May Department Stores acquired all of the Marshall Field's stores and shortly thereafter, Macy's acquired May. Mexico has a large number of department stores based in Mexico, of which the most traditional are El Palacio de Hierro (High end and luxury goods) and Liverpool (Upper-middle income), with its middle income sister store Fabricas de Francia. Sanborns owns over 100 middle income level stores throughout the country. Grupo Carso operates Sears Mexico and two high-end Saks 5th Avenue stores. Other large chains are Coppel and Elektra, which offer items for the bargain price seeker. Wal-Mart operates Suburbia for lower income shoppers, along with stores under the brand names of Wal-Mart, Bodega Aurrera, and Superama. Selfridges was established in 1909 by American-born Harry Gordon Selfridge on Oxford Street. The company's innovative marketing promoted the radical notion of shopping for pleasure rather than necessity and its techniques were adopted by modern department stores the world over. The store was extensively promoted through paid advertising. The shop floors were structured so that goods could be made more accessible to customers. There were elegant restaurants with modest prices, a library, reading and writing rooms, special reception rooms for French, German, American and ""Colonial"" customers, a First Aid Room, and a Silence Room, with soft lights, deep chairs, and double-glazing, all intended to keep customers in the store as long as possible. Staff members were taught to be on hand to assist customers, but not too aggressively, and to sell the merchandise. Selfridge attracted shoppers with educational and scientific exhibits; – in 1909, Louis Blériot's monoplane was exhibited at Selfridges (Blériot was the first to fly over the English Channel), and the first public demonstration of television by John Logie Baird took place in the department store in 1925. The Grands Magasins Dufayel was a huge department store with inexpensive prices built in 1890 in the northern part of Paris, where it reached a very large new customer base in the working class. In a neighborhood with few public spaces, it provided a consumer version of the public square. It educated workers to approach shopping as an exciting social activity not just a routine exercise in obtaining necessities, just as the bourgeoisie did at the famous department stores in the central city. Like the bourgeois stores, it helped transform consumption from a business transaction into a direct relationship between consumer and sought-after goods. Its advertisements promised the opportunity to participate in the newest, most fashionable consumerism at reasonable cost. The latest technology was featured, such as cinemas and exhibits of inventions like X-ray machines (that could be used to fit shoes) and the gramophone. Marshall Field & Company originated in 1852. It was the premier department store on the main shopping street in the Midwest, State Street in Chicago. Upscale shoppers came by train from throughout the region, patronizing nearby hotels. It grew to become a major chain before converting to the Macy's nameplate on 9 September 2006. Marshall Field's Served as a model for other departments stores in that it had exceptional customer service. Field's also brought with it the now famous Frango mints brand that became so closely identified with Marshall Field's and Chicago from the now defunct Frederick & Nelson Department store. Marshall Field's also had the firsts, among many innovations by Marshall Field's. Field's had the first European buying office, which was located in Manchester, England, and the first bridal registry. The company was the first to introduce the concept of the personal shopper, and that service was provided without charge in every Field's store, until the chain's last days under the Marshall Field's name. It was the first store to offer revolving credit and the first department store to use escalators. Marshall Field's book department in the State Street store was legendary; it pioneered the concept of the ""book signing."" Moreover, every year at Christmas, Marshall Field's downtown store windows were filled with animated displays as part of the downtown shopping district display; the ""theme"" window displays became famous for their ingenuity and beauty, and visiting the Marshall Field's windows at Christmas became a tradition for Chicagoans and visitors alike, as popular a local practice as visiting the Walnut Room with its equally famous Christmas tree or meeting ""under the clock"" on State Street. The first department store in the Philippines is the Hoskyn's Department Store of Hoskyn & Co. established in 1877 in Iloilo by the Englishman Henry Hoskyn, nephew of Nicholas Loney, the first British vice-consul in Iloilo. Some of the earliest department stores in the Philippines were located in Manila as early as 1898 with the opening of the American Bazaar, which was later named Beck's. During the course of the American occupation of the Philippines, many department stores were built throughout the city, many of which were located in Escolta. Heacock's, a luxury department store, was considered as the best department store in the Orient. Other department stores included Aguinaldo's, La Puerta del Sol, Estrella del Norte, and the Crystal Arcade, all of which were destroyed during the Battle of Manila in 1945. After the war, department stores were once again alive with the establishment of Shoemart (now SM), and Rustan's. Since the foundation of these companies in the 1950s, there are now more than one hundred department stores to date. At present, due to the huge success of shopping malls, department stores in the Philippines usually are anchor tenants within malls. SM Supermalls and Robinsons Malls are two of the country's most prominent mall chains, all of which has Department Store sections. The five most prevalent chains are Lotte, Hyundai, Shinsegae, Galleria, AK plaza. Lotte Department Store is the largest, operating more than 40 stores (include outlet, young plaza, foreign branches). Hyundai Department Store has about 14 stores (13dept, 1outlet), and there are 10 stores in Shinsegae. Shinsegae has 3 outlet store with Simon. Galleria has 5, AK has 5 stores. Galleriaeast and west is well known by luxury goods. These five department stores are known to people as representative corporations in the field of distirution in South Korea. From fashion items to electric appliances, people can buy various kinds of products. Every weekend, people are fond of going around these department stores, because their location is usually easy to visit. As of 2010 the Shinsegae department store in Centum City, Busan, is the largest department store in the world. In 1877, John Wanamaker opened the United State's first modern department store in a former Pennsylvania Railroad freight terminal in Philadelphia. Wanamakers was the first department store to offer fixed prices marked on every article and also introduced electrical illumination (1878), the telephone (1879), and the use of pneumatic tubes to transport cash and documents (1880) to the department store business. Subsequent department stores founded in Philadelphia included Strawbridge and Clothier, Gimbels, Lit Brothers, and Snellenbergs. The origins of the department store lay in the growth of the conspicuous consumer society at the turn of the 19th century. As the Industrial Revolution accelerated economy expansion, the affluent middle-class grew in size and wealth. This urbanized social group, sharing a culture of consumption and changing fashion, was the catalyst for the retail revolution. As rising prosperity and social mobility increased the number of people, especially women (who found they could shop unaccompanied at department stores without damaging their reputation), with disposable income in the late Georgian period, window shopping was transformed into a leisure activity and entrepreneurs, like the potter Josiah Wedgwood, pioneered the use of marketing techniques to influence the prevailing tastes and preferences of society. In 1849, Horne's began operations and soon became a leading Pittsburgh department store. In 1879, it opened a seven-story landmark which was the first department store in the city's downtown. In 1972, Associated Dry Goods acquired Horne's, and ADG expanded operations of Horne's to several stores in suburban malls throughout the Pittsburgh region as well as in Erie, Pennsylvania and Northeast Ohio. In December 1986, Horne's was acquired by a local investor group following ADG's acquisition by May Department Stores. By 1994, Federated Department Stores acquired the remaining ten Horne's stores and merged them with its Lazarus division, completely ceasing all operations of any store under the Horne's name. The site where the Saint Petersburg Passage sprawls had been devoted to trade since the city's foundation in the early 18th century. It had been occupied by various shops and warehouses (Maly Gostiny Dvor, Schukin Dvor, Apraksin Dvor) until 1846, when Count Essen-Stenbock-Fermor acquired the grounds to build an elite shopping mall for the Russian nobility and wealthy bourgeoisie. Stenbock-Fermor conceived of the Passage as more than a mere shopping mall, but also as a cultural and social centre for the people of St Petersburg. The edifice contained coffee-houses, confectioneries, panorama installations, an anatomical museum, a wax museum, and even a small zoo, described by Dostoyevsky in his extravaganza ""Crocodile, or Passage through the Passage"". The concert hall became renowned as a setting for literary readings attended by the likes of Dostoevsky and Turgenev. Parenthetically, the Passage premises have long been associated with the entertainment industry and still remains home to the Komissarzhevskaya Theatre. Before the 1950s, the department store held an eminent place in both Canada and Australia, during both the Great Depression and World War II. Since then, they have suffered from strong competition from specialist stores. Most recently the competition has intensified with the advent of larger-scale superstores (Jones et al. 1994; Merrilees and Miller 1997). Competition was not the only reason for the department stores' weakening strength; the changing structure of cities also affected them. The compact and centralized 19th century city with its mass transit lines converging on the downtown was a perfect environment for department store growth. But as residents moved out of the downtown areas to the suburbs, the large, downtown department stores became inconvenient and lost business to the newer suburban shopping malls. In 2003, U.S. department store sales were surpassed by big-box store sales for the first time (though some stores may be classified as ""big box"" by physical layout and ""department store"" by merchandise). Department stores today have sections that sell the following: clothing, furniture, home appliances, toys, cosmetics, gardening, toiletries, sporting goods, do it yourself, paint, and hardware and additionally select other lines of products such as food, books, jewelry, electronics, stationery, photographic equipment, baby products, and products for pets. Customers check out near the front of the store or, alternatively, at sales counters within each department. Some are part of a retail chain of many stores, while others may be independent retailers. In the 1970s, they came under heavy pressure from discounters. Since 2010, they have come under even heavier pressure from online stores such as Amazon. In Puerto Rico, various department stores have operated, such as Sears, JC Penney, Macy's, Kmart, Wal-Mart, Marshalls, Burlington Coat Factory, T.J. Maxx, Costco, Sam's Club and others. La New York was a Puerto Rican department store. Topeka, Capri and Pitusa are competitors on the Puerto Rican market that also have hypermarkets operating under their names. Retailers Nordstrom and Saks Fifth Avenue also have plans to come to the Mall of San Juan, a new high-end retail project with over 100 tenants. The mall is set to open in March 2015."
Melbourne,"A long list of AM and FM radio stations broadcast to greater Melbourne. These include ""public"" (i.e., state-owned ABC and SBS) and community stations. Many commercial stations are networked-owned: DMG has Nova 100 and Smooth; ARN controls Gold 104.3 and KIIS 101.1; and Southern Cross Austereo runs both Fox and Triple M. Stations from towns in regional Victoria may also be heard (e.g. 93.9 Bay FM, Geelong). Youth alternatives include ABC Triple J and youth run SYN. Triple J, and similarly PBS and Triple R, strive to play under represented music. JOY 94.9 caters for gay, lesbian, bisexual and transgender audiences. For fans of classical music there are 3MBS and ABC Classic FM. Light FM is a contemporary Christian station. AM stations include ABC: 774, Radio National, and News Radio; also Fairfax affiliates 3AW (talk) and Magic (easy listening). For sport fans and enthusiasts there is SEN 1116. Melbourne has many community run stations that serve alternative interests, such as 3CR and 3KND (Indigenous). Many suburbs have low powered community run stations serving local audiences. Melbourne (/ˈmɛlbərn/, AU i/ˈmɛlbən/) is the capital and most populous city in the Australian state of Victoria, and the second most populous city in Australia and Oceania. The name ""Melbourne"" refers to the area of urban agglomeration (as well as a census statistical division) spanning 9,900 km2 (3,800 sq mi) which comprises the broader metropolitan area, as well as being the common name for its city centre. The metropolis is located on the large natural bay of Port Phillip and expands into the hinterlands towards the Dandenong and Macedon mountain ranges, Mornington Peninsula and Yarra Valley. Melbourne consists of 31 municipalities. It has a population of 4,347,955 as of 2013, and its inhabitants are called Melburnians. Height limits in the Melbourne CBD were lifted in 1958, after the construction of ICI House, transforming the city's skyline with the introduction of skyscrapers. Suburban expansion then intensified, serviced by new indoor malls beginning with Chadstone Shopping Centre. The post-war period also saw a major renewal of the CBD and St Kilda Road which significantly modernised the city. New fire regulations and redevelopment saw most of the taller pre-war CBD buildings either demolished or partially retained through a policy of facadism. Many of the larger suburban mansions from the boom era were also either demolished or subdivided. Another recent environmental issue in Melbourne was the Victorian government project of channel deepening Melbourne Ports by dredging Port Phillip Bay—the Port Phillip Channel Deepening Project. It was subject to controversy and strict regulations among fears that beaches and marine wildlife could be affected by the disturbance of heavy metals and other industrial sediments. Other major pollution problems in Melbourne include levels of bacteria including E. coli in the Yarra River and its tributaries caused by septic systems, as well as litter. Up to 350,000 cigarette butts enter the storm water runoff every day. Several programs are being implemented to minimise beach and river pollution. In February 2010, The Transition Decade, an initiative to transition human society, economics and environment towards sustainability, was launched in Melbourne. Melbourne is experiencing high population growth, generating high demand for housing. This housing boom has increased house prices and rents, as well as the availability of all types of housing. Subdivision regularly occurs in the outer areas of Melbourne, with numerous developers offering house and land packages. However, after 10 years[when?] of planning policies to encourage medium-density and high-density development in existing areas with greater access to public transport and other services, Melbourne's middle and outer-ring suburbs have seen significant brownfields redevelopment. Ship transport is an important component of Melbourne's transport system. The Port of Melbourne is Australia's largest container and general cargo port and also its busiest. The port handled two million shipping containers in a 12-month period during 2007, making it one of the top five ports in the Southern Hemisphere. Station Pier on Port Phillip Bay is the main passenger ship terminal with cruise ships and the Spirit of Tasmania ferries which cross Bass Strait to Tasmania docking there. Ferries and water taxis run from berths along the Yarra River as far upstream as South Yarra and across Port Phillip Bay. An influx of interstate and overseas migrants, particularly Irish, German and Chinese, saw the development of slums including a temporary ""tent city"" established on the southern banks of the Yarra. Chinese migrants founded the Melbourne Chinatown in 1851, which remains the longest continuous Chinese settlement in the Western World. In the aftermath of the Eureka Stockade, mass public support for the plight of the miners resulted in major political changes to the colony, including changes to working conditions across local industries including mining, agriculture and manufacturing. The nationalities involved in the Eureka revolt and Burke and Wills expedition gave an indication of immigration flows in the second half of the nineteenth century. Melbourne has a highly diversified economy with particular strengths in finance, manufacturing, research, IT, education, logistics, transportation and tourism. Melbourne houses the headquarters for many of Australia's largest corporations, including five of the ten largest in the country (based on revenue), and four of the largest six in the country (based on market capitalisation) (ANZ, BHP Billiton (the world's largest mining company), the National Australia Bank and Telstra), as well as such representative bodies and think tanks as the Business Council of Australia and the Australian Council of Trade Unions. Melbourne's suburbs also have the Head Offices of Wesfarmers companies Coles (including Liquorland), Bunnings, Target, K-Mart & Officeworks. The city is home to Australia's largest and busiest seaport which handles more than $75 billion in trade every year and 39% of the nation's container trade. Melbourne Airport provides an entry point for national and international visitors, and is Australia's second busiest airport.[citation needed] The Melbourne rail network has its origins in privately built lines from the 1850s gold rush era, and today the suburban network consists of 209 suburban stations on 16 lines which radiate from the City Loop, a partially underground metro section of the network beneath the Central Business District (Hoddle Grid). Flinders Street Station is Melbourne's busiest railway station, and was the world's busiest passenger station in 1926. It remains a prominent Melbourne landmark and meeting place. The city has rail connections with regional Victorian cities, as well as direct interstate rail services to Sydney and Adelaide and beyond which depart from Melbourne's other major rail terminus, Southern Cross Station in Spencer Street. In the 2013–2014 financial year, the Melbourne rail network recorded 232.0 million passenger trips, the highest in its history. Many rail lines, along with dedicated lines and rail yards are also used for freight. The Overland to Adelaide departs Southern Cross twice a week, while the XPT to Sydney departs twice a day. Three daily newspapers serve Melbourne: the Herald Sun (tabloid), The Age (formerly broadsheet, now compact) and The Australian (national broadsheet). Six free-to-air television stations service Greater Melbourne and Geelong: ABC Victoria, (ABV), SBS Victoria (SBS), Seven Melbourne (HSV), Nine Melbourne (GTV), Ten Melbourne (ATV), C31 Melbourne (MGV) – community television. Each station (excluding C31) broadcasts a primary channel and several multichannels. C31 is only broadcast from the transmitters at Mount Dandenong and South Yarra. Hybrid digital/print media companies such as Broadsheet and ThreeThousand are based in and primarily serve Melbourne. Melbourne is an international cultural centre, with cultural endeavours spanning major events and festivals, drama, musicals, comedy, music, art, architecture, literature, film and television. The climate, waterfront location and nightlife make it one of the most vibrant destinations in Australia. For five years in a row (as of 2015) it has held the top position in a survey by The Economist Intelligence Unit of the world's most liveable cities on the basis of a number of attributes which include its broad cultural offerings. The city celebrates a wide variety of annual cultural events and festivals of all types, including Australia's largest free community festival—Moomba, the Melbourne International Arts Festival, Melbourne International Film Festival, Melbourne International Comedy Festival and the Melbourne Fringe Festival. The culture of the city is an important drawcard for tourists, of which just under two million international overnight visitors and 57.7 million domestic overnight visited during the year ending March 2014. Melbourne is typical of Australian capital cities in that after the turn of the 20th century, it expanded with the underlying notion of a 'quarter acre home and garden' for every family, often referred to locally as the Australian Dream. This, coupled with the popularity of the private automobile after 1945, led to the auto-centric urban structure now present today in the middle and outer suburbs. Much of metropolitan Melbourne is accordingly characterised by low density sprawl, whilst its inner city areas feature predominantly medium-density, transit-oriented urban forms. The city centre, Docklands, St. Kilda Road and Southbank areas feature high-density forms. From 2006, the growth of the city extended into ""green wedges"" and beyond the city's urban growth boundary. Predictions of the city's population reaching 5 million people pushed the state government to review the growth boundary in 2008 as part of its Melbourne @ Five Million strategy. In 2009, Melbourne was less affected by the Late-2000s financial crisis in comparison to other Australian cities. At this time, more new jobs were created in Melbourne than any other Australian city—almost as many as the next two fastest growing cities, Brisbane and Perth, combined, and Melbourne's property market remained strong, resulting in historically high property prices and widespread rent increases. In 2012, the city contained a total of 594 high-rise buildings, with 8 under construction, 71 planned and 39 at proposal stage making the city's skyline the second largest in Australia. The CBD is dominated by modern office buildings including the Rialto Towers (1986), built on the site of several grand classical Victorian buildings, two of which — the Rialto Building (1889) designed by William Pitt and the Winfield Building (1890) designed by Charles D'Ebro and Richard Speight — still remain today and more recently hi-rise apartment buildings including Eureka Tower (2006), which is listed as the 13th tallest residential building in the world in January 2014. The main passenger airport serving the metropolis and the state is Melbourne Airport (also called Tullamarine Airport), which is the second busiest in Australia, and the Port of Melbourne is Australia's busiest seaport for containerised and general cargo. Melbourne has an extensive transport network. The main metropolitan train terminus is Flinders Street Station, and the main regional train and coach terminus is Southern Cross Station. Melbourne is also home to Australia's most extensive freeway network and has the world's largest urban tram network. A brash boosterism that had typified Melbourne during this time ended in the early 1890s with a severe depression of the city's economy, sending the local finance and property industries into a period of chaos during which 16 small ""land banks"" and building societies collapsed, and 133 limited companies went into liquidation. The Melbourne financial crisis was a contributing factor in the Australian economic depression of the 1890s and the Australian banking crisis of 1893. The effects of the depression on the city were profound, with virtually no new construction until the late 1890s. The city is recognised for its mix of modern architecture which intersects with an extensive range of nineteenth and early twentieth century buildings. Some of the most architecturally noteworthy historic buildings include the World Heritage Site-listed Royal Exhibition Building, constructed over a two-year period for the Melbourne International Exhibition in 1880, A.C. Goode House, a Neo Gothic building located on Collins Street designed by Wright, Reed & Beaver (1891), William Pitt's Venetian Gothic style Old Stock Exchange (1888), William Wardell's Gothic Bank (1883) which features some of Melbourne's finest interiors, the incomplete Parliament House, St Paul's Cathedral (1891) and Flinders Street Station (1909), which was the busiest commuter railway station in the world in the mid-1920s. Since the mid-1990s, Melbourne has maintained significant population and employment growth. There has been substantial international investment in the city's industries and property market. Major inner-city urban renewal has occurred in areas such as Southbank, Port Melbourne, Melbourne Docklands and more recently, South Wharf. According to the Australian Bureau of Statistics, Melbourne sustained the highest population increase and economic growth rate of any Australian capital city in the three years ended June 2004. These factors have led to population growth and further suburban expansion through the 2000s. The city is home to many professional franchises/teams in national competitions including: cricket clubs Melbourne Stars, Melbourne Renegades and Victorian Bushrangers, which play in the Big Bash League and other domestic cricket competitions; soccer clubs Melbourne Victory and Melbourne City FC (known until June 2014 as Melbourne Heart), which play in the A-League competition, both teams play their home games at AAMI Park, with the Victory also playing home games at Etihad Stadium. Rugby league club Melbourne Storm which plays in the NRL competition; rugby union clubs Melbourne Rebels and Melbourne Rising, which play in the Super Rugby and National Rugby Championship competitions respectively; netball club Melbourne Vixens, which plays in the trans-Tasman trophy ANZ Championship; basketball club Melbourne United, which plays in the NBL competition; Bulleen Boomers and Dandenong Rangers, which play in the WNBL; ice hockey teams Melbourne Ice and Melbourne Mustangs, who play in the Australian Ice Hockey League; and baseball club Melbourne Aces, which plays in the Australian Baseball League. Rowing is also a large part of Melbourne's sporting identity, with a number of clubs located on the Yarra River, out of which many Australian Olympians trained. The city previously held the nation's premier long distance swimming event the annual Race to Prince's Bridge, in the Yarra River. Melbourne has an integrated public transport system based around extensive train, tram, bus and taxi systems. Flinders Street Station was the world's busiest passenger station in 1927 and Melbourne's tram network overtook Sydney's to become the world's largest in the 1940s, at which time 25% of travellers used public transport but by 2003 it had declined to just 7.6%. The public transport system was privatised in 1999, symbolising the peak of the decline. Despite privatisation and successive governments persisting with auto-centric urban development into the 21st century, there have since been large increases in public transport patronage, with the mode share for commuters increasing to 14.8% and 8.4% of all trips. A target of 20% public transport mode share for Melbourne by 2020 was set by the state government in 2006. Since 2006 public transport patronage has grown by over 20%. In recent years, Melton, Wyndham and Casey, part of the Melbourne statistical division, have recorded the highest growth rate of all local government areas in Australia. Melbourne could overtake Sydney in population by 2028, The ABS has projected in two scenarios that Sydney will remain larger than Melbourne beyond 2056, albeit by a margin of less than 3% compared to a margin of 12% today. Melbourne's population could overtake that of Sydney by 2037 or 2039, according to the first scenario projected by the ABS; primarily due to larger levels of internal migration losses assumed for Sydney. Another study claims that Melbourne will surpass Sydney in population by 2040. Melbourne's CBD, compared with other Australian cities, has comparatively unrestricted height limits and as a result of waves of post-war development contains five of the six tallest buildings in Australia, the tallest of which is the Eureka Tower, situated in Southbank. It has an observation deck near the top from where you can see above all of Melbourne's structures. The Rialto tower, the city's second tallest, remains the tallest building in the old CBD; its observation deck for visitors has recently closed. Melbourne is also an important financial centre. Two of the big four banks, NAB and ANZ, are headquartered in Melbourne. The city has carved out a niche as Australia's leading centre for superannuation (pension) funds, with 40% of the total, and 65% of industry super-funds including the $109 billion-dollar Federal Government Future Fund. The city was rated 41st within the top 50 financial cities as surveyed by the MasterCard Worldwide Centers of Commerce Index (2008), second only to Sydney (12th) in Australia. Melbourne is Australia's second-largest industrial centre. It is the Australian base for a number of significant manufacturers including Boeing, truck-makers Kenworth and Iveco, Cadbury as well as Bombardier Transportation and Jayco, among many others. It is also home to a wide variety of other manufacturers, ranging from petrochemicals and pharmaceuticals to fashion garments, paper manufacturing and food processing. The south-eastern suburb of Scoresby is home to Nintendo's Australian headquarters. The city also boasts a research and development hub for Ford Australia, as well as a global design studio and technical centre for General Motors and Toyota respectively. Australian rules football and cricket are the most popular sports in Melbourne. It is considered the spiritual home of the two sports in Australia. The first official Test cricket match was played at the Melbourne Cricket Ground in March 1877. The origins of Australian rules football can be traced to matches played next to the MCG in 1858. The Australian Football League is headquartered at Docklands Stadium. Nine of the League's teams are based in the Melbourne metropolitan area: Carlton, Collingwood, Essendon, Hawthorn, Melbourne, North Melbourne, Richmond, St Kilda, and Western Bulldogs. Up to five AFL matches are played each week in Melbourne, attracting an average 40,000 people per game. Additionally, the city annually hosts the AFL Grand Final. As the centre of Australia's ""rust belt"", Melbourne experienced an economic downturn between 1989 to 1992, following the collapse of several local financial institutions. In 1992 the newly elected Kennett government began a campaign to revive the economy with an aggressive development campaign of public works coupled with the promotion of the city as a tourist destination with a focus on major events and sports tourism. During this period the Australian Grand Prix moved to Melbourne from Adelaide. Major projects included the construction of a new facility for the Melbourne Museum, Federation Square, the Melbourne Exhibition and Convention Centre, Crown Casino and the CityLink tollway. Other strategies included the privatisation of some of Melbourne's services, including power and public transport, and a reduction in funding to public services such as health, education and public transport infrastructure. During a visit in 1885 English journalist George Augustus Henry Sala coined the phrase ""Marvellous Melbourne"", which stuck long into the twentieth century and is still used today by Melburnians. Growing building activity culminated in a ""land boom"" which, in 1888, reached a peak of speculative development fuelled by consumer confidence and escalating land value. As a result of the boom, large commercial buildings, coffee palaces, terrace housing and palatial mansions proliferated in the city. The establishment of a hydraulic facility in 1887 allowed for the local manufacture of elevators, resulting in the first construction of high-rise buildings; most notably the APA Building, amongst the world's tallest commercial buildings upon completion in 1889. This period also saw the expansion of a major radial rail-based transport network. With the gold rush largely over by 1860, Melbourne continued to grow on the back of continuing gold mining, as the major port for exporting the agricultural products of Victoria, especially wool, and a developing manufacturing sector protected by high tariffs. An extensive radial railway network centred on Melbourne and spreading out across the suburbs and into the countryside was established from the late 1850s. Further major public buildings were begun in the 1860s and 1870s such as the Supreme Court, Government House, and the Queen Victoria Market. The central city filled up with shops and offices, workshops, and warehouses. Large banks and hotels faced the main streets, with fine townhouses in the east end of Collins Street, contrasting with tiny cottages down laneways within the blocks. The Aboriginal population continued to decline with an estimated 80% total decrease by 1863, due primarily to introduced diseases, particularly smallpox, frontier violence and dispossession from their lands. RMIT University was also ranked among the top 51–100 universities in the world in the subjects of: accounting, Business and Management, communication and media studies, computer science and information systems. The Swinburne University of Technology, based in the inner city Melbourne suburb of Hawthorn is ranked 76–100 in the world for Physics by the Academic Ranking of World Universities making Swinburne the only Australian university outside the Group of Eight to achieve a top 100 rating in a science discipline. Deakin University maintains two major campuses in Melbourne and Geelong, and is the third largest university in Victoria. In recent years, the number of international students at Melbourne's universities has risen rapidly, a result of an increasing number of places being made available to full fee paying students. Education in Melbourne is overseen by the Victorian Department of Education and Early Childhood Development (DEECD), whose role is to 'provide policy and planning advice for the delivery of education'. The Hoddle Grid (dimensions of 1 by 1⁄2 mile (1.61 by 0.80 km)) forms the centre of Melbourne's central business district. The grid's southern edge fronts onto the Yarra River. Office, commercial and public developments in the adjoining districts of Southbank and Docklands have made these redeveloped areas into extensions of the CBD in all but name. The city centre has a reputation for its historic and prominent lanes and arcades (most notably Block Place and Royal Arcade) which contain a variety of shops and cafés and are a byproduct of the city's layout. The decade began with the Melbourne International Exhibition in 1880, held in the large purpose-built Exhibition Building. In 1880 a telephone exchange was established and in the same year the foundations of St Paul's, were laid; in 1881 electric light was installed in the Eastern Market, and in the following year a generating station capable of supplying 2,000 incandescent lamps was in operation. In 1885 the first line of the Melbourne cable tramway system was built, becoming one of the worlds most extensive systems by 1890. Melbourne has a temperate oceanic climate (Köppen climate classification Cfb) and is well known for its changeable weather conditions. This is mainly due to Melbourne's location situated on the boundary of the very hot inland areas and the cool southern ocean. This temperature differential is most pronounced in the spring and summer months and can cause very strong cold fronts to form. These cold fronts can be responsible for all sorts of severe weather from gales to severe thunderstorms and hail, large temperature drops, and heavy rain. The local councils are responsible for providing the functions set out in the Local Government Act 1989 such as urban planning and waste management. Most other government services are provided or regulated by the Victorian state government, which governs from Parliament House in Spring Street. These include services which are associated with local government in other countries and include public transport, main roads, traffic control, policing, education above preschool level, health and planning of major infrastructure projects. The state government retains the right to override certain local government decisions, including urban planning, and Melburnian issues often feature prominently in state election. With the wealth brought on by the gold rush following closely on the heels of the establishment of Victoria as a separate colony and the subsequent need for public buildings, a program of grand civic construction soon began. The 1850s and 1860s saw the commencement of Parliament House, the Treasury Building, the Old Melbourne Gaol, Victoria Barracks, the State Library, University, General Post Office, Customs House, the Melbourne Town Hall, St Patrick's cathedral, though many remained uncompleted for decades, with some still not finished. Batman's Treaty with the Aborigines was annulled by the New South Wales governor (who at the time governed all of eastern mainland Australia), with compensation paid to members of the association. In 1836, Governor Bourke declared the city the administrative capital of the Port Phillip District of New South Wales, and commissioned the first plan for the city, the Hoddle Grid, in 1837. The settlement was named Batmania after Batman. However, later that year the settlement was named ""Melbourne"" after the British Prime Minister, William Lamb, 2nd Viscount Melbourne, whose seat was Melbourne Hall in the market town of Melbourne, Derbyshire. On 13 April 1837 the settlement's general post office officially opened with that name. The layout of the inner suburbs on a largely one-mile grid pattern, cut through by wide radial boulevards, and string of gardens surrounding the central city was largely established in the 1850s and 1860s. These areas were rapidly filled from the mid 1850s by the ubiquitous terrace house, as well as detached houses and some grand mansions in large grounds, while some of the major roads developed as shopping streets. Melbourne quickly became a major finance centre, home to several banks, the Royal Mint, and Australia's first stock exchange in 1861. In 1855 the Melbourne Cricket Club secured possession of its now famous ground, the MCG. Members of the Melbourne Football Club codified Australian football in 1859, and Yarra rowing clubs and ""regattas"" became popular about the same time. In 1861 the Melbourne Cup was first run. In 1864 Melbourne acquired its first public monument—the Burke and Wills statue. Melbourne's air quality is generally good and has improved significantly since the 1980s. Like many urban environments, the city faces significant environmental issues, many of them relating to the city's large urban footprint and urban sprawl and the demand for infrastructure and services. One such issue is water usage, drought and low rainfall. Drought in Victoria, low rainfalls and high temperatures deplete Melbourne water supplies and climate change may have a long-term impact on the water supplies of Melbourne. In response to low water supplies and low rainfall due to drought, the government implemented water restrictions and a range of other options including: water recycling schemes for the city, incentives for household water tanks, greywater systems, water consumption awareness initiatives, and other water saving and reuse initiatives; also, in June 2007, the Bracks Government announced that a $3.1 billion Wonthaggi desalination plant would be built on Victoria's south-east coast, capable of treating 150 billion litres of water per year, as well as a 70 km (43 mi) pipeline from the Goulburn area in Victoria's north to Melbourne and a new water pipeline linking Melbourne and Geelong. Both projects are being conducted under controversial Public-Private Partnerships and a multitude of independent reports have found that neither project is required to supply water to the city and that Sustainable Water Management is the best solution. In the meantime, the drought must be weathered. The city reaches south-east through Dandenong to the growth corridor of Pakenham towards West Gippsland, and southward through the Dandenong Creek valley, the Mornington Peninsula and the city of Frankston taking in the peaks of Olivers Hill, Mount Martha and Arthurs Seat, extending along the shores of Port Phillip as a single conurbation to reach the exclusive suburb of Portsea and Point Nepean. In the west, it extends along the Maribyrnong River and its tributaries north towards Sunbury and the foothills of the Macedon Ranges, and along the flat volcanic plain country towards Melton in the west, Werribee at the foothills of the You Yangs granite ridge south west of the CBD. The Little River, and the township of the same name, marks the border between Melbourne and neighbouring Geelong city. Water storage and supply for Melbourne is managed by Melbourne Water, which is owned by the Victorian Government. The organisation is also responsible for management of sewerage and the major water catchments in the region as well as the Wonthaggi desalination plant and North–South Pipeline. Water is stored in a series of reservoirs located within and outside the Greater Melbourne area. The largest dam, the Thomson River Dam, located in the Victorian Alps, is capable of holding around 60% of Melbourne's water capacity, while smaller dams such as the Upper Yarra Dam, Yan Yean Reservoir, and the Cardinia Reservoir carry secondary supplies. In response to attribution of recent climate change, the City of Melbourne, in 2002, set a target to reduce carbon emissions to net zero by 2020 and Moreland City Council established the Zero Moreland program, however not all metropolitan municipalities have followed, with the City of Glen Eira notably deciding in 2009 not to become carbon neutral. Melbourne has one of the largest urban footprints in the world due to its low density housing, resulting in a vast suburban sprawl, with a high level of car dependence and minimal public transport outside of inner areas. Much of the vegetation within the city are non-native species, most of European origin, and in many cases plays host to invasive species and noxious weeds. Significant introduced urban pests include the common myna, feral pigeon, brown rat, European wasp, common starling and red fox. Many outlying suburbs, particularly towards the Yarra Valley and the hills to the north-east and east, have gone for extended periods without regenerative fires leading to a lack of saplings and undergrowth in urbanised native bushland. The Department of Sustainability and Environment partially addresses this problem by regularly burning off. Several national parks have been designated around the urban area of Melbourne, including the Mornington Peninsula National Park, Port Phillip Heads Marine National Park and Point Nepean National Park in the south east, Organ Pipes National Park to the north and Dandenong Ranges National Park to the east. There are also a number of significant state parks just outside Melbourne. Responsibility for regulating pollution falls under the jurisdiction of the EPA Victoria and several local councils. Air pollution, by world standards, is classified as being good. Summer and autumn are the worst times of year for atmospheric haze in the urban area. Melbourne has four airports. Melbourne Airport, at Tullamarine, is the city's main international and domestic gateway and second busiest in Australia. The airport is home base for passenger airlines Jetstar Airways and Tiger Airways Australia and cargo airlines Australian air Express and Toll Priority; and is a major hub for Qantas and Virgin Australia. Avalon Airport, located between Melbourne and Geelong, is a secondary hub of Jetstar. It is also used as a freight and maintenance facility. Buses and taxis are the only forms of public transport to and from the city's main airports. Air Ambulance facilities are available for domestic and international transportation of patients. Melbourne also has a significant general aviation airport, Moorabbin Airport in the city's south east that also handles a small number of passenger flights. Essendon Airport, which was once the city's main airport also handles passenger flights, general aviation and some cargo flights. Melbourne rates highly in education, entertainment, health care, research and development, tourism and sport, making it the world's most liveable city—for the fifth year in a row in 2015, according to the Economist Intelligence Unit. It is a leading financial centre in the Asia-Pacific region, and ranks among the top 30 cities in the world in the Global Financial Centres Index. Referred to as Australia's ""cultural capital"", it is the birthplace of Australian impressionism, Australian rules football, the Australian film and television industries, and Australian contemporary dance such as the Melbourne Shuffle. It is recognised as a UNESCO City of Literature and a major centre for street art, music and theatre. It is home to many of Australia's largest and oldest cultural institutions such as the Melbourne Cricket Ground, the National Gallery of Victoria, the State Library of Victoria and the UNESCO World Heritage-listed Royal Exhibition Building. Melbourne has the largest tram network in the world which had its origins in the city's 1880s land boom. In 2013–2014, 176.9 million passenger trips were made by tram. Melbourne's is Australia's only tram network to comprise more than a single line and consists of 250 km (155.3 mi) of track, 487 trams, 25 routes, and 1,763 tram stops. Around 80 per cent of Melbourne's tram network shares road space with other vehicles, while the rest of the network is separated or are light rail routes. Melbourne's trams are recognised as iconic cultural assets and a tourist attraction. Heritage trams operate on the free City Circle route, intended for visitors to Melbourne, and heritage restaurant trams travel through the city and surrounding areas during the evening. Melbourne is currently building 50 new E Class trams with some already in service in 2014. The E Class trams are about 30 metres long and are superior to the C2 class tram of similar length. Melbourne's bus network consists of almost 300 routes which mainly service the outer suburbs and fill the gaps in the network between rail and tram services. 127.6 million passenger trips were recorded on Melbourne's buses in 2013–2014, an increase of 10.2 percent on the previous year. In May and June 1835, the area which is now central and northern Melbourne was explored by John Batman, a leading member of the Port Phillip Association in Van Diemen's Land (now known as Tasmania), who claimed to have negotiated a purchase of 600,000 acres (2,400 km2) with eight Wurundjeri elders. Batman selected a site on the northern bank of the Yarra River, declaring that ""this will be the place for a village"". Batman then returned to Launceston in Tasmania. In early August 1835 a different group of settlers, including John Pascoe Fawkner, left Launceston on the ship Enterprize. Fawkner was forced to disembark at Georgetown, Tasmania, because of outstanding debts. The remainder of the party continued and arrived at the mouth of the Yarra River on 15 August 1835. On 30 August 1835 the party disembarked and established a settlement at the site of the current Melbourne Immigration Museum. Batman and his group arrived on 2 September 1835 and the two groups ultimately agreed to share the settlement. Melbourne is often referred to as Australia's garden city, and the state of Victoria was once known as the garden state. There is an abundance of parks and gardens in Melbourne, many close to the CBD with a variety of common and rare plant species amid landscaped vistas, pedestrian pathways and tree-lined avenues. Melbourne's parks are often considered the best public parks in all of Australia's major cities. There are also many parks in the surrounding suburbs of Melbourne, such as in the municipalities of Stonnington, Boroondara and Port Phillip, south east of the central business district. The extensive area covered by urban Melbourne is formally divided into hundreds of suburbs (for addressing and postal purposes), and administered as local government areas 31 of which are located within the metropolitan area. Some of Australia's most prominent and well known schools are based in Melbourne. Of the top twenty high schools in Australia according to the Better Education ranking, six are located in Melbourne. There has also been a rapid increase in the number of International students studying in the city. Furthermore, Melbourne was ranked the world's fourth top university city in 2008 after London, Boston and Tokyo in a poll commissioned by the Royal Melbourne Institute of Technology. Melbourne is the home of seven public universities: the University of Melbourne, Monash University, Royal Melbourne Institute of Technology (RMIT University), Deakin University, La Trobe University, Swinburne University of Technology and Victoria University. To counter the trend towards low-density suburban residential growth, the government began a series of controversial public housing projects in the inner city by the Housing Commission of Victoria, which resulted in demolition of many neighbourhoods and a proliferation of high-rise towers. In later years, with the rapid rise of motor vehicle ownership, the investment in freeway and highway developments greatly accelerated the outward suburban sprawl and declining inner city population. The Bolte government sought to rapidly accelerate the modernisation of Melbourne. Major road projects including the remodelling of St Kilda Junction, the widening of Hoddle Street and then the extensive 1969 Melbourne Transportation Plan changed the face of the city into a car-dominated environment. Melbourne is notable as the host city for the 1956 Summer Olympic Games (the first Olympic Games held in the southern hemisphere and Oceania, with all previous games held in Europe and the United States), along with the 2006 Commonwealth Games. Melbourne is so far the southernmost city to host the games. The city is home to three major annual international sporting events: the Australian Open (one of the four Grand Slam tennis tournaments); the Melbourne Cup (horse racing); and the Australian Grand Prix (Formula One). Also, the Australian Masters golf tournament is held at Melbourne since 1979, having been co-sanctioned by the European Tour from 2006 to 2009. Melbourne was proclaimed the ""World's Ultimate Sports City"", in 2006, 2008 and 2010. The city is home to the National Sports Museum, which until 2003 was located outside the members pavilion at the Melbourne Cricket Ground. It reopened in 2008 in the Olympic Stand. Port Phillip is often warmer than the surrounding oceans and/or the land mass, particularly in spring and autumn; this can set up a ""bay effect"" similar to the ""lake effect"" seen in colder climates where showers are intensified leeward of the bay. Relatively narrow streams of heavy showers can often affect the same places (usually the eastern suburbs) for an extended period, while the rest of Melbourne and surrounds stays dry. Overall, Melbourne is, owing to the rain shadow of the Otway Ranges, nonetheless drier than average for southern Victoria. Within the city and surrounds, however, rainfall varies widely, from around 425 millimetres (17 in) at Little River to 1,250 millimetres (49 in) on the eastern fringe at Gembrook. Melbourne receives 48.6 clear days annually. Dewpoint temperatures in the summer range from 9.5 °C (49.1 °F) to 11.7 °C (53.1 °F). After a trend of declining population density since World War II, the city has seen increased density in the inner and western suburbs, aided in part by Victorian Government planning, such as Postcode 3000 and Melbourne 2030 which have aimed to curtail urban sprawl. According to the Australian Bureau of Statistics as of June 2013, inner city Melbourne had the highest population density with 12,400 people per km2. Surrounding inner city suburbs experienced an increase in population density between 2012 and 2013; Carlton (9,000 people per km2) and Fitzroy (7,900). At the time of Australia's federation on 1 January 1901, Melbourne became the seat of government of the federation. The first federal parliament was convened on 9 May 1901 in the Royal Exhibition Building, subsequently moving to the Victorian Parliament House where it was located until 1927, when it was moved to Canberra. The Governor-General of Australia resided at Government House in Melbourne until 1930 and many major national institutions remained in Melbourne well into the twentieth century. Melbourne universities have campuses all over Australia and some internationally. Swinburne University has campuses in Malaysia, while Monash has a research centre based in Prato, Italy. The University of Melbourne, the second oldest university in Australia, was ranked first among Australian universities in the 2010 THES international rankings. The 2012–2013 Times Higher Education Supplement ranked the University of Melbourne as the 28th (30th by QS ranking) best university in the world. Monash University was ranked as the 99th (60th by QS ranking) best university in the world. Both universities are members of the Group of Eight, a coalition of leading Australian tertiary institutions offering comprehensive and leading education. The Story of the Kelly Gang, the world's first feature film, was shot in Melbourne in 1906. Melbourne filmmakers continued to produce bushranger films until they were banned by Victorian politicians in 1912 for the perceived promotion of crime, thus contributing to the decline of one of the silent film era's most productive industries. A notable film shot and set in Melbourne during Australia's cinematic lull is On the Beach (1959). The 1970s saw the rise of the Australian New Wave and its Ozploitation offshoot, instigated by Melbourne-based productions Stork and Alvin Purple. Picnic at Hanging Rock and Mad Max, both shot in and around Melbourne, achieved worldwide acclaim. 2004 saw the construction of Melbourne's largest film and television studio complex, Docklands Studios Melbourne, which has hosted many domestic productions, as well as international features. Melbourne is also home to the headquarters of Village Roadshow Pictures, Australia's largest film production company. Famous modern day actors from Melbourne include Cate Blanchett, Rachel Griffiths, Guy Pearce, Geoffrey Rush and Eric Bana. Between 1836 and 1842 Victorian Aboriginal groups were largely dispossessed[by whom?] of their land. By January 1844, there were said to be 675 Aborigines resident in squalid camps in Melbourne. The British Colonial Office appointed five Aboriginal Protectors for the Aborigines of Victoria, in 1839, however their work was nullified by a land policy that favoured squatters to take possession of Aboriginal lands. By 1845, fewer than 240 wealthy Europeans held all the pastoral licences then issued in Victoria and became a powerful political and economic force in Victoria for generations to come. Melbourne has the largest Greek-speaking population outside of Europe, a population comparable to some larger Greek cities like Larissa and Volos. Thessaloniki is Melbourne's Greek sister city. The Vietnamese surname Nguyen is the second most common in Melbourne's phone book after Smith. The city also features substantial Indian, Sri Lankan, and Malaysian-born communities, in addition to recent South African and Sudanese influxes. The cultural diversity is reflected in the city's restaurants that serve international cuisines. Melbourne's rich and diverse literary history was recognised in 2008 when it became the second UNESCO City of Literature. The State Library of Victoria is one of Australia's oldest cultural institutions and one of many public and university libraries across the city. Melbourne also has Australia's widest range of bookstores, as well the nation's largest publishing sector. The city is home to significant writers' festivals, most notably the Melbourne Writers Festival. Several major literary prizes are open to local writers including the Melbourne Prize for Literature and the Victorian Premier's Literary Awards. Significant novels set in Melbourne include Fergus Hume's The Mystery of a Hansom Cab, Helen Garner's Monkey Grip and Christos Tsiolkas' The Slap. Notable writers and poets from Melbourne include Thomas Browne, C. J. Dennis, Germaine Greer and Peter Carey. Television shows are produced in Melbourne, most notably Neighbours, Kath & Kim, Winners and Losers, Offspring, Underbelly , House Husbands, Wentworth and Miss Fisher's Murder Mysteries, along with national news-based programs such as The Project, Insiders and ABC News Breakfast. Melbourne is also known as the game show capital of Australia; productions such as Million Dollar Minute, Millionaire Hot Seat and Family Feud are all based in Melbourne. Reality television productions such as Dancing with the Stars, MasterChef, The Block and The Real Housewives of Melbourne are all filmed in and around Melbourne. Founded by free settlers from the British Crown colony of Van Diemen's Land on 30 August 1835, in what was then the colony of New South Wales, it was incorporated as a Crown settlement in 1837. It was named ""Melbourne"" by the Governor of New South Wales, Sir Richard Bourke, in honour of the British Prime Minister of the day, William Lamb, 2nd Viscount Melbourne. It was officially declared a city by Queen Victoria in 1847, after which it became the capital of the newly founded colony of Victoria in 1851. During the Victorian gold rush of the 1850s, it was transformed into one of the world's largest and wealthiest cities. After the federation of Australia in 1901, it served as the nation's interim seat of government until 1927. Like many Australian cities, Melbourne has a high dependency on the automobile for transport, particularly in the outer suburban areas where the largest number of cars are bought, with a total of 3.6 million private vehicles using 22,320 km (13,870 mi) of road, and one of the highest lengths of road per capita in the world. The early 20th century saw an increase in popularity of automobiles, resulting in large-scale suburban expansion. By the mid 1950s there was just under 200 passenger vehicles per 1000 people by 2013 there was 600 passenger vehicles per 1000 people. Today it has an extensive network of freeways and arterial roadways used by private vehicles including freight as well as public transport systems including bus and taxis. Major highways feeding into the city include the Eastern Freeway, Monash Freeway and West Gate Freeway (which spans the large West Gate Bridge), whilst other freeways circumnavigate the city or lead to other major cities, including CityLink (which spans the large Bolte Bridge), Eastlink, the Western Ring Road, Calder Freeway, Tullamarine Freeway (main airport link) and the Hume Freeway which links Melbourne and Sydney. Melbourne's live performance institutions date from the foundation of the city, with the first theatre, the Pavilion, opening in 1841. The city's East End Theatre District includes theatres that similarly date from 1850s to the 1920s, including the Princess Theatre, Regent Theatre, Her Majesty's Theatre, Forum Theatre, Comedy Theatre, and the Athenaeum Theatre. The Melbourne Arts Precinct in Southbank is home to Arts Centre Melbourne, which includes the State Theatre, Hamer Hall, the Playhouse and the Fairfax Studio. The Melbourne Recital Centre and Southbank Theatre (principal home of the MTC, which includes the Sumner and Lawler performance spaces) are also located in Southbank. The Sidney Myer Music Bowl, which dates from 1955, is located in the gardens of Kings Domain; and the Palais Theatre is a feature of the St Kilda Beach foreshore. Residential architecture is not defined by a single architectural style, but rather an eclectic mix of houses, townhouses, condominiums, and apartment buildings in the metropolitan area (particularly in areas of urban sprawl). Free standing dwellings with relatively large gardens are perhaps the most common type of housing outside inner city Melbourne. Victorian terrace housing, townhouses and historic Italianate, Tudor revival and Neo-Georgian mansions are all common in neighbourhoods such as Toorak. The discovery of gold in Victoria in mid 1851 led to the Victorian gold rush, and Melbourne, which served as the major port and provided most services for the region, experienced rapid growth. Within months, the city's population had increased by nearly three-quarters, from 25,000 to 40,000 inhabitants. Thereafter, growth was exponential and by 1865, Melbourne had overtaken Sydney as Australia's most populous city. Additionally, Melbourne along with the Victorian regional cities of Ballarat and Geelong became the wealthiest cities in the world during the Gold Rush era. Melbourne is also prone to isolated convective showers forming when a cold pool crosses the state, especially if there is considerable daytime heating. These showers are often heavy and can contain hail and squalls and significant drops in temperature, but they pass through very quickly at times with a rapid clearing trend to sunny and relatively calm weather and the temperature rising back to what it was before the shower. This often occurs in the space of minutes and can be repeated many times in a day, giving Melbourne a reputation for having ""four seasons in one day"", a phrase that is part of local popular culture and familiar to many visitors to the city. The lowest temperature on record is −2.8 °C (27.0 °F), on 21 July 1869. The highest temperature recorded in Melbourne city was 46.4 °C (115.5 °F), on 7 February 2009. While snow is occasionally seen at higher elevations in the outskirts of the city, it has not been recorded in the Central Business District since 1986. Over two-thirds of Melburnians speak only English at home (68.1%). Chinese (mainly Cantonese and Mandarin) is the second-most-common language spoken at home (3.6%), with Greek third, Italian fourth and Vietnamese fifth, each with more than 100,000 speakers. Although Victoria's net interstate migration has fluctuated, the population of the Melbourne statistical division has grown by about 70,000 people a year since 2005. Melbourne has now attracted the largest proportion of international overseas immigrants (48,000) finding it outpacing Sydney's international migrant intake on percentage, along with having strong interstate migration from Sydney and other capitals due to more affordable housing and cost of living. CSL, one of the world's top five biotech companies, and Sigma Pharmaceuticals have their headquarters in Melbourne. The two are the largest listed Australian pharmaceutical companies. Melbourne has an important ICT industry that employs over 60,000 people (one third of Australia's ICT workforce), with a turnover of $19.8 billion and export revenues of $615 million. In addition, tourism also plays an important role in Melbourne's economy, with about 7.6 million domestic visitors and 1.88 million international visitors in 2004. In 2008, Melbourne overtook Sydney with the amount of money that domestic tourists spent in the city, accounting for around $15.8 billion annually. Melbourne has been attracting an increasing share of domestic and international conference markets. Construction began in February 2006 of a $1 billion 5000-seat international convention centre, Hilton Hotel and commercial precinct adjacent to the Melbourne Exhibition and Convention Centre to link development along the Yarra River with the Southbank precinct and multibillion-dollar Docklands redevelopment."
Greeks,"The Greek shipping tradition recovered during Ottoman rule when a substantial merchant middle class developed, which played an important part in the Greek War of Independence. Today, Greek shipping continues to prosper to the extent that Greece has the largest merchant fleet in the world, while many more ships under Greek ownership fly flags of convenience. The most notable shipping magnate of the 20th century was Aristotle Onassis, others being Yiannis Latsis, George Livanos, and Stavros Niarchos. The evolution of Proto-Greek should be considered within the context of an early Paleo-Balkan sprachbund that makes it difficult to delineate exact boundaries between individual languages. The characteristically Greek representation of word-initial laryngeals by prothetic vowels is shared, for one, by the Armenian language, which also seems to share some other phonological and morphological peculiarities of Greek; this has led some linguists to propose a hypothetical closer relationship between Greek and Armenian, although evidence remains scant. The most obvious link between modern and ancient Greeks is their language, which has a documented tradition from at least the 14th century BC to the present day, albeit with a break during the Greek Dark Ages (lasting from the 11th to the 8th century BC). Scholars compare its continuity of tradition to Chinese alone. Since its inception, Hellenism was primarily a matter of common culture and the national continuity of the Greek world is a lot more certain than its demographic. Yet, Hellenism also embodied an ancestral dimension through aspects of Athenian literature that developed and influenced ideas of descent based on autochthony. During the later years of the Eastern Roman Empire, areas such as Ionia and Constantinople experienced a Hellenic revival in language, philosophy, and literature and on classical models of thought and scholarship. This revival provided a powerful impetus to the sense of cultural affinity with ancient Greece and its classical heritage. The cultural changes undergone by the Greeks are, despite a surviving common sense of ethnicity, undeniable. At the same time, the Greeks have retained their language and alphabet, certain values and cultural traditions, customs, a sense of religious and cultural difference and exclusion, (the word barbarian was used by 12th-century historian Anna Komnene to describe non-Greek speakers), a sense of Greek identity and common sense of ethnicity despite the global political and social changes of the past two millennia. There is a sizeable Greek minority of about 105,000 (disputed, sources claim higher) people, in Albania. The Greek minority of Turkey, which numbered upwards of 200,000 people after the 1923 exchange, has now dwindled to a few thousand, after the 1955 Constantinople Pogrom and other state sponsored violence and discrimination. This effectively ended, though not entirely, the three-thousand-year-old presence of Hellenism in Asia Minor. There are smaller Greek minorities in the rest of the Balkan countries, the Levant and the Black Sea states, remnants of the Old Greek Diaspora (pre-19th century). Following the Fall of Constantinople on 29 May 1453, many Greeks sought better employment and education opportunities by leaving for the West, particularly Italy, Central Europe, Germany and Russia. Greeks are greatly credited for the European cultural revolution, later called, the Renaissance. In Greek-inhabited territory itself, Greeks came to play a leading role in the Ottoman Empire, due in part to the fact that the central hub of the empire, politically, culturally, and socially, was based on Western Thrace and Greek Macedonia, both in Northern Greece, and of course was centred on the mainly Greek-populated, former Byzantine capital, Constantinople. As a direct consequence of this situation, Greek-speakers came to play a hugely important role in the Ottoman trading and diplomatic establishment, as well as in the church. Added to this, in the first half of the Ottoman period men of Greek origin made up a significant proportion of the Ottoman army, navy, and state bureaucracy, having been levied as adolescents (along with especially Albanians and Serbs) into Ottoman service through the devshirme. Many Ottomans of Greek (or Albanian or Serb) origin were therefore to be found within the Ottoman forces which governed the provinces, from Ottoman Egypt, to Ottomans occupied Yemen and Algeria, frequently as provincial governors. Notable modern Greek artists include Renaissance painter Dominikos Theotokopoulos (El Greco), Panagiotis Doxaras, Nikolaos Gyzis, Nikiphoros Lytras, Yannis Tsarouchis, Nikos Engonopoulos, Constantine Andreou, Jannis Kounellis, sculptors such as Leonidas Drosis, Georgios Bonanos, Yannoulis Chalepas and Joannis Avramidis, conductor Dimitri Mitropoulos, soprano Maria Callas, composers such as Mikis Theodorakis, Nikos Skalkottas, Iannis Xenakis, Manos Hatzidakis, Eleni Karaindrou, Yanni and Vangelis, one of the best-selling singers worldwide Nana Mouskouri and poets such as Kostis Palamas, Dionysios Solomos, Angelos Sikelianos and Yannis Ritsos. Alexandrian Constantine P. Cavafy and Nobel laureates Giorgos Seferis and Odysseas Elytis are among the most important poets of the 20th century. Novel is also represented by Alexandros Papadiamantis and Nikos Kazantzakis. Greek colonies and communities have been historically established on the shores of the Mediterranean Sea and Black Sea, but the Greek people have always been centered around the Aegean and Ionian seas, where the Greek language has been spoken since the Bronze Age. Until the early 20th century, Greeks were distributed between the Greek peninsula, the western coast of Asia Minor, the Black Sea coast, Cappadocia in central Anatolia, Egypt, the Balkans, Cyprus, and Constantinople. Many of these regions coincided to a large extent with the borders of the Byzantine Empire of the late 11th century and the Eastern Mediterranean areas of ancient Greek colonization. The cultural centers of the Greeks have included Athens, Thessalonica, Alexandria, Smyrna, and Constantinople at various periods. As of 2007, Greece had the eighth highest percentage of tertiary enrollment in the world (with the percentages for female students being higher than for male) while Greeks of the Diaspora are equally active in the field of education. Hundreds of thousands of Greek students attend western universities every year while the faculty lists of leading Western universities contain a striking number of Greek names. Notable modern Greek scientists of modern times include Dimitrios Galanos, Georgios Papanikolaou (inventor of the Pap test), Nicholas Negroponte, Constantin Carathéodory, Manolis Andronikos, Michael Dertouzos, John Argyris, Panagiotis Kondylis, John Iliopoulos (2007 Dirac Prize for his contributions on the physics of the charm quark, a major contribution to the birth of the Standard Model, the modern theory of Elementary Particles), Joseph Sifakis (2007 Turing Award, the ""Nobel Prize"" of Computer Science), Christos Papadimitriou (2002 Knuth Prize, 2012 Gödel Prize), Mihalis Yannakakis (2005 Knuth Prize) and Dimitri Nanopoulos. The roots of Greek success in the Ottoman Empire can be traced to the Greek tradition of education and commerce. It was the wealth of the extensive merchant class that provided the material basis for the intellectual revival that was the prominent feature of Greek life in the half century and more leading to the outbreak of the Greek War of Independence in 1821. Not coincidentally, on the eve of 1821, the three most important centres of Greek learning were situated in Chios, Smyrna and Aivali, all three major centres of Greek commerce. Greek success was also favoured by Greek domination of the Christian Orthodox church. The history of the Greek people is closely associated with the history of Greece, Cyprus, Constantinople, Asia Minor and the Black Sea. During the Ottoman rule of Greece, a number of Greek enclaves around the Mediterranean were cut off from the core, notably in Southern Italy, the Caucasus, Syria and Egypt. By the early 20th century, over half of the overall Greek-speaking population was settled in Asia Minor (now Turkey), while later that century a huge wave of migration to the United States, Australia, Canada and elsewhere created the modern Greek diaspora. Today, Greeks are the majority ethnic group in the Hellenic Republic, where they constitute 93% of the country's population, and the Republic of Cyprus where they make up 78% of the island's population (excluding Turkish settlers in the occupied part of the country). Greek populations have not traditionally exhibited high rates of growth; nonetheless, the population of Greece has shown regular increase since the country's first census in 1828. A large percentage of the population growth since the state's foundation has resulted from annexation of new territories and the influx of 1.5 million Greek refugees after the 1923 population exchange between Greece and Turkey. About 80% of the population of Greece is urban, with 28% concentrated in the city of Athens The Eastern Roman Empire – today conventionally named the Byzantine Empire, a name not in use during its own time – became increasingly influenced by Greek culture after the 7th century, when Emperor Heraclius (AD 575 - 641) decided to make Greek the empire's official language. Certainly from then on, but likely earlier, the Roman and Greek cultures were virtually fused into a single Greco-Roman world. Although the Latin West recognized the Eastern Empire's claim to the Roman legacy for several centuries, after Pope Leo III crowned Charlemagne, king of the Franks, as the ""Roman Emperor"" on 25 December 800, an act which eventually led to the formation of the Holy Roman Empire, the Latin West started to favour the Franks and began to refer to the Eastern Roman Empire largely as the Empire of the Greeks (Imperium Graecorum). For those that remained under the Ottoman Empire's millet system, religion was the defining characteristic of national groups (milletler), so the exonym ""Greeks"" (Rumlar from the name Rhomaioi) was applied by the Ottomans to all members of the Orthodox Church, regardless of their language or ethnic origin. The Greek speakers were the only ethnic group to actually call themselves Romioi, (as opposed to being so named by others) and, at least those educated, considered their ethnicity (genos) to be Hellenic. There were, however, many Greeks who escaped the second-class status of Christians inherent in the Ottoman millet system, according to which Muslims were explicitly awarded senior status and preferential treatment. These Greeks either emigrated, particularly to their fellow Greek Orthodox protector, the Russian Empire, or simply converted to Islam, often only very superficially and whilst remaining crypto-Christian. The most notable examples of large-scale conversion to Turkish Islam among those today defined as Greek Muslims - excluding those who had to convert as a matter of course on being recruited through the devshirme - were to be found in Crete (Cretan Turks), Greek Macedonia (for example among the Vallahades of western Macedonia), and among Pontic Greeks in the Pontic Alps and Armenian Highlands. Several Ottoman sultans and princes were also of part Greek origin, with mothers who were either Greek concubines or princesses from Byzantine noble families, one famous example being sultan Selim the Grim, whose mother Gülbahar Hatun was a Pontic Greek. The terms used to define Greekness have varied throughout history but were never limited or completely identified with membership to a Greek state. By Western standards, the term Greeks has traditionally referred to any native speakers of the Greek language, whether Mycenaean, Byzantine or modern Greek. Byzantine Greeks called themselves Romioi and considered themselves the political heirs of Rome, but at least by the 12th century a growing number of those educated, deemed themselves the heirs of ancient Greece as well, although for most of the Greek speakers, ""Hellene"" still meant pagan. On the eve of the Fall of Constantinople the Last Emperor urged his soldiers to remember that they were the descendants of Greeks and Romans. Notable Greek seafarers include people such as Pytheas of Marseilles, Scylax of Caryanda who sailed to Iberia and beyond, Nearchus, the 6th century merchant and later monk Cosmas Indicopleustes (Cosmas who sailed to India) and the explorer of the Northwestern passage Juan de Fuca. In later times, the Romioi plied the sea-lanes of the Mediterranean and controlled trade until an embargo imposed by the Roman Emperor on trade with the Caliphate opened the door for the later Italian pre-eminence in trade. During and after the Greek War of Independence, Greeks of the diaspora were important in establishing the fledgling state, raising funds and awareness abroad. Greek merchant families already had contacts in other countries and during the disturbances many set up home around the Mediterranean (notably Marseilles in France, Livorno in Italy, Alexandria in Egypt), Russia (Odessa and Saint Petersburg), and Britain (London and Liverpool) from where they traded, typically in textiles and grain. Businesses frequently comprised the extended family, and with them they brought schools teaching Greek and the Greek Orthodox Church. Modern Greek has, in addition to Standard Modern Greek or Dimotiki, a wide variety of dialects of varying levels of mutual intelligibility, including Cypriot, Pontic, Cappadocian, Griko and Tsakonian (the only surviving representative of ancient Doric Greek). Yevanic is the language of the Romaniotes, and survives in small communities in Greece, New York and Israel. In addition to Greek, many Greeks in Greece and the Diaspora are bilingual in other languages or dialects such as English, Arvanitika/Albanian, Aromanian, Macedonian Slavic, Russian and Turkish. Greek culture has evolved over thousands of years, with its beginning in the Mycenaean civilization, continuing through the Classical period, the Roman and Eastern Roman periods and was profoundly affected by Christianity, which it in turn influenced and shaped. Ottoman Greeks had to endure through several centuries of adversity that culminated in genocide in the 20th century but nevertheless included cultural exchanges and enriched both cultures. The Diafotismos is credited with revitalizing Greek culture and giving birth to the synthesis of ancient and medieval elements that characterize it today. The traditional Greek homelands have been the Greek peninsula and the Aegean Sea, the Southern Italy (Magna Graecia), the Black Sea, the Ionian coasts of Asia Minor and the islands of Cyprus and Sicily. In Plato's Phaidon, Socrates remarks, ""we (Greeks) live around a sea like frogs around a pond"" when describing to his friends the Greek cities of the Aegean. This image is attested by the map of the Old Greek Diaspora, which corresponded to the Greek world until the creation of the Greek state in 1832. The sea and trade were natural outlets for Greeks since the Greek peninsula is rocky and does not offer good prospects for agriculture. Greek art has a long and varied history. Greeks have contributed to the visual, literary and performing arts. In the West, ancient Greek art was influential in shaping the Roman and later the modern western artistic heritage. Following the Renaissance in Europe, the humanist aesthetic and the high technical standards of Greek art inspired generations of European artists. Well into the 19th century, the classical tradition derived from Greece played an important role in the art of the western world. In the East, Alexander the Great's conquests initiated several centuries of exchange between Greek, Central Asian and Indian cultures, resulting in Greco-Buddhist art, whose influence reached as far as Japan. In Homer's Iliad, the names Danaans (or Danaoi: Δαναοί) and Argives (Argives: Αργείοι) are used to designate the Greek forces opposed to the Trojans. The myth of Danaus, whose origin is Egypt, is a foundation legend of Argos. His daughters Danaides, were forced in Tartarus to carry a jug to fill a bathtub without a bottom. This myth is connected with a task that can never be fulfilled (Sisyphos) and the name can be derived from the PIE root *danu: ""river"". There is not any satisfactory theory on their origin. Some scholars connect Danaans with the Denyen, one of the groups of the sea peoples who attacked Egypt during the reign of Ramesses III (1187-1156 BCE). The same inscription mentions the Weshesh who might have been the Achaeans. The Denyen seem to have been inhabitants of the city Adana in Cilicia. Pottery similar to that of Mycenae itself has been found in Tarsus of Cilicia and it seems that some refugees from the Aegean went there after the collapse of the Mycenean civilization. These Cilicians seem to have been called Dananiyim, the same word as Danaoi who attacked Egypt in 1191 BC along with the Quaouash (or Weshesh) who may be Achaeans. They were also called Danuna according to a Hittite inscription and the same name is mentioned in the Amarna letters. Julius Pokorny reconstructs the name from the PIE root da:-: ""flow, river"", da:-nu: ""any moving liquid, drops"", da: navo ""people living by the river, Skyth. nomadic people (in Rigveda water-demons, fem.Da:nu primordial goddess), in Greek Danaoi, Egypt. Danuna"". It is also possible that the name Danaans is pre-Greek. A country Danaja with a city Mukana (propaply: Mycenea) is mentioned in inscriptions from Egypt from Amenophis III (1390-1352 BC), Thutmosis III (1437 BC). The total number of Greeks living outside Greece and Cyprus today is a contentious issue. Where Census figures are available, they show around 3 million Greeks outside Greece and Cyprus. Estimates provided by the SAE - World Council of Hellenes Abroad put the figure at around 7 million worldwide. According to George Prevelakis of Sorbonne University, the number is closer to just below 5 million. Integration, intermarriage, and loss of the Greek language influence the self-identification of the Omogeneia. Important centres of the New Greek Diaspora today are London, New York, Melbourne and Toronto. In 2010, the Hellenic Parliament introduced a law that enables Diaspora Greeks in Greece to vote in the elections of the Greek state. This law was later repealed in early 2014. Around 1200 BC, the Dorians, another Greek-speaking people, followed from Epirus. Traditionally, historians have believed that the Dorian invasion caused the collapse of the Mycenaean civilization, but it is likely the main attack was made by seafaring raiders (sea peoples) who sailed into the eastern Mediterranean around 1180 BC. The Dorian invasion was followed by a poorly attested period of migrations, appropriately called the Greek Dark Ages, but by 800 BC the landscape of Archaic and Classical Greece was discernible. In the Hesiodic Catalogue of Women, Graecus is presented as the son of Zeus and Pandora II, sister of Hellen the patriarch of Hellenes. Hellen was the son of Deucalion who ruled around Phthia in central Greece. The Parian Chronicle mentions that when Deucalion became king of Phthia, the previously called Graikoi were named Hellenes. Aristotle notes that the Hellenes were related with Grai/Greeks (Meteorologica I.xiv) a native name of a Dorian tribe in Epirus which was used by the Illyrians. He also claims that the great deluge must have occurred in the region around Dodona, where the Selloi dwelt. However, according to the Greek tradition it is more possible that the homeland of the Greeks was originally in central Greece. A modern theory derives the name Greek (Latin Graeci) from Graikos, ""inhabitant of Graia/Graea,"" a town on the coast of Boeotia. Greek colonists from Graia helped to found Cumae (900 BC) in Italy, where they were called Graeces. When the Romans encountered them they used this name for the colonists and then for all Greeks (Graeci.) The word γραῖα graia ""old woman"" comes from the PIE root *ǵerh2-/*ǵreh2-, ""to grow old"" via Proto-Greek *gera-/grau-iu; the same root later gave γέρας geras (/keras/), ""gift of honour"" in Mycenean Greek. The Germanic languages borrowed the word Greeks with an initial ""k"" sound which probably was their initial sound closest to the Latin ""g"" at the time (Goth. Kreks). The area out of ancient Attica including Boeotia was called Graïke and is connected with the older deluge of Ogyges, the mythological ruler of Boeotia. The region was originally occupied by the Minyans who were autochthonous or Proto-Greek speaking people. In ancient Greek the name Ogygios came to mean ""from earliest days"". The classical period of Greek civilization covers a time spanning from the early 5th century BC to the death of Alexander the Great, in 323 BC (some authors prefer to split this period into 'Classical', from the end of the Persian wars to the end of the Peloponnesian War, and 'Fourth Century', up to the death of Alexander). It is so named because it set the standards by which Greek civilization would be judged in later eras. The Classical period is also described as the ""Golden Age"" of Greek civilization, and its art, philosophy, architecture and literature would be instrumental in the formation and development of Western culture. Greek demonstrates several linguistic features that are shared with other Balkan languages, such as Albanian, Bulgarian and Eastern Romance languages (see Balkan sprachbund), and has absorbed many foreign words, primarily of Western European and Turkish origin. Because of the movements of Philhellenism and the Diafotismos in the 19th century, which emphasized the modern Greeks' ancient heritage, these foreign influences were excluded from official use via the creation of Katharevousa, a somewhat artificial form of Greek purged of all foreign influence and words, as the official language of the Greek state. In 1976, however, the Hellenic Parliament voted to make the spoken Dimotiki the official language, making Katharevousa obsolete. Greek surnames were widely in use by the 9th century supplanting the ancient tradition of using the father’s name, however Greek surnames are most commonly patronymics. Commonly, Greek male surnames end in -s, which is the common ending for Greek masculine proper nouns in the nominative case. Exceptionally, some end in -ou, indicating the genitive case of this proper noun for patronymic reasons. Although surnames in mainland Greece are static today, dynamic and changing patronymic usage survives in middle names where the genitive of father's first name is commonly the middle name (this usage having been passed on to the Russians). In Cyprus, by contrast, surnames follow the ancient tradition of being given according to the father’s name. Finally, in addition to Greek-derived surnames many have Latin, Turkish and Italian origin. In ancient times, the trading and colonizing activities of the Greek tribes and city states spread the Greek culture, religion and language around the Mediterranean and Black Sea basins, especially in Sicily and southern Italy (also known as Magna Grecia), Spain, the south of France and the Black sea coasts. Under Alexander the Great's empire and successor states, Greek and Hellenizing ruling classes were established in the Middle East, India and in Egypt. The Hellenistic period is characterized by a new wave of Greek colonization that established Greek cities and kingdoms in Asia and Africa. Under the Roman Empire, easier movement of people spread Greeks across the Empire and in the eastern territories, Greek became the lingua franca rather than Latin. The modern-day Griko community of southern Italy, numbering about 60,000, may represent a living remnant of the ancient Greek populations of Italy. Most Greeks are Christians, belonging to the Greek Orthodox Church. During the first centuries after Jesus Christ, the New Testament was originally written in Koine Greek, which remains the liturgical language of the Greek Orthodox Church, and most of the early Christians and Church Fathers were Greek-speaking. There are small groups of ethnic Greeks adhering to other Christian denominations like Greek Catholics, Greek Evangelicals, Pentecostals, and groups adhering to other religions including Romaniot and Sephardic Jews and Greek Muslims. About 2,000 Greeks are members of Hellenic Polytheistic Reconstructionism congregations. Another study from 2012 included 150 dental school students from University of Athens, the result showed that light hair colour (blonde/light ash brown) was predominant in 10.7% of the students. 36% had medium hair colour (Light brown/Medium darkest brown). 32% had darkest brown and 21% black (15.3 off black, 6% midnight black). In conclusion the hair colour of young Greeks are mostly brown, ranging from light to dark brown. with significant minorities having black and blonde hair. The same study also showed that the eye colour of the students was 14.6% blue/green, 28% medium (light brown) and 57.4% dark brown. The relationship between ethnic Greek identity and Greek Orthodox religion continued after the creation of the Modern Greek state in 1830. According to the second article of the first Greek constitution of 1822, a Greek was defined as any Christian resident of the Kingdom of Greece, a clause removed by 1840. A century later, when the Treaty of Lausanne was signed between Greece and Turkey in 1923, the two countries agreed to use religion as the determinant for ethnic identity for the purposes of population exchange, although most of the Greeks displaced (over a million of the total 1.5 million) had already been driven out by the time the agreement was signed.[note 1] The Greek genocide, in particular the harsh removal of Pontian Greeks from the southern shore area of the Black Sea, contemporaneous with and following the failed Greek Asia Minor Campaign, was part of this process of Turkification of the Ottoman Empire and the placement of its economy and trade, then largely in Greek hands under ethnic Turkish control. The ethnogenesis of the Greek nation is linked to the development of Pan-Hellenism in the 8th century BC. According to some scholars, the foundational event was the Olympic Games in 776 BC, when the idea of a common Hellenism among the Greek tribes was first translated into a shared cultural experience and Hellenism was primarily a matter of common culture. The works of Homer (i.e. Iliad and Odyssey) and Hesiod (i.e. Theogony) were written in the 8th century BC, becoming the basis of the national religion, ethos, history and mythology. The Oracle of Apollo at Delphi was established in this period. The Greeks of the Classical era made several notable contributions to science and helped lay the foundations of several western scientific traditions, like philosophy, historiography and mathematics. The scholarly tradition of the Greek academies was maintained during Roman times with several academic institutions in Constantinople, Antioch, Alexandria and other centres of Greek learning while Eastern Roman science was essentially a continuation of classical science. Greeks have a long tradition of valuing and investing in paideia (education). Paideia was one of the highest societal values in the Greek and Hellenistic world while the first European institution described as a university was founded in 5th century Constantinople and operated in various incarnations until the city's fall to the Ottomans in 1453. The University of Constantinople was Christian Europe's first secular institution of higher learning since no theological subjects were taught, and considering the original meaning of the world university as a corporation of students, the world’s first university as well. In the religious sphere, this was a period of profound change. The spiritual revolution that took place, saw a waning of the old Greek religion, whose decline beginning in the 3rd century BC continued with the introduction of new religious movements from the East. The cults of deities like Isis and Mithra were introduced into the Greek world. Greek-speaking communities of the Hellenized East were instrumental in the spread of early Christianity in the 2nd and 3rd centuries, and Christianity's early leaders and writers (notably St Paul) were generally Greek-speaking, though none were from Greece. However, Greece itself had a tendency to cling to paganism and was not one of the influential centers of early Christianity: in fact, some ancient Greek religious practices remained in vogue until the end of the 4th century, with some areas such as the southeastern Peloponnese remaining pagan until well into the 10th century AD. Of the new eastern religions introduced into the Greek world, the most successful was Christianity. From the early centuries of the Common Era, the Greeks identified as Romaioi (""Romans""), by that time the name ‘Hellenes’ denoted pagans. While ethnic distinctions still existed in the Roman Empire, they became secondary to religious considerations and the renewed empire used Christianity as a tool to support its cohesion and promoted a robust Roman national identity. Concurrently the secular, urban civilization of late antiquity survived in the Eastern Mediterranean along with Greco-Roman educational system, although it was from Christianity that the culture's essential values were drawn. This age saw the Greeks move towards larger cities and a reduction in the importance of the city-state. These larger cities were parts of the still larger Kingdoms of the Diadochi. Greeks, however, remained aware of their past, chiefly through the study of the works of Homer and the classical authors. An important factor in maintaining Greek identity was contact with barbarian (non-Greek) peoples, which was deepened in the new cosmopolitan environment of the multi-ethnic Hellenistic kingdoms. This led to a strong desire among Greeks to organize the transmission of the Hellenic paideia to the next generation. Greek science, technology and mathematics are generally considered to have reached their peak during the Hellenistic period. Before the establishment of the Modern Greek state, the link between ancient and modern Greeks was emphasized by the scholars of Greek Enlightenment especially by Rigas Feraios. In his ""Political Constitution"", he addresses to the nation as ""the people descendant of the Greeks"". The modern Greek state was created in 1829, when the Greeks liberated a part of their historic homelands, Peloponnese, from the Ottoman Empire. The large Greek diaspora and merchant class were instrumental in transmitting the ideas of western romantic nationalism and philhellenism, which together with the conception of Hellenism, formulated during the last centuries of the Byzantine Empire, formed the basis of the Diafotismos and the current conception of Hellenism. Homer uses the terms Achaeans and Danaans (Δαναοί) as a generic term for Greeks in Iliad, and they were probably a part of the Mycenean civilization. The names Achaioi and Danaoi seem to be pre-Dorian belonging to the people who were overthrown. They were forced to the region that later bore the name Achaea after the Dorian invasion. In the 5th century BC, they were redefined as contemporary speakers of Aeolic Greek which was spoken mainly in Thessaly, Boeotia and Lesbos. There are many controversial theories on the origin of the Achaeans. According to one view, the Achaeans were one of the fair-headed tribes of upper Europe, who pressed down over the Alps during the early Iron age (1300 BC) to southern Europe. Another theory suggests that the Peloponnesian Dorians were the Achaeans. These theories are rejected by other scholars who, based on linguistic criteria, suggest that the Achaeans were mainland pre-Dorian Greeks. There is also the theory that there was an Achaean ethnos that migrated from Asia minor to lower Thessaly prior to 2000 BC. Some Hittite texts mention a nation lying to the west called Ahhiyava or Ahhiya. Egyptian documents refer to Ekwesh, one of the groups of sea peoples who attached Egypt during the reign of Merneptah (1213-1203 BCE), who may have been Achaeans. The most widely used symbol is the flag of Greece, which features nine equal horizontal stripes of blue alternating with white representing the nine syllables of the Greek national motto Eleftheria i thanatos (freedom or death), which was the motto of the Greek War of Independence. The blue square in the upper hoist-side corner bears a white cross, which represents Greek Orthodoxy. The Greek flag is widely used by the Greek Cypriots, although Cyprus has officially adopted a neutral flag to ease ethnic tensions with the Turkish Cypriot minority – see flag of Cyprus). Greeks from Cyprus have a similar history of emigration, usually to the English-speaking world because of the island's colonization by the British Empire. Waves of emigration followed the Turkish invasion of Cyprus in 1974, while the population decreased between mid-1974 and 1977 as a result of emigration, war losses, and a temporary decline in fertility. After the ethnic cleansing of a third of the Greek population of the island in 1974, there was also an increase in the number of Greek Cypriots leaving, especially for the Middle East, which contributed to a decrease in population that tapered off in the 1990s. Today more than two-thirds of the Greek population in Cyprus is urban. Homer refers to the ""Hellenes"" (/ˈhɛliːnz/) as a relatively small tribe settled in Thessalic Phthia, with its warriors under the command of Achilleus. The Parian Chronicle says that Phthia was the homeland of the Hellenes and that this name was given to those previously called Greeks (Γραικοί). In Greek mythology, Hellen, the patriarch of Hellenes, was son of Pyrrha and Deucalion, who ruled around Phthia, the only survivors after the great deluge. It seems that the myth was invented when the Greek tribes started to separate from each other in certain areas of Greece and it indicates their common origin. Aristotle names ancient Hellas as an area in Epirus between Dodona and the Achelous river, the location of the great deluge of Deucalion, a land occupied by the Selloi and the ""Greeks"" who later came to be known as ""Hellenes"". Selloi were the priests of Dodonian Zeus and the word probably means ""sacrificers"" (compare Gothic saljan, ""present, sacrifice""). There is currently no satisfactory etymology of the name Hellenes. Some scholars assert that the name Selloi changed to Sellanes and then to Hellanes-Hellenes. However this etymology connects the name Hellenes with the Dorians who occupied Epirus and the relation with the name Greeks given by the Romans becomes uncertain. The name Hellenes seems to be older and it was probably used by the Greeks with the establishment of the Great Amphictyonic League. This was an ancient association of Greek tribes with twelve founders which was organized to protect the great temples of Apollo in Delphi (Phocis) and of Demeter near Thermopylae (Locris). According to the legend it was founded after the Trojan War by the eponymous Amphictyon, brother of Hellen. The Greeks of classical antiquity idealized their Mycenaean ancestors and the Mycenaean period as a glorious era of heroes, closeness of the gods and material wealth. The Homeric Epics (i.e. Iliad and Odyssey) were especially and generally accepted as part of the Greek past and it was not until the 19th century that scholars began to question Homer's historicity. As part of the Mycenaean heritage that survived, the names of the gods and goddesses of Mycenaean Greece (e.g. Zeus, Poseidon and Hades) became major figures of the Olympian Pantheon of later antiquity. In any case, Alexander's toppling of the Achaemenid Empire, after his victories at the battles of the Granicus, Issus and Gaugamela, and his advance as far as modern-day Pakistan and Tajikistan, provided an important outlet for Greek culture, via the creation of colonies and trade routes along the way. While the Alexandrian empire did not survive its creator's death intact, the cultural implications of the spread of Hellenism across much of the Middle East and Asia were to prove long lived as Greek became the lingua franca, a position it retained even in Roman times. Many Greeks settled in Hellenistic cities like Alexandria, Antioch and Seleucia. Two thousand years later, there are still communities in Pakistan and Afghanistan, like the Kalash, who claim to be descended from Greek settlers. A distinct Greek political identity re-emerged in the 11th century in educated circles and became more forceful after the fall of Constantinople to the Crusaders of the Fourth Crusade in 1204, so that when the empire was revived in 1261, it became in many ways a Greek national state. That new notion of nationhood engendered a deep interest in the classical past culminating in the ideas of the Neoplatonist philosopher Gemistus Pletho, who abandoned Christianity. However, it was the combination of Orthodox Christianity with a specifically Greek identity that shaped the Greeks' notion of themselves in the empire's twilight years. The interest in the Classical Greek heritage was complemented by a renewed emphasis on Greek Orthodox identity, which was reinforced in the late Medieval and Ottoman Greeks' links with their fellow Orthodox Christians in the Russian Empire. These were further strengthened following the fall of the Empire of Trebizond in 1461, after which and until the second Russo-Turkish War of 1828-29 hundreds of thousands of Pontic Greeks fled or migrated from the Pontic Alps and Armenian Highlands to southern Russia and the Russian South Caucasus (see also Greeks in Russia, Greeks in Armenia, Greeks in Georgia, and Caucasian Greeks)."
London,"Outward urban expansion is now prevented by the Metropolitan Green Belt, although the built-up area extends beyond the boundary in places, resulting in a separately defined Greater London Urban Area. Beyond this is the vast London commuter belt. Greater London is split for some purposes into Inner London and Outer London. The city is split by the River Thames into North and South, with an informal central London area in its interior. The coordinates of the nominal centre of London, traditionally considered to be the original Eleanor Cross at Charing Cross near the junction of Trafalgar Square and Whitehall, are approximately 51°30′26″N 00°07′39″W﻿ / ﻿51.50722°N 0.12750°W﻿ / 51.50722; -0.12750. The 2011 census showed that 36.7 per cent of Greater London's population were born outside the UK. The table to the right shows the 30 most common countries of birth of London residents in 2011, the date of the last published UK Census. A portion of the German-born population are likely to be British nationals born to parents serving in the British Armed Forces in Germany. Estimates produced by the Office for National Statistics indicate that the five largest foreign-born groups living in London in the period July 2009 to June 2010 were those born in India, Poland, the Republic of Ireland, Bangladesh and Nigeria. The London Natural History Society suggest that London is ""one of the World's Greenest Cities"" with more than 40 percent green space or open water. They indicate that 2000 species of flowering plant have been found growing there and that the tidal Thames supports 120 species of fish. They also state that over 60 species of bird nest in central London and that their members have recorded 47 species of butterfly, 1173 moths and more than 270 kinds of spider around London. London's wetland areas support nationally important populations of many water birds. London has 38 Sites of Special Scientific Interest (SSSIs), two National Nature Reserves and 76 Local Nature Reserves. Some international railway services to Continental Europe were operated during the 20th century as boat trains, such as the Admiraal de Ruijter to Amsterdam and the Night Ferry to Paris and Brussels. The opening of the Channel Tunnel in 1994 connected London directly to the continental rail network, allowing Eurostar services to begin. Since 2007, high-speed trains link St. Pancras International with Lille, Paris, Brussels and European tourist destinations via the High Speed 1 rail link and the Channel Tunnel. The first high-speed domestic trains started in June 2009 linking Kent to London. There are plans for a second high speed line linking London to the Midlands, North West England, and Yorkshire. In 2003, a congestion charge was introduced to reduce traffic volumes in the city centre. With a few exceptions, motorists are required to pay £10 per day to drive within a defined zone encompassing much of central London. Motorists who are residents of the defined zone can buy a greatly reduced season pass. London government initially expected the Congestion Charge Zone to increase daily peak period Underground and bus users by 20,000 people, reduce road traffic by 10 to 15 per cent, increase traffic speeds by 10 to 15 per cent, and reduce queues by 20 to 30 per cent. Over the course of several years, the average number of cars entering the centre of London on a weekday was reduced from 195,000 to 125,000 cars – a 35-per-cent reduction of vehicles driven per day. London has a diverse range of peoples and cultures, and more than 300 languages are spoken within Greater London. The Office for National Statistics estimated its mid-2014 population to be 8,538,689, the largest of any municipality in the European Union, and accounting for 12.5 percent of the UK population. London's urban area is the second most populous in the EU, after Paris, with 9,787,426 inhabitants according to the 2011 census. The city's metropolitan area is one of the most populous in Europe with 13,879,757 inhabitants,[note 4] while the Greater London Authority states the population of the city-region (covering a large part of the south east) as 22.7 million. London was the world's most populous city from around 1831 to 1925. London i/ˈlʌndən/ is the capital and most populous city of England and the United Kingdom. Standing on the River Thames in the south eastern part of the island of Great Britain, London has been a major settlement for two millennia. It was founded by the Romans, who named it Londinium. London's ancient core, the City of London, largely retains its 1.12-square-mile (2.9 km2) medieval boundaries and in 2011 had a resident population of 7,375, making it the smallest city in England. Since at least the 19th century, the term London has also referred to the metropolis developed around this core. The bulk of this conurbation forms Greater London,[note 1] a region of England governed by the Mayor of London and the London Assembly.[note 2] The conurbation also covers two English counties: the small district of the City of London and the county of Greater London. The latter constitutes the vast majority of London, though historically it was split between Middlesex (a now abolished county), Essex, Surrey, Kent and Hertfordshire. The largest parks in the central area of London are three of the eight Royal Parks, namely Hyde Park and its neighbour Kensington Gardens in the west, and Regent's Park to the north. Hyde Park in particular is popular for sports and sometimes hosts open-air concerts. Regent's Park contains London Zoo, the world's oldest scientific zoo, and is near the tourist attraction of Madame Tussauds Wax Museum. Primrose Hill in the northern part of Regent's Park at 256 feet (78 m) is a popular spot to view the city skyline. Primarily starting in the mid-1960s, London became a centre for the worldwide youth culture, exemplified by the Swinging London subculture associated with the King's Road, Chelsea and Carnaby Street. The role of trendsetter was revived during the punk era. In 1965 London's political boundaries were expanded to take into account the growth of the urban area and a new Greater London Council was created. During The Troubles in Northern Ireland, London was subjected to bombing attacks by the Provisional IRA. Racial inequality was highlighted by the 1981 Brixton riot. London was instrumental in the development of punk music, with figures such as the Sex Pistols, The Clash, and Vivienne Westwood all based in the city. More recent artists to emerge from the London music scene include George Michael, Kate Bush, Seal, Siouxsie and the Banshees, Bush, the Spice Girls, Jamiroquai, Blur, The Prodigy, Gorillaz, Mumford & Sons, Coldplay, Amy Winehouse, Adele, Ed Sheeran and One Direction. London is also a centre for urban music. In particular the genres UK garage, drum and bass, dubstep and grime evolved in the city from the foreign genres of hip hop and reggae, alongside local drum and bass. Black music station BBC Radio 1Xtra was set up to support the rise of home-grown urban music both in London and in the rest of the UK. Within London, both the City of London and the City of Westminster have city status and both the City of London and the remainder of Greater London are counties for the purposes of lieutenancies. The area of Greater London has incorporated areas that were once part of the historic counties of Middlesex, Kent, Surrey, Essex and Hertfordshire. London's status as the capital of England, and later the United Kingdom, has never been granted or confirmed officially—by statute or in written form.[note 6] The administration of London is formed of two tiers—a city-wide, strategic tier and a local tier. City-wide administration is coordinated by the Greater London Authority (GLA), while local administration is carried out by 33 smaller authorities. The GLA consists of two elected components; the Mayor of London, who has executive powers, and the London Assembly, which scrutinises the mayor's decisions and can accept or reject the mayor's budget proposals each year. The headquarters of the GLA is City Hall, Southwark; the mayor is Boris Johnson. The mayor's statutory planning strategy is published as the London Plan, which was most recently revised in 2011. The local authorities are the councils of the 32 London boroughs and the City of London Corporation. They are responsible for most local services, such as local planning, schools, social services, local roads and refuse collection. Certain functions, such as waste management, are provided through joint arrangements. In 2009–2010 the combined revenue expenditure by London councils and the GLA amounted to just over £22 billion (£14.7 billion for the boroughs and £7.4 billion for the GLA). London is a leading global city, with strengths in the arts, commerce, education, entertainment, fashion, finance, healthcare, media, professional services, research and development, tourism, and transport all contributing to its prominence. It is one of the world's leading financial centres and has the fifth-or sixth-largest metropolitan area GDP in the world depending on measurement.[note 3] London is a world cultural capital. It is the world's most-visited city as measured by international arrivals and has the world's largest city airport system measured by passenger traffic. London is one of the world's leading investment destinations, hosting more international retailers and ultra high-net-worth individuals than any other city. London's 43 universities form the largest concentration of higher education institutes in Europe, and a 2014 report placed it first in the world university rankings. According to the report London also ranks first in the world in software, multimedia development and design, and shares first position in technology readiness. In 2012, London became the first city to host the modern Summer Olympic Games three times. The Monument in the City of London provides views of the surrounding area while commemorating the Great Fire of London, which originated nearby. Marble Arch and Wellington Arch, at the north and south ends of Park Lane respectively, have royal connections, as do the Albert Memorial and Royal Albert Hall in Kensington. Nelson's Column is a nationally recognised monument in Trafalgar Square, one of the focal points of central London. Older buildings are mainly brick built, most commonly the yellow London stock brick or a warm orange-red variety, often decorated with carvings and white plaster mouldings. Following his victory in the Battle of Hastings, William, Duke of Normandy, was crowned King of England in the newly finished Westminster Abbey on Christmas Day 1066. William constructed the Tower of London, the first of the many Norman castles in England to be rebuilt in stone, in the southeastern corner of the city, to intimidate the native inhabitants. In 1097, William II began the building of Westminster Hall, close by the abbey of the same name. The hall became the basis of a new Palace of Westminster. The majority of British Jews live in London, with significant Jewish communities in Stamford Hill, Stanmore, Golders Green, Finchley, Hampstead, Hendon and Edgware in North London. Bevis Marks Synagogue in the City of London is affiliated to London's historic Sephardic Jewish community. It is the only synagogue in Europe which has held regular services continuously for over 300 years. Stanmore and Canons Park Synagogue has the largest membership of any single Orthodox synagogue in the whole of Europe, overtaking Ilford synagogue (also in London) in 1998. The community set up the London Jewish Forum in 2006 in response to the growing significance of devolved London Government. Transport is one of the four main areas of policy administered by the Mayor of London, however the mayor's financial control does not extend to the longer distance rail network that enters London. In 2007 he assumed responsibility for some local lines, which now form the London Overground network, adding to the existing responsibility for the London Underground, trams and buses. The public transport network is administered by Transport for London (TfL) and is one of the most extensive in the world. There is a variety of annual events, beginning with the relatively new New Year's Day Parade, fireworks display at the London Eye, the world's second largest street party, the Notting Hill Carnival is held during the late August Bank Holiday each year. Traditional parades include November's Lord Mayor's Show, a centuries-old event celebrating the annual appointment of a new Lord Mayor of the City of London with a procession along the streets of the City, and June's Trooping the Colour, a formal military pageant performed by regiments of the Commonwealth and British armies to celebrate the Queen's Official Birthday. Islington's 1 mile (1.6 km) long Upper Street, extending northwards from Angel, has more bars and restaurants than any other street in the United Kingdom. Europe's busiest shopping area is Oxford Street, a shopping street nearly 1 mile (1.6 km) long, making it the longest shopping street in the United Kingdom. Oxford Street is home to vast numbers of retailers and department stores, including the world-famous Selfridges flagship store. Knightsbridge, home to the equally renowned Harrods department store, lies to the south-west. London is a major international air transport hub with the busiest city airspace in the world. Eight airports use the word London in their name, but most traffic passes through six of these. London Heathrow Airport, in Hillingdon, West London, is the busiest airport in the world for international traffic, and is the major hub of the nation's flag carrier, British Airways. In March 2008 its fifth terminal was opened. There were plans for a third runway and a sixth terminal; however, these were cancelled by the Coalition Government on 12 May 2010. The pilgrims in Geoffrey Chaucer's late 14th-century Canterbury Tales set out for Canterbury from London – specifically, from the Tabard inn, Southwark. William Shakespeare spent a large part of his life living and working in London; his contemporary Ben Jonson was also based there, and some of his work—most notably his play The Alchemist—was set in the city. A Journal of the Plague Year (1722) by Daniel Defoe is a fictionalisation of the events of the 1665 Great Plague. Later important depictions of London from the 19th and early 20th centuries are Dickens' novels, and Arthur Conan Doyle's Sherlock Holmes stories. Modern writers pervasively influenced by the city include Peter Ackroyd, author of a ""biography"" of London, and Iain Sinclair, who writes in the genre of psychogeography. Close to Richmond Park is Kew Gardens which has the world's largest collection of living plants. In 2003, the gardens were put on the UNESCO list of World Heritage Sites. There are also numerous parks administered by London's borough Councils, including Victoria Park in the East End and Battersea Park in the centre. Some more informal, semi-natural open spaces also exist, including the 320-hectare (790-acre) Hampstead Heath of North London, and Epping Forest, which covers 2,476 hectares (6,118.32 acres) in the east. Both are controlled by the City of London Corporation. Hampstead Heath incorporates Kenwood House, the former stately home and a popular location in the summer months where classical musical concerts are held by the lake, attracting thousands of people every weekend to enjoy the music, scenery and fireworks. Epping Forest is a popular venue for various outdoor activities, including mountain biking, walking, horse riding, golf, angling, and orienteering. Among other inhabitants of London are 10,000 foxes, so that there are now 16 foxes for every square mile (2.6 square kilometres) of London. These urban foxes are noticeably bolder than their country cousins, sharing the pavement with pedestrians and raising cubs in people's backyards. Foxes have even sneaked into the Houses of Parliament, where one was found asleep on a filing cabinet. Another broke into the grounds of Buckingham Palace, reportedly killing some of Queen Elizabeth II's prized pink flamingos. Generally, however, foxes and city folk appear to get along. A survey in 2001 by the London-based Mammal Society found that 80 percent of 3,779 respondents who volunteered to keep a diary of garden mammal visits liked having them around. This sample cannot be taken to represent Londoners as a whole. London is home to five major medical schools – Barts and The London School of Medicine and Dentistry (part of Queen Mary), King's College London School of Medicine (the largest medical school in Europe), Imperial College School of Medicine, UCL Medical School and St George's, University of London – and has a large number of affiliated teaching hospitals. It is also a major centre for biomedical research, and three of the UK's five academic health science centres are based in the city – Imperial College Healthcare, King's Health Partners and UCL Partners (the largest such centre in Europe). The Vikings established Danelaw over much of the eastern and northern part of England with its boundary roughly stretching from London to Chester. It was an area of political and geographical control imposed by the Viking incursions which was formally agreed to by the Danish warlord, Guthrum and west-Saxon king, Alfred the Great in 886 AD. The Anglo-Saxon Chronicle recorded that London was ""refounded"" by Alfred the Great in 886. Archaeological research shows that this involved abandonment of Lundenwic and a revival of life and trade within the old Roman walls. London then grew slowly until about 950, after which activity increased dramatically. The 2011 census recorded that 2,998,264 people or 36.7% of London's population are foreign-born making London the city with the second largest immigrant population, behind New York City, in terms of absolute numbers. The table to the right shows the most common countries of birth of London residents. Note that some of the German-born population, in 18th position, are British citizens from birth born to parents serving in the British Armed Forces in Germany. With increasing industrialisation, London's population grew rapidly throughout the 19th and early 20th centuries, and it was for some time in the late 19th and early 20th centuries the most populous city in the world. Its population peaked at 8,615,245 in 1939 immediately before the outbreak of the Second World War, but had declined to 7,192,091 at the 2001 Census. However, the population then grew by just over a million between the 2001 and 2011 Censuses, to reach 8,173,941 in the latter enumeration. In the latter half of the 19th century the locale of South Kensington was developed as ""Albertopolis"", a cultural and scientific quarter. Three major national museums are there: the Victoria and Albert Museum (for the applied arts), the Natural History Museum and the Science Museum. The National Portrait Gallery was founded in 1856 to house depictions of figures from British history; its holdings now comprise the world's most extensive collection of portraits. The national gallery of British art is at Tate Britain, originally established as an annexe of the National Gallery in 1897. The Tate Gallery, as it was formerly known, also became a major centre for modern art; in 2000 this collection moved to Tate Modern, a new gallery housed in the former Bankside Power Station. There are a number of business schools in London, including the London School of Business and Finance, Cass Business School (part of City University London), Hult International Business School, ESCP Europe, European Business School London, Imperial College Business School and the London Business School. London is also home to many specialist arts education institutions, including the Academy of Live and Recorded Arts, Central School of Ballet, LAMDA, London College of Contemporary Arts (LCCA), London Contemporary Dance School, National Centre for Circus Arts, RADA, Rambert School of Ballet and Contemporary Dance, the Royal College of Art, the Royal College of Music and Trinity Laban. London's bus network is one of the largest in the world, running 24 hours a day, with about 8,500 buses, more than 700 bus routes and around 19,500 bus stops. In 2013, the network had more than 2 billion commuter trips per annum, more than the Underground. Around £850 million is taken in revenue each year. London has the largest wheelchair accessible network in the world and, from the 3rd quarter of 2007, became more accessible to hearing and visually impaired passengers as audio-visual announcements were introduced. The distinctive red double-decker buses are an internationally recognised trademark of London transport along with black cabs and the Tube. During the English Civil War the majority of Londoners supported the Parliamentary cause. After an initial advance by the Royalists in 1642 culminating in the battles of Brentford and Turnham Green, London was surrounded by defensive perimeter wall known as the Lines of Communication. The lines were built by an up to 20,000 people, and were completed in under two months. The fortifications failed their only test when the New Model Army entered London in 1647, and they were levelled by Parliament the same year. Summers are generally warm and sometimes hot. London's average July high is 24 °C (75.2 °F). On average London will see 31 days above 25 °C (77.0 °F) each year, and 4.2 days above 30.0 °C (86.0 °F) every year. During the 2003 European heat wave there were 14 consecutive days above 30 °C (86.0 °F) and 2 consecutive days where temperatures reached 38 °C (100.4 °F), leading to hundreds of heat related deaths. Winters are generally cool and damp with little temperature variation. Snowfall does occur from time to time, and can cause travel disruption when this happens. Spring and autumn are mixed seasons and can be pleasant. As a large city, London has a considerable urban heat island effect, making the centre of London at times 5 °C (9 °F) warmer than the suburbs and outskirts. The effect of this can be seen below when comparing London Heathrow, 15 miles west of London, with the London Weather Centre, in the city centre. London is also home to sizeable Muslim, Hindu, Sikh, and Jewish communities. Notable mosques include the East London Mosque in Tower Hamlets, London Central Mosque on the edge of Regent's Park and the Baitul Futuh Mosque of the Ahmadiyya Muslim Community. Following the oil boom, increasing numbers of wealthy Hindus and Middle-Eastern Muslims have based themselves around Mayfair and Knightsbridge in West London. There are large Muslim communities in the eastern boroughs of Tower Hamlets and Newham. Large Hindu communities are in the north-western boroughs of Harrow and Brent, the latter of which is home to Europe's largest Hindu temple, Neasden Temple. London is also home to 42 Hindu temples. There are Sikh communities in East and West London, particularly in Southall, home to one of the largest Sikh populations and the largest Sikh temple outside India. Three Aviva Premiership rugby union teams are based in London, (London Irish, Saracens, and Harlequins), although currently only Harlequins and Saracens play their home games within Greater London. London Scottish and London Welsh play in the RFU Championship club and other rugby union clubs in the city include Richmond F.C., Rosslyn Park F.C., Westcombe Park R.F.C. and Blackheath F.C.. Twickenham Stadium in south-west London is the national rugby union stadium, and has a capacity of 82,000 now that the new south stand has been completed. Other mammals found in Greater London are hedgehogs, rats, mice, rabbit, shrew, vole, and squirrels, In wilder areas of Outer London, such as Epping Forest, a wide variety of mammals are found including hare, badger, field, bank and water vole, wood mouse, yellow-necked mouse, mole, shrew, and weasel, in addition to fox, squirrel and hedgehog. A dead otter was found at The Highway, in Wapping, about a mile from the Tower Bridge, which would suggest that they have begun to move back after being absent a hundred years from the city. Ten of England's eighteen species of bats have been recorded in Epping Forest: soprano, nathusius and common pipistrelles, noctule, serotine, barbastelle, daubenton's, brown Long-eared, natterer's and leisler's. London has been the setting for many works of literature. The literary centres of London have traditionally been hilly Hampstead and (since the early 20th century) Bloomsbury. Writers closely associated with the city are the diarist Samuel Pepys, noted for his eyewitness account of the Great Fire, Charles Dickens, whose representation of a foggy, snowy, grimy London of street sweepers and pickpockets has been a major influence on people's vision of early Victorian London, and Virginia Woolf, regarded as one of the foremost modernist literary figures of the 20th century. London is the world's most expensive office market for the last three years according to world property journal (2015) report. As of 2015[update] the residential property in London is worth $2.2 trillion - same value as that of Brazil annual GDP. The city has the highest property prices of any European city according to the Office for National Statistics and the European Office of Statistics. On average the price per square metre in central London is €24,252 (April 2014). This is higher than the property prices in other G8 European capital cities; Berlin €3,306, Rome €6,188 and Paris €11,229. London contains four World Heritage Sites: the Tower of London; Kew Gardens; the site comprising the Palace of Westminster, Westminster Abbey, and St Margaret's Church; and the historic settlement of Greenwich (in which the Royal Observatory, Greenwich marks the Prime Meridian, 0° longitude, and GMT). Other famous landmarks include Buckingham Palace, the London Eye, Piccadilly Circus, St Paul's Cathedral, Tower Bridge, Trafalgar Square, and The Shard. London is home to numerous museums, galleries, libraries, sporting events and other cultural institutions, including the British Museum, National Gallery, Tate Modern, British Library and 40 West End theatres. The London Underground is the oldest underground railway network in the world. Along with professional services, media companies are concentrated in London and the media distribution industry is London's second most competitive sector. The BBC is a significant employer, while other broadcasters also have headquarters around the City. Many national newspapers are edited in London. London is a major retail centre and in 2010 had the highest non-food retail sales of any city in the world, with a total spend of around £64.2 billion. The Port of London is the second-largest in the United Kingdom, handling 45 million tonnes of cargo each year. London is home to many museums, galleries, and other institutions, many of which are free of admission charges and are major tourist attractions as well as playing a research role. The first of these to be established was the British Museum in Bloomsbury, in 1753. Originally containing antiquities, natural history specimens and the national library, the museum now has 7 million artefacts from around the globe. In 1824 the National Gallery was founded to house the British national collection of Western paintings; this now occupies a prominent position in Trafalgar Square. In 1762, George III acquired Buckingham House and it was enlarged over the next 75 years. During the 18th century, London was dogged by crime, and the Bow Street Runners were established in 1750 as a professional police force. In total, more than 200 offences were punishable by death, including petty theft. Most children born in the city died before reaching their third birthday. The coffeehouse became a popular place to debate ideas, with growing literacy and the development of the printing press making news widely available; and Fleet Street became the centre of the British press. London is one of the major classical and popular music capitals of the world and is home to major music corporations, such as EMI and Warner Music Group as well as countless bands, musicians and industry professionals. The city is also home to many orchestras and concert halls, such as the Barbican Arts Centre (principal base of the London Symphony Orchestra and the London Symphony Chorus), Cadogan Hall (Royal Philharmonic Orchestra) and the Royal Albert Hall (The Proms). London's two main opera houses are the Royal Opera House and the London Coliseum. The UK's largest pipe organ is at the Royal Albert Hall. Other significant instruments are at the cathedrals and major churches. Several conservatoires are within the city: Royal Academy of Music, Royal College of Music, Guildhall School of Music and Drama and Trinity Laban. London was the world's largest city from about 1831 to 1925. London's overcrowded conditions led to cholera epidemics, claiming 14,000 lives in 1848, and 6,000 in 1866. Rising traffic congestion led to the creation of the world's first local urban rail network. The Metropolitan Board of Works oversaw infrastructure expansion in the capital and some of the surrounding counties; it was abolished in 1889 when the London County Council was created out of those areas of the counties surrounding the capital. London was bombed by the Germans during the First World War while during the Second World War, the Blitz and other bombings by the German Luftwaffe, killed over 30,000 Londoners and destroyed large tracts of housing and other buildings across the city. Immediately after the war, the 1948 Summer Olympics were held at the original Wembley Stadium, at a time when London had barely recovered from the war. London has numerous venues for rock and pop concerts, including the world's busiest arena the o2 arena and other large arenas such as Earls Court, Wembley Arena, as well as many mid-sized venues, such as Brixton Academy, the Hammersmith Apollo and the Shepherd's Bush Empire. Several music festivals, including the Wireless Festival, South West Four, Lovebox, and Hyde Park's British Summer Time are all held in London. The city is home to the first and original Hard Rock Cafe and the Abbey Road Studios where The Beatles recorded many of their hits. In the 1960s, 1970s and 1980s, musicians and groups like Elton John, Pink Floyd, David Bowie, Queen, The Kinks, The Rolling Stones, The Who, Eric Clapton, Led Zeppelin, The Small Faces, Iron Maiden, Fleetwood Mac, Elvis Costello, Cat Stevens, The Police, The Cure, Madness, The Jam, Dusty Springfield, Phil Collins, Rod Stewart and Sade, derived their sound from the streets and rhythms vibrating through London. A number of world-leading education institutions are based in London. In the 2014/15 QS World University Rankings, Imperial College London is ranked joint 2nd in the world (alongside The University of Cambridge), University College London (UCL) is ranked 5th, and King's College London (KCL) is ranked 16th. The London School of Economics has been described as the world's leading social science institution for both teaching and research. The London Business School is considered one of the world's leading business schools and in 2015 its MBA programme was ranked second best in the world by the Financial Times. With the collapse of Roman rule in the early 5th century, London ceased to be a capital and the walled city of Londinium was effectively abandoned, although Roman civilisation continued in the St Martin-in-the-Fields area until around 450. From around 500, an Anglo-Saxon settlement known as Lundenwic developed in the same area, slightly to the west of the old Roman city. By about 680, it had revived sufficiently to become a major port, although there is little evidence of large-scale production of goods. From the 820s the town declined because of repeated Viking invasions. There are three recorded Viking assaults on London; two of which were successful in 851 and 886 AD, although they were defeated during the attack of 994 AD. The majority of primary and secondary schools and further-education colleges in London are controlled by the London boroughs or otherwise state-funded; leading examples include City and Islington College, Ealing, Hammersmith and West London College, Leyton Sixth Form College, Tower Hamlets College and Bethnal Green Academy. There are also a number of private schools and colleges in London, some old and famous, such as City of London School, Harrow, St Paul's School, Haberdashers' Aske's Boys' School, University College School, The John Lyon School, Highgate School and Westminster School. Within the City of Westminster in London the entertainment district of the West End has its focus around Leicester Square, where London and world film premieres are held, and Piccadilly Circus, with its giant electronic advertisements. London's theatre district is here, as are many cinemas, bars, clubs and restaurants, including the city's Chinatown district (in Soho), and just to the east is Covent Garden, an area housing speciality shops. The city is the home of Andrew Lloyd Webber, whose musicals have dominated the West End theatre since the late 20th century. The United Kingdom's Royal Ballet, English National Ballet, Royal Opera and English National Opera are based in London and perform at the Royal Opera House, the London Coliseum, Sadler's Wells Theatre and the Royal Albert Hall as well as touring the country. There are 366 railway stations in the London Travelcard Zones on an extensive above-ground suburban railway network. South London, particularly, has a high concentration of railways as it has fewer Underground lines. Most rail lines terminate around the centre of London, running into eighteen terminal stations, with the exception of the Thameslink trains connecting Bedford in the north and Brighton in the south via Luton and Gatwick airports. London has Britain's busiest station by number of passengers – Waterloo, with over 184 million people using the interchange station complex (which includes Waterloo East station) each year. Clapham Junction is the busiest station in Europe by the number of trains passing. A number of universities in London are outside the University of London system, including Brunel University, City University London, Imperial College London, Kingston University, London Metropolitan University, Middlesex University, University of East London, University of West London and University of Westminster, (with over 34,000 students, the largest unitary university in London), London South Bank University, Middlesex University, University of the Arts London (the largest university of art, design, fashion, communication and the performing arts in Europe), University of East London, the University of West London and the University of Westminster. In addition there are three international universities in London – Regent's University London, Richmond, The American International University in London and Schiller International University. Stansted Airport, north east of London in Essex, is a local UK hub and Luton Airport to the north of London in Bedfordshire, caters mostly for cheap short-haul flights. London City Airport, the smallest and most central airport, in Newham, East London, is focused on business travellers, with a mixture of full service short-haul scheduled flights and considerable business jet traffic. London Southend Airport, east of London in Essex, is a smaller, regional airport that mainly caters for cheap short-haul flights. During the 12th century, the institutions of central government, which had hitherto accompanied the royal English court as it moved around the country, grew in size and sophistication and became increasingly fixed in one place. In most cases this was Westminster, although the royal treasury, having been moved from Winchester, came to rest in the Tower. While the City of Westminster developed into a true capital in governmental terms, its distinct neighbour, the City of London, remained England's largest city and principal commercial centre, and it flourished under its own unique administration, the Corporation of London. In 1100, its population was around 18,000; by 1300 it had grown to nearly 100,000. Although there is evidence of scattered Brythonic settlements in the area, the first major settlement was founded by the Romans after the invasion of 43 AD. This lasted only until around 61, when the Iceni tribe led by Queen Boudica stormed it, burning it to the ground. The next, heavily planned, incarnation of Londinium prospered, and it superseded Colchester as the capital of the Roman province of Britannia in 100. At its height in the 2nd century, Roman London had a population of around 60,000. Greater London encompasses a total area of 1,583 square kilometres (611 sq mi), an area which had a population of 7,172,036 in 2001 and a population density of 4,542 inhabitants per square kilometre (11,760/sq mi). The extended area known as the London Metropolitan Region or the London Metropolitan Agglomeration, comprises a total area of 8,382 square kilometres (3,236 sq mi) has a population of 13,709,000 and a population density of 1,510 inhabitants per square kilometre (3,900/sq mi). Modern London stands on the Thames, its primary geographical feature, a navigable river which crosses the city from the south-west to the east. The Thames Valley is a floodplain surrounded by gently rolling hills including Parliament Hill, Addington Hills, and Primrose Hill. The Thames was once a much broader, shallower river with extensive marshlands; at high tide, its shores reached five times their present width. With 120,000 students in London, the federal University of London is the largest contact teaching university in the UK. It includes four large multi-faculty universities – King's College London, Queen Mary, Royal Holloway and UCL – and a number of smaller and more specialised institutions including Birkbeck, the Courtauld Institute of Art, Goldsmiths, Guildhall School of Music and Drama, the Institute of Education, the London Business School, the London School of Economics, the London School of Hygiene & Tropical Medicine, the Royal Academy of Music, the Central School of Speech and Drama, the Royal Veterinary College and the School of Oriental and African Studies. Members of the University of London have their own admissions procedures, and some award their own degrees. London is one of the leading tourist destinations in the world and in 2015 was ranked as the most visited city in the world with over 65 million visits. It is also the top city in the world by visitor cross-border spending, estimated at US$20.23 billion in 2015 Tourism is one of London's prime industries, employing the equivalent of 350,000 full-time workers in 2003, and the city accounts for 54% of all inbound visitor spend in UK. As of 2016 London is rated as the world top ranked city destination by TripAdvisor users. From 1898, it was commonly accepted that the name was of Celtic origin and meant place belonging to a man called *Londinos; this explanation has since been rejected. Richard Coates put forward an explanation in 1998 that it is derived from the pre-Celtic Old European *(p)lowonida, meaning 'river too wide to ford', and suggested that this was a name given to the part of the River Thames which flows through London; from this, the settlement gained the Celtic form of its name, *Lowonidonjon; this requires quite a serious amendment however. The ultimate difficulty lies in reconciling the Latin form Londinium with the modern Welsh Llundain, which should demand a form *(h)lōndinion (as opposed to *londīnion), from earlier *loundiniom. The possibility cannot be ruled out that the Welsh name was borrowed back in from English at a later date, and thus cannot be used as a basis from which to reconstruct the original name. Although the majority of journeys involving central London are made by public transport, car travel is common in the suburbs. The inner ring road (around the city centre), the North and South Circular roads (in the suburbs), and the outer orbital motorway (the M25, outside the built-up area) encircle the city and are intersected by a number of busy radial routes—but very few motorways penetrate into inner London. A plan for a comprehensive network of motorways throughout the city (the Ringways Plan) was prepared in the 1960s but was mostly cancelled in the early 1970s. The M25 is the longest ring-road motorway in the world at 121.5 mi (195.5 km) long. The A1 and M1 connect London to Leeds, and Newcastle and Edinburgh. London is the seat of the Government of the United Kingdom. Many government departments are based close to the Palace of Westminster, particularly along Whitehall, including the Prime Minister's residence at 10 Downing Street. The British Parliament is often referred to as the ""Mother of Parliaments"" (although this sobriquet was first applied to England itself by John Bright) because it has been the model for most other parliamentary systems. There are 73 Members of Parliament (MPs) from London, who correspond to local parliamentary constituencies in the national Parliament. As of May 2015, 45 are from the Labour Party, 27 are Conservatives, and one is a Liberal Democrat. Policing in Greater London, with the exception of the City of London, is provided by the Metropolitan Police Service, overseen by the Mayor through the Mayor's Office for Policing and Crime (MOPAC). The City of London has its own police force – the City of London Police. The British Transport Police are responsible for police services on National Rail, London Underground, Docklands Light Railway and Tramlink services. A fourth police force in London, the Ministry of Defence Police, do not generally become involved with policing the general public. There are many accents that are traditionally thought of as London accents. The most well known of the London accents long ago acquired the Cockney label, which is heard both in London itself, and across the wider South East England region more generally. The accent of a 21st-century 'Londoner' varies widely; what is becoming more and more common amongst the under-30s however is some fusion of Cockney with a whole array of 'ethnic' accents, in particular Caribbean, which form an accent labelled Multicultural London English (MLE). The other widely heard and spoken accent is RP (Received Pronunciation) in various forms, which can often be heard in the media and many of other traditional professions and beyond, although this accent is not limited to London and South East England, and can also be heard selectively throughout the whole UK amongst certain social groupings. The region covers an area of 1,579 square kilometres (610 sq mi). The population density is 5,177 inhabitants per square kilometre (13,410/sq mi), more than ten times that of any other British region. In terms of population, London is the 19th largest city and the 18th largest metropolitan region in the world. As of 2014[update], London has the largest number of billionaires (British Pound Sterling) in the world, with 72 residing in the city. London ranks as one of the most expensive cities in the world, alongside Tokyo and Moscow. In the dense areas, most of the concentration is via medium- and high-rise buildings. London's skyscrapers such as 30 St Mary Axe, Tower 42, the Broadgate Tower and One Canada Square are mostly in the two financial districts, the City of London and Canary Wharf. High-rise development is restricted at certain sites if it would obstruct protected views of St Paul's Cathedral and other historic buildings. Nevertheless, there are a number of very tall skyscrapers in central London (see Tall buildings in London), including the 95-storey Shard London Bridge, the tallest building in the European Union. Walking is a popular recreational activity in London. Areas that provide for walks include Wimbledon Common, Epping Forest, Hampton Court Park, Hampstead Heath, the eight Royal Parks, canals and disused railway tracks. Access to canals and rivers has improved recently, including the creation of the Thames Path, some 28 miles (45 km) of which is within Greater London, and The Wandle Trail; this runs 12 miles (19 km) through South London along the River Wandle, a tributary of the River Thames. Other long distance paths, linking green spaces, have also been created, including the Capital Ring, the Green Chain Walk, London Outer Orbital Path (""Loop""), Jubilee Walkway, Lea Valley Walk, and the Diana, Princess of Wales Memorial Walk. London's first and only cable car, known as the Emirates Air Line, opened in June 2012. Crossing the River Thames, linking Greenwich Peninsula and the Royal Docks in the east of the city, the cable car is integrated with London's Oyster Card ticketing system, although special fares are charged. Costing £60 million to build, it carries over 3,500 passengers every day, although this is very much lower than its capacity. Similar to the Santander Cycles bike hire scheme, the cable car is sponsored in a 10-year deal by the airline Emirates. London is a major global centre of higher education teaching and research and its 43 universities form the largest concentration of higher education institutes in Europe. According to the QS World University Rankings 2015/16, London has the greatest concentration of top class universities in the world and the international student population around 110,000 which is also more than any other city in the world. A 2014 PricewaterhouseCoopers report termed London as the global capital of higher education London's most popular sport is football and it has fourteen League football clubs, including five in the Premier League: Arsenal, Chelsea, Crystal Palace, Tottenham Hotspur, and West Ham United. Among other professional teams based in London include Fulham, Queens Park Rangers, Millwall and Charlton Athletic. In May 2012, Chelsea became the first London club to win the UEFA Champions League. Aside from Arsenal, Chelsea and Tottenham, none of the other London clubs have ever won the national league title. During the Tudor period the Reformation produced a gradual shift to Protestantism, much of London passing from church to private ownership. The traffic in woollen cloths shipped undyed and undressed from London to the nearby shores of the Low Countries, where it was considered indispensable. But the tentacles of English maritime enterprise hardly extended beyond the seas of north-west Europe. The commercial route to Italy and the Mediterranean Sea normally lay through Antwerp and over the Alps; any ships passing through the Strait of Gibraltar to or from England were likely to be Italian or Ragusan. Upon the re-opening of the Netherlands to English shipping in January 1565, there ensued a strong outburst of commercial activity. The Royal Exchange was founded. Mercantilism grew, and monopoly trading companies such as the East India Company were established, with trade expanding to the New World. London became the principal North Sea port, with migrants arriving from England and abroad. The population rose from an estimated 50,000 in 1530 to about 225,000 in 1605. Two recent discoveries indicate probable very early settlements near the Thames in the London area. In 1999, the remains of a Bronze Age bridge were found on the foreshore north of Vauxhall Bridge. This bridge either crossed the Thames, or went to a now lost island in the river. Dendrology dated the timbers to 1500 BC. In 2010 the foundations of a large timber structure, dated to 4500 BC, were found on the Thames foreshore, south of Vauxhall Bridge. The function of the mesolithic structure is not known. Both structures are on South Bank, at a natural crossing point where the River Effra flows into the River Thames. London's buildings are too diverse to be characterised by any particular architectural style, partly because of their varying ages. Many grand houses and public buildings, such as the National Gallery, are constructed from Portland stone. Some areas of the city, particularly those just west of the centre, are characterised by white stucco or whitewashed buildings. Few structures in central London pre-date the Great Fire of 1666, these being a few trace Roman remains, the Tower of London and a few scattered Tudor survivors in the City. Further out is, for example, the Tudor period Hampton Court Palace, England's oldest surviving Tudor palace, built by Cardinal Thomas Wolsey c.1515. Across London, Black and Asian children outnumber White British children by about six to four in state schools. Altogether at the 2011 census, of London's 1,624,768 population aged 0 to 15, 46.4 per cent were White, 19.8 per cent were Asian, 19 per cent were Black, 10.8 per cent were Mixed and 4 per cent represented another ethnic group. In January 2005, a survey of London's ethnic and religious diversity claimed that there were more than 300 languages spoken in London and more than 50 non-indigenous communities with a population of more than 10,000. Figures from the Office for National Statistics show that, in 2010[update], London's foreign-born population was 2,650,000 (33 per cent), up from 1,630,000 in 1997. Greater London's population declined steadily in the decades after the Second World War, from an estimated peak of 8.6 million in 1939 to around 6.8 million in the 1980s. The principal ports for London moved downstream to Felixstowe and Tilbury, with the London Docklands area becoming a focus for regeneration, including the Canary Wharf development. This was borne out of London's ever-increasing role as a major international financial centre during the 1980s. The Thames Barrier was completed in the 1980s to protect London against tidal surges from the North Sea. The Greater London Council was abolished in 1986, which left London as the only large metropolis in the world without a central administration. In 2000, London-wide government was restored, with the creation of the Greater London Authority. To celebrate the start of the 21st century, the Millennium Dome, London Eye and Millennium Bridge were constructed. On 6 July 2005 London was awarded the 2012 Summer Olympics, making London the first city to stage the Olympic Games three times. In January 2015, Greater London's population was estimated to be 8.63 million, the highest level since 1939. London's largest industry is finance, and its financial exports make it a large contributor to the UK's balance of payments. Around 325,000 people were employed in financial services in London until mid-2007. London has over 480 overseas banks, more than any other city in the world. Over 85 percent (3.2 million) of the employed population of greater London works in the services industries. Because of its prominent global role, London's economy had been affected by the Late-2000s financial crisis. However, by 2010 the City has recovered; put in place new regulatory powers, proceeded to regain lost ground and re-established London's economic dominance. The City of London is home to the Bank of England, London Stock Exchange, and Lloyd's of London insurance market. Herds of red and fallow deer also roam freely within much of Richmond and Bushy Park. A cull takes place each November and February to ensure numbers can be sustained. Epping Forest is also known for its fallow deer, which can frequently be seen in herds to the north of the Forest. A rare population of melanistic, black fallow deer is also maintained at the Deer Sanctuary near Theydon Bois. Muntjac deer, which escaped from deer parks at the turn of the twentieth century, are also found in the forest. While Londoners are accustomed to wildlife such as birds and foxes sharing the city, more recently urban deer have started becoming a regular feature, and whole herds of fallow and white-tailed deer come into residential areas at night to take advantage of the London's green spaces. The London Fire Brigade is the statutory fire and rescue service for Greater London. It is run by the London Fire and Emergency Planning Authority and is the third largest fire service in the world. National Health Service ambulance services are provided by the London Ambulance Service (LAS) NHS Trust, the largest free-at-the-point-of-use emergency ambulance service in the world. The London Air Ambulance charity operates in conjunction with the LAS where required. Her Majesty's Coastguard and the Royal National Lifeboat Institution operate on the River Thames, which is under the jurisdiction of the Port of London Authority from Teddington Lock to the sea. London has played a significant role in the film industry, and has major studios at Ealing and a special effects and post-production community centred in Soho. Working Title Films has its headquarters in London. London has been the setting for films including Oliver Twist (1948), Scrooge (1951), Peter Pan (1953), The 101 Dalmatians (1961), My Fair Lady (1964), Mary Poppins (1964), Blowup (1966), The Long Good Friday (1980), Notting Hill (1999), Love Actually (2003), V For Vendetta (2005), Sweeney Todd: The Demon Barber Of Fleet Street (2008) and The King's Speech (2010). Notable actors and filmmakers from London include; Charlie Chaplin, Alfred Hitchcock, Michael Caine, Helen Mirren, Gary Oldman, Christopher Nolan, Jude Law, Tom Hardy, Keira Knightley and Daniel Day-Lewis. As of 2008[update], the British Academy Film Awards have taken place at the Royal Opera House. London is a major centre for television production, with studios including BBC Television Centre, The Fountain Studios and The London Studios. Many television programmes have been set in London, including the popular television soap opera EastEnders, broadcast by the BBC since 1985. By the 11th century, London was beyond all comparison the largest town in England. Westminster Abbey, rebuilt in the Romanesque style by King Edward the Confessor, was one of the grandest churches in Europe. Winchester had previously been the capital of Anglo-Saxon England, but from this time on, London became the main forum for foreign traders and the base for defence in time of war. In the view of Frank Stenton: ""It had the resources, and it was rapidly developing the dignity and the political self-consciousness appropriate to a national capital."""
Age_of_Enlightenment,"Coffeehouses represent a turning point in history during which people discovered that they could have enjoyable social lives within their communities. Coffeeshops became homes away from home for many who sought, for the first time, to engage in discourse with their neighbors and discuss intriguing and thought-provoking matters, especially those regarding philosophy to politics. Coffeehouses were essential to the Enlightenment, for they were centers of free-thinking and self-discovery. Although many coffeehouse patrons were scholars, a great deal were not. Coffeehouses attracted a diverse set of people, including not only the educated wealthy but also members of the bourgeoisie and the lower class. While it may seem positive that patrons, being doctors, lawyers, merchants, etc. represented almost all classes, the coffeeshop environment sparked fear in those who sought to preserve class distinction. One of the most popular critiques of the coffeehouse claimed that it ""allowed promiscuous association among people from different rungs of the social ladder, from the artisan to the aristocrat"" and was therefore compared to Noah's Ark, receiving all types of animals, clean or unclean. This unique culture served as a catalyst for journalism when Joseph Addison and Richard Steele recognized its potential as an audience. Together, Steele and Addison published The Spectator (1711), a daily publication which aimed, through fictional narrator Mr. Spectator, both to entertain and to provoke discussion regarding serious philosophical matters. The predominant educational psychology from the 1750s onward, especially in northern European countries was associationism, the notion that the mind associates or dissociates ideas through repeated routines. In addition to being conducive to Enlightenment ideologies of liberty, self-determination and personal responsibility, it offered a practical theory of the mind that allowed teachers to transform longstanding forms of print and manuscript culture into effective graphic tools of learning for the lower and middle orders of society. Children were taught to memorize facts through oral and graphic methods that originated during the Renaissance. The term ""Enlightenment"" emerged in English in the later part of the 19th century, with particular reference to French philosophy, as the equivalent of the French term 'Lumières' (used first by Dubos in 1733 and already well established by 1751). From Immanuel Kant's 1784 essay ""Beantwortung der Frage: Was ist Aufklärung?"" (""Answering the Question: What is Enlightenment?"") the German term became 'Aufklärung' (aufklären = to illuminate; sich aufklären = to clear up). However, scholars have never agreed on a definition of the Enlightenment, or on its chronological or geographical extent. Terms like ""les Lumières"" (French), ""illuminismo"" (Italian), ""ilustración"" (Spanish) and ""Aufklärung"" (German) referred to partly overlapping movements. Not until the late nineteenth century did English scholars agree they were talking about ""the Enlightenment."" However, the prime example of reference works that systematized scientific knowledge in the age of Enlightenment were universal encyclopedias rather than technical dictionaries. It was the goal of universal encyclopedias to record all human knowledge in a comprehensive reference work. The most well-known of these works is Denis Diderot and Jean le Rond d'Alembert's Encyclopédie, ou dictionnaire raisonné des sciences, des arts et des métiers. The work, which began publication in 1751, was composed of thirty-five volumes and over 71 000 separate entries. A great number of the entries were dedicated to describing the sciences and crafts in detail, and provided intellectuals across Europe with a high-quality survey of human knowledge. In d'Alembert's Preliminary Discourse to the Encyclopedia of Diderot, the work's goal to record the extent of human knowledge in the arts and sciences is outlined: The increased consumption of reading materials of all sorts was one of the key features of the ""social"" Enlightenment. Developments in the Industrial Revolution allowed consumer goods to be produced in greater quantities at lower prices, encouraging the spread of books, pamphlets, newspapers and journals – ""media of the transmission of ideas and attitudes"". Commercial development likewise increased the demand for information, along with rising populations and increased urbanisation. However, demand for reading material extended outside of the realm of the commercial, and outside the realm of the upper and middle classes, as evidenced by the Bibliothèque Bleue. Literacy rates are difficult to gauge, but in France at least, the rates doubled over the course of the 18th century. Reflecting the decreasing influence of religion, the number of books about science and art published in Paris doubled from 1720 to 1780, while the number of books about religion dropped to just one-tenth of the total. As musicians depended more and more on public support, public concerts became increasingly popular and helped supplement performers' and composers' incomes. The concerts also helped them to reach a wider audience. Handel, for example, epitomized this with his highly public musical activities in London. He gained considerable fame there with performances of his operas and oratorios. The music of Haydn and Mozart, with their Viennese Classical styles, are usually regarded as being the most in line with the Enlightenment ideals. There is little consensus on the precise beginning of the Age of Enlightenment; the beginning of the 18th century (1701) or the middle of the 17th century (1650) are often used as epochs. French historians usually place the period, called the Siècle des Lumières (Century of Enlightenments), between 1715 and 1789, from the beginning of the reign of Louis XV until the French Revolution. If taken back to the mid-17th century, the Enlightenment would trace its origins to Descartes' Discourse on Method, published in 1637. In France, many cited the publication of Isaac Newton's Principia Mathematica in 1687. It is argued by several historians and philosophers that the beginning of the Enlightenment is when Descartes shifted the epistemological basis from external authority to internal certainty by his cogito ergo sum published in 1637. As to its end, most scholars use the last years of the century, often choosing the French Revolution of 1789 or the beginning of the Napoleonic Wars (1804–15) as a convenient point in time with which to date the end of the Enlightenment. Several Americans, especially Benjamin Franklin and Thomas Jefferson, played a major role in bringing Enlightenment ideas to the New World and in influencing British and French thinkers. Franklin was influential for his political activism and for his advances in physics. The cultural exchange during the Age of Enlightenment ran in both directions across the Atlantic. Thinkers such as Paine, Locke, and Rousseau all take Native American cultural practices as examples of natural freedom. The Americans closely followed English and Scottish political ideas, as well as some French thinkers such as Montesquieu. As deists, they were influenced by ideas of John Toland (1670–1722) and Matthew Tindal (1656–1733). During the Enlightenment there was a great emphasis upon liberty, democracy, republicanism and religious tolerance. Attempts to reconcile science and religion resulted in a widespread rejection of prophecy, miracle and revealed religion in preference for Deism – especially by Thomas Paine in The Age of Reason and by Thomas Jefferson in his short Jefferson Bible – from which all supernatural aspects were removed. Hume and other Scottish Enlightenment thinkers developed a 'science of man', which was expressed historically in works by authors including James Burnett, Adam Ferguson, John Millar, and William Robertson, all of whom merged a scientific study of how humans behaved in ancient and primitive cultures with a strong awareness of the determining forces of modernity. Modern sociology largely originated from this movement, and Hume's philosophical concepts that directly influenced James Madison (and thus the U.S. Constitution) and as popularised by Dugald Stewart, would be the basis of classical liberalism. The Enlightenment – known in French as the Siècle des Lumières, the Century of Enlightenment, and in German as the Aufklärung – was a philosophical movement which dominated the world of ideas in Europe in the 18th century. The Enlightenment included a range of ideas centered on reason as the primary source of authority and legitimacy, and came to advance ideals such as liberty, progress, tolerance, fraternity, constitutional government and ending the abuses of the church and state. In France, the central doctrines of the Lumières were individual liberty and religious tolerance, in opposition to the principle of absolute monarchy and the fixed dogmas of the Roman Catholic Church. The Enlightenment was marked by increasing empiricism, scientific rigor, and reductionism, along with increased questioning of religious orthodoxy. Historians have long debated the extent to which the secret network of Freemasonry was a main factor in the Enlightenment. The leaders of the Enlightenment included Freemasons such as Diderot, Montesquieu, Voltaire, Pope, Horace Walpole, Sir Robert Walpole, Mozart, Goethe, Frederick the Great, Benjamin Franklin, and George Washington. Norman Davies said that Freemasonry was a powerful force on behalf of Liberalism in Europe, from about 1700 to the twentieth century. It expanded rapidly during the Age of Enlightenment, reaching practically every country in Europe. It was especially attractive to powerful aristocrats and politicians as well as intellectuals, artists and political activists. Alexis de Tocqueville described the French Revolution as the inevitable result of the radical opposition created in the 18th century between the monarchy and the men of letters of the Enlightenment. These men of letters constituted a sort of ""substitute aristocracy that was both all-powerful and without real power"". This illusory power came from the rise of ""public opinion"", born when absolutist centralization removed the nobility and the bourgeoisie from the political sphere. The ""literary politics"" that resulted promoted a discourse of equality and was hence in fundamental opposition to the monarchical regime. De Tocqueville ""clearly designates ... the cultural effects of transformation in the forms of the exercise of power"". Nevertheless, it took another century before cultural approach became central to the historiography, as typified by Robert Darnton, The Business of Enlightenment: A Publishing History of the Encyclopédie, 1775–1800 (1979). Jonathan Israel called the journals the most influential cultural innovation of European intellectual culture. They shifted the attention of the ""cultivated public"" away from established authorities to novelty and innovation, and promoted the ""enlightened"" ideals of toleration and intellectual objectivity. Being a source of knowledge derived from science and reason, they were an implicit critique of existing notions of universal truth monopolized by monarchies, parliaments, and religious authorities. They also advanced Christian enlightenment that upheld ""the legitimacy of God-ordained authority""—the Bible—in which there had to be agreement between the biblical and natural theories. The Age of Enlightenment was preceded by and closely associated with the scientific revolution. Earlier philosophers whose work influenced the Enlightenment included Francis Bacon, Descartes, Locke, and Spinoza. The major figures of the Enlightenment included Cesare Beccaria, Voltaire, Denis Diderot, Jean-Jacques Rousseau, David Hume, Adam Smith, and Immanuel Kant. Some European rulers, including Catherine II of Russia, Joseph II of Austria and Frederick I of Prussia, tried to apply Enlightenment thought on religious and political tolerance, which became known as enlightened absolutism. The Americans Benjamin Franklin and Thomas Jefferson came to Europe during the period and contributed actively to the scientific and political debate, and the ideals of the Enlightenment were incorporated into the United States Declaration of Independence and the Constitution of the United States. Locke is known for his statement that individuals have a right to ""Life, Liberty and Property"", and his belief that the natural right to property is derived from labor. Tutored by Locke, Anthony Ashley-Cooper, 3rd Earl of Shaftesbury wrote in 1706: ""There is a mighty Light which spreads its self over the world especially in those two free Nations of England and Holland; on whom the Affairs of Europe now turn"". Locke's theory of natural rights has influenced many political documents, including the United States Declaration of Independence and the French National Constituent Assembly's Declaration of the Rights of Man and of the Citizen. A number of novel ideas about religion developed with the Enlightenment, including Deism and talk of atheism. Deism, according to Thomas Paine, is the simple belief in God the Creator, with no reference to the Bible or any other miraculous source. Instead, the Deist relies solely on personal reason to guide his creed, which was eminently agreeable to many thinkers of the time. Atheism was much discussed, but there were few proponents. Wilson and Reill note that, ""In fact, very few enlightened intellectuals, even when they were vocal critics of Christianity, were true atheists. Rather, they were critics of orthodox belief, wedded rather to skepticism, deism, vitalism, or perhaps pantheism."" Some followed Pierre Bayle and argued that atheists could indeed be moral men. Many others like Voltaire held that without belief in a God who punishes evil, the moral order of society was undermined. That is, since atheists gave themselves to no Supreme Authority and no law, and had no fear of eternal consequences, they were far more likely to disrupt society. Bayle (1647–1706) observed that in his day, ""prudent persons will always maintain an appearance of [religion]."". He believed that even atheists could hold concepts of honor and go beyond their own self-interest to create and interact in society. Locke said that if there were no God and no divine law, the result would be moral anarchy: every individual ""could have no law but his own will, no end but himself. He would be a god to himself, and the satisfaction of his own will the sole measure and end of all his actions"". The ""Radical Enlightenment"" promoted the concept of separating church and state, an idea that often credited to English philosopher John Locke (1632–1704). According to his principle of the social contract, Locke said that the government lacked authority in the realm of individual conscience, as this was something rational people could not cede to the government for it or others to control. For Locke, this created a natural right in the liberty of conscience, which he said must therefore remain protected from any government authority. There were two distinct lines of Enlightenment thought: the radical enlightenment, inspired by the philosophy of Spinoza, advocating democracy, individual liberty, freedom of expression, and eradication of religious authority; and a second, more moderate variety, supported by René Descartes, John Locke, Christian Wolff, Isaac Newton and others, which sought accommodation between reform and the traditional systems of power and faith. Both lines of thought were opposed by the conservative Counter-Enlightenment. The first significant work that expressed scientific theory and knowledge expressly for the laity, in the vernacular, and with the entertainment of readers in mind, was Bernard de Fontenelle's Conversations on the Plurality of Worlds (1686). The book was produced specifically for women with an interest in scientific writing and inspired a variety of similar works. These popular works were written in a discursive style, which was laid out much more clearly for the reader than the complicated articles, treatises, and books published by the academies and scientists. Charles Leadbetter's Astronomy (1727) was advertised as ""a Work entirely New"" that would include ""short and easie  [sic] Rules and Astronomical Tables."" The first French introduction to Newtonianism and the Principia was Eléments de la philosophie de Newton, published by Voltaire in 1738. Émilie du Châtelet's translation of the Principia, published after her death in 1756, also helped to spread Newton's theories beyond scientific academies and the university. Francesco Algarotti, writing for a growing female audience, published Il Newtonianism per le dame, which was a tremendously popular work and was translated from Italian into English by Elizabeth Carter. A similar introduction to Newtonianism for women was produced by Henry Pembarton. His A View of Sir Isaac Newton's Philosophy was published by subscription. Extant records of subscribers show that women from a wide range of social standings purchased the book, indicating the growing number of scientifically inclined female readers among the middling class. During the Enlightenment, women also began producing popular scientific works themselves. Sarah Trimmer wrote a successful natural history textbook for children titled The Easy Introduction to the Knowledge of Nature (1782), which was published for many years after in eleven editions. Many of the leading universities associated with Enlightenment progressive principles were located in northern Europe, with the most renowned being the universities of Leiden, Göttingen, Halle, Montpellier, Uppsala and Edinburgh. These universities, especially Edinburgh, produced professors whose ideas had a significant impact on Britain's North American colonies and, later, the American Republic. Within the natural sciences, Edinburgh's medical also led the way in chemistry, anatomy and pharmacology. In other parts of Europe, the universities and schools of France and most of Europe were bastions of traditionalism and were not hospitable to the Enlightenment. In France, the major exception was the medical university at Montpellier. The desire to explore, record and systematize knowledge had a meaningful impact on music publications. Jean-Jacques Rousseau's Dictionnaire de musique (published 1767 in Geneva and 1768 in Paris) was a leading text in the late 18th century. This widely available dictionary gave short definitions of words like genius and taste, and was clearly influenced by the Enlightenment movement. Another text influenced by Enlightenment values was Charles Burney's A General History of Music: From the Earliest Ages to the Present Period (1776), which was a historical survey and an attempt to rationalize elements in music systematically over time. Recently, musicologists have shown renewed interest in the ideas and consequences of the Enlightenment. For example, Rose Rosengard Subotnik's Deconstructive Variations (subtitled Music and Reason in Western Society) compares Mozart's Die Zauberflöte (1791) using the Enlightenment and Romantic perspectives, and concludes that the work is ""an ideal musical representation of the Enlightenment"". French historians traditionally place the Enlightenment between 1715, the year that Louis XIV died, and 1789, the beginning of the French Revolution. Some recent historians begin the period in the 1620s, with the start of the scientific revolution. The Philosophes, the French term for the philosophers of the period, widely circulated their ideas through meetings at scientific academies, Masonic lodges, literary salons and coffee houses, and through printed books and pamphlets. The ideas of the Enlightenment undermined the authority of the monarchy and the church, and paved the way for the revolutions of the 18th and 19th centuries. A variety of 19th-century movements, including liberalism and neo-classicism, trace their intellectual heritage back to the Enlightenment. In Russia, the government began to actively encourage the proliferation of arts and sciences in the mid-18th century. This era produced the first Russian university, library, theatre, public museum, and independent press. Like other enlightened despots, Catherine the Great played a key role in fostering the arts, sciences, and education. She used her own interpretation of Enlightenment ideals, assisted by notable international experts such as Voltaire (by correspondence) and, in residence, world class scientists such as Leonhard Euler and Peter Simon Pallas. The national Enlightenment differed from its Western European counterpart in that it promoted further modernization of all aspects of Russian life and was concerned with attacking the institution of serfdom in Russia. The Russian enlightenment centered on the individual instead of societal enlightenment and encouraged the living of an enlightened life. Jonathan Israel rejects the attempts of postmodern and Marxian historians to understand the revolutionary ideas of the period purely as by-products of social and economic transformations. He instead focuses on the history of ideas in the period from 1650 to the end of the 18th century, and claims that it was the ideas themselves that caused the change that eventually led to the revolutions of the latter half of the 18th century and the early 19th century. Israel argues that until the 1650s Western civilization ""was based on a largely shared core of faith, tradition and authority"". The creation of the public sphere has been associated with two long-term historical trends: the rise of the modern nation state and the rise of capitalism. The modern nation state, in its consolidation of public power, created by counterpoint a private realm of society independent of the state, which allowed for the public sphere. Capitalism also increased society's autonomy and self-awareness, and an increasing need for the exchange of information. As the nascent public sphere expanded, it embraced a large variety of institutions; the most commonly cited were coffee houses and cafés, salons and the literary public sphere, figuratively localized in the Republic of Letters. In France, the creation of the public sphere was helped by the aristocracy's move from the King's palace at Versailles to Paris in about 1720, since their rich spending stimulated the trade in luxuries and artistic creations, especially fine paintings. One of the most important developments that the Enlightenment era brought to the discipline of science was its popularization. An increasingly literate population seeking knowledge and education in both the arts and the sciences drove the expansion of print culture and the dissemination of scientific learning. The new literate population was due to a high rise in the availability of food. This enabled many people to rise out of poverty, and instead of paying more for food, they had money for education. Popularization was generally part of an overarching Enlightenment ideal that endeavoured ""to make information available to the greatest number of people."" As public interest in natural philosophy grew during the 18th century, public lecture courses and the publication of popular texts opened up new roads to money and fame for amateurs and scientists who remained on the periphery of universities and academies. More formal works included explanations of scientific theories for individuals lacking the educational background to comprehend the original scientific text. Sir Isaac Newton's celebrated Philosophiae Naturalis Principia Mathematica was published in Latin and remained inaccessible to readers without education in the classics until Enlightenment writers began to translate and analyze the text in the vernacular. The massive work was arranged according to a ""tree of knowledge."" The tree reflected the marked division between the arts and sciences, which was largely a result of the rise of empiricism. Both areas of knowledge were united by philosophy, or the trunk of the tree of knowledge. The Enlightenment's desacrilization of religion was pronounced in the tree's design, particularly where theology accounted for a peripheral branch, with black magic as a close neighbour. As the Encyclopédie gained popularity, it was published in quarto and octavo editions after 1777. The quarto and octavo editions were much less expensive than previous editions, making the Encyclopédie more accessible to the non-elite. Robert Darnton estimates that there were approximately 25 000 copies of the Encyclopédie in circulation throughout France and Europe before the French Revolution. The extensive, yet affordable encyclopedia came to represent the transmission of Enlightenment and scientific education to an expanding audience. The strongest contribution of the French Academies to the public sphere comes from the concours académiques (roughly translated as 'academic contests') they sponsored throughout France. These academic contests were perhaps the most public of any institution during the Enlightenment. The practice of contests dated back to the Middle Ages, and was revived in the mid-17th century. The subject matter had previously been generally religious and/or monarchical, featuring essays, poetry, and painting. By roughly 1725, however, this subject matter had radically expanded and diversified, including ""royal propaganda, philosophical battles, and critical ruminations on the social and political institutions of the Old Regime."" Topics of public controversy were also discussed such as the theories of Newton and Descartes, the slave trade, women's education, and justice in France. In France, the established men of letters (gens de lettres) had fused with the elites (les grands) of French society by the mid-18th century. This led to the creation of an oppositional literary sphere, Grub Street, the domain of a ""multitude of versifiers and would-be authors"". These men came to London to become authors, only to discover that the literary market simply could not support large numbers of writers, who, in any case, were very poorly remunerated by the publishing-bookselling guilds. Frederick the Great, the king of Prussia from 1740 to 1786, saw himself as a leader of the Enlightenment and patronized philosophers and scientists at his court in Berlin. Voltaire, who had been imprisoned and maltreated by the French government, was eager to accept Frederick's invitation to live at his palace. Frederick explained, ""My principal occupation is to combat ignorance and prejudice ... to enlighten minds, cultivate morality, and to make people as happy as it suits human nature, and as the means at my disposal permit."" As the economy and the middle class expanded, there was an increasing number of amateur musicians. One manifestation of this involved women, who became more involved with music on a social level. Women were already engaged in professional roles as singers, and increased their presence in the amateur performers' scene, especially with keyboard music. Music publishers begin to print music that amateurs could understand and play. The majority of the works that were published were for keyboard, voice and keyboard, and chamber ensemble. After these initial genres were popularized, from the mid-century on, amateur groups sang choral music, which then became a new trend for publishers to capitalize on. The increasing study of the fine arts, as well as access to amateur-friendly published works, led to more people becoming interested in reading and discussing music. Music magazines, reviews, and critical works which suited amateurs as well as connoisseurs began to surface. Though much of Enlightenment political thought was dominated by social contract theorists, both David Hume and Adam Ferguson criticized this camp. Hume's essay Of the Original Contract argues that governments derived from consent are rarely seen, and civil government is grounded in a ruler's habitual authority and force. It is precisely because of the ruler's authority over-and-against the subject, that the subject tacitly consents; Hume says that the subjects would ""never imagine that their consent made him sovereign"", rather the authority did so. Similarly, Ferguson did not believe citizens built the state, rather polities grew out of social development. In his 1767 An Essay on the History of Civil Society, Ferguson uses the four stages of progress, a theory that was very popular in Scotland at the time, to explain how humans advance from a hunting and gathering society to a commercial and civil society without ""signing"" a social contract. The first technical dictionary was drafted by John Harris and entitled Lexicon Technicum: Or, An Universal English Dictionary of Arts and Sciences. Harris' book avoided theological and biographical entries; instead it concentrated on science and technology. Published in 1704, the Lexicon technicum was the first book to be written in English that took a methodical approach to describing mathematics and commercial arithmetic along with the physical sciences and navigation. Other technical dictionaries followed Harris' model, including Ephraim Chambers' Cyclopaedia (1728), which included five editions, and was a substantially larger work than Harris'. The folio edition of the work even included foldout engravings. The Cyclopaedia emphasized Newtonian theories, Lockean philosophy, and contained thorough examinations of technologies, such as engraving, brewing, and dyeing. Bertrand Russell saw the Enlightenment as a phase in a progressive development, which began in antiquity, and that reason and challenges to the established order were constant ideals throughout that time. Russell said that the Enlightenment was ultimately born out of the Protestant reaction against the Catholic counter-reformation, and that philosophical views such as affinity for democracy against monarchy originated among 16th-century Protestants to justify their desire to break away from the Catholic Church. Though many of these philosophical ideals were picked up by Catholics, Russell argues, by the 18th century the Enlightenment was the principal manifestation of the schism that began with Martin Luther. In the mid-18th century, Paris became the center of an explosion of philosophic and scientific activity challenging traditional doctrines and dogmas. The philosophic movement was led by Voltaire and Jean-Jacques Rousseau, who argued for a society based upon reason rather than faith and Catholic doctrine, for a new civil order based on natural law, and for science based on experiments and observation. The political philosopher Montesquieu introduced the idea of a separation of powers in a government, a concept which was enthusiastically adopted by the authors of the United States Constitution. While the Philosophes of the French Enlightenment were not revolutionaries, and many were members of the nobility, their ideas played an important part in undermining the legitimacy of the Old Regime and shaping the French Revolution. Enlightenment scholars sought to curtail the political power of organized religion and thereby prevent another age of intolerant religious war. Spinoza determined to remove politics from contemporary and historical theology (e.g. disregarding Judaic law). Moses Mendelssohn advised affording no political weight to any organized religion, but instead recommended that each person follow what they found most convincing. A good religion based in instinctive morals and a belief in God should not theoretically need force to maintain order in its believers, and both Mendelssohn and Spinoza judged religion on its moral fruits, not the logic of its theology. The first English coffeehouse opened in Oxford in 1650. Brian Cowan said that Oxford coffeehouses developed into ""penny universities"", offering a locus of learning that was less formal than structured institutions. These penny universities occupied a significant position in Oxford academic life, as they were frequented by those consequently referred to as the ""virtuosi"", who conducted their research on some of the resulting premises. According to Cowan, ""the coffeehouse was a place for like-minded scholars to congregate, to read, as well as learn from and to debate with each other, but was emphatically not a university institution, and the discourse there was of a far different order than any university tutorial."" Along with secular matters, readers also favoured an alphabetical ordering scheme over cumbersome works arranged along thematic lines. The historian Charles Porset, commenting on alphabetization, has said that ""as the zero degree of taxonomy, alphabetical order authorizes all reading strategies; in this respect it could be considered an emblem of the Enlightenment."" For Porset, the avoidance of thematic and hierarchical systems thus allows free interpretation of the works and becomes an example of egalitarianism. Encyclopedias and dictionaries also became more popular during the Age of Reason as the number of educated consumers who could afford such texts began to multiply. In the later half of the 18th century, the number of dictionaries and encyclopedias published by decade increased from 63 between 1760 and 1769 to approximately 148 in the decade proceeding the French Revolution (1780–1789). Along with growth in numbers, dictionaries and encyclopedias also grew in length, often having multiple print runs that sometimes included in supplemented editions. Intellectuals such as Robert Darnton and Jürgen Habermas have focused on the social conditions of the Enlightenment. Habermas described the creation of the ""bourgeois public sphere"" in 18th-century Europe, containing the new venues and modes of communication allowing for rational exchange. Habermas said that the public sphere was bourgeois, egalitarian, rational, and independent from the state, making it the ideal venue for intellectuals to critically examine contemporary politics and society, away from the interference of established authority. While the public sphere is generally an integral component of the social study of the Enlightenment, other historians have questioned whether the public sphere had these characteristics. Broadly speaking, Enlightenment science greatly valued empiricism and rational thought, and was embedded with the Enlightenment ideal of advancement and progress. The study of science, under the heading of natural philosophy, was divided into physics and a conglomerate grouping of chemistry and natural history, which included anatomy, biology, geology, mineralogy, and zoology. As with most Enlightenment views, the benefits of science were not seen universally; Rousseau criticized the sciences for distancing man from nature and not operating to make people happier. Science during the Enlightenment was dominated by scientific societies and academies, which had largely replaced universities as centres of scientific research and development. Societies and academies were also the backbone of the maturation of the scientific profession. Another important development was the popularization of science among an increasingly literate population. Philosophes introduced the public to many scientific theories, most notably through the Encyclopédie and the popularization of Newtonianism by Voltaire and Émilie du Châtelet. Some historians have marked the 18th century as a drab period in the history of science; however, the century saw significant advancements in the practice of medicine, mathematics, and physics; the development of biological taxonomy; a new understanding of magnetism and electricity; and the maturation of chemistry as a discipline, which established the foundations of modern chemistry. John Locke, one of the most influential Enlightenment thinkers, based his governance philosophy in social contract theory, a subject that permeated Enlightenment political thought. The English philosopher Thomas Hobbes ushered in this new debate with his work Leviathan in 1651. Hobbes also developed some of the fundamentals of European liberal thought: the right of the individual; the natural equality of all men; the artificial character of the political order (which led to the later distinction between civil society and the state); the view that all legitimate political power must be ""representative"" and based on the consent of the people; and a liberal interpretation of law which leaves people free to do whatever the law does not explicitly forbid. German historian Reinhart Koselleck claimed that ""On the Continent there were two social structures that left a decisive imprint on the Age of Enlightenment: the Republic of Letters and the Masonic lodges."" Scottish professor Thomas Munck argues that ""although the Masons did promote international and cross-social contacts which were essentially non-religious and broadly in agreement with enlightened values, they can hardly be described as a major radical or reformist network in their own right."" Many of the Masons values seemed to greatly appeal to Enlightenment values and thinkers. Diderot discusses the link between Freemason ideals and the enlightenment in D'Alembert's Dream, exploring masonry as a way of spreading enlightenment beliefs. Historian Margaret Jacob stresses the importance of the Masons in indirectly inspiring enlightened political thought. On the negative side, Daniel Roche contests claims that Masonry promoted egalitarianism. He argues that the lodges only attracted men of similar social backgrounds. The presence of noble women in the French ""lodges of adoption"" that formed in the 1780s was largely due to the close ties shared between these lodges and aristocratic society. A healthy, and legal, publishing industry existed throughout Europe, although established publishers and book sellers occasionally ran afoul of the law. The Encyclopédie, for example, condemned not only by the King but also by Clement XII, nevertheless found its way into print with the help of the aforementioned Malesherbes and creative use of French censorship law. But many works were sold without running into any legal trouble at all. Borrowing records from libraries in England, Germany and North America indicate that more than 70 percent of books borrowed were novels. Less than 1 percent of the books were of a religious nature, indicating the general trend of declining religiosity. The vast majority of the reading public could not afford to own a private library, and while most of the state-run ""universal libraries"" set up in the 17th and 18th centuries were open to the public, they were not the only sources of reading material. On one end of the spectrum was the Bibliothèque Bleue, a collection of cheaply produced books published in Troyes, France. Intended for a largely rural and semi-literate audience these books included almanacs, retellings of medieval romances and condensed versions of popular novels, among other things. While some historians have argued against the Enlightenment's penetration into the lower classes, the Bibliothèque Bleue represents at least a desire to participate in Enlightenment sociability. Moving up the classes, a variety of institutions offered readers access to material without needing to buy anything. Libraries that lent out their material for a small price started to appear, and occasionally bookstores would offer a small lending library to their patrons. Coffee houses commonly offered books, journals and sometimes even popular novels to their customers. The Tatler and The Spectator, two influential periodicals sold from 1709 to 1714, were closely associated with coffee house culture in London, being both read and produced in various establishments in the city. This is an example of the triple or even quadruple function of the coffee house: reading material was often obtained, read, discussed and even produced on the premises. Both Locke and Rousseau developed social contract theories in Two Treatises of Government and Discourse on Inequality, respectively. While quite different works, Locke, Hobbes, and Rousseau agreed that a social contract, in which the government's authority lies in the consent of the governed, is necessary for man to live in civil society. Locke defines the state of nature as a condition in which humans are rational and follow natural law; in which all men are born equal and with the right to life, liberty and property. However, when one citizen breaks the Law of Nature, both the transgressor and the victim enter into a state of war, from which it is virtually impossible to break free. Therefore, Locke said that individuals enter into civil society to protect their natural rights via an ""unbiased judge"" or common authority, such as courts, to appeal to. Contrastingly, Rousseau's conception relies on the supposition that ""civil man"" is corrupted, while ""natural man"" has no want he cannot fulfill himself. Natural man is only taken out of the state of nature when the inequality associated with private property is established. Rousseau said that people join into civil society via the social contract to achieve unity while preserving individual freedom. This is embodied in the sovereignty of the general will, the moral and collective legislative body constituted by citizens. The writers of Grub Street, the Grub Street Hacks, were left feeling bitter about the relative success of the men of letters, and found an outlet for their literature which was typified by the libelle. Written mostly in the form of pamphlets, the libelles ""slandered the court, the Church, the aristocracy, the academies, the salons, everything elevated and respectable, including the monarchy itself"". Le Gazetier cuirassé by Charles Théveneau de Morande was a prototype of the genre. It was Grub Street literature that was most read by the public during the Enlightenment. More importantly, according to Darnton, the Grub Street hacks inherited the ""revolutionary spirit"" once displayed by the philosophes, and paved the way for the French Revolution by desacralizing figures of political, moral and religious authority in France. Cesare Beccaria, a jurist and one of the great Enlightenment writers, became famous for his masterpiece Of Crimes and Punishments (1764), which was later translated into 22 languages. Another prominent intellectual was Francesco Mario Pagano, who wrote important studies such as Saggi Politici (Political Essays, 1783), one of the major works of the Enlightenment in Naples, and Considerazioni sul processo criminale (Considerations on the criminal trial, 1787), which established him as an international authority on criminal law. In several nations, rulers welcomed leaders of the Enlightenment at court and asked them to help design laws and programs to reform the system, typically to build stronger national states. These rulers are called ""enlightened despots"" by historians. They included Frederick the Great of Prussia, Catherine the Great of Russia, Leopold II of Tuscany, and Joseph II of Austria. Joseph was over-enthusiastic, announcing so many reforms that had so little support that revolts broke out and his regime became a comedy of errors and nearly all his programs were reversed. Senior ministers Pombal in Portugal and Struensee in Denmark also governed according to Enlightenment ideals. In Poland, the model constitution of 1791 expressed Enlightenment ideals, but was in effect for only one year as the nation was partitioned among its neighbors. More enduring were the cultural achievements, which created a nationalist spirit in Poland. The Enlightenment has been frequently linked to the French Revolution of 1789. One view of the political changes that occurred during the Enlightenment is that the ""consent of the governed"" philosophy as delineated by Locke in Two Treatises of Government (1689) represented a paradigm shift from the old governance paradigm under feudalism known as the ""divine right of kings"". In this view, the revolutions of the late 1700s and early 1800s were caused by the fact that this governance paradigm shift often could not be resolved peacefully, and therefore violent revolution was the result. Clearly a governance philosophy where the king was never wrong was in direct conflict with one whereby citizens by natural law had to consent to the acts and rulings of their government. The major opponent of Freemasonry was the Roman Catholic Church, so that in countries with a large Catholic element, such as France, Italy, Spain, and Mexico, much of the ferocity of the political battles involve the confrontation between what Davies calls the reactionary Church and enlightened Freemasonry. Even in France, Masons did not act as a group. American historians, while noting that Benjamin Franklin and George Washington were indeed active Masons, have downplayed the importance of Freemasonry in causing the American Revolution because the Masonic order was non-political and included both Patriots and their enemy the Loyalists. The word ""public"" implies the highest level of inclusivity – the public sphere by definition should be open to all. However, this sphere was only public to relative degrees. Enlightenment thinkers frequently contrasted their conception of the ""public"" with that of the people: Condorcet contrasted ""opinion"" with populace, Marmontel ""the opinion of men of letters"" with ""the opinion of the multitude,"" and d'Alembert the ""truly enlightened public"" with ""the blind and noisy multitude"". Additionally, most institutions of the public sphere excluded both women and the lower classes. Cross-class influences occurred through noble and lower class participation in areas such as the coffeehouses and the Masonic lodges. The target audience of natural history was French polite society, evidenced more by the specific discourse of the genre than by the generally high prices of its works. Naturalists catered to polite society's desire for erudition – many texts had an explicit instructive purpose. However, natural history was often a political affair. As E. C. Spary writes, the classifications used by naturalists ""slipped between the natural world and the social ... to establish not only the expertise of the naturalists over the natural, but also the dominance of the natural over the social"". The idea of taste (le goût) was a social indicator: to truly be able to categorize nature, one had to have the proper taste, an ability of discretion shared by all members of polite society. In this way natural history spread many of the scientific developments of the time, but also provided a new source of legitimacy for the dominant class. From this basis, naturalists could then develop their own social ideals based on their scientific works. Enlightenment era religious commentary was a response to the preceding century of religious conflict in Europe, especially the Thirty Years' War. Theologians of the Enlightenment wanted to reform their faith to its generally non-confrontational roots and to limit the capacity for religious controversy to spill over into politics and warfare while still maintaining a true faith in God. For moderate Christians, this meant a return to simple Scripture. John Locke abandoned the corpus of theological commentary in favor of an ""unprejudiced examination"" of the Word of God alone. He determined the essence of Christianity to be a belief in Christ the redeemer and recommended avoiding more detailed debate. Thomas Jefferson in the Jefferson Bible went further; he dropped any passages dealing with miracles, visitations of angels, and the resurrection of Jesus after his death. He tried to extract the practical Christian moral code of the New Testament. A genre that greatly rose in importance was that of scientific literature. Natural history in particular became increasingly popular among the upper classes. Works of natural history include René-Antoine Ferchault de Réaumur's Histoire naturelle des insectes and Jacques Gautier d'Agoty's La Myologie complète, ou description de tous les muscles du corps humain (1746). Outside ancien régime France, natural history was an important part of medicine and industry, encompassing the fields of botany, zoology, meteorology, hydrology and mineralogy. Students in Enlightenment universities and academies were taught these subjects to prepare them for careers as diverse as medicine and theology. As shown by M D Eddy, natural history in this context was a very middle class pursuit and operated as a fertile trading zone for the interdisciplinary exchange of diverse scientific ideas. Enlightenment historiography began in the period itself, from what Enlightenment figures said about their work. A dominant element was the intellectual angle they took. D'Alembert's Preliminary Discourse of l'Encyclopédie provides a history of the Enlightenment which comprises a chronological list of developments in the realm of knowledge – of which the Encyclopédie forms the pinnacle. In 1783, Jewish philosopher Moses Mendelssohn referred to Enlightenment as a process by which man was educated in the use of reason. Immanuel Kant called Enlightenment ""man's release from his self-incurred tutelage"", tutelage being ""man's inability to make use of his understanding without direction from another"". ""For Kant, Enlightenment was mankind's final coming of age, the emancipation of the human consciousness from an immature state of ignorance."" The German scholar Ernst Cassirer called the Enlightenment ""a part and a special phase of that whole intellectual development through which modern philosophic thought gained its characteristic self-confidence and self-consciousness"". According to historian Roy Porter, the liberation of the human mind from a dogmatic state of ignorance is the epitome of what the Age of Enlightenment was trying to capture. Many women played an essential part in the French Enlightenment, due to the role they played as salonnières in Parisian salons, as the contrast to the male philosophes. The salon was the principal social institution of the republic, and ""became the civil working spaces of the project of Enlightenment."" Women, as salonnières, were ""the legitimate governors of [the] potentially unruly discourse"" that took place within. While women were marginalized in the public culture of the Ancien Régime, the French Revolution destroyed the old cultural and economic restraints of patronage and corporatism (guilds), opening French society to female participation, particularly in the literary sphere. The influence of science also began appearing more commonly in poetry and literature during the Enlightenment. Some poetry became infused with scientific metaphor and imagery, while other poems were written directly about scientific topics. Sir Richard Blackmore committed the Newtonian system to verse in Creation, a Philosophical Poem in Seven Books (1712). After Newton's death in 1727, poems were composed in his honour for decades. James Thomson (1700–1748) penned his ""Poem to the Memory of Newton,"" which mourned the loss of Newton, but also praised his science and legacy. During the Age of Enlightenment, Freemasons comprised an international network of like-minded men, often meeting in secret in ritualistic programs at their lodges. they promoted the ideals of the Enlightenment, and helped diffuse these values across Britain and France and other places. Freemasonry as a systematic creed with its own myths, values and set of rituals originated in Scotland around 1600 and spread first to England and then across the Continent in the eighteenth century. They fostered new codes of conduct – including a communal understanding of liberty and equality inherited from guild sociability – ""liberty, fraternity, and equality"" Scottish soldiers and Jacobite Scots brought to the Continent ideals of fraternity which reflected not the local system of Scottish customs but the institutions and ideals originating in the English Revolution against royal absolutism. Freemasonry was particularly prevalent in France – by 1789, there were perhaps as many as 100,000 French Masons, making Freemasonry the most popular of all Enlightenment associations. The Freemasons displayed a passion for secrecy and created new degrees and ceremonies. Similar societies, partially imitating Freemasonry, emerged in France, Germany, Sweden and Russia. One example was the ""Illuminati"" founded in Bavaria in 1776, which was copied after the Freemasons but was never part of the movement. The Illuminati was an overtly political group, which most Masonic lodges decidedly were not. The most influential publication of the Enlightenment was the Encyclopédie, compiled by Denis Diderot and (until 1759) by Jean le Rond d'Alembert and a team of 150 scientists and philosophers. It was published between 1751 and 1772 in thirty-five volumes, and spread the ideas of the Enlightenment across Europe and beyond. Other landmark publications were the Dictionnaire philosophique (Philosophical Dictionary, 1764) and Letters on the English (1733) written by Voltaire; Rousseau's Discourse on Inequality (1754) and The Social Contract (1762); and Montesquieu's Spirit of the Laws (1748). The ideas of the Enlightenment played a major role in inspiring the French Revolution, which began in 1789. After the Revolution, the Enlightenment was followed by an opposing intellectual movement known as Romanticism. In England, the Royal Society of London also played a significant role in the public sphere and the spread of Enlightenment ideas. It was founded by a group of independent scientists and given a royal charter in 1662. The Society played a large role in spreading Robert Boyle's experimental philosophy around Europe, and acted as a clearinghouse for intellectual correspondence and exchange. Boyle was ""a founder of the experimental world in which scientists now live and operate,"" and his method based knowledge on experimentation, which had to be witnessed to provide proper empirical legitimacy. This is where the Royal Society came into play: witnessing had to be a ""collective act"", and the Royal Society's assembly rooms were ideal locations for relatively public demonstrations. However, not just any witness was considered to be credible; ""Oxford professors were accounted more reliable witnesses than Oxfordshire peasants."" Two factors were taken into account: a witness's knowledge in the area; and a witness's ""moral constitution"". In other words, only civil society were considered for Boyle's public. Immanuel Kant (1724–1804) tried to reconcile rationalism and religious belief, individual freedom and political authority, as well as map out a view of the public sphere through private and public reason. Kant's work continued to shape German thought, and indeed all of European philosophy, well into the 20th century. Mary Wollstonecraft was one of England's earliest feminist philosophers. She argued for a society based on reason, and that women, as well as men, should be treated as rational beings. She is best known for her work A Vindication of the Rights of Woman (1791). Although the existence of dictionaries and encyclopedias spanned into ancient times, the texts changed from simply defining words in a long running list to far more detailed discussions of those words in 18th-century encyclopedic dictionaries. The works were part of an Enlightenment movement to systematize knowledge and provide education to a wider audience than the elite. As the 18th century progressed, the content of encyclopedias also changed according to readers' tastes. Volumes tended to focus more strongly on secular affairs, particularly science and technology, rather than matters of theology. The Enlightenment took hold in most European countries, often with a specific local emphasis. For example, in France it became associated with anti-government and anti-Church radicalism while in Germany it reached deep into the middle classes and where it expressed a spiritualistic and nationalistic tone without threatening governments or established churches. Government responses varied widely. In France, the government was hostile, and the philosophes fought against its censorship, sometimes being imprisoned or hounded into exile. The British government for the most part ignored the Enlightenment's leaders in England and Scotland, although it did give Isaac Newton a knighthood and a very lucrative government office. These views on religious tolerance and the importance of individual conscience, along with the social contract, became particularly influential in the American colonies and the drafting of the United States Constitution. Thomas Jefferson called for a ""wall of separation between church and state"" at the federal level. He previously had supported successful efforts to disestablish the Church of England in Virginia, and authored the Virginia Statute for Religious Freedom. Jefferson's political ideals were greatly influenced by the writings of John Locke, Francis Bacon, and Isaac Newton whom he considered the three greatest men that ever lived. Both Rousseau and Locke's social contract theories rest on the presupposition of natural rights, which are not a result of law or custom, but are things that all men have in pre-political societies, and are therefore universal and inalienable. The most famous natural right formulation comes from John Locke in his Second Treatise, when he introduces the state of nature. For Locke the law of nature is grounded on mutual security, or the idea that one cannot infringe on another's natural rights, as every man is equal and has the same inalienable rights. These natural rights include perfect equality and freedom, and the right to preserve life and property. Locke also argued against slavery on the basis that enslaving yourself goes against the law of nature; you cannot surrender your own rights, your freedom is absolute and no one can take it from you. Additionally, Locke argues that one person cannot enslave another because it is morally reprehensible, although he introduces a caveat by saying that enslavement of a lawful captive in time of war would not go against one's natural rights. The context for the rise of the public sphere was the economic and social change commonly associated with the Industrial Revolution: ""economic expansion, increasing urbanization, rising population and improving communications in comparison to the stagnation of the previous century""."" Rising efficiency in production techniques and communication lowered the prices of consumer goods and increased the amount and variety of goods available to consumers (including the literature essential to the public sphere). Meanwhile, the colonial experience (most European states had colonial empires in the 18th century) began to expose European society to extremely heterogeneous cultures, leading to the breaking down of ""barriers between cultural systems, religious divides, gender differences and geographical areas"". Masonic lodges created a private model for public affairs. They ""reconstituted the polity and established a constitutional form of self-government, complete with constitutions and laws, elections and representatives."" In other words, the micro-society set up within the lodges constituted a normative model for society as a whole. This was especially true on the Continent: when the first lodges began to appear in the 1730s, their embodiment of British values was often seen as threatening by state authorities. For example, the Parisian lodge that met in the mid 1720s was composed of English Jacobite exiles. Furthermore, freemasons all across Europe explicitly linked themselves to the Enlightenment as a whole. In French lodges, for example, the line ""As the means to be enlightened I search for the enlightened"" was a part of their initiation rites. British lodges assigned themselves the duty to ""initiate the unenlightened"". This did not necessarily link lodges to the irreligious, but neither did this exclude them from the occasional heresy. In fact, many lodges praised the Grand Architect, the masonic terminology for the deistic divine being who created a scientifically ordered universe. The Enlightenment has always been contested territory. Its supporters ""hail it as the source of everything that is progressive about the modern world. For them, it stands for freedom of thought, rational inquiry, critical thinking, religious tolerance, political liberty, scientific achievement, the pursuit of happiness, and hope for the future."" However, its detractors accuse it of 'shallow' rationalism, naïve optimism, unrealistic universalism, and moral darkness. From the start there was a Counter-Enlightenment in which conservative and clerical defenders of traditional religion attacked materialism and skepticism as evil forces that encouraged immorality. By 1794, they pointed to the Terror during the French Revolution as confirmation of their predictions. As the Enlightenment was ending, Romantic philosophers argued that excessive dependence on reason was a mistake perpetuated by the Enlightenment, because it disregarded the bonds of history, myth, faith and tradition that were necessary to hold society together. In Germany, practical reference works intended for the uneducated majority became popular in the 18th century. The Marperger Curieuses Natur-, Kunst-, Berg-, Gewerkund Handlungs-Lexicon (1712) explained terms that usefully described the trades and scientific and commercial education. Jablonksi Allgemeines Lexicon (1721) was better known than the Handlungs-Lexicon, and underscored technical subjects rather than scientific theory. For example, over five columns of text were dedicated to wine, while geometry and logic were allocated only twenty-two and seventeen lines, respectively. The first edition of the Encyclopædia Britannica (1771) was modelled along the same lines as the German lexicons. One of the primary elements of the culture of the Enlightenment was the rise of the public sphere, a ""realm of communication marked by new arenas of debate, more open and accessible forms of urban public space and sociability, and an explosion of print culture,"" in the late 17th century and 18th century. Elements of the public sphere included: it was egalitarian, it discussed the domain of ""common concern,"" and argument was founded on reason. Habermas uses the term ""common concern"" to describe those areas of political/social knowledge and discussion that were previously the exclusive territory of the state and religious authorities, now open to critical examination by the public sphere. The values of this bourgeois public sphere included holding reason to be supreme, considering everything to be open to criticism (the public sphere is critical), and the opposition of secrecy of all sorts. More importantly, the contests were open to all, and the enforced anonymity of each submission guaranteed that neither gender nor social rank would determine the judging. Indeed, although the ""vast majority"" of participants belonged to the wealthier strata of society (""the liberal arts, the clergy, the judiciary, and the medical profession""), there were some cases of the popular classes submitting essays, and even winning. Similarly, a significant number of women participated – and won – the competitions. Of a total of 2300 prize competitions offered in France, women won 49 – perhaps a small number by modern standards, but very significant in an age in which most women did not have any academic training. Indeed, the majority of the winning entries were for poetry competitions, a genre commonly stressed in women's education. The debating societies discussed an extremely wide range of topics. Before the Enlightenment, most intellectual debates revolved around ""confessional"" – that is, Catholic, Lutheran, Reformed (Calvinist), or Anglican issues, and the main aim of these debates was to establish which bloc of faith ought to have the ""monopoly of truth and a God-given title to authority"". After this date everything thus previously rooted in tradition was questioned and often replaced by new concepts in the light of philosophical reason. After the second half of the 17th century and during the 18th century, a ""general process of rationalization and secularization set in,"" and confessional disputes were reduced to a secondary status in favor of the ""escalating contest between faith and incredulity"". The history of Academies in France during the Enlightenment begins with the Academy of Science, founded in 1635 in Paris. It was closely tied to the French state, acting as an extension of a government seriously lacking in scientists. It helped promote and organize new disciplines, and it trained new scientists. It also contributed to the enhancement of scientists' social status, considering them to be the ""most useful of all citizens"". Academies demonstrate the rising interest in science along with its increasing secularization, as evidenced by the small number of clerics who were members (13 percent). The presence of the French academies in the public sphere cannot be attributed to their membership; although the majority of their members were bourgeois, the exclusive institution was only open to elite Parisian scholars. They perceived themselves as ""interpreters of the sciences for the people"". For example, it was with this in mind that academicians took it upon themselves to disprove the popular pseudo-science of mesmerism. In addition to debates on religion, societies discussed issues such as politics and the role of women. It is important to note, however, that the critical subject matter of these debates did not necessarily translate into opposition to the government. In other words, the results of the debate quite frequently upheld the status quo. From a historical standpoint, one of the most important features of the debating society was their openness to the public; women attended and even participated in almost every debating society, which were likewise open to all classes providing they could pay the entrance fee. Once inside, spectators were able to participate in a largely egalitarian form of sociability that helped spread Enlightenment ideas. Francis Hutcheson, a moral philosopher, described the utilitarian and consequentialist principle that virtue is that which provides, in his words, ""the greatest happiness for the greatest numbers"". Much of what is incorporated in the scientific method (the nature of knowledge, evidence, experience, and causation) and some modern attitudes towards the relationship between science and religion were developed by his protégés David Hume and Adam Smith. Hume became a major figure in the skeptical philosophical and empiricist traditions of philosophy. Science came to play a leading role in Enlightenment discourse and thought. Many Enlightenment writers and thinkers had backgrounds in the sciences and associated scientific advancement with the overthrow of religion and traditional authority in favour of the development of free speech and thought. Scientific progress during the Enlightenment included the discovery of carbon dioxide (fixed air) by the chemist Joseph Black, the argument for deep time by the geologist James Hutton, and the invention of the steam engine by James Watt. The experiments of Lavoisier were used to create the first modern chemical plants in Paris, and the experiments of the Montgolfier Brothers enabled them to launch the first manned flight in a hot-air balloon on 21 November 1783, from the Château de la Muette, near the Bois de Boulogne. Across continental Europe, but in France especially, booksellers and publishers had to negotiate censorship laws of varying strictness. The Encyclopédie, for example, narrowly escaped seizure and had to be saved by Malesherbes, the man in charge of the French censure. Indeed, many publishing companies were conveniently located outside France so as to avoid overzealous French censors. They would smuggle their merchandise across the border, where it would then be transported to clandestine booksellers or small-time peddlers. The records of clandestine booksellers may give a better representation of what literate Frenchmen might have truly read, since their clandestine nature provided a less restrictive product choice. In one case, political books were the most popular category, primarily libels and pamphlets. Readers were more interested in sensationalist stories about criminals and political corruption than they were in political theory itself. The second most popular category, ""general works"" (those books ""that did not have a dominant motif and that contained something to offend almost everyone in authority"") demonstrated a high demand for generally low-brow subversive literature. However, these works never became part of literary canon, and are largely forgotten today as a result. The Café Procope was established in Paris in 1686; by the 1720s there were around 400 cafés in the city. The Café Procope in particular became a center of Enlightenment, welcoming such celebrities as Voltaire and Rousseau. The Café Procope was where Diderot and D'Alembert decided to create the Encyclopédie. The cafés were one of the various ""nerve centers"" for bruits publics, public noise or rumour. These bruits were allegedly a much better source of information than were the actual newspapers available at the time. In the Scottish Enlightenment, Scotland's major cities created an intellectual infrastructure of mutually supporting institutions such as universities, reading societies, libraries, periodicals, museums and masonic lodges. The Scottish network was ""predominantly liberal Calvinist, Newtonian, and 'design' oriented in character which played a major role in the further development of the transatlantic Enlightenment"". In France, Voltaire said ""we look to Scotland for all our ideas of civilization."" The focus of the Scottish Enlightenment ranged from intellectual and economic matters to the specifically scientific as in the work of William Cullen, physician and chemist; James Anderson, an agronomist; Joseph Black, physicist and chemist; and James Hutton, the first modern geologist. Scientific academies and societies grew out of the Scientific Revolution as the creators of scientific knowledge in contrast to the scholasticism of the university. During the Enlightenment, some societies created or retained links to universities. However, contemporary sources distinguished universities from scientific societies by claiming that the university's utility was in the transmission of knowledge, while societies functioned to create knowledge. As the role of universities in institutionalized science began to diminish, learned societies became the cornerstone of organized science. Official scientific societies were chartered by the state in order to provide technical expertise. Most societies were granted permission to oversee their own publications, control the election of new members, and the administration of the society. After 1700, a tremendous number of official academies and societies were founded in Europe, and by 1789 there were over seventy official scientific societies. In reference to this growth, Bernard de Fontenelle coined the term ""the Age of Academies"" to describe the 18th century. The first scientific and literary journals were established during the Enlightenment. The first journal, the Parisian Journal des Sçavans, appeared in 1665. However, it was not until 1682 that periodicals began to be more widely produced. French and Latin were the dominant languages of publication, but there was also a steady demand for material in German and Dutch. There was generally low demand for English publications on the Continent, which was echoed by England's similar lack of desire for French works. Languages commanding less of an international market – such as Danish, Spanish and Portuguese – found journal success more difficult, and more often than not, a more international language was used instead. French slowly took over Latin's status as the lingua franca of learned circles. This in turn gave precedence to the publishing industry in Holland, where the vast majority of these French language periodicals were produced. Coffeehouses were especially important to the spread of knowledge during the Enlightenment because they created a unique environment in which people from many different walks of life gathered and shared ideas. They were frequently criticized by nobles who feared the possibility of an environment in which class and its accompanying titles and privileges were disregarded. Such an environment was especially intimidating to monarchs who derived much of their power from the disparity between classes of people. If classes were to join together under the influence of Enlightenment thinking, they might recognize the all-encompassing oppression and abuses of their monarchs and, because of their size, might be able to carry out successful revolts. Monarchs also resented the idea of their subjects convening as one to discuss political matters, especially those concerning foreign affairs - rulers thought political affairs to be their business only, a result of their supposed divine right to rule.  The Republic of Letters was the sum of a number of Enlightenment ideals: an egalitarian realm governed by knowledge that could act across political boundaries and rival state power. It was a forum that supported ""free public examination of questions regarding religion or legislation"". Immanuel Kant considered written communication essential to his conception of the public sphere; once everyone was a part of the ""reading public"", then society could be said to be enlightened. The people who participated in the Republic of Letters, such as Diderot and Voltaire, are frequently known today as important Enlightenment figures. Indeed, the men who wrote Diderot's Encyclopédie arguably formed a microcosm of the larger ""republic""."
Pain,"A much smaller number of people are insensitive to pain due to an inborn abnormality of the nervous system, known as ""congenital insensitivity to pain"". Children with this condition incur carelessly-repeated damage to their tongues, eyes, joints, skin, and muscles. Some die before adulthood, and others have a reduced life expectancy.[citation needed] Most people with congenital insensitivity to pain have one of five hereditary sensory and autonomic neuropathies (which includes familial dysautonomia and congenital insensitivity to pain with anhidrosis). These conditions feature decreased sensitivity to pain together with other neurological abnormalities, particularly of the autonomic nervous system. A very rare syndrome with isolated congenital insensitivity to pain has been linked with mutations in the SCN9A gene, which codes for a sodium channel (Nav1.7) necessary in conducting pain nerve stimuli. Spinal cord fibers dedicated to carrying A-delta fiber pain signals, and others that carry both A-delta and C fiber pain signals up the spinal cord to the thalamus in the brain have been identified. Other spinal cord fibers, known as wide dynamic range neurons, respond to A-delta and C fibers, but also to the large A-beta fibers that carry touch, pressure and vibration signals. Pain-related activity in the thalamus spreads to the insular cortex (thought to embody, among other things, the feeling that distinguishes pain from other homeostatic emotions such as itch and nausea) and anterior cingulate cortex (thought to embody, among other things, the motivational element of pain); and pain that is distinctly located also activates the primary and secondary somatosensory cortices. Melzack and Casey's 1968 picture of the dimensions of pain is as influential today as ever, firmly framing theory and guiding research in the functional neuroanatomy and psychology of pain. Paraplegia, the loss of sensation and voluntary motor control after serious spinal cord damage, may be accompanied by girdle pain at the level of the spinal cord damage, visceral pain evoked by a filling bladder or bowel, or, in five to ten per cent of paraplegics, phantom body pain in areas of complete sensory loss. This phantom body pain is initially described as burning or tingling but may evolve into severe crushing or pinching pain, or the sensation of fire running down the legs or of a knife twisting in the flesh. Onset may be immediate or may not occur until years after the disabling injury. Surgical treatment rarely provides lasting relief. The pain signal travels from the periphery to the spinal cord along an A-delta or C fiber. Because the A-delta fiber is thicker than the C fiber, and is thinly sheathed in an electrically insulating material (myelin), it carries its signal faster (5–30 m/s) than the unmyelinated C fiber (0.5–2 m/s). Pain evoked by the (faster) A-delta fibers is described as sharp and is felt first. This is followed by a duller pain, often described as burning, carried by the C fibers. These first order neurons enter the spinal cord via Lissauer's tract. Cultural barriers can also keep a person from telling someone they are in pain. Religious beliefs may prevent the individual from seeking help. They may feel certain pain treatment is against their religion. They may not report pain because they feel it is a sign that death is near. Many people fear the stigma of addiction and avoid pain treatment so as not to be prescribed potentially addicting drugs. Many Asians do not want to lose respect in society by admitting they are in pain and need help, believing the pain should be borne in silence, while other cultures feel they should report pain right away and get immediate relief. Gender can also be a factor in reporting pain. Sexual differences can be the result of social and cultural expectations, with women expected to be emotional and show pain and men stoic, keeping pain to themselves. Pain is usually transitory, lasting only until the noxious stimulus is removed or the underlying damage or pathology has healed, but some painful conditions, such as rheumatoid arthritis, peripheral neuropathy, cancer and idiopathic pain, may persist for years. Pain that lasts a long time is called chronic or persistent, and pain that resolves quickly is called acute. Traditionally, the distinction between acute and chronic pain has relied upon an arbitrary interval of time from onset; the two most commonly used markers being 3 months and 6 months since the onset of pain, though some theorists and researchers have placed the transition from acute to chronic pain at 12 months.:93 Others apply acute to pain that lasts less than 30 days, chronic to pain of more than six months' duration, and subacute to pain that lasts from one to six months. A popular alternative definition of chronic pain, involving no arbitrarily fixed durations, is ""pain that extends beyond the expected period of healing"". Chronic pain may be classified as cancer pain or else as benign. The Multidimensional Pain Inventory (MPI) is a questionnaire designed to assess the psychosocial state of a person with chronic pain. Analysis of MPI results by Turk and Rudy (1988) found three classes of chronic pain patient: ""(a) dysfunctional, people who perceived the severity of their pain to be high, reported that pain interfered with much of their lives, reported a higher degree of psychological distress caused by pain, and reported low levels of activity; (b) interpersonally distressed, people with a common perception that significant others were not very supportive of their pain problems; and (c) adaptive copers, patients who reported high levels of social support, relatively low levels of pain and perceived interference, and relatively high levels of activity."" Combining the MPI characterization of the person with their IASP five-category pain profile is recommended for deriving the most useful case description. Pain is the most common reason for physician consultation in most developed countries. It is a major symptom in many medical conditions, and can interfere with a person's quality of life and general functioning. Psychological factors such as social support, hypnotic suggestion, excitement, or distraction can significantly affect pain's intensity or unpleasantness. In some arguments put forth in physician-assisted suicide or euthanasia debates, pain has been used as an argument to permit terminally ill patients to end their lives.  In 1955, DC Sinclair and G Weddell developed peripheral pattern theory, based on a 1934 suggestion by John Paul Nafe. They proposed that all skin fiber endings (with the exception of those innervating hair cells) are identical, and that pain is produced by intense stimulation of these fibers. Another 20th-century theory was gate control theory, introduced by Ronald Melzack and Patrick Wall in the 1965 Science article ""Pain Mechanisms: A New Theory"". The authors proposed that both thin (pain) and large diameter (touch, pressure, vibration) nerve fibers carry information from the site of injury to two destinations in the dorsal horn of the spinal cord, and that the more large fiber activity relative to thin fiber activity at the inhibitory cell, the less pain is felt. Both peripheral pattern theory and gate control theory have been superseded by more modern theories of pain[citation needed]. A person's self-report is the most reliable measure of pain, with health care professionals tending to underestimate severity. A definition of pain widely employed in nursing, emphasizing its subjective nature and the importance of believing patient reports, was introduced by Margo McCaffery in 1968: ""Pain is whatever the experiencing person says it is, existing whenever he says it does"". To assess intensity, the patient may be asked to locate their pain on a scale of 0 to 10, with 0 being no pain at all, and 10 the worst pain they have ever felt. Quality can be established by having the patient complete the McGill Pain Questionnaire indicating which words best describe their pain. Local anesthetic injections into the nerves or sensitive areas of the stump may relieve pain for days, weeks, or sometimes permanently, despite the drug wearing off in a matter of hours; and small injections of hypertonic saline into the soft tissue between vertebrae produces local pain that radiates into the phantom limb for ten minutes or so and may be followed by hours, weeks or even longer of partial or total relief from phantom pain. Vigorous vibration or electrical stimulation of the stump, or current from electrodes surgically implanted onto the spinal cord, all produce relief in some patients. Differences in pain perception and tolerance thresholds are associated with, among other factors, ethnicity, genetics, and sex. People of Mediterranean origin report as painful some radiant heat intensities that northern Europeans describe as nonpainful. And Italian women tolerate less intense electric shock than Jewish or Native American women. Some individuals in all cultures have significantly higher than normal pain perception and tolerance thresholds. For instance, patients who experience painless heart attacks have higher pain thresholds for electric shock, muscle cramp and heat. The International Association for the Study of Pain advocates that the relief of pain should be recognized as a human right, that chronic pain should be considered a disease in its own right, and that pain medicine should have the full status of a specialty. It is a specialty only in China and Australia at this time. Elsewhere, pain medicine is a subspecialty under disciplines such as anesthesiology, physiatry, neurology, palliative medicine and psychiatry. In 2011, Human Rights Watch alerted that tens of millions of people worldwide are still denied access to inexpensive medications for severe pain. When a person is non-verbal and cannot self-report pain, observation becomes critical, and specific behaviors can be monitored as pain indicators. Behaviors such as facial grimacing and guarding indicate pain, as well as an increase or decrease in vocalizations, changes in routine behavior patterns and mental status changes. Patients experiencing pain may exhibit withdrawn social behavior and possibly experience a decreased appetite and decreased nutritional intake. A change in condition that deviates from baseline such as moaning with movement or when manipulating a body part, and limited range of motion are also potential pain indicators. In patients who possess language but are incapable of expressing themselves effectively, such as those with dementia, an increase in confusion or display of aggressive behaviors or agitation may signal that discomfort exists, and further assessment is necessary. The prevalence of phantom pain in upper limb amputees is nearly 82%, and in lower limb amputees is 54%. One study found that eight days after amputation, 72 percent of patients had phantom limb pain, and six months later, 65 percent reported it. Some amputees experience continuous pain that varies in intensity or quality; others experience several bouts a day, or it may occur only once every week or two. It is often described as shooting, crushing, burning or cramping. If the pain is continuous for a long period, parts of the intact body may become sensitized, so that touching them evokes pain in the phantom limb, or phantom limb pain may accompany urination or defecation. A number of meta-analyses have found clinical hypnosis to be effective in controlling pain associated with diagnostic and surgical procedures in both adults and children, as well as pain associated with cancer and childbirth. A 2007 review of 13 studies found evidence for the efficacy of hypnosis in the reduction of chronic pain in some conditions, though the number of patients enrolled in the studies was low, bringing up issues of power to detect group differences, and most lacked credible controls for placebo and/or expectation. The authors concluded that ""although the findings provide support for the general applicability of hypnosis in the treatment of chronic pain, considerably more research will be needed to fully determine the effects of hypnosis for different chronic-pain conditions."" The most reliable method for assessing pain in most humans is by asking a question: a person may report pain that cannot be detected by any known physiological measure. However, like infants (Latin infans meaning ""unable to speak""), animals cannot answer questions about whether they feel pain; thus the defining criterion for pain in humans cannot be applied to them. Philosophers and scientists have responded to this difficulty in a variety of ways. René Descartes for example argued that animals lack consciousness and therefore do not experience pain and suffering in the way that humans do. Bernard Rollin of Colorado State University, the principal author of two U.S. federal laws regulating pain relief for animals, writes that researchers remained unsure into the 1980s as to whether animals experience pain, and that veterinarians trained in the U.S. before 1989 were simply taught to ignore animal pain. In his interactions with scientists and other veterinarians, he was regularly asked to ""prove"" that animals are conscious, and to provide ""scientifically acceptable"" grounds for claiming that they feel pain. Carbone writes that the view that animals feel pain differently is now a minority view. Academic reviews of the topic are more equivocal, noting that although the argument that animals have at least simple conscious thoughts and feelings has strong support, some critics continue to question how reliably animal mental states can be determined. The ability of invertebrate species of animals, such as insects, to feel pain and suffering is also unclear. Pain is a distressing feeling often caused by intense or damaging stimuli, such as stubbing a toe, burning a finger, putting alcohol on a cut, and bumping the ""funny bone"". Because it is a complex, subjective phenomenon, defining pain has been a challenge. The International Association for the Study of Pain's widely used definition states: ""Pain is an unpleasant sensory and emotional experience associated with actual or potential tissue damage, or described in terms of such damage."" In medical diagnosis, pain is a symptom. In his book, The Greatest Show on Earth: The Evidence for Evolution, biologist Richard Dawkins grapples with the question of why pain has to be so very painful. He describes the alternative as a simple, mental raising of a ""red flag"". To argue why that red flag might be insufficient, Dawkins explains that drives must compete with each other within living beings. The most fit creature would be the one whose pains are well balanced. Those pains which mean certain death when ignored will become the most powerfully felt. The relative intensities of pain, then, may resemble the relative importance of that risk to our ancestors (lack of food, too much cold, or serious injuries are felt as agony, whereas minor damage is felt as mere discomfort). This resemblance will not be perfect, however, because natural selection can be a poor designer. The result is often glitches in animals, including supernormal stimuli. Such glitches help explain pains which are not, or at least no longer directly adaptive (e.g. perhaps some forms of toothache, or injury to fingernails). In 1994, responding to the need for a more useful system for describing chronic pain, the International Association for the Study of Pain (IASP) classified pain according to specific characteristics: (1) region of the body involved (e.g. abdomen, lower limbs), (2) system whose dysfunction may be causing the pain (e.g., nervous, gastrointestinal), (3) duration and pattern of occurrence, (4) intensity and time since onset, and (5) etiology. However, this system has been criticized by Clifford J. Woolf and others as inadequate for guiding research and treatment. Woolf suggests three classes of pain : (1) nociceptive pain, (2) inflammatory pain which is associated with tissue damage and the infiltration of immune cells, and (3) pathological pain which is a disease state caused by damage to the nervous system or by its abnormal function (e.g. fibromyalgia, irritable bowel syndrome, tension type headache, etc.). Breakthrough pain is transitory acute pain that comes on suddenly and is not alleviated by the patient's normal pain management. It is common in cancer patients who often have background pain that is generally well-controlled by medications, but who also sometimes experience bouts of severe pain that from time to time ""breaks through"" the medication. The characteristics of breakthrough cancer pain vary from person to person and according to the cause. Management of breakthrough pain can entail intensive use of opioids, including fentanyl. Although unpleasantness is an essential part of the IASP definition of pain, it is possible to induce a state described as intense pain devoid of unpleasantness in some patients, with morphine injection or psychosurgery. Such patients report that they have pain but are not bothered by it; they recognize the sensation of pain but suffer little, or not at all. Indifference to pain can also rarely be present from birth; these people have normal nerves on medical investigations, and find pain unpleasant, but do not avoid repetition of the pain stimulus. Nociceptive pain may also be divided into ""visceral"", ""deep somatic"" and ""superficial somatic"" pain. Visceral structures are highly sensitive to stretch, ischemia and inflammation, but relatively insensitive to other stimuli that normally evoke pain in other structures, such as burning and cutting. Visceral pain is diffuse, difficult to locate and often referred to a distant, usually superficial, structure. It may be accompanied by nausea and vomiting and may be described as sickening, deep, squeezing, and dull. Deep somatic pain is initiated by stimulation of nociceptors in ligaments, tendons, bones, blood vessels, fasciae and muscles, and is dull, aching, poorly-localized pain. Examples include sprains and broken bones. Superficial pain is initiated by activation of nociceptors in the skin or other superficial tissue, and is sharp, well-defined and clearly located. Examples of injuries that produce superficial somatic pain include minor wounds and minor (first degree) burns.  Sugar taken orally reduces the total crying time but not the duration of the first cry in newborns undergoing a painful procedure (a single lancing of the heel). It does not moderate the effect of pain on heart rate and a recent single study found that sugar did not significantly affect pain-related electrical activity in the brains of newborns one second after the heel lance procedure. Sweet oral liquid moderately reduces the incidence and duration of crying caused by immunization injection in children between one and twelve months of age. In 1968 Ronald Melzack and Kenneth Casey described pain in terms of its three dimensions: ""sensory-discriminative"" (sense of the intensity, location, quality and duration of the pain), ""affective-motivational"" (unpleasantness and urge to escape the unpleasantness), and ""cognitive-evaluative"" (cognitions such as appraisal, cultural values, distraction and hypnotic suggestion). They theorized that pain intensity (the sensory discriminative dimension) and unpleasantness (the affective-motivational dimension) are not simply determined by the magnitude of the painful stimulus, but ""higher"" cognitive activities can influence perceived intensity and unpleasantness. Cognitive activities ""may affect both sensory and affective experience or they may modify primarily the affective-motivational dimension. Thus, excitement in games or war appears to block both dimensions of pain, while suggestion and placebos may modulate the affective-motivational dimension and leave the sensory-discriminative dimension relatively undisturbed."" (p. 432) The paper ends with a call to action: ""Pain can be treated not only by trying to cut down the sensory input by anesthetic block, surgical intervention and the like, but also by influencing the motivational-affective and cognitive factors as well."" (p. 435) People with long-term pain frequently display psychological disturbance, with elevated scores on the Minnesota Multiphasic Personality Inventory scales of hysteria, depression and hypochondriasis (the ""neurotic triad""). Some investigators have argued that it is this neuroticism that causes acute pain to turn chronic, but clinical evidence points the other way, to chronic pain causing neuroticism. When long-term pain is relieved by therapeutic intervention, scores on the neurotic triad and anxiety fall, often to normal levels. Self-esteem, often low in chronic pain patients, also shows improvement once pain has resolved. Physical pain is an important political topic in relation to various issues, including pain management policy, drug control, animal rights or animal welfare, torture, and pain compliance. In various contexts, the deliberate infliction of pain in the form of corporal punishment is used as retribution for an offence, or for the purpose of disciplining or reforming a wrongdoer, or to deter attitudes or behaviour deemed unacceptable. In some cultures, extreme practices such as mortification of the flesh or painful rites of passage are highly regarded. Wilhelm Erb's (1874) ""intensive"" theory, that a pain signal can be generated by intense enough stimulation of any sensory receptor, has been soundly disproved. Some sensory fibers do not differentiate between noxious and non-noxious stimuli, while others, nociceptors, respond only to noxious, high intensity stimuli. At the peripheral end of the nociceptor, noxious stimuli generate currents that, above a given threshold, begin to send signals along the nerve fiber to the spinal cord. The ""specificity"" (whether it responds to thermal, chemical or mechanical features of its environment) of a nociceptor is determined by which ion channels it expresses at its peripheral end. Dozens of different types of nociceptor ion channels have so far been identified, and their exact functions are still being determined. In 1644, René Descartes theorized that pain was a disturbance that passed down along nerve fibers until the disturbance reached the brain, a development that transformed the perception of pain from a spiritual, mystical experience to a physical, mechanical sensation[citation needed]. Descartes's work, along with Avicenna's, prefigured the 19th-century development of specificity theory. Specificity theory saw pain as ""a specific sensation, with its own sensory apparatus independent of touch and other senses"". Another theory that came to prominence in the 18th and 19th centuries was intensive theory, which conceived of pain not as a unique sensory modality, but an emotional state produced by stronger than normal stimuli such as intense light, pressure or temperature. By the mid-1890s, specificity was backed mostly by physiologists and physicians, and the intensive theory was mostly backed by psychologists. However, after a series of clinical observations by Henry Head and experiments by Max von Frey, the psychologists migrated to specificity almost en masse, and by century's end, most textbooks on physiology and psychology were presenting pain specificity as fact. The experience of pain has many cultural dimensions. For instance, the way in which one experiences and responds to pain is related to sociocultural characteristics, such as gender, ethnicity, and age. An aging adult may not respond to pain in the way that a younger person would. Their ability to recognize pain may be blunted by illness or the use of multiple prescription drugs. Depression may also keep the older adult from reporting they are in pain. The older adult may also quit doing activities they love because it hurts too much. Decline in self-care activities (dressing, grooming, walking, etc.) may also be indicators that the older adult is experiencing pain. The older adult may refrain from reporting pain because they are afraid they will have to have surgery or will be put on a drug they might become addicted to. They may not want others to see them as weak, or may feel there is something impolite or shameful in complaining about pain, or they may feel the pain is deserved punishment for past transgressions. The presence of pain in an animal cannot be known for certain, but it can be inferred through physical and behavioral reactions. Specialists currently believe that all vertebrates can feel pain, and that certain invertebrates, like the octopus, might too. As for other animals, plants, or other entities, their ability to feel physical pain is at present a question beyond scientific reach, since no mechanism is known by which they could have such a feeling. In particular, there are no known nociceptors in groups such as plants, fungi, and most insects, except for instance in fruit flies. Pain is the most common reason for people to use complementary and alternative medicine. An analysis of the 13 highest quality studies of pain treatment with acupuncture, published in January 2009, concluded there is little difference in the effect of real, sham and no acupuncture. However other reviews have found benefit. Additionally, there is tentative evidence for a few herbal medicine. There is interest in the relationship between vitamin D and pain, but the evidence so far from controlled trials for such a relationship, other than in osteomalacia, is unconvincing. Nociceptive pain is caused by stimulation of peripheral nerve fibers that respond to stimuli approaching or exceeding harmful intensity (nociceptors), and may be classified according to the mode of noxious stimulation. The most common categories are ""thermal"" (e.g. heat or cold), ""mechanical"" (e.g. crushing, tearing, shearing, etc.) and ""chemical"" (e.g. iodine in a cut or chemicals released during inflammation). Some nociceptors respond to more than one of these modalities and are consequently designated polymodal."
Oxygen,"Reactive oxygen species, such as superoxide ion (O−
2) and hydrogen peroxide (H
2O
2), are dangerous by-products of oxygen use in organisms. Parts of the immune system of higher organisms create peroxide, superoxide, and singlet oxygen to destroy invading microbes. Reactive oxygen species also play an important role in the hypersensitive response of plants against pathogen attack. Oxygen is toxic to obligately anaerobic organisms, which were the dominant form of early life on Earth until O
2 began to accumulate in the atmosphere about 2.5 billion years ago during the Great Oxygenation Event, about a billion years after the first appearance of these organisms. Oxygen toxicity to the lungs and central nervous system can also occur in deep scuba diving and surface supplied diving. Prolonged breathing of an air mixture with an O
2 partial pressure more than 60 kPa can eventually lead to permanent pulmonary fibrosis. Exposure to a O
2 partial pressures greater than 160 kPa (about 1.6 atm) may lead to convulsions (normally fatal for divers). Acute oxygen toxicity (causing seizures, its most feared effect for divers) can occur by breathing an air mixture with 21% O
2 at 66 m or more of depth; the same thing can occur by breathing 100% O
2 at only 6 m. Many major classes of organic molecules in living organisms, such as proteins, nucleic acids, carbohydrates, and fats, contain oxygen, as do the major inorganic compounds that are constituents of animal shells, teeth, and bone. Most of the mass of living organisms is oxygen as it is a part of water, the major constituent of lifeforms. Oxygen is used in cellular respiration and released by photosynthesis, which uses the energy of sunlight to produce oxygen from water. It is too chemically reactive to remain a free element in air without being continuously replenished by the photosynthetic action of living organisms. Another form (allotrope) of oxygen, ozone (O
3), strongly absorbs UVB radiation and consequently the high-altitude ozone layer helps protect the biosphere from ultraviolet radiation, but is a pollutant near the surface where it is a by-product of smog. At even higher low earth orbit altitudes, sufficient atomic oxygen is present to cause erosion for spacecraft. Oxygen, as a supposed mild euphoric, has a history of recreational use in oxygen bars and in sports. Oxygen bars are establishments, found in Japan, California, and Las Vegas, Nevada since the late 1990s that offer higher than normal O
2 exposure for a fee. Professional athletes, especially in American football, also sometimes go off field between plays to wear oxygen masks in order to get a ""boost"" in performance. The pharmacological effect is doubtful; a placebo effect is a more likely explanation. Available studies support a performance boost from enriched O
2 mixtures only if they are breathed during aerobic exercise. Among the most important classes of organic compounds that contain oxygen are (where ""R"" is an organic group): alcohols (R-OH); ethers (R-O-R); ketones (R-CO-R); aldehydes (R-CO-H); carboxylic acids (R-COOH); esters (R-COO-R); acid anhydrides (R-CO-O-CO-R); and amides (R-C(O)-NR
2). There are many important organic solvents that contain oxygen, including: acetone, methanol, ethanol, isopropanol, furan, THF, diethyl ether, dioxane, ethyl acetate, DMF, DMSO, acetic acid, and formic acid. Acetone ((CH
3)
2CO) and phenol (C
6H
5OH) are used as feeder materials in the synthesis of many different substances. Other important organic compounds that contain oxygen are: glycerol, formaldehyde, glutaraldehyde, citric acid, acetic anhydride, and acetamide. Epoxides are ethers in which the oxygen atom is part of a ring of three atoms. In this dioxygen, the two oxygen atoms are chemically bonded to each other. The bond can be variously described based on level of theory, but is reasonably and simply described as a covalent double bond that results from the filling of molecular orbitals formed from the atomic orbitals of the individual oxygen atoms, the filling of which results in a bond order of two. More specifically, the double bond is the result of sequential, low-to-high energy, or Aufbau, filling of orbitals, and the resulting cancellation of contributions from the 2s electrons, after sequential filling of the low σ and σ* orbitals; σ overlap of the two atomic 2p orbitals that lie along the O-O molecular axis and π overlap of two pairs of atomic 2p orbitals perpendicular to the O-O molecular axis, and then cancellation of contributions from the remaining two of the six 2p electrons after their partial filling of the lowest π and π* orbitals. The element is found in almost all biomolecules that are important to (or generated by) life. Only a few common complex biomolecules, such as squalene and the carotenes, contain no oxygen. Of the organic compounds with biological relevance, carbohydrates contain the largest proportion by mass of oxygen. All fats, fatty acids, amino acids, and proteins contain oxygen (due to the presence of carbonyl groups in these acids and their ester residues). Oxygen also occurs in phosphate (PO3−
4) groups in the biologically important energy-carrying molecules ATP and ADP, in the backbone and the purines (except adenine) and pyrimidines of RNA and DNA, and in bones as calcium phosphate and hydroxylapatite. Hyperbaric (high-pressure) medicine uses special oxygen chambers to increase the partial pressure of O
2 around the patient and, when needed, the medical staff. Carbon monoxide poisoning, gas gangrene, and decompression sickness (the 'bends') are sometimes treated using these devices. Increased O
2 concentration in the lungs helps to displace carbon monoxide from the heme group of hemoglobin. Oxygen gas is poisonous to the anaerobic bacteria that cause gas gangrene, so increasing its partial pressure helps kill them. Decompression sickness occurs in divers who decompress too quickly after a dive, resulting in bubbles of inert gas, mostly nitrogen and helium, forming in their blood. Increasing the pressure of O
2 as soon as possible is part of the treatment. Free oxygen gas was almost nonexistent in Earth's atmosphere before photosynthetic archaea and bacteria evolved, probably about 3.5 billion years ago. Free oxygen first appeared in significant quantities during the Paleoproterozoic eon (between 3.0 and 2.3 billion years ago). For the first billion years, any free oxygen produced by these organisms combined with dissolved iron in the oceans to form banded iron formations. When such oxygen sinks became saturated, free oxygen began to outgas from the oceans 3–2.7 billion years ago, reaching 10% of its present level around 1.7 billion years ago. John Dalton's original atomic hypothesis assumed that all elements were monatomic and that the atoms in compounds would normally have the simplest atomic ratios with respect to one another. For example, Dalton assumed that water's formula was HO, giving the atomic mass of oxygen as 8 times that of hydrogen, instead of the modern value of about 16. In 1805, Joseph Louis Gay-Lussac and Alexander von Humboldt showed that water is formed of two volumes of hydrogen and one volume of oxygen; and by 1811 Amedeo Avogadro had arrived at the correct interpretation of water's composition, based on what is now called Avogadro's law and the assumption of diatomic elemental molecules.[a] Oxygen gas (O
2) can be toxic at elevated partial pressures, leading to convulsions and other health problems.[j] Oxygen toxicity usually begins to occur at partial pressures more than 50 kilopascals (kPa), equal to about 50% oxygen composition at standard pressure or 2.5 times the normal sea-level O
2 partial pressure of about 21 kPa. This is not a problem except for patients on mechanical ventilators, since gas supplied through oxygen masks in medical applications is typically composed of only 30%–50% O
2 by volume (about 30 kPa at standard pressure). (although this figure also is subject to wide variation, depending on type of mask). Oxygen is a chemical element with symbol O and atomic number 8. It is a member of the chalcogen group on the periodic table and is a highly reactive nonmetal and oxidizing agent that readily forms compounds (notably oxides) with most elements. By mass, oxygen is the third-most abundant element in the universe, after hydrogen and helium. At standard temperature and pressure, two atoms of the element bind to form dioxygen, a colorless and odorless diatomic gas with the formula O
2. Diatomic oxygen gas constitutes 20.8% of the Earth's atmosphere. However, monitoring of atmospheric oxygen levels show a global downward trend, because of fossil-fuel burning. Oxygen is the most abundant element by mass in the Earth's crust as part of oxide compounds such as silicon dioxide, making up almost half of the crust's mass. Oxygen is present in the atmosphere in trace quantities in the form of carbon dioxide (CO
2). The Earth's crustal rock is composed in large part of oxides of silicon (silica SiO
2, as found in granite and quartz), aluminium (aluminium oxide Al
2O
3, in bauxite and corundum), iron (iron(III) oxide Fe
2O
3, in hematite and rust), and calcium carbonate (in limestone). The rest of the Earth's crust is also made of oxygen compounds, in particular various complex silicates (in silicate minerals). The Earth's mantle, of much larger mass than the crust, is largely composed of silicates of magnesium and iron. The other major method of producing O
2 gas involves passing a stream of clean, dry air through one bed of a pair of identical zeolite molecular sieves, which absorbs the nitrogen and delivers a gas stream that is 90% to 93% O
2. Simultaneously, nitrogen gas is released from the other nitrogen-saturated zeolite bed, by reducing the chamber operating pressure and diverting part of the oxygen gas from the producer bed through it, in the reverse direction of flow. After a set cycle time the operation of the two beds is interchanged, thereby allowing for a continuous supply of gaseous oxygen to be pumped through a pipeline. This is known as pressure swing adsorption. Oxygen gas is increasingly obtained by these non-cryogenic technologies (see also the related vacuum swing adsorption). In 1891 Scottish chemist James Dewar was able to produce enough liquid oxygen to study. The first commercially viable process for producing liquid oxygen was independently developed in 1895 by German engineer Carl von Linde and British engineer William Hampson. Both men lowered the temperature of air until it liquefied and then distilled the component gases by boiling them off one at a time and capturing them. Later, in 1901, oxyacetylene welding was demonstrated for the first time by burning a mixture of acetylene and compressed O
2. This method of welding and cutting metal later became common. Singlet oxygen is a name given to several higher-energy species of molecular O
2 in which all the electron spins are paired. It is much more reactive towards common organic molecules than is molecular oxygen per se. In nature, singlet oxygen is commonly formed from water during photosynthesis, using the energy of sunlight. It is also produced in the troposphere by the photolysis of ozone by light of short wavelength, and by the immune system as a source of active oxygen. Carotenoids in photosynthetic organisms (and possibly also in animals) play a major role in absorbing energy from singlet oxygen and converting it to the unexcited ground state before it can cause harm to tissues. The unusually high concentration of oxygen gas on Earth is the result of the oxygen cycle. This biogeochemical cycle describes the movement of oxygen within and between its three main reservoirs on Earth: the atmosphere, the biosphere, and the lithosphere. The main driving factor of the oxygen cycle is photosynthesis, which is responsible for modern Earth's atmosphere. Photosynthesis releases oxygen into the atmosphere, while respiration and decay remove it from the atmosphere. In the present equilibrium, production and consumption occur at the same rate of roughly 1/2000th of the entire atmospheric oxygen per year. In the late 17th century, Robert Boyle proved that air is necessary for combustion. English chemist John Mayow (1641–1679) refined this work by showing that fire requires only a part of air that he called spiritus nitroaereus or just nitroaereus. In one experiment he found that placing either a mouse or a lit candle in a closed container over water caused the water to rise and replace one-fourteenth of the air's volume before extinguishing the subjects. From this he surmised that nitroaereus is consumed in both respiration and combustion. People who climb mountains or fly in non-pressurized fixed-wing aircraft sometimes have supplemental O
2 supplies.[h] Passengers traveling in (pressurized) commercial airplanes have an emergency supply of O
2 automatically supplied to them in case of cabin depressurization. Sudden cabin pressure loss activates chemical oxygen generators above each seat, causing oxygen masks to drop. Pulling on the masks ""to start the flow of oxygen"" as cabin safety instructions dictate, forces iron filings into the sodium chlorate inside the canister. A steady stream of oxygen gas is then produced by the exothermic reaction. Free oxygen also occurs in solution in the world's water bodies. The increased solubility of O
2 at lower temperatures (see Physical properties) has important implications for ocean life, as polar oceans support a much higher density of life due to their higher oxygen content. Water polluted with plant nutrients such as nitrates or phosphates may stimulate growth of algae by a process called eutrophication and the decay of these organisms and other biomaterials may reduce amounts of O
2 in eutrophic water bodies. Scientists assess this aspect of water quality by measuring the water's biochemical oxygen demand, or the amount of O
2 needed to restore it to a normal concentration. This combination of cancellations and σ and π overlaps results in dioxygen's double bond character and reactivity, and a triplet electronic ground state. An electron configuration with two unpaired electrons as found in dioxygen (see the filled π* orbitals in the diagram), orbitals that are of equal energy—i.e., degenerate—is a configuration termed a spin triplet state. Hence, the ground state of the O
2 molecule is referred to as triplet oxygen.[b] The highest energy, partially filled orbitals are antibonding, and so their filling weakens the bond order from three to two. Because of its unpaired electrons, triplet oxygen reacts only slowly with most organic molecules, which have paired electron spins; this prevents spontaneous combustion. In the triplet form, O
2 molecules are paramagnetic. That is, they impart magnetic character to oxygen when it is in the presence of a magnetic field, because of the spin magnetic moments of the unpaired electrons in the molecule, and the negative exchange energy between neighboring O
2 molecules. Liquid oxygen is attracted to a magnet to a sufficient extent that, in laboratory demonstrations, a bridge of liquid oxygen may be supported against its own weight between the poles of a powerful magnet.[c] Uptake of O
2 from the air is the essential purpose of respiration, so oxygen supplementation is used in medicine. Treatment not only increases oxygen levels in the patient's blood, but has the secondary effect of decreasing resistance to blood flow in many types of diseased lungs, easing work load on the heart. Oxygen therapy is used to treat emphysema, pneumonia, some heart disorders (congestive heart failure), some disorders that cause increased pulmonary artery pressure, and any disease that impairs the body's ability to take up and use gaseous oxygen. Oxygen storage methods include high pressure oxygen tanks, cryogenics and chemical compounds. For reasons of economy, oxygen is often transported in bulk as a liquid in specially insulated tankers, since one liter of liquefied oxygen is equivalent to 840 liters of gaseous oxygen at atmospheric pressure and 20 °C (68 °F). Such tankers are used to refill bulk liquid oxygen storage containers, which stand outside hospitals and other institutions with a need for large volumes of pure oxygen gas. Liquid oxygen is passed through heat exchangers, which convert the cryogenic liquid into gas before it enters the building. Oxygen is also stored and shipped in smaller cylinders containing the compressed gas; a form that is useful in certain portable medical applications and oxy-fuel welding and cutting. Oxygen was discovered independently by Carl Wilhelm Scheele, in Uppsala, in 1773 or earlier, and Joseph Priestley in Wiltshire, in 1774, but Priestley is often given priority because his work was published first. The name oxygen was coined in 1777 by Antoine Lavoisier, whose experiments with oxygen helped to discredit the then-popular phlogiston theory of combustion and corrosion. Its name derives from the Greek roots ὀξύς oxys, ""acid"", literally ""sharp"", referring to the sour taste of acids and -γενής -genes, ""producer"", literally ""begetter"", because at the time of naming, it was mistakenly thought that all acids required oxygen in their composition. Common uses of oxygen includes the production cycle of steel, plastics and textiles, brazing, welding and cutting of steels and other metals, rocket propellant, in oxygen therapy and life support systems in aircraft, submarines, spaceflight and diving. In the meantime, on August 1, 1774, an experiment conducted by the British clergyman Joseph Priestley focused sunlight on mercuric oxide (HgO) inside a glass tube, which liberated a gas he named ""dephlogisticated air"". He noted that candles burned brighter in the gas and that a mouse was more active and lived longer while breathing it. After breathing the gas himself, he wrote: ""The feeling of it to my lungs was not sensibly different from that of common air, but I fancied that my breast felt peculiarly light and easy for some time afterwards."" Priestley published his findings in 1775 in a paper titled ""An Account of Further Discoveries in Air"" which was included in the second volume of his book titled Experiments and Observations on Different Kinds of Air. Because he published his findings first, Priestley is usually given priority in the discovery. Highly combustible materials that leave little residue, such as wood or coal, were thought to be made mostly of phlogiston; whereas non-combustible substances that corrode, such as iron, contained very little. Air did not play a role in phlogiston theory, nor were any initial quantitative experiments conducted to test the idea; instead, it was based on observations of what happens when something burns, that most common objects appear to become lighter and seem to lose something in the process. The fact that a substance like wood gains overall weight in burning was hidden by the buoyancy of the gaseous combustion products. Indeed, one of the first clues that the phlogiston theory was incorrect was that metals, too, gain weight in rusting (when they were supposedly losing phlogiston). Trioxygen (O
3) is usually known as ozone and is a very reactive allotrope of oxygen that is damaging to lung tissue. Ozone is produced in the upper atmosphere when O
2 combines with atomic oxygen made by the splitting of O
2 by ultraviolet (UV) radiation. Since ozone absorbs strongly in the UV region of the spectrum, the ozone layer of the upper atmosphere functions as a protective radiation shield for the planet. Near the Earth's surface, it is a pollutant formed as a by-product of automobile exhaust. The metastable molecule tetraoxygen (O
4) was discovered in 2001, and was assumed to exist in one of the six phases of solid oxygen. It was proven in 2006 that this phase, created by pressurizing O
2 to 20 GPa, is in fact a rhombohedral O
8 cluster. This cluster has the potential to be a much more powerful oxidizer than either O
2 or O
3 and may therefore be used in rocket fuel. A metallic phase was discovered in 1990 when solid oxygen is subjected to a pressure of above 96 GPa and it was shown in 1998 that at very low temperatures, this phase becomes superconducting. Oxygen is the most abundant chemical element by mass in the Earth's biosphere, air, sea and land. Oxygen is the third most abundant chemical element in the universe, after hydrogen and helium. About 0.9% of the Sun's mass is oxygen. Oxygen constitutes 49.2% of the Earth's crust by mass and is the major component of the world's oceans (88.8% by mass). Oxygen gas is the second most common component of the Earth's atmosphere, taking up 20.8% of its volume and 23.1% of its mass (some 1015 tonnes).[d] Earth is unusual among the planets of the Solar System in having such a high concentration of oxygen gas in its atmosphere: Mars (with 0.1% O
2 by volume) and Venus have far lower concentrations. The O
2 surrounding these other planets is produced solely by ultraviolet radiation impacting oxygen-containing molecules such as carbon dioxide. Paleoclimatologists measure the ratio of oxygen-18 and oxygen-16 in the shells and skeletons of marine organisms to determine what the climate was like millions of years ago (see oxygen isotope ratio cycle). Seawater molecules that contain the lighter isotope, oxygen-16, evaporate at a slightly faster rate than water molecules containing the 12% heavier oxygen-18; this disparity increases at lower temperatures. During periods of lower global temperatures, snow and rain from that evaporated water tends to be higher in oxygen-16, and the seawater left behind tends to be higher in oxygen-18. Marine organisms then incorporate more oxygen-18 into their skeletons and shells than they would in a warmer climate. Paleoclimatologists also directly measure this ratio in the water molecules of ice core samples that are up to several hundreds of thousands of years old. Oxygen presents two spectrophotometric absorption bands peaking at the wavelengths 687 and 760 nm. Some remote sensing scientists have proposed using the measurement of the radiance coming from vegetation canopies in those bands to characterize plant health status from a satellite platform. This approach exploits the fact that in those bands it is possible to discriminate the vegetation's reflectance from its fluorescence, which is much weaker. The measurement is technically difficult owing to the low signal-to-noise ratio and the physical structure of vegetation; but it has been proposed as a possible method of monitoring the carbon cycle from satellites on a global scale. Breathing pure O
2 in space applications, such as in some modern space suits, or in early spacecraft such as Apollo, causes no damage due to the low total pressures used. In the case of spacesuits, the O
2 partial pressure in the breathing gas is, in general, about 30 kPa (1.4 times normal), and the resulting O
2 partial pressure in the astronaut's arterial blood is only marginally more than normal sea-level O
2 partial pressure (for more information on this, see space suit and arterial blood gas). The common allotrope of elemental oxygen on Earth is called dioxygen, O
2. It is the form that is a major part of the Earth's atmosphere (see Occurrence). O2 has a bond length of 121 pm and a bond energy of 498 kJ·mol−1, which is smaller than the energy of other double bonds or pairs of single bonds in the biosphere and responsible for the exothermic reaction of O2 with any organic molecule. Due to its energy content, O2 is used by complex forms of life, such as animals, in cellular respiration (see Biological role). Other aspects of O
2 are covered in the remainder of this article. Concentrated O
2 will allow combustion to proceed rapidly and energetically. Steel pipes and storage vessels used to store and transmit both gaseous and liquid oxygen will act as a fuel; and therefore the design and manufacture of O
2 systems requires special training to ensure that ignition sources are minimized. The fire that killed the Apollo 1 crew in a launch pad test spread so rapidly because the capsule was pressurized with pure O
2 but at slightly more than atmospheric pressure, instead of the 1⁄3 normal pressure that would be used in a mission.[k] Due to its electronegativity, oxygen forms chemical bonds with almost all other elements to give corresponding oxides. The surface of most metals, such as aluminium and titanium, are oxidized in the presence of air and become coated with a thin film of oxide that passivates the metal and slows further corrosion. Many oxides of the transition metals are non-stoichiometric compounds, with slightly less metal than the chemical formula would show. For example, the mineral FeO (wüstite) is written as Fe
1 − xO, where x is usually around 0.05. Oxygen is more soluble in water than nitrogen is. Water in equilibrium with air contains approximately 1 molecule of dissolved O
2 for every 2 molecules of N
2, compared to an atmospheric ratio of approximately 1:4. The solubility of oxygen in water is temperature-dependent, and about twice as much (14.6 mg·L−1) dissolves at 0 °C than at 20 °C (7.6 mg·L−1). At 25 °C and 1 standard atmosphere (101.3 kPa) of air, freshwater contains about 6.04 milliliters (mL) of oxygen per liter, whereas seawater contains about 4.95 mL per liter. At 5 °C the solubility increases to 9.0 mL (50% more than at 25 °C) per liter for water and 7.2 mL (45% more) per liter for sea water. Oxygen gas can also be produced through electrolysis of water into molecular oxygen and hydrogen. DC electricity must be used: if AC is used, the gases in each limb consist of hydrogen and oxygen in the explosive ratio 2:1. Contrary to popular belief, the 2:1 ratio observed in the DC electrolysis of acidified water does not prove that the empirical formula of water is H2O unless certain assumptions are made about the molecular formulae of hydrogen and oxygen themselves. A similar method is the electrocatalytic O
2 evolution from oxides and oxoacids. Chemical catalysts can be used as well, such as in chemical oxygen generators or oxygen candles that are used as part of the life-support equipment on submarines, and are still part of standard equipment on commercial airliners in case of depressurization emergencies. Another air separation technology involves forcing air to dissolve through ceramic membranes based on zirconium dioxide by either high pressure or an electric current, to produce nearly pure O
2 gas. One of the first known experiments on the relationship between combustion and air was conducted by the 2nd century BCE Greek writer on mechanics, Philo of Byzantium. In his work Pneumatica, Philo observed that inverting a vessel over a burning candle and surrounding the vessel's neck with water resulted in some water rising into the neck. Philo incorrectly surmised that parts of the air in the vessel were converted into the classical element fire and thus were able to escape through pores in the glass. Many centuries later Leonardo da Vinci built on Philo's work by observing that a portion of air is consumed during combustion and respiration. In one experiment, Lavoisier observed that there was no overall increase in weight when tin and air were heated in a closed container. He noted that air rushed in when he opened the container, which indicated that part of the trapped air had been consumed. He also noted that the tin had increased in weight and that increase was the same as the weight of the air that rushed back in. This and other experiments on combustion were documented in his book Sur la combustion en général, which was published in 1777. In that work, he proved that air is a mixture of two gases; 'vital air', which is essential to combustion and respiration, and azote (Gk. ἄζωτον ""lifeless""), which did not support either. Azote later became nitrogen in English, although it has kept the name in French and several other European languages. Planetary geologists have measured different abundances of oxygen isotopes in samples from the Earth, the Moon, Mars, and meteorites, but were long unable to obtain reference values for the isotope ratios in the Sun, believed to be the same as those of the primordial solar nebula. Analysis of a silicon wafer exposed to the solar wind in space and returned by the crashed Genesis spacecraft has shown that the Sun has a higher proportion of oxygen-16 than does the Earth. The measurement implies that an unknown process depleted oxygen-16 from the Sun's disk of protoplanetary material prior to the coalescence of dust grains that formed the Earth. By the late 19th century scientists realized that air could be liquefied, and its components isolated, by compressing and cooling it. Using a cascade method, Swiss chemist and physicist Raoul Pierre Pictet evaporated liquid sulfur dioxide in order to liquefy carbon dioxide, which in turn was evaporated to cool oxygen gas enough to liquefy it. He sent a telegram on December 22, 1877 to the French Academy of Sciences in Paris announcing his discovery of liquid oxygen. Just two days later, French physicist Louis Paul Cailletet announced his own method of liquefying molecular oxygen. Only a few drops of the liquid were produced in either case so no meaningful analysis could be conducted. Oxygen was liquified in stable state for the first time on March 29, 1883 by Polish scientists from Jagiellonian University, Zygmunt Wróblewski and Karol Olszewski. Highly concentrated sources of oxygen promote rapid combustion. Fire and explosion hazards exist when concentrated oxidants and fuels are brought into close proximity; an ignition event, such as heat or a spark, is needed to trigger combustion. Oxygen is the oxidant, not the fuel, but nevertheless the source of most of the chemical energy released in combustion. Combustion hazards also apply to compounds of oxygen with a high oxidative potential, such as peroxides, chlorates, nitrates, perchlorates, and dichromates because they can donate oxygen to a fire. Oxygen condenses at 90.20 K (−182.95 °C, −297.31 °F), and freezes at 54.36 K (−218.79 °C, −361.82 °F). Both liquid and solid O
2 are clear substances with a light sky-blue color caused by absorption in the red (in contrast with the blue color of the sky, which is due to Rayleigh scattering of blue light). High-purity liquid O
2 is usually obtained by the fractional distillation of liquefied air. Liquid oxygen may also be produced by condensation out of air, using liquid nitrogen as a coolant. It is a highly reactive substance and must be segregated from combustible materials."
Ministry_of_Defence_(United_Kingdom),"Additionally, there are a number of Assistant Chiefs of Defence Staff, including the Assistant Chief of the Defence Staff (Reserves and Cadets) and the Defence Services Secretary in the Royal Household of the Sovereign of the United Kingdom, who is also the Assistant Chief of Defence Staff (Personnel). The Ministry of Defence (MoD) is the British government department responsible for implementing the defence policy set by Her Majesty's Government, and is the headquarters of the British Armed Forces. The current Chief of the Defence Staff, the professional head of the British Armed Forces, is General Sir Nicholas Houghton, late Green Howards. He is supported by the Vice Chief of the Defence Staff, by the professional heads of the three services of HM Armed Forces and by the Commander of Joint Forces Command. There are also three Deputy Chiefs of the Defence Staff with particular remits, Deputy Chief of the Defence Staff (Capability), Deputy CDS (Personnel and Training) and Deputy CDS (Operations). The Surgeon General, represents the Defence Medical Services on the Defence Staff, and is the clinical head of that service. The MoD states that its principal objectives are to defend the United Kingdom of Great Britain and Northern Ireland and its interests and to strengthen international peace and stability. With the collapse of the Soviet Union and the end of the Cold War, the MoD does not foresee any short-term conventional military threat; rather, it has identified weapons of mass destruction, international terrorism, and failed and failing states as the overriding threats to Britain's interests. The MoD also manages day-to-day running of the armed forces, contingency planning and defence procurement. The Ministry of Defence is one of the United Kingdom's largest landowners, owning 227,300 hectares of land and foreshore (either freehold or leasehold) at April 2014, which was valued at ""about £20 billion"". The MoD also has ""rights of access"" to a further 222,000 hectares. In total, this is about 1.8% of the UK land mass. The total annual cost to support the defence estate is ""in excess of £3.3 billion"". In 2013 it was found that the Ministry of Defence had overspent on its equipment budget by £6.5bn on orders that could take up to 39 years to fulfil. The Ministry of Defence has been criticised in the past for poor management and financial control, investing in projects that have taken up to 10 and even as much as 15 years to be delivered. The Ministers and Chiefs of the Defence Staff are supported by a number of civilian, scientific and professional military advisors. The Permanent Under-Secretary of State for Defence (generally known as the Permanent Secretary) is the senior civil servant at the MoD. His or her role is to ensure the MoD operates effectively as a department of the government. The most notable fraud conviction was that of Gordon Foxley, head of defence procurement at the Ministry of Defence from 1981 to 1984. Police claimed he received at least £3.5m in total in corrupt payments, such as substantial bribes from overseas arms contractors aiming to influence the allocation of contracts. From 1946 to 1964 five Departments of State did the work of the modern Ministry of Defence: the Admiralty, the War Office, the Air Ministry, the Ministry of Aviation, and an earlier form of the Ministry of Defence. These departments merged in 1964; the defence functions of the Ministry of Aviation Supply merged into the Ministry of Defence in 1971. In April 2008, a £90m contract was signed with Boeing for a ""quick fix"" solution, so they can fly by 2010: QinetiQ will downgrade the Chinooks—stripping out some of their more advanced equipment. During the 1920s and 1930s, British civil servants and politicians, looking back at the performance of the state during World War I, concluded that there was a need for greater co-ordination between the three Services that made up the armed forces of the United Kingdom—the British Army, the Royal Navy, and the Royal Air Force. The formation of a united ministry of defence was rejected by David Lloyd George's coalition government in 1921; but the Chiefs of Staff Committee was formed in 1923, for the purposes of inter-Service co-ordination. As rearmament became a concern during the 1930s, Stanley Baldwin created the position of Minister for Coordination of Defence. Lord Chatfield held the post until the fall of Neville Chamberlain's government in 1940; his success was limited by his lack of control over the existing Service departments and his limited political influence. The Strategic Defence and Security Review 2015 included £178 billion investment in new equipment and capabilities. The review set a defence policy with four primary missions for the Armed Forces: Following the end of the Cold War, the threat of direct conventional military confrontation with other states has been replaced by terrorism. Sir Richard Dannatt predicted British forces to be involved in combating ""predatory non-state actors"" for the foreseeable future, in what he called an ""era of persistent conflict"". He told the Chatham House think tank that the fight against al-Qaeda and other militant Islamist groups was ""probably the fight of our generation"". Henry VIII's wine cellar at the Palace of Whitehall, built in 1514–1516 for Cardinal Wolsey, is in the basement of Main Building, and is used for entertainment. The entire vaulted brick structure of the cellar was encased in steel and concrete and relocated nine feet to the west and nearly 19 feet (5.8 m) deeper in 1949, when construction was resumed at the site after World War II. This was carried out without any significant damage to the structure. The defence estate is divided as training areas & ranges (84.0%), research & development (5.4%), airfields (3.4%), barracks & camps (2.5%), storage & supply depots (1.6%), and other (3.0%). These are largely managed by the Defence Infrastructure Organisation. Dannatt criticised a remnant ""Cold War mentality"", with military expenditures based on retaining a capability against a direct conventional strategic threat; He said currently only 10% of the MoD's equipment programme budget between 2003 and 2018 was to be invested in the ""land environment""—at a time when Britain was engaged in land-based wars in Afghanistan and Iraq. The MoD has been criticised for an ongoing fiasco, having spent £240m on eight Chinook HC3 helicopters which only started to enter service in 2010, years after they were ordered in 1995 and delivered in 2001. A National Audit Office report reveals that the helicopters have been stored in air conditioned hangars in Britain since their 2001[why?] delivery, while troops in Afghanistan have been forced to rely on helicopters which are flying with safety faults. By the time the Chinooks are airworthy, the total cost of the project could be as much as £500m. The headquarters of the MoD are in Whitehall and are now known as Main Building. This structure is neoclassical in style and was originally built between 1938 and 1959 to designs by Vincent Harris to house the Air Ministry and the Board of Trade. The northern entrance in Horse Guards Avenue is flanked by two monumental statues, Earth and Water, by Charles Wheeler. Opposite stands the Gurkha Monument, sculpted by Philip Jackson and unveiled in 1997 by Queen Elizabeth II. Within it is the Victoria Cross and George Cross Memorial, and nearby are memorials to the Fleet Air Arm and RAF (to its east, facing the riverside). A major refurbishment of the building was completed under a PFI contract by Skanska in 2004. A government report covered by the Guardian in 2002 indicates that between 1940 and 1979, the Ministry of Defence ""turned large parts of the country into a giant laboratory to conduct a series of secret germ warfare tests on the public"" and many of these tests ""involved releasing potentially dangerous chemicals and micro-organisms over vast swaths of the population without the public being told."" The Ministry of Defence claims that these trials were to simulate germ warfare and that the tests were harmless. Still, families who have been in the area of many of the tests are experiencing children with birth defects and physical and mental handicaps and many are asking for a public inquiry. According to the report these tests affected estimated millions of people including one period between 1961 and 1968 where ""more than a million people along the south coast of England, from Torquay to the New Forest, were exposed to bacteria including e.coli and bacillus globigii, which mimics anthrax."" Two scientists commissioned by the Ministry of Defence stated that these trials posed no risk to the public. This was confirmed by Sue Ellison, a representative of Porton Down who said that the results from these trials ""will save lives, should the country or our forces face an attack by chemical and biological weapons."" Asked whether such tests are still being carried out, she said: ""It is not our policy to discuss ongoing research."" It is unknown whether or not the harmlessness of the trials was known at the time of their occurrence. In October 2009, the MoD was heavily criticized for withdrawing the bi-annual non-operational training £20m budget for the volunteer Territorial Army (TA), ending all non-operational training for 6 months until April 2010. The government eventually backed down and restored the funding. The TA provides a small percentage of the UK's operational troops. Its members train on weekly evenings and monthly weekends, as well as two-week exercises generally annually and occasionally bi-annually for troops doing other courses. The cuts would have meant a significant loss of personnel and would have had adverse effects on recruitment. The 1998 Strategic Defence Review and the 2003 Delivering Security in a Changing World White Paper outlined the following posture for the British Armed Forces: The Defence Committee—Third Report ""Defence Equipment 2009"" cites an article from the Financial Times website stating that the Chief of Defence Materiel, General Sir Kevin O’Donoghue, had instructed staff within Defence Equipment and Support (DE&S) through an internal memorandum to reprioritize the approvals process to focus on supporting current operations over the next three years; deterrence related programmes; those that reflect defence obligations both contractual or international; and those where production contracts are already signed. The report also cites concerns over potential cuts in the defence science and technology research budget; implications of inappropriate estimation of Defence Inflation within budgetary processes; underfunding in the Equipment Programme; and a general concern over striking the appropriate balance over a short-term focus (Current Operations) and long-term consequences of failure to invest in the delivery of future UK defence capabilities on future combatants and campaigns. The then Secretary of State for Defence, Bob Ainsworth MP, reinforced this reprioritisation of focus on current operations and had not ruled out ""major shifts"" in defence spending. In the same article the First Sea Lord and Chief of the Naval Staff, Admiral Sir Mark Stanhope, Royal Navy, acknowledged that there was not enough money within the defence budget and it is preparing itself for tough decisions and the potential for cutbacks. According to figures published by the London Evening Standard the defence budget for 2009 is ""more than 10% overspent"" (figures cannot be verified) and the paper states that this had caused Gordon Brown to say that the defence spending must be cut. The MoD has been investing in IT to cut costs and improve services for its personnel. The MoD has since been regarded as a leader in elaborating the post-Cold War organising concept of ""defence diplomacy"". As a result of the Strategic Defence and Security Review 2010, Prime Minister David Cameron signed a 50-year treaty with French President Nicolas Sarkozy that would have the two countries co-operate intensively in military matters. The UK is establishing air and naval bases in the Persian Gulf, located in the UAE and Bahrain. A presence in Oman is also being considered. Winston Churchill, on forming his government in 1940, created the office of Minister of Defence to exercise ministerial control over the Chiefs of Staff Committee and to co-ordinate defence matters. The post was held by the Prime Minister of the day until Clement Attlee's government introduced the Ministry of Defence Act of 1946. The new ministry was headed by a Minister of Defence who possessed a seat in the Cabinet. The three existing service Ministers—the Secretary of State for War, the First Lord of the Admiralty, and the Secretary of State for Air—remained in direct operational control of their respective services, but ceased to attend Cabinet."
War_on_Terror,"In 2005, the UN Security Council adopted Resolution 1624 concerning incitement to commit acts of terrorism and the obligations of countries to comply with international human rights laws. Although both resolutions require mandatory annual reports on counter-terrorism activities by adopting nations, the United States and Israel have both declined to submit reports. In the same year, the United States Department of Defense and the Chairman of the Joint Chiefs of Staff issued a planning document, by the name ""National Military Strategic Plan for the War on Terrorism"", which stated that it constituted the ""comprehensive military plan to prosecute the Global War on Terror for the Armed Forces of the United States...including the findings and recommendations of the 9/11 Commission and a rigorous examination with the Department of Defense"". Because the actions involved in the ""war on terrorism"" are diffuse, and the criteria for inclusion are unclear, political theorist Richard Jackson has argued that ""the 'war on terrorism' therefore, is simultaneously a set of actual practices—wars, covert operations, agencies, and institutions—and an accompanying series of assumptions, beliefs, justifications, and narratives—it is an entire language or discourse."" Jackson cites among many examples a statement by John Ashcroft that ""the attacks of September 11 drew a bright line of demarcation between the civil and the savage"". Administration officials also described ""terrorists"" as hateful, treacherous, barbarous, mad, twisted, perverted, without faith, parasitical, inhuman, and, most commonly, evil. Americans, in contrast, were described as brave, loving, generous, strong, resourceful, heroic, and respectful of human rights. The USA PATRIOT Act of October 2001 dramatically reduces restrictions on law enforcement agencies' ability to search telephone, e-mail communications, medical, financial, and other records; eases restrictions on foreign intelligence gathering within the United States; expands the Secretary of the Treasury's authority to regulate financial transactions, particularly those involving foreign individuals and entities; and broadens the discretion of law enforcement and immigration authorities in detaining and deporting immigrants suspected of terrorism-related acts. The act also expanded the definition of terrorism to include domestic terrorism, thus enlarging the number of activities to which the USA PATRIOT Act's expanded law enforcement powers could be applied. A new Terrorist Finance Tracking Program monitored the movements of terrorists' financial resources (discontinued after being revealed by The New York Times). Global telecommunication usage, including those with no links to terrorism, is being collected and monitored through the NSA electronic surveillance program. The Patriot Act is still in effect. In a major split in the ranks of Al Qaeda's organization, the Iraqi franchise, known as Al Qaeda in Iraq covertly invaded Syria and the Levant and began participating in the ongoing Syrian Civil War, gaining enough support and strength to re-invade Iraq's western provinces under the name of the Islamic State of Iraq and the Levant (ISIS/ISIL), taking over much of the country in a blitzkrieg-like action and combining the Iraq insurgency and Syrian Civil War into a single conflict. Due to their extreme brutality and a complete change in their overall ideology, Al Qaeda's core organization in Central Asia eventually denounced ISIS and directed their affiliates to cut off all ties with this organization. Many analysts[who?] believe that because of this schism, Al Qaeda and ISIL are now in a competition to retain the title of the world's most powerful terrorist organization. On 14 September 2009, U.S. Special Forces killed two men and wounded and captured two others near the Somali village of Baarawe. Witnesses claim that helicopters used for the operation launched from French-flagged warships, but that could not be confirmed. A Somali-based al-Qaida affiliated group, the Al-Shabaab, has confirmed the death of ""sheik commander"" Saleh Ali Saleh Nabhan along with an unspecified number of militants. Nabhan, a Kenyan, was wanted in connection with the 2002 Mombasa attacks. U.S. President Barack Obama has rarely used the term, but in his inaugural address on 20 January 2009, he stated ""Our nation is at war, against a far-reaching network of violence and hatred."" In March 2009 the Defense Department officially changed the name of operations from ""Global War on Terror"" to ""Overseas Contingency Operation"" (OCO). In March 2009, the Obama administration requested that Pentagon staff members avoid use of the term, instead using ""Overseas Contingency Operation"". Basic objectives of the Bush administration ""war on terror"", such as targeting al Qaeda and building international counterterrorism alliances, remain in place. In December 2012, Jeh Johnson, the General Counsel of the Department of Defense, stated that the military fight will be replaced by a law enforcement operation when speaking at Oxford University, predicting that al Qaeda will be so weakened to be ineffective, and has been ""effectively destroyed"", and thus the conflict will not be an armed conflict under international law. In May 2013, Obama stated that the goal is ""to dismantle specific networks of violent extremists that threaten America""; which coincided with the U.S. Office of Management and Budget having changed the wording from ""Overseas Contingency Operations"" to ""Countering Violent Extremism"" in 2010. In September 2009, a U.S. Drone strike reportedly killed Ilyas Kashmiri, who was the chief of Harkat-ul-Jihad al-Islami, a Kashmiri militant group associated with Al-Qaeda. Kashmiri was described by Bruce Riedel as a 'prominent' Al-Qaeda member, while others described him as the head of military operations for Al-Qaeda. Waziristan had now become the new battlefield for Kashmiri militants, who were now fighting NATO in support of Al-Qaeda. On 8 July 2012, Al-Badar Mujahideen, a breakaway faction of Kashmir centric terror group Hizbul Mujahideen, on conclusion of their two-day Shuhada Conference called for mobilisation of resources for continuation of jihad in Kashmir. The first ground attack came at the Battle of Umm Qasr on 21 March 2003 when a combined force of British, American and Polish forces seized control of the port city of Umm Qasr. Baghdad, Iraq's capital city, fell to American forces in April 2003 and Saddam Hussein's government quickly dissolved. On 1 May 2003, Bush announced that major combat operations in Iraq had ended. However, an insurgency arose against the U.S.-led coalition and the newly developing Iraqi military and post-Saddam government. The insurgency, which included al-Qaeda affiliated groups, led to far more coalition casualties than the invasion. Other elements of the insurgency were led by fugitive members of President Hussein's Ba'ath regime, which included Iraqi nationalists and pan-Arabists. Many insurgency leaders are Islamists and claim to be fighting a religious war to reestablish the Islamic Caliphate of centuries past. Iraq's former president, Saddam Hussein was captured by U.S. forces in December 2003. He was executed in 2006. The British 16th Air Assault Brigade (later reinforced by Royal Marines) formed the core of the force in southern Afghanistan, along with troops and helicopters from Australia, Canada and the Netherlands. The initial force consisted of roughly 3,300 British, 2,000 Canadian, 1,400 from the Netherlands and 240 from Australia, along with special forces from Denmark and Estonia and small contingents from other nations. The monthly supply of cargo containers through Pakistani route to ISAF in Afghanistan is over 4,000 costing around 12 billion in Pakistani Rupees. The Obama administration began to reengage in Iraq with a series of airstrikes aimed at ISIS beginning on 10 August 2014. On 9 September 2014 President Obama said that he had the authority he needed to take action to destroy the militant group known as the Islamic State of Iraq and the Levant, citing the 2001 Authorization for Use of Military Force Against Terrorists, and thus did not require additional approval from Congress. The following day on 10 September 2014 President Barack Obama made a televised speech about ISIL, which he stated ""Our objective is clear: We will degrade, and ultimately destroy, ISIL through a comprehensive and sustained counter-terrorism strategy"". Obama has authorized the deployment of additional U.S. Forces into Iraq, as well as authorizing direct military operations against ISIL within Syria. On the night of 21/22 September the United States, Saudi Arabia, Bahrain, the UAE, Jordan and Qatar started air attacks against ISIS in Syria.[citation needed] The origins of al-Qaeda can be traced to the Soviet war in Afghanistan (December 1979 – February 1989). The United States, United Kingdom, Saudi Arabia, Pakistan, and the People's Republic of China supported the Islamist Afghan mujahadeen guerillas against the military forces of the Soviet Union and the Democratic Republic of Afghanistan. A small number of ""Afghan Arab"" volunteers joined the fight against the Soviets, including Osama bin Laden, but there is no evidence they received any external assistance. In May 1996 the group World Islamic Front for Jihad Against Jews and Crusaders (WIFJAJC), sponsored by bin Laden (and later re-formed as al-Qaeda), started forming a large base of operations in Afghanistan, where the Islamist extremist regime of the Taliban had seized power earlier in the year. In February 1998, Osama bin Laden signed a fatwā, as head of al-Qaeda, declaring war on the West and Israel, later in May of that same year al-Qaeda released a video declaring war on the U.S. and the West. Political interest groups have stated that these laws remove important restrictions on governmental authority, and are a dangerous encroachment on civil liberties, possible unconstitutional violations of the Fourth Amendment. On 30 July 2003, the American Civil Liberties Union (ACLU) filed the first legal challenge against Section 215 of the Patriot Act, claiming that it allows the FBI to violate a citizen's First Amendment rights, Fourth Amendment rights, and right to due process, by granting the government the right to search a person's business, bookstore, and library records in a terrorist investigation, without disclosing to the individual that records were being searched. Also, governing bodies in a number of communities have passed symbolic resolutions against the act. Following the ceasefire agreement that suspended hostilities (but not officially ended) in the 1991 Gulf War, the United States and its allies instituted and began patrolling Iraqi no-fly zones, to protect Iraq's Kurdish and Shi'a Arab population—both of which suffered attacks from the Hussein regime before and after the Gulf War—in Iraq's northern and southern regions, respectively. U.S. forces continued in combat zone deployments through November 1995 and launched Operation Desert Fox against Iraq in 1998 after it failed to meet U.S. demands of ""unconditional cooperation"" in weapons inspections. On the morning of 11 September 2001, 19 men affiliated with al-Qaeda hijacked four airliners all bound for California. Once the hijackers assumed control of the airliners, they told the passengers that they had the bomb on board and would spare the lives of passengers and crew once their demands were met – no passenger and crew actually suspected that they would use the airliners as suicide weapons since it had never happened before in history. The hijackers – members of al-Qaeda's Hamburg cell – intentionally crashed two airliners into the Twin Towers of the World Trade Center in New York City. Both buildings collapsed within two hours from fire damage related to the crashes, destroying nearby buildings and damaging others. The hijackers crashed a third airliner into the Pentagon in Arlington County, Virginia, just outside Washington D.C. The fourth plane crashed into a field near Shanksville, Pennsylvania, after some of its passengers and flight crew attempted to retake control of the plane, which the hijackers had redirected toward Washington D.C., to target the White House, or the U.S. Capitol. No flights had survivors. A total of 2,977 victims and the 19 hijackers perished in the attacks. On 12 January 2002, Musharraf gave a speech against Islamic extremism. He unequivocally condemned all acts of terrorism and pledged to combat Islamic extremism and lawlessness within Pakistan itself. He stated that his government was committed to rooting out extremism and made it clear that the banned militant organizations would not be allowed to resurface under any new name. He said, ""the recent decision to ban extremist groups promoting militancy was taken in the national interest after thorough consultations. It was not taken under any foreign influence"". Criticism of the War on Terror addresses the issues, morality, efficiency, economics, and other questions surrounding the War on Terror and made against the phrase itself, calling it a misnomer. The notion of a ""war"" against ""terrorism"" has proven highly contentious, with critics charging that it has been exploited by participating governments to pursue long-standing policy/military objectives, reduce civil liberties, and infringe upon human rights. It is argued that the term war is not appropriate in this context (as in War on Drugs), since there is no identifiable enemy, and that it is unlikely international terrorism can be brought to an end by military means. In the following months, NATO took a wide range of measures to respond to the threat of terrorism. On 22 November 2002, the member states of the Euro-Atlantic Partnership Council (EAPC) decided on a Partnership Action Plan against Terrorism, which explicitly states, ""EAPC States are committed to the protection and promotion of fundamental freedoms and human rights, as well as the rule of law, in combating terrorism."" NATO started naval operations in the Mediterranean Sea designed to prevent the movement of terrorists or weapons of mass destruction as well as to enhance the security of shipping in general called Operation Active Endeavour. Following the 11 September 2001 attacks, former President of Pakistan Pervez Musharraf sided with the U.S. against the Taliban government in Afghanistan after an ultimatum by then U.S. President George W. Bush. Musharraf agreed to give the U.S. the use of three airbases for Operation Enduring Freedom. United States Secretary of State Colin Powell and other U.S. administration officials met with Musharraf. On 19 September 2001, Musharraf addressed the people of Pakistan and stated that, while he opposed military tactics against the Taliban, Pakistan risked being endangered by an alliance of India and the U.S. if it did not cooperate. In 2006, Musharraf testified that this stance was pressured by threats from the U.S., and revealed in his memoirs that he had ""war-gamed"" the United States as an adversary and decided that it would end in a loss for Pakistan. Support for the U.S. cooled when America made clear its determination to invade Iraq in late 2002. Even so, many of the ""coalition of the willing"" countries that unconditionally supported the U.S.-led military action have sent troops to Afghanistan, particular neighboring Pakistan, which has disowned its earlier support for the Taliban and contributed tens of thousands of soldiers to the conflict. Pakistan was also engaged in the War in North-West Pakistan (Waziristan War). Supported by U.S. intelligence, Pakistan was attempting to remove the Taliban insurgency and al-Qaeda element from the northern tribal areas. The Taliban regrouped in western Pakistan and began to unleash an insurgent-style offensive against Coalition forces in late 2002. Throughout southern and eastern Afghanistan, firefights broke out between the surging Taliban and Coalition forces. Coalition forces responded with a series of military offensives and an increase in the amount of troops in Afghanistan. In February 2010, Coalition forces launched Operation Moshtarak in southern Afghanistan along with other military offensives in the hopes that they would destroy the Taliban insurgency once and for all. Peace talks are also underway between Taliban affiliated fighters and Coalition forces. In September 2014, Afghanistan and the United States signed a security agreement, which permits United States and NATO forces to remain in Afghanistan until at least 2024. The United States and other NATO and non-NATO forces are planning to withdraw; with the Taliban claiming it has defeated the United States and NATO, and the Obama Administration viewing it as a victory. In December 2014, ISAF encasing its colors, and Resolute Support began as the NATO operation in Afghanistan. Continued United States operations within Afghanistan will continue under the name ""Operation Freedom's Sentinel"". Subsequently, in October 2001, U.S. forces (with UK and coalition allies) invaded Afghanistan to oust the Taliban regime. On 7 October 2001, the official invasion began with British and U.S. forces conducting airstrike campaigns over enemy targets. Kabul, the capital city of Afghanistan, fell by mid-November. The remaining al-Qaeda and Taliban remnants fell back to the rugged mountains of eastern Afghanistan, mainly Tora Bora. In December, Coalition forces (the U.S. and its allies) fought within that region. It is believed that Osama bin Laden escaped into Pakistan during the battle. On 7 August 1998, al-Qaeda struck the U.S. embassies in Kenya and Tanzania, killing 224 people, including 12 Americans. In retaliation, U.S. President Bill Clinton launched Operation Infinite Reach, a bombing campaign in Sudan and Afghanistan against targets the U.S. asserted were associated with WIFJAJC, although others have questioned whether a pharmaceutical plant in Sudan was used as a chemical warfare plant. The plant produced much of the region's antimalarial drugs and around 50% of Sudan's pharmaceutical needs. The strikes failed to kill any leaders of WIFJAJC or the Taliban. Other critics, such as Francis Fukuyama, note that ""terrorism"" is not an enemy, but a tactic; calling it a ""war on terror"", obscures differences between conflicts such as anti-occupation insurgents and international mujahideen. With a military presence in Iraq and Afghanistan and its associated collateral damage Shirley Williams maintains this increases resentment and terrorist threats against the West. There is also perceived U.S. hypocrisy, media-induced hysteria, and that differences in foreign and security policy have damaged America's image in most of the world. On 16 September 2001, at Camp David, President George W. Bush used the phrase war on terrorism in an unscripted and controversial comment when he said, ""This crusade – this war on terrorism – is going to take a while, ... "" Bush later apologized for this remark due to the negative connotations the term crusade has to people, e.g. of Muslim faith. The word crusade was not used again. On 20 September 2001, during a televised address to a joint session of congress, Bush stated that, ""(o)ur 'war on terror' begins with al-Qaeda, but it does not end there. It will not end until every terrorist group of global reach has been found, stopped, and defeated."" In a 'Letter to American People' written by Osama bin Laden in 2002, he stated that one of the reasons he was fighting America is because of its support of India on the Kashmir issue. While on a trip to Delhi in 2002, U.S. Secretary of Defense Donald Rumsfeld suggested that Al-Qaeda was active in Kashmir, though he did not have any hard evidence. An investigation in 2002 unearthed evidence that Al-Qaeda and its affiliates were prospering in Pakistan-administered Kashmir with tacit approval of Pakistan's National Intelligence agency Inter-Services Intelligence. A team of Special Air Service and Delta Force was sent into Indian-administered Kashmir in 2002 to hunt for Osama bin Laden after reports that he was being sheltered by the Kashmiri militant group Harkat-ul-Mujahideen. U.S. officials believed that Al-Qaeda was helping organize a campaign of terror in Kashmir in order to provoke conflict between India and Pakistan. Fazlur Rehman Khalil, the leader of the Harkat-ul-Mujahideen, signed al-Qaeda's 1998 declaration of holy war, which called on Muslims to attack all Americans and their allies. Indian sources claimed that In 2006, Al-Qaeda claimed they had established a wing in Kashmir; this worried the Indian government. India also claimed that Al-Qaeda has strong ties with the Kashmir militant groups Lashkar-e-Taiba and Jaish-e-Mohammed in Pakistan. While on a visit to Pakistan in January 2010, U.S. Defense secretary Robert Gates stated that Al-Qaeda was seeking to destabilize the region and planning to provoke a nuclear war between India and Pakistan. The Authorization for Use of Military Force Against Terrorists or ""AUMF"" was made law on 14 September 2001, to authorize the use of United States Armed Forces against those responsible for the attacks on 11 September 2001. It authorized the President to use all necessary and appropriate force against those nations, organizations, or persons he determines planned, authorized, committed, or aided the terrorist attacks that occurred on 11 September 2001, or harbored such organizations or persons, in order to prevent any future acts of international terrorism against the United States by such nations, organizations or persons. Congress declares this is intended to constitute specific statutory authorization within the meaning of section 5(b) of the War Powers Resolution of 1973. In addition to military efforts abroad, in the aftermath of 9/11 the Bush Administration increased domestic efforts to prevent future attacks. Various government bureaucracies that handled security and military functions were reorganized. A new cabinet-level agency called the United States Department of Homeland Security was created in November 2002 to lead and coordinate the largest reorganization of the U.S. federal government since the consolidation of the armed forces into the Department of Defense.[citation needed] In January 2002, the United States Special Operations Command, Pacific deployed to the Philippines to advise and assist the Armed Forces of the Philippines in combating Filipino Islamist groups. The operations were mainly focused on removing the Abu Sayyaf group and Jemaah Islamiyah (JI) from their stronghold on the island of Basilan. The second portion of the operation was conducted as a humanitarian program called ""Operation Smiles"". The goal of the program was to provide medical care and services to the region of Basilan as part of a ""Hearts and Minds"" program. Joint Special Operations Task Force – Philippines disbanded in June 2014, ending a 14-year mission. After JSOTF-P disbanded, as late as November 2014, American forces continued to operate in the Philippines under the name ""PACOM Augmentation Team"". The use of drones by the Central Intelligence Agency in Pakistan to carry out operations associated with the Global War on Terror sparks debate over sovereignty and the laws of war. The U.S. Government uses the CIA rather than the U.S. Air Force for strikes in Pakistan in order to avoid breaching sovereignty through military invasion. The United States was criticized by[according to whom?] a report on drone warfare and aerial sovereignty for abusing the term 'Global War on Terror' to carry out military operations through government agencies without formally declaring war. The conflict in northern Mali began in January 2012 with radical Islamists (affiliated to al-Qaeda) advancing into northern Mali. The Malian government had a hard time maintaining full control over their country. The fledgling government requested support from the international community on combating the Islamic militants. In January 2013, France intervened on behalf of the Malian government's request and deployed troops into the region. They launched Operation Serval on 11 January 2013, with the hopes of dislodging the al-Qaeda affiliated groups from northern Mali. In 2002, the Musharraf-led government took a firm stand against the jihadi organizations and groups promoting extremism, and arrested Maulana Masood Azhar, head of the Jaish-e-Mohammed, and Hafiz Muhammad Saeed, chief of the Lashkar-e-Taiba, and took dozens of activists into custody. An official ban was imposed on the groups on 12 January. Later that year, the Saudi born Zayn al-Abidn Muhammed Hasayn Abu Zubaydah was arrested by Pakistani officials during a series of joint U.S.-Pakistan raids. Zubaydah is said to have been a high-ranking al-Qaeda official with the title of operations chief and in charge of running al-Qaeda training camps. Other prominent al-Qaeda members were arrested in the following two years, namely Ramzi bin al-Shibh, who is known to have been a financial backer of al-Qaeda operations, and Khalid Sheikh Mohammed, who at the time of his capture was the third highest-ranking official in al-Qaeda and had been directly in charge of the planning for the 11 September attacks."
Chinese_characters,"One of the most complex characters found in modern Chinese dictionaries[g] is 齉 (U+9F49) (nàng,  listen (help·info), pictured below, middle image), meaning ""snuffle"" (that is, a pronunciation marred by a blocked nose), with ""just"" thirty-six strokes. However, this is not in common use. The most complex character that can be input using the Microsoft New Phonetic IME 2002a for traditional Chinese is 龘 (dá, ""the appearance of a dragon flying""). It is composed of the dragon radical represented three times, for a total of 16 × 3 = 48 strokes. Among the most complex characters in modern dictionaries and also in frequent modern use are 籲 (yù, ""to implore""), with 32 strokes; 鬱 (yù, ""luxuriant, lush; gloomy""), with 29 strokes, as in 憂鬱 (yōuyù, ""depressed""); 豔 (yàn, ""colorful""), with 28 strokes; and 釁 (xìn, ""quarrel""), with 25 strokes, as in 挑釁 (tiǎoxìn, ""to pick a fight""). Also in occasional modern use is 鱻 (xiān ""fresh""; variant of 鮮 xiān) with 33 strokes. After Kim Jong Il, the second ruler of North Korea, died in December 2011, Kim Jong Un stepped up and began mandating the use of Hanja as a source of definition for the Korean language. Currently, it is said that North Korea teaches around 3,000 Hanja characters to North Korean students, and in some cases, the characters appear within advertisements and newspapers. However, it is also said that the authorities implore students not to use the characters in public. Due to North Korea's strict isolationism, accurate reports about hanja use in North Korea are hard to obtain. In the Republic of China (Taiwan), which uses traditional Chinese characters, the Ministry of Education's Chángyòng Guózì Biāozhǔn Zìtǐ Biǎo (常用國字標準字體表, Chart of Standard Forms of Common National Characters) lists 4,808 characters; the Cì Chángyòng Guózì Biāozhǔn Zìtǐ Biǎo (次常用國字標準字體表, Chart of Standard Forms of Less-Than-Common National Characters) lists another 6,341 characters. The Chinese Standard Interchange Code (CNS11643)—the official national encoding standard—supports 48,027 characters, while the most widely used encoding scheme, BIG-5, supports only 13,053. In addition, there are a number of dialect characters (方言字) that are not used in formal written Chinese but represent colloquial terms in non-Mandarin varieties of Chinese. One such variety is Written Cantonese, in widespread use in Hong Kong even for certain formal documents, due to the former British colonial administration's recognition of Cantonese for use for official purposes. In Taiwan, there is also an informal body of characters used to represent Hokkien Chinese. Many varieties have specific characters for words exclusive to them. For example, the vernacular character 㓾, pronounced cii11 in Hakka, means ""to kill"". Furthermore, Shanghainese and Sichuanese also have their own series of written text, but these are not widely used in actual texts, Mandarin being the preference for all mainland regions. Modern examples particularly include Chinese characters for SI units. In Chinese these units are disyllabic and standardly written with two characters, as 厘米 límǐ ""centimeter"" (厘 centi-, 米 meter) or 千瓦 qiānwǎ ""kilowatt"". However, in the 19th century these were often written via compound characters, pronounced disyllabically, such as 瓩 for 千瓦 or 糎 for 厘米 – some of these characters were also used in Japan, where they were pronounced with borrowed European readings instead. These have now fallen out of general use, but are occasionally seen. Less systematic examples include 圕 túshūguǎn ""library"", a contraction of 圖書館, A four-morpheme word, 社会主义 shèhuì zhǔyì ""socialism"", is commonly written with a single character formed by combining the last character, 义, with the radical of the first, 社, yielding roughly 礻义. Examples are 河 hé ""river"", 湖 hú ""lake"", 流 liú ""stream"", 沖 chōng ""riptide"" (or ""flush""), 滑 huá ""slippery"". All these characters have on the left a radical of three short strokes (氵), which is a reduced form of the character 水 shuǐ meaning ""water"", indicating that the character has a semantic connection with water. The right-hand side in each case is a phonetic indicator. For example, in the case of 沖 chōng (Old Chinese *ɡ-ljuŋ), the phonetic indicator is 中 zhōng (Old Chinese *k-ljuŋ), which by itself means ""middle"". In this case it can be seen that the pronunciation of the character is slightly different from that of its phonetic indicator; the process of historical phonetic change means that the composition of such characters can sometimes seem arbitrary today. For instance, to look up the character where the sound is not known, e.g., 松 (pine tree), the user first determines which part of the character is the radical (here 木), then counts the number of strokes in the radical (four), and turns to the radical index (usually located on the inside front or back cover of the dictionary). Under the number ""4"" for radical stroke count, the user locates 木, then turns to the page number listed, which is the start of the listing of all the characters containing this radical. This page will have a sub-index giving remainder stroke numbers (for the non-radical portions of characters) and page numbers. The right half of the character also contains four strokes, so the user locates the number 4, and turns to the page number given. From there, the user must scan the entries to locate the character he or she is seeking. Some dictionaries have a sub-index which lists every character containing each radical, and if the user knows the number of strokes in the non-radical portion of the character, he or she can locate the correct page directly. Most modern Chinese dictionaries and Chinese dictionaries sold to English speakers use the traditional radical-based character index in a section at the front, while the main body of the dictionary arranges the main character entries alphabetically according to their pinyin spelling. To find a character with unknown sound using one of these dictionaries, the reader finds the radical and stroke number of the character, as before, and locates the character in the radical index. The character's entry will have the character's pronunciation in pinyin written down; the reader then turns to the main dictionary section and looks up the pinyin spelling alphabetically. In addition to strictness in character size and shape, Chinese characters are written with very precise rules. The most important rules regard the strokes employed, stroke placement, and stroke order. Just as each region that uses Chinese characters has standardized character forms, each also has standardized stroke orders, with each standard being different. Most characters can be written with just one correct stroke order, though some words also have many valid stroke orders, which may occasionally result in different stroke counts. Some characters are also written with different stroke orders due to character simplification. According to the Rev. John Gulick: ""The inhabitants of other Asiatic nations, who have had occasion to represent the words of their several languages by Chinese characters, have as a rule used unaspirated characters for the sounds, g, d, b. The Muslims from Arabia and Persia have followed this method … The Mongols, Manchu, and Japanese also constantly select unaspirated characters to represent the sounds g, d, b, and j of their languages. These surrounding Asiatic nations, in writing Chinese words in their own alphabets, have uniformly used g, d, b, & c., to represent the unaspirated sounds."" Written Japanese also includes a pair of syllabaries known as kana, derived by simplifying Chinese characters selected to represent syllables of Japanese. The syllabaries differ because they sometimes selected different characters for a syllable, and because they used different strategies to reduce these characters for easy writing: the angular katakana were obtained by selecting a part of each character, while hiragana were derived from the cursive forms of whole characters. Modern Japanese writing uses a composite system, using kanji for word stems, hiragana for inflexional endings and grammatical words, and katakana to transcribe non-Chinese loanwords as well as serve as a method to emphasize native words (similar to how italics are used in Romance languages). In the years after World War II, the Japanese government also instituted a series of orthographic reforms. Some characters were given simplified forms called shinjitai 新字体 (lit. ""new character forms"", the older forms were then labelled the kyūjitai 旧字体, lit. ""old character forms""). The number of characters in common use was restricted, and formal lists of characters to be learned during each grade of school were established, first the 1850-character tōyō kanji 当用漢字 list in 1945, the 1945-character jōyō kanji 常用漢字 list in 1981, and a 2136-character reformed version of the jōyō kanji in 2010. Many variant forms of characters and obscure alternatives for common characters were officially discouraged. This was done with the goal of facilitating learning for children and simplifying kanji use in literature and periodicals. These are simply guidelines, hence many characters outside these standards are still widely known and commonly used, especially those used for personal and place names (for the latter, see jinmeiyō kanji),[citation needed] as well as for some common words such as ""dragon"" (Japanese kana: たつ, Rōmaji: tatsu) in which both the shinjitai 竜 and the kyūjitai 龍 forms of the kanji are both acceptable and widely known amongst native Japanese speakers. The majority of simplified characters are drawn from conventional abbreviated forms, or ancient standard forms. For example, the orthodox character 來 lái (""come"") was written with the structure 来 in the clerical script (隶书 / 隸書, lìshū) of the Han dynasty. This clerical form uses one fewer stroke, and was thus adopted as a simplified form. The character 雲 yún (""cloud"") was written with the structure 云 in the oracle bone script of the Shang dynasty, and had remained in use later as a phonetic loan in the meaning of ""to say"" while the 雨 radical was added to differentiate meanings. The simplified form adopts the original structure. Chinese character dictionaries often allow users to locate entries in several ways. Many Chinese, Japanese, and Korean dictionaries of Chinese characters list characters in radical order: characters are grouped together by radical, and radicals containing fewer strokes come before radicals containing more strokes (radical-and-stroke sorting). Under each radical, characters are listed by their total number of strokes. It is often also possible to search for characters by sound, using pinyin (in Chinese dictionaries), zhuyin (in Taiwanese dictionaries), kana (in Japanese dictionaries) or hangul (in Korean dictionaries). Most dictionaries also allow searches by total number of strokes, and individual dictionaries often allow other search methods as well. While new characters can be easily coined by writing on paper, they are difficult to represent on a computer – they must generally be represented as a picture, rather than as text – which presents a significant barrier to their use or widespread adoption. Compare this with the use of symbols as names in 20th century musical albums such as Led Zeppelin IV (1971) and Love Symbol Album (1993); an album cover may potentially contain any graphics, but in writing and other computation these symbols are difficult to use. Semantic-phonetic compounds or pictophonetic compounds are by far the most numerous characters. These characters are composed of two parts: one of a limited set of characters (the semantic indicator, often graphically simplified) which suggests the general meaning of the compound character, and another character (the phonetic indicator) whose pronunciation suggests the pronunciation of the compound character. In most cases the semantic indicator is also the radical under which the character is listed in dictionaries. Chinese characters represent words of the language using several strategies. A few characters, including some of the most commonly used, were originally pictograms, which depicted the objects denoted, or simple ideograms, in which meaning was expressed iconically. Some other words were expressed by compound ideograms, but the vast majority were written using the rebus principle, in which a character for a similarly sounding word was either simply borrowed or (more commonly) extended with a disambiguating semantic marker to form a phono-semantic compound character. Regular script has been attributed to Zhong Yao, of the Eastern Han to Cao Wei period (c. 151–230 AD), who has been called the ""father of regular script"". However, some scholars postulate that one person alone could not have developed a new script which was universally adopted, but could only have been a contributor to its gradual formation. The earliest surviving pieces written in regular script are copies of Yao's works, including at least one copied by Wang Xizhi. This new script, which is the dominant modern Chinese script, developed out of a neatly written form of early semi-cursive, with addition of the pause (頓/顿 dùn) technique to end horizontal strokes, plus heavy tails on strokes which are written to the downward-right diagonal. Thus, early regular script emerged from a neat, formal form of semi-cursive, which had itself emerged from neo-clerical (a simplified, convenient form of clerical script). It then matured further in the Eastern Jin dynasty in the hands of the ""Sage of Calligraphy"", Wang Xizhi, and his son Wang Xianzhi. It was not, however, in widespread use at that time, and most writers continued using neo-clerical, or a somewhat semi-cursive form of it, for daily writing, while the conservative bafen clerical script remained in use on some stelae, alongside some semi-cursive, but primarily neo-clerical. New characters can in principle be coined at any time, just as new words can be, but they may not be adopted. Significant historically recent coinages date to scientific terms of the 19th century. Specifically, Chinese coined new characters for chemical elements – see chemical elements in East Asian languages – which continue to be used and taught in schools in China and Taiwan. In Japan, in the Meiji era (specifically, late 19th century), new characters were coined for some (but not all) SI units, such as 粁 (米 ""meter"" + 千 ""thousand, kilo-"") for kilometer. These kokuji (Japanese-coinages) have found use in China as well – see Chinese characters for SI units for details. The Shang dynasty oracle bone script and the Zhou dynasty scripts found on Chinese bronze inscriptions are no longer used; the oldest script that is still in use today is the Seal Script (篆書(书), zhuànshū). It evolved organically out of the Spring and Autumn period Zhou script, and was adopted in a standardized form under the first Emperor of China, Qin Shi Huang. The seal script, as the name suggests, is now used only in artistic seals. Few people are still able to read it effortlessly today, although the art of carving a traditional seal in the script remains alive; some calligraphers also work in this style. There is a clear trend toward the exclusive use of hangul in day-to-day South Korean society. Hanja are still used to some extent, particularly in newspapers, weddings, place names and calligraphy (although it is nowhere near the extent of kanji use in day-to-day Japanese society). Hanja is also extensively used in situations where ambiguity must be avoided,[citation needed] such as academic papers, high-level corporate reports, government documents, and newspapers; this is due to the large number of homonyms that have resulted from extensive borrowing of Chinese words. It was not until the Northern and Southern dynasties that regular script rose to dominant status. During that period, regular script continued evolving stylistically, reaching full maturity in the early Tang dynasty. Some call the writing of the early Tang calligrapher Ouyang Xun (557–641) the first mature regular script. After this point, although developments in the art of calligraphy and in character simplification still lay ahead, there were no more major stages of evolution for the mainstream script. The art of writing Chinese characters is called Chinese calligraphy. It is usually done with ink brushes. In ancient China, Chinese calligraphy is one of the Four Arts of the Chinese Scholars. There is a minimalist set of rules of Chinese calligraphy. Every character from the Chinese scripts is built into a uniform shape by means of assigning it a geometric area in which the character must occur. Each character has a set number of brushstrokes; none must be added or taken away from the character to enhance it visually, lest the meaning be lost. Finally, strict regularity is not required, meaning the strokes may be accentuated for dramatic effect of individual style. Calligraphy was the means by which scholars could mark their thoughts and teachings for immortality, and as such, represent some of the more precious treasures that can be found from ancient China. Contrary to the popular belief of there being only one script per period, there were in fact multiple scripts in use during the Han period. Although mature clerical script, also called 八分 (bāfēn) script, was dominant at that time, an early type of cursive script was also in use by the Han by at least as early as 24 BC (during the very late Western Han period),[b] incorporating cursive forms popular at the time, well as many elements from the vulgar writing of the Warring State of Qin. By around the time of the Eastern Jin dynasty, this Han cursive became known as 章草 zhāngcǎo (also known as 隶草 / 隸草 lìcǎo today), or in English sometimes clerical cursive, ancient cursive, or draft cursive. Some believe that the name, based on 章 zhāng meaning ""orderly"", arose because the script was a more orderly form of cursive than the modern form, which emerged during the Eastern Jin dynasty and is still in use today, called 今草 jīncǎo or ""modern cursive"". Although most of the simplified Chinese characters in use today are the result of the works moderated by the government of the People's Republic of China in the 1950s and 60s, character simplification predates the republic's formation in 1949. One of the earliest proponents of character simplification was Lufei Kui, who proposed in 1909 that simplified characters should be used in education. In the years following the May Fourth Movement in 1919, many anti-imperialist Chinese intellectuals sought ways to modernise China. In the 1930s and 1940s, discussions on character simplification took place within the Kuomintang government, and many Chinese intellectuals and writers have long maintained that character simplification would help boost literacy in China. In many world languages, literacy has been promoted as a justification for spelling reforms. The People's Republic of China issued its first round of official character simplifications in two documents, the first in 1956 and the second in 1964. In the 1950s and 1960s, while confusion about simplified characters was still rampant, transitional characters that mixed simplified parts with yet-to-be simplified parts of characters together appeared briefly, then disappeared. One man who has encountered this problem is Taiwanese politician Yu Shyi-kun, due to the rarity of the last character in his name. Newspapers have dealt with this problem in varying ways, including using software to combine two existing, similar characters, including a picture of the personality, or, especially as is the case with Yu Shyi-kun, simply substituting a homophone for the rare character in the hope that the reader would be able to make the correct inference. Taiwanese political posters, movie posters etc. will often add the bopomofo phonetic symbols next to such a character. Japanese newspapers may render such names and words in katakana instead of kanji, and it is accepted practice for people to write names for which they are unsure of the correct kanji in katakana instead. When learning how to write hanja, students are taught to memorize the native Korean pronunciation for the hanja's meaning and the Sino-Korean pronunciations (the pronunciation based on the Chinese pronunciation of the characters) for each hanja respectively so that students know what the syllable and meaning is for a particular hanja. For example, the name for the hanja 水 is 물 수 (mul-su) in which 물 (mul) is the native Korean pronunciation for ""water"", while 수 (su) is the Sino-Korean pronunciation of the character. The naming of hanja is similar to if ""water"" were named ""water-aqua"", ""horse-equus"", or ""gold-aurum"" based on a hybridization of both the English and the Latin names. Other examples include 사람 인 (saram-in) for 人 ""person/people"", 큰 대 (keun-dae) for 大 ""big/large//great"", 작을 소 (jakeul-so) for 小 ""small/little"", 아래 하 (arae-ha) for 下 ""underneath/below/low"", 아비 부 (abi-bu) for 父 ""father"", and 나라이름 한 (naraimreum-han) for 韓 ""Han/Korea"". The use of such contractions is as old as Chinese characters themselves, and they have frequently been found in religious or ritual use. In the Oracle Bone script, personal names, ritual items, and even phrases such as 受又(祐) shòu yòu ""receive blessings"" are commonly contracted into single characters. A dramatic example is that in medieval manuscripts 菩薩 púsà ""bodhisattva"" (simplified: 菩萨) is sometimes written with a single character formed of a 2×2 grid of four 十 (derived from the grass radical over two 十). However, for the sake of consistency and standardization, the CPC seeks to limit the use of such polysyllabic characters in public writing to ensure that every character only has one syllable. Chinese characters are primarily morphosyllabic, meaning that most Chinese morphemes are monosyllabic and are written with a single character, though in modern Chinese most words are disyllabic and dimorphemic, consisting of two syllables, each of which is a morpheme. In modern Chinese 10% of morphemes only occur as part of a given compound. However, a few morphemes are disyllabic, some of them dating back to Classical Chinese. Excluding foreign loan words, these are typically words for plants and small animals. They are usually written with a pair of phono-semantic compound characters sharing a common radical. Examples are 蝴蝶 húdié ""butterfly"" and 珊瑚 shānhú ""coral"". Note that the 蝴 hú of húdié and the 瑚 hú of shānhú have the same phonetic, 胡, but different radicals (""insect"" and ""jade"", respectively). Neither exists as an independent morpheme except as a poetic abbreviation of the disyllabic word. A commonly seen example is the double happiness symbol 囍, formed as a ligature of 喜喜 and referred to by its disyllabic name (simplified Chinese: 双喜; traditional Chinese: 雙喜; pinyin: shuāngxǐ). In handwriting, numbers are very frequently squeezed into one space or combined – common ligatures include 廿 niàn, ""twenty"", normally read as 二十 èrshí, 卅 sà, ""thirty"", normally read as 三十 sānshí, and 卌 xì ""forty"", normally read as 四十 ""sìshí"". In some cases counters are also merged into one character, such as 七十人 qīshí rén ""seventy people"". Another common abbreviation is 门 with a ""T"" written inside it, for 問題, 问题, wèntí (""question; problem""), where the ""T"" is from pinyin for the second syllable tí 题. Since polysyllabic characters are often non-standard, they are often excluded incharcter dictionaries. Chinese characters are logograms used in the writing of Chinese and some other Asian languages. In Standard Chinese they are called Hanzi (simplified Chinese: 汉字; traditional Chinese: 漢字). They have been adapted to write a number of other languages including: Japanese, where they are known as kanji, Korean, where they are known as hanja, and Vietnamese in a system known as chữ Nôm. Collectively, they are known as CJKV characters. In English, they are sometimes called Han characters. Chinese characters constitute the oldest continuously used system of writing in the world. By virtue of their widespread current use in East Asia, and historic use throughout the Sinosphere, Chinese characters are among the most widely adopted writing systems in the world. In Old Chinese, (e.g. Classical Chinese) most words were monosyllabic and there was a close correspondence between characters and words. In modern Chinese (esp. Mandarin Chinese), characters do not necessarily correspond to words; indeed the majority of Chinese words today consist of two or more characters due to the merging and loss of sounds in the Chinese language over time. Rather, a character almost always corresponds to a single syllable that is also a morpheme. However, there are a few exceptions to this general correspondence, including bisyllabic morphemes (written with two characters), bimorphemic syllables (written with two characters) and cases where a single character represents a polysyllabic word or phrase. The following is a comparison of Chinese characters in the Standard Form of National Characters, a common traditional Chinese standard used in Taiwan, the Table of General Standard Chinese Characters, the standard for Mainland Chinese simplified Chinese characters, and the jōyō kanji, the standard for Japanese kanji. Generally, the jōyō kanji are more similar to traditional Chinese characters than simplified Chinese characters are to traditional Chinese characters. ""Simplified"" refers to having significant differences from the Taiwan standard, not necessarily being a newly created character or a newly performed substitution. The characters in the Hong Kong standard and the Kangxi Dictionary are also known as ""Traditional,"" but are not shown. The earliest confirmed evidence of the Chinese script yet discovered is the body of inscriptions on oracle bones from the late Shang dynasty (c. 1200–1050 BC). These symbols, carved on pieces of bone and turtle shell being sold as ""dragon bones"" for medicinal purposes, were identified as Chinese writing by scholars in 1899. By 1928, the source of the oracle bones had been traced to a village near Anyang in Henan Province, which was excavated by the Academia Sinica between 1928 and 1937. Over 150,000 fragments have been found. Although Chinese characters in Vietnam are now limited to ceremonial uses, they were once in widespread use. Until the early 20th century, Literary Chinese was used in Vietnam for all official and scholarly writing. Around the 13th century the Nôm script was developed to record folk literature in the Vietnamese language. The script used Chinese characters to represent both borrowed Sino-Vietnamese vocabulary and native words with similar pronunciation or meaning. In addition thousands of new compound characters were created to write Vietnamese words. This process resulted in a highly complex system that was never mastered by more than 5% of the population. Both Literary Chinese and Nôm were replaced in the early 20th century by Vietnamese written with the Latin-based Vietnamese alphabet. Seal script, which had evolved slowly in the state of Qin during the Eastern Zhou dynasty, became standardized and adopted as the formal script for all of China in the Qin dynasty (leading to a popular misconception that it was invented at that time), and was still widely used for decorative engraving and seals (name chops, or signets) in the Han dynasty period. However, despite the Qin script standardization, more than one script remained in use at the time. For example, a little-known, rectilinear and roughly executed kind of common (vulgar) writing had for centuries coexisted with the more formal seal script in the Qin state, and the popularity of this vulgar writing grew as the use of writing itself became more widespread. By the Warring States period, an immature form of clerical script called ""early clerical"" or ""proto-clerical"" had already developed in the state of Qin based upon this vulgar writing, and with influence from seal script as well. The coexistence of the three scripts – small seal, vulgar and proto-clerical, with the latter evolving gradually in the Qin to early Han dynasties into clerical script – runs counter to the traditional belief that the Qin dynasty had one script only, and that clerical script was suddenly invented in the early Han dynasty from the small seal script. The traditional picture of an orderly series of scripts, each one invented suddenly and then completely displacing the previous one, has been conclusively demonstrated to be fiction by the archaeological finds and scholarly research of the later 20th and early 21st centuries. Gradual evolution and the coexistence of two or more scripts was more often the case. As early as the Shang dynasty, oracle-bone script coexisted as a simplified form alongside the normal script of bamboo books (preserved in typical bronze inscriptions), as well as the extra-elaborate pictorial forms (often clan emblems) found on many bronzes. By the late Eastern Han period, an early form of semi-cursive script appeared, developing out of a cursively written form of neo-clerical script[c] and simple cursive. This semi-cursive script was traditionally attributed to Liu Desheng c. 147–188 AD,[d] although such attributions refer to early masters of a script rather than to their actual inventors, since the scripts generally evolved into being over time. Qiu gives examples of early semi-cursive script, showing that it had popular origins rather than being purely Liu’s invention. The cursive script (草書(书), cǎoshū, literally ""grass script"") is used informally. The basic character shapes are suggested, rather than explicitly realized, and the abbreviations are sometimes extreme. Despite being cursive to the point where individual strokes are no longer differentiable and the characters often illegible to the untrained eye, this script (also known as draft) is highly revered for the beauty and freedom that it embodies. Some of the simplified Chinese characters adopted by the People's Republic of China, and some simplified characters used in Japan, are derived from the cursive script. The Japanese hiragana script is also derived from this script. In times past, until the 15th century, in Korea, Literary Chinese was the dominant form of written communication, prior to the creation of hangul, the Korean alphabet. Much of the vocabulary, especially in the realms of science and sociology, comes directly from Chinese, comparable to Latin or Greek root words in European languages. However, due to the lack of tones in Korean,[citation needed] as the words were imported from Chinese, many dissimilar characters took on identical sounds, and subsequently identical spelling in hangul.[citation needed] Chinese characters are sometimes used to this day for either clarification in a practical manner, or to give a distinguished appearance, as knowledge of Chinese characters is considered a high class attribute and an indispensable part of a classical education.[citation needed] It is also observed that the preference for Chinese characters is treated as being conservative and Confucian. In recent decades, a series of inscribed graphs and pictures have been found at Neolithic sites in China, including Jiahu (c. 6500 BC), Dadiwan and Damaidi from the 6th millennium BC, and Banpo (5th millennium BC). Often these finds are accompanied by media reports that push back the purported beginnings of Chinese writing by thousands of years. However, because these marks occur singly, without any implied context, and are made crudely and simply, Qiu Xigui concluded that ""we do not have any basis for stating that these constituted writing nor is there reason to conclude that they were ancestral to Shang dynasty Chinese characters."" They do however demonstrate a history of sign use in the Yellow River valley during the Neolithic through to the Shang period. Just as Roman letters have a characteristic shape (lower-case letters mostly occupying the x-height, with ascenders or descenders on some letters), Chinese characters occupy a more or less square area in which the components of every character are written to fit in order to maintain a uniform size and shape, especially with small printed characters in Ming and sans-serif styles. Because of this, beginners often practise writing on squared graph paper, and the Chinese sometimes use the term ""Square-Block Characters"" (方块字 / 方塊字, fāngkuàizì), sometimes translated as tetragraph, in reference to Chinese characters. The use of traditional Chinese characters versus simplified Chinese characters varies greatly, and can depend on both the local customs and the medium. Before the official reform, character simplifications were not officially sanctioned and generally adopted vulgar variants and idiosyncratic substitutions. Orthodox variants were mandatory in printed works, while the (unofficial) simplified characters would be used in everyday writing or quick notes. Since the 1950s, and especially with the publication of the 1964 list, the People's Republic of China has officially adopted simplified Chinese characters for use in mainland China, while Hong Kong, Macau, and the Republic of China (Taiwan) were not affected by the reform. There is no absolute rule for using either system, and often it is determined by what the target audience understands, as well as the upbringing of the writer. Based on studies of these bronze inscriptions, it is clear that, from the Shang dynasty writing to that of the Western Zhou and early Eastern Zhou, the mainstream script evolved in a slow, unbroken fashion, until assuming the form that is now known as seal script in the late Eastern Zhou in the state of Qin, without any clear line of division. Meanwhile, other scripts had evolved, especially in the eastern and southern areas during the late Zhou dynasty, including regional forms, such as the gǔwén (""ancient forms"") of the eastern Warring States preserved as variant forms in the Han dynasty character dictionary Shuowen Jiezi, as well as decorative forms such as bird and insect scripts. Occasionally a bisyllabic word is written with two characters that contain the same radical, as in 蝴蝶 húdié ""butterfly"", where both characters have the insect radical 虫. A notable example is pipa (a Chinese lute, also a fruit, the loquat, of similar shape) – originally written as 批把 with the hand radical, referring to the down and up strokes when playing this instrument, which was then changed to 枇杷 (tree radical), which is still used for the fruit, while the character was changed to 琵琶 when referring to the instrument. In other cases a compound word may coincidentally share a radical without this being meaningful. Even the Zhonghua Zihai does not include characters in the Chinese family of scripts created to represent non-Chinese languages. Characters formed by Chinese principles in other languages include the roughly 1,500 Japanese-made kokuji given in the Kokuji no Jiten, the Korean-made gukja, the over 10,000 Sawndip characters still in use in Guangxi, and the almost 20,000 Nôm characters formerly used in Vietnam.[citation needed] More divergent descendents of Chinese script include Tangut script, which created over 5,000 characters with similar strokes but different formation principles to Chinese characters. Modern Chinese has many homophones; thus the same spoken syllable may be represented by many characters, depending on meaning. A single character may also have a range of meanings, or sometimes quite distinct meanings; occasionally these correspond to different pronunciations. Cognates in the several varieties of Chinese are generally written with the same character. They typically have similar meanings, but often quite different pronunciations. In other languages, most significantly today in Japanese and sometimes in Korean, characters are used to represent Chinese loanwords, to represent native words independent of the Chinese pronunciation, and as purely phonetic elements based on their pronunciation in the historical variety of Chinese from which they were acquired. These foreign adaptations of Chinese pronunciation are known as Sino-Xenic pronunciations, and have been useful in the reconstruction of Middle Chinese. In certain cases compound words and set phrases may be contracted into single characters. Some of these can be considered logograms, where characters represent whole words rather than syllable-morphemes, though these are generally instead considered ligatures or abbreviations (similar to scribal abbreviations, such as & for ""et""), and as non-standard. These do see use, particularly in handwriting or decoration, but also in some cases in print. In Chinese, these ligatures are called héwén (合文), héshū (合書) or hétǐzì (合体字), and in the special case of combining two characters, these are known as ""two-syllable Chinese characters"" (双音节汉字, 雙音節漢字). Chinese characters number in the tens of thousands, though most of them are minor graphic variants encountered only in historical texts. Studies in China have shown that functional literacy in written Chinese requires a knowledge of between three and four thousand characters. In Japan, 2,136 are taught through secondary school (the Jōyō kanji); hundreds more are in everyday use. There are various national standard lists of characters, forms, and pronunciations. Simplified forms of certain characters are used in China, Singapore, and Malaysia; the corresponding traditional characters are used in Taiwan, Hong Kong, Macau, and to a limited extent in South Korea. In Japan, common characters are written in post-WWII Japan-specific simplified forms (shinjitai), which are closer to traditional forms than Chinese simplifications, while uncommon characters are written in Japanese traditional forms (kyūjitai), which are virtually identical to Chinese traditional forms. In South Korea, when Chinese characters are used they are of the traditional variant and are almost identical to those used in places like Taiwan and Hong Kong. Teaching of Chinese characters in South Korea starts in the 7th grade and continues until the 12th grade where 1,800 total characters are taught albeit these characters are only used in certain cases (on signs, academic papers, historical writings, etc.) and are slowly declining in use. The total number of Chinese characters from past to present remains unknowable because new ones are developed all the time – for instance, brands may create new characters when none of the existing ones allow for the intended meaning. Chinese characters are theoretically an open set and anyone can create new characters, though such inventions are rarely included in official character sets. The number of entries in major Chinese dictionaries is the best means of estimating the historical growth of character inventory. Modified radicals and new variants are two common reasons for the ever-increasing number of characters. There are about 300 radicals and 100 are in common use. Creating a new character by modifying the radical is an easy way to disambiguate homographs among xíngshēngzì pictophonetic compounds. This practice began long before the standardization of Chinese script by Qin Shi Huang and continues to the present day. The traditional 3rd-person pronoun tā (他 ""he, she, it""), which is written with the ""person radical"", illustrates modifying significs to form new characters. In modern usage, there is a graphic distinction between tā (她 ""she"") with the ""woman radical"", tā (牠 ""it"") with the ""animal radical"", tā (它 ""it"") with the ""roof radical"", and tā (祂 ""He"") with the ""deity radical"", One consequence of modifying radicals is the fossilization of rare and obscure variant logographs, some of which are not even used in Classical Chinese. For instance, he 和 ""harmony, peace"", which combines the ""grain radical"" with the ""mouth radical"", has infrequent variants 咊 with the radicals reversed and 龢 with the ""flute radical"". The People's Republic of China issued its first round of official character simplifications in two documents, the first in 1956 and the second in 1964. A second round of character simplifications (known as erjian, or ""second round simplified characters"") was promulgated in 1977. It was poorly received, and in 1986 the authorities rescinded the second round completely, while making six revisions to the 1964 list, including the restoration of three traditional characters that had been simplified: 叠 dié, 覆 fù, 像 xiàng. In China, which uses simplified Chinese characters, the Xiàndài Hànyǔ Chángyòng Zìbiǎo (现代汉语常用字表, Chart of Common Characters of Modern Chinese) lists 2,500 common characters and 1,000 less-than-common characters, while the Xiàndài Hànyǔ Tōngyòng Zìbiǎo (现代汉语通用字表, Chart of Generally Utilized Characters of Modern Chinese) lists 7,000 characters, including the 3,500 characters already listed above. GB2312, an early version of the national encoding standard used in the People's Republic of China, has 6,763 code points. GB18030, the modern, mandatory standard, has a much higher number. The New Hànyǔ Shuǐpíng Kǎoshì (汉语水平考试, Chinese Proficiency Test) covers approximately 2,600 characters at its highest level (level six). Regular script typefaces are also commonly used, but not as common as Ming or sans-serif typefaces for body text. Regular script typefaces are often used to teach students Chinese characters, and often aim to match the standard forms of the region where they are meant to be used. Most typefaces in the Song dynasty were regular script typefaces which resembled a particular person's handwriting (e.g. the handwriting of Ouyang Xun, Yan Zhenqing, or Liu Gongquan), while most modern regular script typefaces tend toward anonymity and regularity. Although most often associated with the People's Republic of China, character simplification predates the 1949 communist victory. Caoshu, cursive written text, almost always includes character simplification, and simplified forms have always existed in print, albeit not for the most formal works. In the 1930s and 1940s, discussions on character simplification took place within the Kuomintang government, and a large number of Chinese intellectuals and writers have long maintained that character simplification would help boost literacy in China. Indeed, this desire by the Kuomintang to simplify the Chinese writing system (inherited and implemented by the Communist Party of China) also nursed aspirations of some for the adoption of a phonetic script based on the Latin script, and spawned such inventions as the Gwoyeu Romatzyh. There are also some extremely complex characters which have understandably become rather rare. According to Joël Bellassen (1989), the most complex Chinese character is /𪚥 (U+2A6A5) zhé  listen (help·info), meaning ""verbose"" and containing sixty-four strokes; this character fell from use around the 5th century. It might be argued, however, that while containing the most strokes, it is not necessarily the most complex character (in terms of difficulty), as it simply requires writing the same sixteen-stroke character 龍 lóng (lit. ""dragon"") four times in the space for one. Another 64-stroke character is /𠔻 (U+2053B) zhèng composed of 興 xīng/xìng (lit. ""flourish"") four times."
"Tucson,_Arizona","Interstate 10, which runs southeast to northwest through town, connects Tucson to Phoenix to the northwest on the way to its western terminus in Santa Monica, California, and to Las Cruces, New Mexico and El Paso, Texas toward its eastern terminus in Jacksonville, Florida. I-19 runs south from Tucson toward Nogales and the U.S.-Mexico border. I-19 is the only Interstate highway that uses ""kilometer posts"" instead of ""mileposts"", although the speed limits are marked in miles per hour instead of kilometers per hour. On Sentinel Peak (also known as ""'A' Mountain""), just west of downtown, there is a giant ""A"" in honor of the University of Arizona. Starting in about 1916, a yearly tradition developed for freshmen to whitewash the ""A"", which was visible for miles. However, at the beginning of the Iraq War, anti-war activists painted it black. This was followed by a paint scuffle where the ""A"" was painted various colors until the city council intervened. It is now red, white and blue except when it is white or another color decided by a biennial election. Because of the three-color paint scheme often used, the shape of the A can be vague and indistinguishable from the rest of the peak. The top of Sentinel Peak, which is accessible by road, offers an outstanding scenic view of the city looking eastward. A parking lot located near the summit of Sentinel Peak was formerly a popular place to watch sunsets or view the city lights at night. South Tucson is actually the name of an independent, incorporated town of 1 sq mi (2.6 km2), completely surrounded by the city of Tucson, sitting just south of downtown. South Tucson has a colorful, dynamic history. It was first incorporated in 1936, and later reincorporated in 1940. The population consists of about 83% Mexican-American and 10% Native American residents. South Tucson is widely known for its many Mexican restaurants and the architectural styles which include bright outdoor murals, many of which have been painted over due to city policy. At the end of the first decade of the 21st century, downtown Tucson underwent a revitalization effort by city planners and the business community. The primary project was Rio Nuevo, a large retail and community center that has been stalled in planning for more than ten years. Downtown is generally regarded as the area bordered by 17th Street to the south, I-10 to the west, and 6th Street to the north, and Toole Avenue and the Union Pacific (formerly Southern Pacific) railroad tracks, site of the historic train depot and ""Locomotive #1673"", built in 1900. Downtown is divided into the Presidio District, the Barrio Viejo, and the Congress Street Arts and Entertainment District. Some authorities include the 4th Avenue shopping district, which is set just northeast of the rest of downtown and connected by an underpass beneath the UPRR tracks. Tucson was probably first visited by Paleo-Indians, known to have been in southern Arizona about 12,000 years ago. Recent archaeological excavations near the Santa Cruz River have located a village site dating from 2100 BC.[citation needed] The floodplain of the Santa Cruz River was extensively farmed during the Early Agricultural period, circa 1200 BC to AD 150. These people constructed irrigation canals and grew corn, beans, and other crops while gathering wild plants and hunting. The Early Ceramic period occupation of Tucson saw the first extensive use of pottery vessels for cooking and storage. The groups designated as the Hohokam lived in the area from AD 600 to 1450 and are known for their vast irrigation canal systems and their red-on-brown pottery.[citation needed] East Tucson is relatively new compared to other parts of the city, developed between the 1950s and the 1970s,[citation needed] with developments such as Desert Palms Park. It is generally classified as the area of the city east of Swan Road, with above-average real estate values relative to the rest of the city. The area includes urban and suburban development near the Rincon Mountains. East Tucson includes Saguaro National Park East. Tucson's ""Restaurant Row"" is also located on the east side, along with a significant corporate and financial presence. Restaurant Row is sandwiched by three of Tucson's storied Neighborhoods: Harold Bell Wright Estates, named after the famous author's ranch which occupied some of that area prior to the depression; the Tucson Country Club (the third to bear the name Tucson Country Club), and the Dorado Country Club. Tucson's largest office building is 5151 East Broadway in east Tucson, completed in 1975. The first phases of Williams Centre, a mixed-use, master-planned development on Broadway near Craycroft Road, were opened in 1987. Park Place, a recently renovated shopping center, is also located along Broadway (west of Wilmot Road). Since 2009, the Tucson Festival of Books has been held annually over a two-day period in March at the University of Arizona. By 2010 it had become the fourth largest book festival in the United States, with 450 authors and 80,000 attendees. In addition to readings and lectures, it features a science fair, varied entertainment, food, and exhibitors ranging from local retailers and publishers to regional and national nonprofit organizations. In 2011, the Festival began presenting a Founder's Award; recipients include Elmore Leonard and R.L. Stine. The Tucson metro area is served by many local television stations and is the 68th largest designated market area (DMA) in the U.S. with 433,310 homes (0.39% of the total U.S.). It is limited to the three counties of southeastern Arizona (Pima, Santa Cruz, and Cochise) The major television networks serving Tucson are: KVOA 4 (NBC), KGUN 9 (ABC), KMSB-TV 11 (Fox), KOLD-TV 13 (CBS), KTTU 18 (My Network TV) and KWBA 58 (The CW). KUAT-TV 6 is a PBS affiliate run by the University of Arizona (as is sister station KUAS 27). Tucson's Sun Tran bus system serves greater Tucson with standard, express, regional shuttle, and on-demand shuttle bus service. It was awarded Best Transit System in 1988 and 2005. A 3.9-mile streetcar line, Sun Link, connects the University of Arizona campus with 4th Avenue, downtown, and the Mercado District west of Interstate 10 and the Santa Cruz River. Ten-minute headway passenger service began July 25, 2014. The streetcar utilizes Sun Tran's card payment and transfer system, connecting with the University of Arizona's CatTran shuttles, Amtrak, and Greyhound intercity bus service. Near the intersection of Craycroft and Ft. Lowell Roads are the remnants of the Historic Fort Lowell. This area has become one of Tucson's iconic neighborhoods. In 1891, the Fort was abandoned and much of the interior was stripped of their useful components and it quickly fell into ruin. In 1900, three of the officer buildings were purchased for use as a sanitarium. The sanitarium was then sold to Harvey Adkins in 1928. The Bolsius family Pete, Nan and Charles Bolsius purchased and renovated surviving adobe buildings of the Fort – transforming them into spectacular artistic southwestern architectural examples. Their woodwork, plaster treatment and sense of proportion drew on their Dutch heritage and New Mexican experience. Other artists and academics throughout the middle of the 20th century, including: Win Ellis, Jack Maul, Madame Cheruy, Giorgio Belloli, Charels Bode, Veronica Hughart, Edward and Rosamond Spicer, Hazel Larson Archer and Ruth Brown, renovated adobes, built homes and lived in the area. The artist colony attracted writers and poets including beat generation Alan Harrington and Jack Kerouac whose visit is documented in his iconic book On the Road. This rural pocket in the middle of the city is listed on the National Register of Historic Places. Each year in February the neighborhood celebrates its history in the City Landmark it owns and restored the San Pedro Chapel. To prevent further loss of groundwater, Tucson has been involved in water conservation and groundwater preservation efforts, shifting away from its reliance on a series of Tucson area wells in favor of conservation, consumption-based pricing for residential and commercial water use, and new wells in the more sustainable Avra Valley aquifer, northwest of the city. An allocation from the Central Arizona Project Aqueduct (CAP), which passes more than 300 mi (480 km) across the desert from the Colorado River, has been incorporated into the city's water supply, annually providing over 20 million gallons of ""recharged"" water which is pumped into the ground to replenish water pumped out. Since 2001, CAP water has allowed the city to remove or turn off over 80 wells. A combination of urban and suburban development, the West Side is generally defined as the area west of I-10. Western Tucson encompasses the banks of the Santa Cruz River and the foothills of the Tucson Mountains, and includes the International Wildlife Museum, Sentinel Peak, and the Marriott Starr Pass Resort & Spa, located in the wealthy enclave known as Starr Pass. Moving past the Tucson Mountains, travelers find themselves in the area commonly referred to as ""west of"" Tucson or ""Old West Tucson"". A large undulating plain extending south into the Altar Valley, rural residential development predominates, but here you will also find major attractions including Saguaro National Park West, the Arizona-Sonora Desert Museum, and the Old Tucson Studios movie set/theme park. Another popular event held in February, which is early spring in Tucson, is the Fiesta de los Vaqueros, or rodeo week, founded by winter visitor, Leighton Kramer. While at its heart the Fiesta is a sporting event, it includes what is billed as ""the world's largest non-mechanized parade"". The Rodeo Parade is a popular event as most schools give two rodeo days off instead of Presidents Day. The exception to this is Presidio High (a non-public charter school), which doesn't get either. Western wear is seen throughout the city as corporate dress codes are cast aside during the Fiesta. The Fiesta de los Vaqueros marks the beginning of the rodeo season in the United States. The University of Arizona Wildcats sports teams, most notably the men's basketball and women's softball teams have strong local interest. The men's basketball team, formerly coached by Hall of Fame head coach Lute Olson and currently coached by Sean Miller, has made 25 straight NCAA Tournaments and won the 1997 National Championship. Arizona's Softball team has reached the NCAA National Championship game 12 times and has won 8 times, most recently in 2007. The university's swim teams have gained international recognition, with swimmers coming from as far as Japan and Africa to train with the coach Frank Busch who has also worked with the U.S. Olympic swim team for a number of years. Both men and women's swim teams recently[when?] won the NCAA National Championships. At the airport, where records have been kept since 1930, the record maximum temperature was 117 °F (47 °C) on June 26, 1990, and the record minimum temperature was 16 °F (−9 °C) on January 4, 1949. There is an average of 145.0 days annually with highs of 90 °F (32 °C) or higher and an average of 16.9 days with lows reaching or below the freezing mark. Measurable precipitation falls on an average of 53 days. The wettest year was 1983 with 21.86 in (555 mm) of precipitation, and the driest year was 1953 with 5.34 in (136 mm). The most rainfall in one month was 7.93 in (201 mm) in August 1955. The most rainfall in 24 hours was 3.93 in (100 mm) on July 29, 1958. Snow at the airport averages only 1.1 in (2.8 cm) annually. The most snow received in one year was 8.3 in (21 cm) and the most snow in one month was 6.8 in (17 cm) in December 1971. Much of Tucson's economic development has been centered on the development of the University of Arizona, which is currently the second largest employer in the city. Davis-Monthan Air Force Base, located on the southeastern edge of the city, also provides many jobs for Tucson residents. Its presence, as well as the presence of the US Army Intelligence Center (Fort Huachuca, the largest employer in the region in nearby Sierra Vista), has led to the development of a significant number of high-tech industries, including government contractors, in the area. The city of Tucson is also a major hub for the Union Pacific Railroad's Sunset Route that links the Los Angeles ports with the South/Southeast regions of the country. Tucson has one daily newspaper, the morning Arizona Daily Star. Wick Communications publishes the daily legal paper The Daily Territorial, while Boulder, Colo.-based 10/13 Communications publishes Tucson Weekly (an ""alternative"" publication), Inside Tucson Business and the Explorer. TucsonSentinel.com is a nonprofit independent online news organization. Tucson Lifestyle Magazine, Lovin' Life News, DesertLeaf, and Zócalo Magazine are monthly publications covering arts, architecture, decor, fashion, entertainment, business, history, and other events. The Arizona Daily Wildcat is the University of Arizona's student newspaper, and the Aztec News is the Pima Community College student newspaper. The New Vision is the newspaper for the Roman Catholic Diocese of Tucson, and the Arizona Jewish Post is the newspaper of the Jewish Federation of Southern Arizona. The monsoon can begin any time from mid-June to late July, with an average start date around July 3. It typically continues through August and sometimes into September. During the monsoon, the humidity is much higher than the rest of the year. It begins with clouds building up from the south in the early afternoon followed by intense thunderstorms and rainfall, which can cause flash floods. The evening sky at this time of year is often pierced with dramatic lightning strikes. Large areas of the city do not have storm sewers, so monsoon rains flood the main thoroughfares, usually for no longer than a few hours. A few underpasses in Tucson have ""feet of water"" scales painted on their supports to discourage fording by automobiles during a rainstorm. Arizona traffic code Title 28-910, the so-called ""Stupid Motorist Law"", was instituted in 1995 to discourage people from entering flooded roadways. If the road is flooded and a barricade is in place, motorists who drive around the barricade can be charged up to $2000 for costs involved in rescuing them. Despite all warnings and precautions, however, three Tucson drivers have drowned between 2004 and 2010. Tucson is known for being a trailblazer in voluntary partial publicly financed campaigns. Since 1985, both mayoral and council candidates have been eligible to receive matching public funds from the city. To become eligible, council candidates must receive 200 donations of $10 or more (300 for a mayoral candidate). Candidates must then agree to spending limits equal to 33¢ for every registered Tucson voter, or $79,222 in 2005 (the corresponding figures for mayor are 64¢ per registered voter, or $142,271 in 2003). In return, candidates receive matching funds from the city at a 1:1 ratio of public money to private donations. The only other limitation is that candidates may not exceed 75% of the limit by the date of the primary. Many cities, such as San Francisco and New York City, have copied this system, albeit with more complex spending and matching formulas. Tucson's largest park, Reid Park, is located in midtown and includes Reid Park Zoo and Hi Corbett Field. Speedway Boulevard, a major east-west arterial road in central Tucson, was named the ""ugliest street in America"" by Life magazine in the early 1970s, quoting Tucson Mayor James Corbett. Despite this, Speedway Boulevard was awarded ""Street of the Year"" by Arizona Highways in the late 1990s. According to David Leighton, historical writer for the Arizona Daily Star newspaper, Speedway Boulevard derives its name from an old horse racetrack, known as ""The Harlem River Speedway,"" more commonly called ""The Speedway,"" in New York City. The street was called ""The Speedway,"" from 1904 to about 1906 before the word ""The"" was taken out. Catalina Highway stretches 25 miles (40 km) and the entire mountain range is one of Tucson's most popular vacation spots for cycling, hiking, rock climbing, camping, birding, and wintertime snowboarding and skiing. Near the top of Mt. Lemmon is the town of Summerhaven. In Summerhaven, visitors will find log houses and cabins, a general store, and various shops, as well as numerous hiking trails. Near Summerhaven is the road to Ski Valley which hosts a ski lift, several runs, a giftshop, and nearby restaurant. Tucson's primary electrical power source is a coal and natural gas power-plant managed by Tucson Electric Power that is situated within the city limits on the south-western boundary of Davis-Monthan Air-force base adjacent to Interstate-10. The air pollution generated has raised some concerns as the Sundt operating station has been online since 1962 as is exempt from many pollution standards and controls due to its age. Solar has been gaining ground in Tucson with its ideal over 300 days of sunshine climate. Federal, state, and even local utility credits and incentives have also enticed residents to equip homes with solar systems. Davis-Monthan AFB has a 3.3 Megawatt (MW) ground-mounted solar photovoltaic (PV) array and a 2.7 MW rooftop-mounted PV array, both of which are located in the Base Housing area. The base will soon have the largest solar-generating capacity in the United States Department of Defense after awarding a contract on September 10, 2010, to SunEdison to construct a 14.5 MW PV field on the northwestern side of the base. Tucson (/ˈtuːsɒn/ /tuːˈsɒn/) is a city and the county seat of Pima County, Arizona, United States, and home to the University of Arizona. The 2010 United States Census put the population at 520,116, while the 2013 estimated population of the entire Tucson metropolitan statistical area (MSA) was 996,544. The Tucson MSA forms part of the larger Tucson-Nogales combined statistical area (CSA), with a total population of 980,263 as of the 2010 Census. Tucson is the second-largest populated city in Arizona behind Phoenix, both of which anchor the Arizona Sun Corridor. The city is  located 108 miles (174 km) southeast of Phoenix and 60 mi (97 km) north of the U.S.-Mexico border. Tucson is the 33rd largest city and the 59th largest metropolitan area in the United States. Roughly 150 Tucson companies are involved in the design and manufacture of optics and optoelectronics systems, earning Tucson the nickname Optics Valley. The League of American Bicyclists gave Tucson a gold rating for bicycle friendliness in late April 2007. Tucson hosts the largest perimeter cycling event in the United States. The ride called ""El Tour de Tucson"" happens in November on the Saturday before Thanksgiving. El Tour de Tucson produced and promoted by Perimeter Bicycling has as many as 10,000 participants from all over the world, annually. Tucson is one of only nine cities in the U.S. to receive a gold rating or higher for cycling friendliness from the League of American Bicyclists. The city is known for its winter cycling opportunities. Both road and mountain biking are popular in and around Tucson with trail areas including Starr Pass and Fantasy Island. Arizona, south of the Gila River was legally bought from Mexico in the Gadsden Purchase on June 8, 1854. Tucson became a part of the United States of America, although the American military did not formally take over control until March 1856. In 1857 Tucson became a stage station on the San Antonio-San Diego Mail Line and in 1858 became 3rd division headquarters of the Butterfield Overland Mail until the line shut down in March 1861. The Overland Mail Corporation attempted to continue running, however following the Bascom Affair, devastating Apache attacks on the stations and coaches ended operations in August 1861.[citation needed] In general, Tucson and Pima County support the Democratic Party, as opposed the state's largest metropolitan area, Phoenix, which usually supports the Republican Party. Congressional redistricting in 2013, following the publication of the 2010 Census, divided the Tucson area into three Federal Congressional districts (the first, second and third of Arizona). The city center is in the 3rd District, represented by Raul Grijalva, a Democrat, since 2003, while the more affluent residential areas to the south and east are in the 2nd District, represented by Republican Martha McSally since 2015, and the exurbs north and west between Tucson and Phoenix in the 3rd District are represented by Democrat Ann Kirkpatrick since 2008. The United States Postal Service operates post offices in Tucson. The Tucson Main Post Office is located at 1501 South Cherrybell Stravenue. Tracks include Tucson Raceway Park and Rillito Downs. Tucson Raceway Park hosts NASCAR-sanctioned auto racing events and is one of only two asphalt short tracks in Arizona. Rillito Downs is an in-town destination on weekends in January and February each year. This historic track held the first organized quarter horse races in the world, and they are still racing there. The racetrack is threatened by development. The Moltacqua racetrack, was another historic horse racetrack located on what is now Sabino Canyon Road and Vactor Ranch Trail, but it no longer exists. Tucson is located 118 mi (190 km) southeast of Phoenix and 60 mi (97 km) north of the United States - Mexico border. The 2010 United States Census puts the city's population at 520,116 with a metropolitan area population at 980,263. In 2009, Tucson ranked as the 32nd largest city and 52nd largest metropolitan area in the United States. A major city in the Arizona Sun Corridor, Tucson is the largest city in southern Arizona, the second largest in the state after Phoenix. It is also the largest city in the area of the Gadsden Purchase. As of 2015, The Greater Tucson Metro area has exceeded a population of 1 million. By 1900, 7,531 people lived in the city. The population increased gradually to 13,913 in 1910. At about this time, the U.S. Veterans Administration had begun construction on the present Veterans Hospital. Many veterans who had been gassed in World War I and were in need of respiratory therapy began coming to Tucson after the war, due to the clean dry air. Over the following years the city continued to grow, with the population increasing to 20,292 in 1920 and 36,818 in 1940. In 2006 the population of Pima County, in which Tucson is located, passed one million while the City of Tucson's population was 535,000. Also on the north side is the suburban community of Catalina Foothills, located in the foothills of the Santa Catalina Mountains just north of the city limits. This community includes among the area's most expensive homes, sometimes multimillion-dollar estates. The Foothills area is generally defined as north of River Road, east of Oracle Road, and west of Sabino Creek. Some of the Tucson area's major resorts are located in the Catalina Foothills, including the Hacienda Del Sol, Westin La Paloma Resort, Loews Ventana Canyon Resort and Canyon Ranch Resort. La Encantada, an upscale outdoor shopping mall, is also in the Foothills. In an effort to conserve water, Tucson is recharging groundwater supplies by running part of its share of CAP water into various open portions of local rivers to seep into their aquifer. Additional study is scheduled to determine the amount of water that is lost through evaporation from the open areas, especially during the summer. The City of Tucson already provides reclaimed water to its inhabitants, but it is only used for ""applications such as irrigation, dust control, and industrial uses."" These resources have been in place for more than 27 years, and deliver to over 900 locations. For the past 25 years, the Tucson Folk Festival has taken place the first Saturday and Sunday of May in downtown Tucson's El Presidio Park. In addition to nationally known headline acts each evening, the Festival highlights over 100 local and regional musicians on five stages is one of the largest free festivals in the country. All stages are within easy walking distance. Organized by the Tucson Kitchen Musicians Association, volunteers make this festival possible. KXCI 91.3-FM, Arizona's only community radio station, is a major partner, broadcasting from the Plaza Stage throughout the weekend. In addition, there are numerous workshops, events for children, sing-alongs, and a popular singer/songwriter contest. Musicians typically play 30-minute sets, supported by professional audio staff volunteers. A variety of food and crafts are available at the festival, as well as local micro-brews. All proceeds from sales go to fund future festivals. Jesuit missionary Eusebio Francisco Kino visited the Santa Cruz River valley in 1692, and founded the Mission San Xavier del Bac in 1700 about 7 mi (11 km) upstream from the site of the settlement of Tucson. A separate Convento settlement was founded downstream along the Santa Cruz River, near the base of what is now ""A"" mountain. Hugo O'Conor, the founding father of the city of Tucson, Arizona authorized the construction of a military fort in that location, Presidio San Agustín del Tucsón, on August 20, 1775 (near the present downtown Pima County Courthouse). During the Spanish period of the presidio, attacks such as the Second Battle of Tucson were repeatedly mounted by Apaches. Eventually the town came to be called ""Tucson"" and became a part of Sonora after Mexico gained independence from Spain in 1821. The Tucson Padres played at Kino Veterans Memorial Stadium from 2011 to 2013. They served as the AAA affiliate of the San Diego Padres. The team, formerly known as the Portland Beavers, was temporarily relocated to Tucson from Portland while awaiting the building of a new stadium in Escondido. Legal issues derailed the plans to build the Escondido stadium, so they moved to El Paso, Texas for the 2014 season. Previously, the Tucson Sidewinders, a triple-A affiliate of the Arizona Diamondbacks, won the Pacific Coast League championship and unofficial AAA championship in 2006. The Sidewinders played in Tucson Electric Park and were in the Pacific Conference South of the PCL. The Sidewinders were sold in 2007 and moved to Reno, Nevada after the 2008 season. They now compete as the Reno Aces. At the University of Arizona, where records have been kept since 1894, the record maximum temperature was 115 °F (46 °C) on June 19, 1960, and July 28, 1995, and the record minimum temperature was 6 °F (−14 °C) on January 7, 1913. There are an average of 150.1 days annually with highs of 90 °F (32 °C) or higher and an average of 26.4 days with lows reaching or below the freezing mark. Average annual precipitation is 11.15 in (283 mm). There is an average of 49 days with measurable precipitation. The wettest year was 1905 with 24.17 in (614 mm) and the driest year was 1924 with 5.07 in (129 mm). The most precipitation in one month was 7.56 in (192 mm) in July 1984. The most precipitation in 24 hours was 4.16 in (106 mm) on October 1, 1983. Annual snowfall averages 0.7 in (1.8 cm). The most snow in one year was 7.2 in (18 cm) in 1987. The most snow in one month was 6.0 in (15 cm) in January 1898 and March 1922. The city's elevation is 2,643 ft (806 m) above sea level (as measured at the Tucson International Airport). Tucson is situated on an alluvial plain in the Sonoran desert, surrounded by five minor ranges of mountains: the Santa Catalina Mountains and the Tortolita Mountains to the north, the Santa Rita Mountains to the south, the Rincon Mountains to the east, and the Tucson Mountains to the west. The high point of the Santa Catalina Mountains is 9,157 ft (2,791 m) Mount Lemmon, the southernmost ski destination in the continental U.S., while the Tucson Mountains include 4,687 ft (1,429 m) Wasson Peak. The highest point in the area is Mount Wrightson, found in the Santa Rita Mountains at 9,453 ft (2,881 m) above sea level. The community of Casas Adobes is also on the Northwest Side, with the distinction of being Tucson's first suburb, established in the late 1940s. Casas Adobes is centered on the historic Casas Adobes Plaza (built in 1948). Casas Adobes is also home to Tohono Chul Park (a nature preserve) near the intersection of North Oracle Road and West Ina Road. The attempted assassination of Representative Gabrielle Giffords, and the murders of chief judge for the U.S. District Court for Arizona, John Roll and five other people on January 8, 2011, occurred at the La Toscana Village in Casas Adobes. The Foothills Mall is also located on the northwest side in Casas Adobes. Perhaps the biggest sustainability problem in Tucson, with its high desert climate, is potable water supply. The state manages all water in Arizona through its Arizona Department of Water Resources (ADWR). The primary consumer of water is Agriculture (including golf courses), which consumes about 69% of all water. Municipal (which includes residential use) accounts for about 25% of use. Energy consumption and availability is another sustainability issue. However, with over 300 days of full sun a year, Tucson has demonstrated its potential to be an ideal solar energy producer. The accomplished and awarded writers (poets, novelists, dramatists, nonfiction writers) who have lived in Tucson include Edward Abbey, Erskine Caldwell, Barbara Kingsolver and David Foster Wallace. Some were associated with the University of Arizona, but many were independent writers who chose to make Tucson their home. The city is particularly active in publishing and presenting contemporary innovative poetry in various ways. Examples are the Chax Press, a publisher of poetry books in trade and book arts editions, and the University of Arizona Poetry Center, which has a sizable poetry library and presents readings, conferences, and workshops. As one of the oldest parts of town, Central Tucson is anchored by the Broadway Village shopping center designed by local architect Josias Joesler at the intersection of Broadway Boulevard and Country Club Road. The 4th Avenue Shopping District between downtown and the University and the Lost Barrio just East of downtown, also have many unique and popular stores. Local retail business in Central Tucson is densely concentrated along Fourth Avenue and the Main Gate Square on University Boulevard near the UA campus. The El Con Mall is also located in the eastern part of midtown. Southeast Tucson continues to experience rapid residential development. The area includes Davis-Monthan Air Force Base. The area is considered to be south of Golf Links Road. It is the home of Santa Rita High School, Chuck Ford Park (Lakeside Park), Lakeside Lake, Lincoln Park (upper and lower), The Lakecrest Neighborhoods, and Pima Community College East Campus. The Atterbury Wash with its access to excellent bird watching is also located in the Southeast Tucson area. The suburban community of Rita Ranch houses many of the military families from Davis-Monthan, and is near the southeastern-most expansion of the current city limits. Close by Rita Ranch and also within the city limits lies Civano, a planned development meant to showcase ecologically sound building practices and lifestyles. The City of Tucson, Pima County, the State of Arizona, and the private sector have all made commitments to create a growing, healthy economy[citation needed] with advanced technology industry sectors as its foundation. Raytheon Missile Systems (formerly Hughes Aircraft Co.), Texas Instruments, IBM, Intuit Inc., Universal Avionics, Honeywell Aerospace, Sunquest Information Systems, Sanofi-Aventis, Ventana Medical Systems, Inc., and Bombardier Aerospace all have a significant presence in Tucson. Roughly 150 Tucson companies are involved in the design and manufacture of optics and optoelectronics systems, earning Tucson the nickname ""Optics Valley"". Tucson is commonly known as ""The Old Pueblo"". While the exact origin of this nickname is uncertain, it is commonly traced back to Mayor R. N. ""Bob"" Leatherwood. When rail service was established to the city on March 20, 1880, Leatherwood celebrated the fact by sending telegrams to various leaders, including the President of the United States and the Pope, announcing that the ""ancient and honorable pueblo"" of Tucson was now connected by rail to the outside world. The term became popular with newspaper writers who often abbreviated it as ""A. and H. Pueblo"". This in turn transformed into the current form of ""The Old Pueblo"". Cycling is popular in Tucson due to its flat terrain and dry climate. Tucson and Pima County maintain an extensive network of marked bike routes, signal crossings, on-street bike lanes, mountain-biking trails, and dedicated shared-use paths. The Loop is a network of seven linear parks comprising over 100 mi (160 km) of paved, vehicle-free trails that encircles the majority of the city with links to Marana and Oro Valley. The Tucson-Pima County Bicycle Advisory Committee (TPCBAC) serves in an advisory capacity to local governments on issues relating to bicycle recreation, transportation, and safety. Tucson was awarded a gold rating for bicycle-friendliness by the League of American Bicyclists in 2006. Both the council members and the mayor serve four-year terms; none face term limits. Council members are nominated by their wards via a ward-level primary held in September. The top vote-earners from each party then compete at-large for their ward's seat on the November ballot. In other words, on election day the whole city votes on all the council races up for that year. Council elections are severed: Wards 1, 2, and 4 (as well as the mayor) are up for election in the same year (most recently 2011), while Wards 3, 5, and 6 share another year (most recently 2013). Central Tucson is bicycle-friendly. To the east of the University of Arizona, Third Street is bike-only except for local traffic and passes by the historic homes of the Sam Hughes neighborhood. To the west, E. University Boulevard leads to the Fourth Avenue Shopping District. To the North, N. Mountain Avenue has a full bike-only lane for half of the 3.5 miles (5.6 km) to the Rillito River Park bike and walk multi-use path. To the south, N. Highland Avenue leads to the Barraza-Aviation Parkway bicycle path. The expansive area northwest of the city limits is diverse, ranging from the rural communities of Catalina and parts of the town of Marana, the small suburb of Picture Rocks, the affluent town of Oro Valley in the western foothills of the Santa Catalina Mountains, and residential areas in the northeastern foothills of the Tucson Mountains. Continental Ranch (Marana), Dove Mountain (Marana), and Rancho Vistoso (Oro Valley) are all masterplanned communities located in the Northwest, where thousands of residents live. The Procession, held at sundown, consists of a non-motorized parade through downtown Tucson featuring many floats, sculptures, and memorials, in which the community is encouraged to participate. The parade is followed by performances on an outdoor stage, culminating in the burning of an urn in which written prayers have been collected from participants and spectators. The event is organized and funded by the non-profit arts organization Many Mouths One Stomach, with the assistance of many volunteers and donations from the public and local businesses. Winters in Tucson are mild relative to other parts of the United States. Daytime highs in the winter range between 64 and 75 °F (18 and 24 °C), with overnight lows between 30 and 44 °F (−1 and 7 °C). Tucson typically averages one hard freeze per winter season, with temperatures dipping to the mid or low-20s (−7 to −4 °C), but this is typically limited to only a very few nights. Although rare, snow has been known to fall in Tucson, usually a light dusting that melts within a day. The most recent snowfall was on February 20, 2013 when 2.0 inches of snow blanketed the city, the largest snowfall since 1987. From 1877 to 1878, the area suffered a rash of stagecoach robberies. Most notable, however, were the two holdups committed by masked road-agent William Whitney Brazelton. Brazelton held up two stages in the summer of 1878 near Point of Mountain Station approximately 17 mi (27 km) northwest of Tucson. John Clum, of Tombstone, Arizona fame was one of the passengers. Brazelton was eventually tracked down and killed on Monday August 19, 1878, in a mesquite bosque along the Santa Cruz River 3 miles (5 km) south of Tucson by Pima County Sheriff Charles A. Shibell and his citizen's posse. Brazelton had been suspected of highway robbery not only in the Tucson area, but also in the Prescott region and Silver City, New Mexico area as well. Brazelton's crimes prompted John J. Valentine, Sr. of Wells, Fargo & Co. to send special agent and future Pima County sheriff Bob Paul to investigate. Fort Lowell, then east of Tucson, was established to help protect settlers from Apache attacks. In 1882, Frank Stilwell was implicated in the murder of Morgan Earp by Cowboy Pete Spence's wife, Marietta, at the coroner's inquest on Morgan Earp's shooting. The coroner's jury concluded that Spence, Stilwell, Frederick Bode, and Florentino ""Indian Charlie"" Cruz were the prime suspects in the assassination of Morgan Earp. :250 Deputy U.S. Marshal Wyatt Earp gathered a few trusted friends and accompanied Virgil Earp and his family as they traveled to Benson for a train ride to California. They found Stilwell lying in wait for Virgil in the Tucson station and killed him on the tracks. After killing Stilwell, Wyatt deputized others and rode on a vendetta, killing three more cowboys over the next few days before leaving the state. Tucson has a desert climate (Köppen BWh), with two major seasons, summer and winter; plus three minor seasons: fall, spring, and the monsoon. Tucson averages 11.8 inches (299.7 mm) of precipitation per year, more than most other locations with desert climates, but it still qualifies due to its high evapotranspiration; in other words, it experiences a high net loss of water. A similar scenario is seen in Alice Springs, Australia, which averages 11 inches (279.4 mm) a year, but has a desert climate. As of the census of 2010, there were 520,116 people, 229,762 households, and 112,455 families residing in the city. The population density was 2,500.1 inhabitants per square mile (965.3/km²). There were 209,609 housing units at an average density of 1,076.7 per square mile (415.7/km²). The racial makeup of the city was 69.7% White (down from 94.8% in 1970), 5.0% Black or African-American, 2.7% Native American, 2.9% Asian, 0.2% Pacific Islander, 16.9% from other races, and 3.8% from two or more races. Hispanic or Latino of any race were 41.6% of the population. Non-Hispanic Whites were 47.2% of the population in 2010, down from 72.8% in 1970."
Architecture,"The architecture of different parts of Asia developed along different lines from that of Europe; Buddhist, Hindu and Sikh architecture each having different characteristics. Buddhist architecture, in particular, showed great regional diversity. Hindu temple architecture, which developed around the 3rd century BCE, is governed by concepts laid down in the Shastras, and is concerned with expressing the macrocosm and the microcosm. In many Asian countries, pantheistic religion led to architectural forms that were designed specifically to enhance the natural landscape. On the difference between the ideals of architecture and mere construction, the renowned 20th-century architect Le Corbusier wrote: ""You employ stone, wood, and concrete, and with these materials you build houses and palaces: that is construction. Ingenuity is at work. But suddenly you touch my heart, you do me good. I am happy and I say: This is beautiful. That is Architecture"". Meanwhile, the Industrial Revolution laid open the door for mass production and consumption. Aesthetics became a criterion for the middle class as ornamented products, once within the province of expensive craftsmanship, became cheaper under machine production. In Renaissance Europe, from about 1400 onwards, there was a revival of Classical learning accompanied by the development of Renaissance Humanism which placed greater emphasis on the role of the individual in society than had been the case during the Medieval period. Buildings were ascribed to specific architects – Brunelleschi, Alberti, Michelangelo, Palladio – and the cult of the individual had begun. There was still no dividing line between artist, architect and engineer, or any of the related vocations, and the appellation was often one of regional preference. Environmental sustainability has become a mainstream issue, with profound effect on the architectural profession. Many developers, those who support the financing of buildings, have become educated to encourage the facilitation of environmentally sustainable design, rather than solutions based primarily on immediate cost. Major examples of this can be found in Passive solar building design, greener roof designs, biodegradable materials, and more attention to a structure's energy usage. This major shift in architecture has also changed architecture schools to focus more on the environment. Sustainability in architecture was pioneered by Frank Lloyd Wright, in the 1960s by Buckminster Fuller and in the 1970s by architects such as Ian McHarg and Sim Van der Ryn in the US and Brenda and Robert Vale in the UK and New Zealand. There has been an acceleration in the number of buildings which seek to meet green building sustainable design principles. Sustainable practices that were at the core of vernacular architecture increasingly provide inspiration for environmentally and socially sustainable contemporary techniques. The U.S. Green Building Council's LEED (Leadership in Energy and Environmental Design) rating system has been instrumental in this. To restrict the meaning of (architectural) formalism to art for art's sake is not only reactionary; it can also be a purposeless quest for perfection or originality which degrades form into a mere instrumentality"". The 19th-century English art critic, John Ruskin, in his Seven Lamps of Architecture, published 1849, was much narrower in his view of what constituted architecture. Architecture was the ""art which so disposes and adorns the edifices raised by men ... that the sight of them"" contributes ""to his mental health, power, and pleasure"". It is widely assumed that architectural success was the product of a process of trial and error, with progressively less trial and more replication as the results of the process proved increasingly satisfactory. What is termed vernacular architecture continues to be produced in many parts of the world. Indeed, vernacular buildings make up most of the built world that people experience every day. Early human settlements were mostly rural. Due to a surplus in production the economy began to expand resulting in urbanization thus creating urban areas which grew and evolved very rapidly in some cases, such as that of Çatal Höyük in Anatolia and Mohenjo Daro of the Indus Valley Civilization in modern-day Pakistan. When modern architecture was first practiced, it was an avant-garde movement with moral, philosophical, and aesthetic underpinnings. Immediately after World War I, pioneering modernist architects sought to develop a completely new style appropriate for a new post-war social and economic order, focused on meeting the needs of the middle and working classes. They rejected the architectural practice of the academic refinement of historical styles which served the rapidly declining aristocratic order. The approach of the Modernist architects was to reduce buildings to pure forms, removing historical references and ornament in favor of functionalist details. Buildings displayed their functional and structural elements, exposing steel beams and concrete surfaces instead of hiding them behind decorative forms. While the notion that structural and aesthetic considerations should be entirely subject to functionality was met with both popularity and skepticism, it had the effect of introducing the concept of ""function"" in place of Vitruvius' ""utility"". ""Function"" came to be seen as encompassing all criteria of the use, perception and enjoyment of a building, not only practical but also aesthetic, psychological and cultural. According to Vitruvius, the architect should strive to fulfill each of these three attributes as well as possible. Leon Battista Alberti, who elaborates on the ideas of Vitruvius in his treatise, De Re Aedificatoria, saw beauty primarily as a matter of proportion, although ornament also played a part. For Alberti, the rules of proportion were those that governed the idealised human figure, the Golden mean. The most important aspect of beauty was therefore an inherent part of an object, rather than something applied superficially; and was based on universal, recognisable truths. The notion of style in the arts was not developed until the 16th century, with the writing of Vasari: by the 18th century, his Lives of the Most Excellent Painters, Sculptors, and Architects had been translated into Italian, French, Spanish and English. Texts on architecture have been written since ancient time. These texts provided both general advice and specific formal prescriptions or canons. Some examples of canons are found in the writings of the 1st-century BCE Roman Architect Vitruvius. Some of the most important early examples of canonic architecture are religious. One such reaction to the cold aesthetic of modernism and Brutalism is the school of metaphoric architecture, which includes such things as biomorphism and zoomorphic architecture, both using nature as the primary source of inspiration and design. While it is considered by some to be merely an aspect of postmodernism, others consider it to be a school in its own right and a later development of expressionist architecture. The earliest surviving written work on the subject of architecture is De architectura, by the Roman architect Vitruvius in the early 1st century AD. According to Vitruvius, a good building should satisfy the three principles of firmitas, utilitas, venustas, commonly known by the original translation – firmness, commodity and delight. An equivalent in modern English would be: Many architects resisted modernism, finding it devoid of the decorative richness of historical styles. As the first generation of modernists began to die after WWII, a second generation of architects including Paul Rudolph, Marcel Breuer, and Eero Saarinen tried to expand the aesthetics of modernism with Brutalism, buildings with expressive sculptural facades made of unfinished concrete. But an even new younger postwar generation critiqued modernism and Brutalism for being too austere, standardized, monotone, and not taking into account the richness of human experience offered in historical buildings across time and in different places and cultures. In the early 19th century, Augustus Welby Northmore Pugin wrote Contrasts (1836) that, as the titled suggested, contrasted the modern, industrial world, which he disparaged, with an idealized image of neo-medieval world. Gothic architecture, Pugin believed, was the only ""true Christian form of architecture."" Beginning in the late 1950s and 1960s, architectural phenomenology emerged as an important movement in the early reaction against modernism, with architects like Charles Moore in the USA, Christian Norberg-Schulz in Norway, and Ernesto Nathan Rogers and Vittorio Gregotti in Italy, who collectively popularized an interest in a new contemporary architecture aimed at expanding human experience using historical buildings as models and precedents. Postmodernism produced a style that combined contemporary building technology and cheap materials, with the aesthetics of older pre-modern and non-modern styles, from high classical architecture to popular or vernacular regional building styles. Robert Venturi famously defined postmodern architecture as a ""decorated shed"" (an ordinary building which is functionally designed inside and embellished on the outside), and upheld it against modernist and brutalist ""ducks"" (buildings with unnecessarily expressive tectonic forms). Islamic architecture began in the 7th century CE, incorporating architectural forms from the ancient Middle East and Byzantium, but also developing features to suit the religious and social needs of the society. Examples can be found throughout the Middle East, North Africa, Spain and the Indian Sub-continent. The widespread application of the pointed arch was to influence European architecture of the Medieval period. Building first evolved out of the dynamics between needs (shelter, security, worship, etc.) and means (available building materials and attendant skills). As human cultures developed and knowledge began to be formalized through oral traditions and practices, building became a craft, and ""architecture"" is the name given to the most highly formalized and respected versions of that craft. The architecture and urbanism of the Classical civilizations such as the Greek and the Roman evolved from civic ideals rather than religious or empirical ones and new building types emerged. Architectural ""style"" developed in the form of the Classical orders. A revival of the Classical style in architecture was accompanied by a burgeoning of science and engineering which affected the proportions and structure of buildings. At this stage, it was still possible for an artist to design a bridge as the level of structural calculations involved was within the scope of the generalist. Architecture has to do with planning and designing form, space and ambience to reflect functional, technical, social, environmental and aesthetic considerations. It requires the creative manipulation and coordination of materials and technology, and of light and shadow. Often, conflicting requirements must be resolved. The practice of Architecture also encompasses the pragmatic aspects of realizing buildings and structures, including scheduling, cost estimation and construction administration. Documentation produced by architects, typically drawings, plans and technical specifications, defines the structure and/or behavior of a building or other kind of system that is to be or has been constructed. In Europe during the Medieval period, guilds were formed by craftsmen to organise their trades and written contracts have survived, particularly in relation to ecclesiastical buildings. The role of architect was usually one with that of master mason, or Magister lathomorum as they are sometimes described in contemporary documents. Architects such as Frank Lloyd Wright developed Organic architecture, in which the form was defined by its environment and purpose, with an aim to promote harmony between human habitation and the natural world with prime examples being Robie House and Fallingwater. Around the beginning of the 20th century, a general dissatisfaction with the emphasis on revivalist architecture and elaborate decoration gave rise to many new lines of thought that served as precursors to Modern Architecture. Notable among these is the Deutscher Werkbund, formed in 1907 to produce better quality machine made objects. The rise of the profession of industrial design is usually placed here. Following this lead, the Bauhaus school, founded in Weimar, Germany in 1919, redefined the architectural bounds prior set throughout history, viewing the creation of a building as the ultimate synthesis—the apex—of art, craft, and technology. In many ancient civilizations, such as those of Egypt and Mesopotamia, architecture and urbanism reflected the constant engagement with the divine and the supernatural, and many ancient cultures resorted to monumentality in architecture to represent symbolically the political power of the ruler, the ruling elite, or the state itself. Architects such as Mies van der Rohe, Philip Johnson and Marcel Breuer worked to create beauty based on the inherent qualities of building materials and modern construction techniques, trading traditional historic forms for simplified geometric forms, celebrating the new means and methods made possible by the Industrial Revolution, including steel-frame construction, which gave birth to high-rise superstructures. By mid-century, Modernism had morphed into the International Style, an aesthetic epitomized in many ways by the Twin Towers of New York's World Trade Center designed by Minoru Yamasaki. For Ruskin, the aesthetic was of overriding significance. His work goes on to state that a building is not truly a work of architecture unless it is in some way ""adorned"". For Ruskin, a well-constructed, well-proportioned, functional building needed string courses or rustication, at the very least. In the late 20th century a new concept was added to those included in the compass of both structure and function, the consideration of sustainability, hence sustainable architecture. To satisfy the contemporary ethos a building should be constructed in a manner which is environmentally friendly in terms of the production of its materials, its impact upon the natural and built environment of its surrounding area and the demands that it makes upon non-sustainable power sources for heating, cooling, water and waste management and lighting. The major architectural undertakings were the buildings of abbeys and cathedrals. From about 900 CE onwards, the movements of both clerics and tradesmen carried architectural knowledge across Europe, resulting in the pan-European styles Romanesque and Gothic. Architecture (Latin architectura, from the Greek ἀρχιτέκτων arkhitekton ""architect"", from ἀρχι- ""chief"" and τέκτων ""builder"") is both the process and the product of planning, designing, and constructing buildings and other physical structures. Architectural works, in the material form of buildings, are often perceived as cultural symbols and as works of art. Historical civilizations are often identified with their surviving architectural achievements. Nunzia Rondanini stated, ""Through its aesthetic dimension architecture goes beyond the functional aspects that it has in common with other human sciences. Through its own particular way of expressing values, architecture can stimulate and influence social life without presuming that, in and of itself, it will promote social development.' Early Asian writings on architecture include the Kao Gong Ji of China from the 7th–5th centuries BCE; the Shilpa Shastras of ancient India and Manjusri Vasthu Vidya Sastra of Sri Lanka. Since the 1980s, as the complexity of buildings began to increase (in terms of structural systems, services, energy and technologies), the field of architecture became multi-disciplinary with specializations for each project type, technological expertise or project delivery methods. In addition, there has been an increased separation of the 'design' architect [Notes 1] from the 'project' architect who ensures that the project meets the required standards and deals with matters of liability.[Notes 2] The preparatory processes for the design of any large building have become increasingly complicated, and require preliminary studies of such matters as durability, sustainability, quality, money, and compliance with local laws. A large structure can no longer be the design of one person but must be the work of many. Modernism and Postmodernism have been criticised by some members of the architectural profession who feel that successful architecture is not a personal, philosophical, or aesthetic pursuit by individualists; rather it has to consider everyday needs of people and use technology to create liveable environments, with the design process being informed by studies of behavioral, environmental, and social sciences. Among the philosophies that have influenced modern architects and their approach to building design are rationalism, empiricism, structuralism, poststructuralism, and phenomenology. Concurrently, the recent movements of New Urbanism, Metaphoric architecture and New Classical Architecture promote a sustainable approach towards construction, that appreciates and develops smart growth, architectural tradition and classical design. This in contrast to modernist and globally uniform architecture, as well as leaning against solitary housing estates and suburban sprawl. With the emerging knowledge in scientific fields and the rise of new materials and technology, architecture and engineering began to separate, and the architect began to concentrate on aesthetics and the humanist aspects, often at the expense of technical aspects of building design. There was also the rise of the ""gentleman architect"" who usually dealt with wealthy clients and concentrated predominantly on visual qualities derived usually from historical prototypes, typified by the many country houses of Great Britain that were created in the Neo Gothic or Scottish Baronial styles. Formal architectural training in the 19th century, for example at École des Beaux-Arts in France, gave much emphasis to the production of beautiful drawings and little to context and feasibility. Effective architects generally received their training in the offices of other architects, graduating to the role from draughtsmen or clerks. Vernacular architecture became increasingly ornamental. House builders could use current architectural design in their work by combining features found in pattern books and architectural journals."
Cotton,"Most cotton in the United States, Europe and Australia is harvested mechanically, either by a cotton picker, a machine that removes the cotton from the boll without damaging the cotton plant, or by a cotton stripper, which strips the entire boll off the plant. Cotton strippers are used in regions where it is too windy to grow picker varieties of cotton, and usually after application of a chemical defoliant or the natural defoliation that occurs after a freeze. Cotton is a perennial crop in the tropics, and without defoliation or freezing, the plant will continue to grow. In Iran (Persia), the history of cotton dates back to the Achaemenid era (5th century BC); however, there are few sources about the planting of cotton in pre-Islamic Iran. The planting of cotton was common in Merv, Ray and Pars of Iran. In Persian poets' poems, especially Ferdowsi's Shahname, there are references to cotton (""panbe"" in Persian). Marco Polo (13th century) refers to the major products of Persia, including cotton. John Chardin, a French traveler of the 17th century who visited the Safavid Persia, spoke approvingly of the vast cotton farms of Persia. Though known since antiquity the commercial growing of cotton in Egypt only started in 1820's, following a Frenchman, by the name of M. Jumel, propositioning the then ruler, Mohamed Ali Pasha, that he could earn a substantial income by growing an extra-long staple Maho (Barbadence) cotton, in Lower Egypt, for the French market. Mohamed Ali Pasha accepted the proposition and granted himself the monopoly on the sale and export of cotton in Egypt; and later dictated cotton should be grown in preference to other crops. By the time of the American Civil war annual exports had reached $16 million (120,000 bales), which rose to $56 million by 1864, primarily due to the loss of the Confederate supply on the world market. Exports continued to grow even after the reintroduction of US cotton, produced now by a paid workforce, and Egyptian exports reached 1.2 million bales a year by 1903. The era of manufactured fibers began with the development of rayon in France in the 1890s. Rayon is derived from a natural cellulose and cannot be considered synthetic, but requires extensive processing in a manufacturing process, and led the less expensive replacement of more naturally derived materials. A succession of new synthetic fibers were introduced by the chemicals industry in the following decades. Acetate in fiber form was developed in 1924. Nylon, the first fiber synthesized entirely from petrochemicals, was introduced as a sewing thread by DuPont in 1936, followed by DuPont's acrylic in 1944. Some garments were created from fabrics based on these fibers, such as women's hosiery from nylon, but it was not until the introduction of polyester into the fiber marketplace in the early 1950s that the market for cotton came under threat. The rapid uptake of polyester garments in the 1960s caused economic hardship in cotton-exporting economies, especially in Central American countries, such as Nicaragua, where cotton production had boomed tenfold between 1950 and 1965 with the advent of cheap chemical pesticides. Cotton production recovered in the 1970s, but crashed to pre-1960 levels in the early 1990s. Production capacity in Britain and the United States was improved by the invention of the cotton gin by the American Eli Whitney in 1793. Before the development of cotton gins, the cotton fibers had to be pulled from the seeds tediously by hand. By the late 1700s a number of crude ginning machines had been developed. However, to produce a bale of cotton required over 600 hours of human labor, making large-scale production uneconomical in the United States, even with the use of humans as slave labor. The gin that Whitney manufactured (the Holmes design) reduced the hours down to just a dozen or so per bale. Although Whitney patented his own design for a cotton gin, he manufactured a prior design from Henry Odgen Holmes, for which Holmes filed a patent in 1796. Improving technology and increasing control of world markets allowed British traders to develop a commercial chain in which raw cotton fibers were (at first) purchased from colonial plantations, processed into cotton cloth in the mills of Lancashire, and then exported on British ships to captive colonial markets in West Africa, India, and China (via Shanghai and Hong Kong). In addition to concerns over subsidies, the cotton industries of some countries are criticized for employing child labor and damaging workers' health by exposure to pesticides used in production. The Environmental Justice Foundation has campaigned against the prevalent use of forced child and adult labor in cotton production in Uzbekistan, the world's third largest cotton exporter. The international production and trade situation has led to ""fair trade"" cotton clothing and footwear, joining a rapidly growing market for organic clothing, fair fashion or ""ethical fashion"". The fair trade system was initiated in 2005 with producers from Cameroon, Mali and Senegal. India's cotton-processing sector gradually declined during British expansion in India and the establishment of colonial rule during the late 18th and early 19th centuries. This was largely due to aggressive colonialist mercantile policies of the British East India Company, which made cotton processing and manufacturing workshops in India uncompetitive. Indian markets were increasingly forced to supply only raw cotton and, by British-imposed law, to purchase manufactured textiles from Britain.[citation needed] While Brazil was fighting the US through the WTO's Dispute Settlement Mechanism against a heavily subsidized cotton industry, a group of four least-developed African countries – Benin, Burkina Faso, Chad, and Mali – also known as ""Cotton-4"" have been the leading protagonist for the reduction of US cotton subsidies through negotiations. The four introduced a ""Sectoral Initiative in Favour of Cotton"", presented by Burkina Faso's President Blaise Compaoré during the Trade Negotiations Committee on 10 June 2003. Beginning as a self-help program in the mid-1960s, the Cotton Research and Promotion Program (CRPP) was organized by U.S. cotton producers in response to cotton's steady decline in market share. At that time, producers voted to set up a per-bale assessment system to fund the program, with built-in safeguards to protect their investments. With the passage of the Cotton Research and Promotion Act of 1966, the program joined forces and began battling synthetic competitors and re-establishing markets for cotton. Today, the success of this program has made cotton the best-selling fiber in the U.S. and one of the best-selling fibers in the world.[citation needed] During the late medieval period, cotton became known as an imported fiber in northern Europe, without any knowledge of how it was derived, other than that it was a plant. Because Herodotus had written in his Histories, Book III, 106, that in India trees grew in the wild producing wool, it was assumed that the plant was a tree, rather than a shrub. This aspect is retained in the name for cotton in several Germanic languages, such as German Baumwolle, which translates as ""tree wool"" (Baum means ""tree""; Wolle means ""wool""). Noting its similarities to wool, people in the region could only imagine that cotton must be produced by plant-borne sheep. John Mandeville, writing in 1350, stated as fact the now-preposterous belief: ""There grew there [India] a wonderful tree which bore tiny lambs on the endes of its branches. These branches were so pliable that they bent down to allow the lambs to feed when they are hungrie  [sic]."" (See Vegetable Lamb of Tartary.) By the end of the 16th century, cotton was cultivated throughout the warmer regions in Asia and the Americas. The 25,000 cotton growers in the United States of America are heavily subsidized at the rate of $2 billion per year although China now provides the highest overall level of cotton sector support. The future of these subsidies is uncertain and has led to anticipatory expansion of cotton brokers' operations in Africa. Dunavant expanded in Africa by buying out local operations. This is only possible in former British colonies and Mozambique; former French colonies continue to maintain tight monopolies, inherited from their former colonialist masters, on cotton purchases at low fixed prices. Successful cultivation of cotton requires a long frost-free period, plenty of sunshine, and a moderate rainfall, usually from 600 to 1,200 mm (24 to 47 in). Soils usually need to be fairly heavy, although the level of nutrients does not need to be exceptional. In general, these conditions are met within the seasonally dry tropics and subtropics in the Northern and Southern hemispheres, but a large proportion of the cotton grown today is cultivated in areas with less rainfall that obtain the water from irrigation. Production of the crop for a given year usually starts soon after harvesting the preceding autumn. Cotton is naturally a perennial but is grown as an annual to help control pests. Planting time in spring in the Northern hemisphere varies from the beginning of February to the beginning of June. The area of the United States known as the South Plains is the largest contiguous cotton-growing region in the world. While dryland (non-irrigated) cotton is successfully grown in this region, consistent yields are only produced with heavy reliance on irrigation water drawn from the Ogallala Aquifer. Since cotton is somewhat salt and drought tolerant, this makes it an attractive crop for arid and semiarid regions. As water resources get tighter around the world, economies that rely on it face difficulties and conflict, as well as potential environmental problems. For example, improper cropping and irrigation practices have led to desertification in areas of Uzbekistan, where cotton is a major export. In the days of the Soviet Union, the Aral Sea was tapped for agricultural irrigation, largely of cotton, and now salination is widespread. Historically, in North America, one of the most economically destructive pests in cotton production has been the boll weevil. Due to the US Department of Agriculture's highly successful Boll Weevil Eradication Program (BWEP), this pest has been eliminated from cotton in most of the United States. This program, along with the introduction of genetically engineered Bt cotton (which contains a bacterial gene that codes for a plant-produced protein that is toxic to a number of pests such as cotton bollworm and pink bollworm), has allowed a reduction in the use of synthetic insecticides. A public genome sequencing effort of cotton was initiated in 2007 by a consortium of public researchers. They agreed on a strategy to sequence the genome of cultivated, tetraploid cotton. ""Tetraploid"" means that cultivated cotton actually has two separate genomes within its nucleus, referred to as the A and D genomes. The sequencing consortium first agreed to sequence the D-genome relative of cultivated cotton (G. raimondii, a wild Central American cotton species) because of its small size and limited number of repetitive elements. It is nearly one-third the number of bases of tetraploid cotton (AD), and each chromosome is only present once.[clarification needed] The A genome of G. arboreum would be sequenced next. Its genome is roughly twice the size of G. raimondii's. Part of the difference in size between the two genomes is the amplification of retrotransposons (GORGE). Once both diploid genomes are assembled, then research could begin sequencing the actual genomes of cultivated cotton varieties. This strategy is out of necessity; if one were to sequence the tetraploid genome without model diploid genomes, the euchromatic DNA sequences of the AD genomes would co-assemble and the repetitive elements of AD genomes would assembly independently into A and D sequences respectively. Then there would be no way to untangle the mess of AD sequences without comparing them to their diploid counterparts. GM cotton acreage in India grew at a rapid rate, increasing from 50,000 hectares in 2002 to 10.6 million hectares in 2011. The total cotton area in India was 12.1 million hectares in 2011, so GM cotton was grown on 88% of the cotton area. This made India the country with the largest area of GM cotton in the world. A long-term study on the economic impacts of Bt cotton in India, published in the Journal PNAS in 2012, showed that Bt cotton has increased yields, profits, and living standards of smallholder farmers. The U.S. GM cotton crop was 4.0 million hectares in 2011 the second largest area in the world, the Chinese GM cotton crop was third largest by area with 3.9 million hectares and Pakistan had the fourth largest GM cotton crop area of 2.6 million hectares in 2011. The initial introduction of GM cotton proved to be a success in Australia – the yields were equivalent to the non-transgenic varieties and the crop used much less pesticide to produce (85% reduction). The subsequent introduction of a second variety of GM cotton led to increases in GM cotton production until 95% of the Australian cotton crop was GM in 2009 making Australia the country with the fifth largest GM cotton crop in the world. Other GM cotton growing countries in 2011 were Argentina, Myanmar, Burkina Faso, Brazil, Mexico, Colombia, South Africa and Costa Rica. Organic cotton is generally understood as cotton from plants not genetically modified and that is certified to be grown without the use of any synthetic agricultural chemicals, such as fertilizers or pesticides. Its production also promotes and enhances biodiversity and biological cycles. In the United States, organic cotton plantations are required to enforce the National Organic Program (NOP). This institution determines the allowed practices for pest control, growing, fertilizing, and handling of organic crops. As of 2007, 265,517 bales of organic cotton were produced in 24 countries, and worldwide production was growing at a rate of more than 50% per year. The fiber is most often spun into yarn or thread and used to make a soft, breathable textile. The use of cotton for fabric is known to date to prehistoric times; fragments of cotton fabric dated from 5000 BC have been excavated in Mexico and the Indus Valley Civilization in Ancient India (modern-day Pakistan and some parts of India). Although cultivated since antiquity, it was the invention of the cotton gin that lowered the cost of production that led to its widespread use, and it is the most widely used natural fiber cloth in clothing today. But Bt cotton is ineffective against many cotton pests, however, such as plant bugs, stink bugs, and aphids; depending on circumstances it may still be desirable to use insecticides against these. A 2006 study done by Cornell researchers, the Center for Chinese Agricultural Policy and the Chinese Academy of Science on Bt cotton farming in China found that after seven years these secondary pests that were normally controlled by pesticide had increased, necessitating the use of pesticides at similar levels to non-Bt cotton and causing less profit for farmers because of the extra expense of GM seeds. However, a 2009 study by the Chinese Academy of Sciences, Stanford University and Rutgers University refuted this. They concluded that the GM cotton effectively controlled bollworm. The secondary pests were mostly miridae (plant bugs) whose increase was related to local temperature and rainfall and only continued to increase in half the villages studied. Moreover, the increase in insecticide use for the control of these secondary insects was far smaller than the reduction in total insecticide use due to Bt cotton adoption. A 2012 Chinese study concluded that Bt cotton halved the use of pesticides and doubled the level of ladybirds, lacewings and spiders. The International Service for the Acquisition of Agri-biotech Applications (ISAAA) said that, worldwide, GM cotton was planted on an area of 25 million hectares in 2011. This was 69% of the worldwide total area planted in cotton. The cottonseed which remains after the cotton is ginned is used to produce cottonseed oil, which, after refining, can be consumed by humans like any other vegetable oil. The cottonseed meal that is left generally is fed to ruminant livestock; the gossypol remaining in the meal is toxic to monogastric animals. Cottonseed hulls can be added to dairy cattle rations for roughage. During the American slavery period, cotton root bark was used in folk remedies as an abortifacient, that is, to induce a miscarriage. Gossypol was one of the many substances found in all parts of the cotton plant and it was described by the scientists as 'poisonous pigment'. It also appears to inhibit the development of sperm or even restrict the mobility of the sperm. Also, it is thought to interfere with the menstrual cycle by restricting the release of certain hormones. The public sector effort continues with the goal to create a high-quality, draft genome sequence from reads generated by all sources. The public-sector effort has generated Sanger reads of BACs, fosmids, and plasmids as well as 454 reads. These later types of reads will be instrumental in assembling an initial draft of the D genome. In 2010, two companies (Monsanto and Illumina), completed enough Illumina sequencing to cover the D genome of G. raimondii about 50x. They announced that they would donate their raw reads to the public. This public relations effort gave them some recognition for sequencing the cotton genome. Once the D genome is assembled from all of this raw material, it will undoubtedly assist in the assembly of the AD genomes of cultivated varieties of cotton, but a lot of hard work remains. The earliest evidence of cotton use in South Asia has been found at the site of Mehrgarh, Pakistan, where cotton threads have been found preserved in copper beads; these finds have been dated to Neolithic (between 6000 and 5000 BCE). Cotton cultivation in the region is dated to the Indus Valley Civilization, which covered parts of modern eastern Pakistan and northwestern India between 3300 and 1300 BCE The Indus cotton industry was well-developed and some methods used in cotton spinning and fabrication continued to be used until the industrialization of India. Between 2000 and 1000 BC cotton became widespread across much of India. For example, it has been found at the site of Hallus in Karnataka dating from around 1000 BC. By the 1840s, India was no longer capable of supplying the vast quantities of cotton fibers needed by mechanized British factories, while shipping bulky, low-price cotton from India to Britain was time-consuming and expensive. This, coupled with the emergence of American cotton as a superior type (due to the longer, stronger fibers of the two domesticated native American species, Gossypium hirsutum and Gossypium barbadense), encouraged British traders to purchase cotton from plantations in the United States and plantations in the Caribbean. By the mid-19th century, ""King Cotton"" had become the backbone of the southern American economy. In the United States, cultivating and harvesting cotton became the leading occupation of slaves. Cotton remained a key crop in the Southern economy after emancipation and the end of the Civil War in 1865. Across the South, sharecropping evolved, in which landless black and white farmers worked land owned by others in return for a share of the profits. Some farmers rented the land and bore the production costs themselves. Until mechanical cotton pickers were developed, cotton farmers needed additional labor to hand-pick cotton. Picking cotton was a source of income for families across the South. Rural and small town school systems had split vacations so children could work in the fields during ""cotton-picking."" Cotton is used to make a number of textile products. These include terrycloth for highly absorbent bath towels and robes; denim for blue jeans; cambric, popularly used in the manufacture of blue work shirts (from which we get the term ""blue-collar""); and corduroy, seersucker, and cotton twill. Socks, underwear, and most T-shirts are made from cotton. Bed sheets often are made from cotton. Cotton also is used to make yarn used in crochet and knitting. Fabric also can be made from recycled or recovered cotton that otherwise would be thrown away during the spinning, weaving, or cutting process. While many fabrics are made completely of cotton, some materials blend cotton with other fibers, including rayon and synthetic fibers such as polyester. It can either be used in knitted or woven fabrics, as it can be blended with elastine to make a stretchier thread for knitted fabrics, and apparel such as stretch jeans. The largest producers of cotton, currently (2009), are China and India, with annual production of about 34 million bales and 33.4 million bales, respectively; most of this production is consumed by their respective textile industries. The largest exporters of raw cotton are the United States, with sales of $4.9 billion, and Africa, with sales of $2.1 billion. The total international trade is estimated to be $12 billion. Africa's share of the cotton trade has doubled since 1980. Neither area has a significant domestic textile industry, textile manufacturing having moved to developing nations in Eastern and South Asia such as India and China. In Africa, cotton is grown by numerous small holders. Dunavant Enterprises, based in Memphis, Tennessee, is the leading cotton broker in Africa, with hundreds of purchasing agents. It operates cotton gins in Uganda, Mozambique, and Zambia. In Zambia, it often offers loans for seed and expenses to the 180,000 small farmers who grow cotton for it, as well as advice on farming methods. Cargill also purchases cotton in Africa for export. Cotton linters are fine, silky fibers which adhere to the seeds of the cotton plant after ginning. These curly fibers typically are less than 1⁄8 inch (3.2 mm) long. The term also may apply to the longer textile fiber staple lint as well as the shorter fuzzy fibers from some upland species. Linters are traditionally used in the manufacture of paper and as a raw material in the manufacture of cellulose. In the UK, linters are referred to as ""cotton wool"". This can also be a refined product (absorbent cotton in U.S. usage) which has medical, cosmetic and many other practical uses. The first medical use of cotton wool was by Sampson Gamgee at the Queen's Hospital (later the General Hospital) in Birmingham, England. The advent of the Industrial Revolution in Britain provided a great boost to cotton manufacture, as textiles emerged as Britain's leading export. In 1738, Lewis Paul and John Wyatt, of Birmingham, England, patented the roller spinning machine, as well as the flyer-and-bobbin system for drawing cotton to a more even thickness using two sets of rollers that traveled at different speeds. Later, the invention of the James Hargreaves' spinning jenny in 1764, Richard Arkwright's spinning frame in 1769 and Samuel Crompton's spinning mule in 1775 enabled British spinners to produce cotton yarn at much higher rates. From the late 18th century on, the British city of Manchester acquired the nickname ""Cottonopolis"" due to the cotton industry's omnipresence within the city, and Manchester's role as the heart of the global cotton trade. During the American Civil War, American cotton exports slumped due to a Union blockade on Southern ports, and also because of a strategic decision by the Confederate government to cut exports, hoping to force Britain to recognize the Confederacy or enter the war. This prompted the main purchasers of cotton, Britain and France, to turn to Egyptian cotton. British and French traders invested heavily in cotton plantations. The Egyptian government of Viceroy Isma'il took out substantial loans from European bankers and stock exchanges. After the American Civil War ended in 1865, British and French traders abandoned Egyptian cotton and returned to cheap American exports,[citation needed] sending Egypt into a deficit spiral that led to the country declaring bankruptcy in 1876, a key factor behind Egypt's occupation by the British Empire in 1882. Genetically modified (GM) cotton was developed to reduce the heavy reliance on pesticides. The bacterium Bacillus thuringiensis (Bt) naturally produces a chemical harmful only to a small fraction of insects, most notably the larvae of moths and butterflies, beetles, and flies, and harmless to other forms of life. The gene coding for Bt toxin has been inserted into cotton, causing cotton, called Bt cotton, to produce this natural insecticide in its tissues. In many regions, the main pests in commercial cotton are lepidopteran larvae, which are killed by the Bt protein in the transgenic cotton they eat. This eliminates the need to use large amounts of broad-spectrum insecticides to kill lepidopteran pests (some of which have developed pyrethroid resistance). This spares natural insect predators in the farm ecology and further contributes to noninsecticide pest management. Cotton lisle is a finely-spun, tightly twisted type of cotton that is noted for being strong and durable. Lisle is composed of two strands that have each been twisted an extra twist per inch than ordinary yarns and combined to create a single thread. The yarn is spun so that it is compact and solid. This cotton is used mainly for underwear, stockings, and gloves. Colors applied to this yarn are noted for being more brilliant than colors applied to softer yarn. This type of thread was first made in the city of Lisle, France (now Lille), hence its name."
States_of_Germany,"The Basic Law of the Federal Republic of Germany, the federal constitution, stipulates that the structure of each Federal State's government must ""conform to the principles of republican, democratic, and social government, based on the rule of law"" (Article 28). Most of the states are governed by a cabinet led by a Ministerpräsident (Minister-President), together with a unicameral legislative body known as the Landtag (State Diet). The states are parliamentary republics and the relationship between their legislative and executive branches mirrors that of the federal system: the legislatures are popularly elected for four or five years (depending on the state), and the Minister-President is then chosen by a majority vote among the Landtag's members. The Minister-President appoints a cabinet to run the state's agencies and to carry out the executive duties of the state's government. Municipalities (Gemeinden): Every rural district and every Amt is subdivided into municipalities, while every urban district is a municipality in its own right. There are (as of 6 March 2009[update]) 12,141 municipalities, which are the smallest administrative units in Germany. Cities and towns are municipalities as well, also having city rights or town rights (Stadtrechte). Nowadays, this is mostly just the right to be called a city or town. However, in former times there were many other privileges, including the right to impose local taxes or to allow industry only within city limits. Later, the constitution was amended to state that the citizens of the 16 states had successfully achieved the unity of Germany in free self-determination and that the Basic Law thus applied to the entire German people. Article 23, which had allowed ""any other parts of Germany"" to join, was rephrased. It had been used in 1957 to reintegrate the Saar Protectorate as the Saarland into the Federal Republic, and this was used as a model for German reunification in 1990. The amended article now defines the participation of the Federal Council and the 16 German states in matters concerning the European Union. During the Allied occupation of Germany after World War II, internal borders were redrawn by the Allied military governments. No single state comprised more than 30% of either population or territory; this was intended to prevent any one state from being as dominant within Germany as Prussia had been in the past. Initially, only seven of the pre-War states remained: Baden (in part), Bavaria (reduced in size), Bremen, Hamburg, Hesse (enlarged), Saxony, and Thuringia. The states with hyphenated names, such as Rhineland-Palatinate, North Rhine-Westphalia, and Saxony-Anhalt, owed their existence to the occupation powers and were created out of mergers of former Prussian provinces and smaller states. Former German territory that lie east of the Oder-Neisse Line fell under either Polish or Soviet administration but attempts were made at least symbolically not to abandon sovereignty well into the 1960s. However, no attempts were made to establish new states in these territories as they lay outside the jurisdiction of West Germany at that time. The debate on a new delimitation of the German territory started in 1919 as part of discussions about the new constitution. Hugo Preuss, the father of the Weimar Constitution, drafted a plan to divide the German Reich into 14 roughly equal-sized states. His proposal was turned down due to opposition of the states and concerns of the government. Article 18 of the constitution enabled a new delimitation of the German territory but set high hurdles: Three fifth of the votes handed in, and at least the majority of the population are necessary to decide on the alteration of territory. In fact, until 1933 there were only four changes in the configuration of the German states: The 7 Thuringian states were merged in 1920, whereby Coburg opted for Bavaria, Pyrmont joined Prussia in 1922, and Waldeck did so in 1929. Any later plans to break up the dominating Prussia into smaller states failed because political circumstances were not favorable to state reforms. Federalism is one of the entrenched constitutional principles of Germany. According to the German constitution (called Grundgesetz or in English Basic Law), some topics, such as foreign affairs and defense, are the exclusive responsibility of the federation (i.e., the federal level), while others fall under the shared authority of the states and the federation; the states retain residual legislative authority for all other areas, including ""culture"", which in Germany includes not only topics such as financial promotion of arts and sciences, but also most forms of education and job training. Though international relations including international treaties are primarily the responsibility of the federal level, the constituent states have certain limited powers in this area: in matters that affect them directly, the states defend their interests at the federal level through the Bundesrat (literally Federal Council, the upper house of the German Federal Parliament) and in areas where they have legislative authority they have limited powers to conclude international treaties ""with the consent of the federal government"". Paragraph 6 of Article 29 stated that if a petition was successful a referendum should be held within three years. Since the deadline passed on 5 May 1958 without anything happening the Hesse state government filed a constitutional complaint with the Federal Constitutional Court in October 1958. The complaint was dismissed in July 1961 on the grounds that Article 29 had made the new delimitation of the federal territory an exclusively federal matter. At the same time, the Court reaffirmed the requirement for a territorial revision as a binding order to the relevant constitutional bodies. Germany is a federal republic consisting of sixteen federal states (German: Bundesland, or Land).[a] Since today's Germany was formed from an earlier collection of several states, it has a federal constitution, and the constituent states retain a measure of sovereignty. With an emphasis on geographical conditions, Berlin and Hamburg are frequently called Stadtstaaten (city-states), as is the Free Hanseatic City of Bremen, which in fact includes the cities of Bremen and Bremerhaven. The remaining 13 states are called Flächenländer (literally: area states). In the Paris Agreements of 23 October 1954, France offered to establish an independent ""Saarland"", under the auspices of the Western European Union (WEU), but on 23 October 1955 in the Saar Statute referendum the Saar electorate rejected this plan by 67.7% to 32.3% (out of a 96.5% turnout: 423,434 against, 201,975 for) despite the public support of Federal German Chancellor Konrad Adenauer for the plan. The rejection of the plan by the Saarlanders was interpreted as support for the Saar to join the Federal Republic of Germany. Federalism has a long tradition in German history. The Holy Roman Empire comprised many petty states numbering more than 300 around 1796. The number of territories was greatly reduced during the Napoleonic Wars (1796–1814). After the Congress of Vienna (1815), 39 states formed the German Confederation. The Confederation was dissolved after the Austro-Prussian War and replaced by a North German Federation under Prussian hegemony; this war left Prussia dominant in Germany, and German nationalism would compel the remaining independent states to ally with Prussia in the Franco-Prussian War of 1870–71, and then to accede to the crowning of King Wilhelm of Prussia as German Emperor. The new German Empire included 25 states (three of them, Hanseatic cities) and the imperial territory of Alsace-Lorraine. The empire was dominated by Prussia, which controlled 65% of the territory and 62% of the population. After the territorial losses of the Treaty of Versailles, the remaining states continued as republics of a new German federation. These states were gradually de facto abolished and reduced to provinces under the Nazi regime via the Gleichschaltung process, as the states administratively were largely superseded by the Nazi Gau system. The municipalities have two major policy responsibilities. First, they administer programs authorized by the federal or state government. Such programs typically relate to youth, schools, public health, and social assistance. Second, Article 28(2) of the Basic Law guarantees the municipalities ""the right to regulate on their own responsibility all the affairs of the local community within the limits set by law."" Under this broad statement of competence, local governments can justify a wide range of activities. For instance, many municipalities develop and expand the economic infrastructure of their communities through the development of industrial trading estates. The Districts of Germany (Kreise) are administrative districts, and every state except the city-states of Berlin, Hamburg, and Bremen consists of ""rural districts"" (Landkreise), District-free Towns/Cities (Kreisfreie Städte, in Baden-Württemberg also called ""urban districts"", or Stadtkreise), cities that are districts in their own right, or local associations of a special kind (Kommunalverbände besonderer Art), see below. The state Free Hanseatic City of Bremen consists of two urban districts, while Berlin and Hamburg are states and urban districts at the same time. Local associations of a special kind are an amalgamation of one or more Landkreise with one or more Kreisfreie Städte to form a replacement of the aforementioned administrative entities at the district level. They are intended to implement simplification of administration at that level. Typically, a district-free city or town and its urban hinterland are grouped into such an association, or Kommunalverband besonderer Art. Such an organization requires the issuing of special laws by the governing state, since they are not covered by the normal administrative structure of the respective states. In 1952, following a referendum, Baden, Württemberg-Baden, and Württemberg-Hohenzollern merged into Baden-Württemberg. In 1957, the Saar Protectorate rejoined the Federal Republic as the Saarland. German reunification in 1990, in which the German Democratic Republic (East Germany) ascended into the Federal Republic, resulted in the addition of the re-established eastern states of Brandenburg, Mecklenburg-West Pomerania (in German Mecklenburg-Vorpommern), Saxony (Sachsen), Saxony-Anhalt (Sachsen-Anhalt), and Thuringia (Thüringen), as well as the reunification of West and East Berlin into Berlin and its establishment as a full and equal state. A regional referendum in 1996 to merge Berlin with surrounding Brandenburg as ""Berlin-Brandenburg"" failed to reach the necessary majority vote in Brandenburg, while a majority of Berliners voted in favour of the merger. A new delimitation of the federal territory has been discussed since the Federal Republic was founded in 1949 and even before. Committees and expert commissions advocated a reduction of the number of states; academics (Rutz, Miegel, Ottnad etc.) and politicians (Döring, Apel, and others) made proposals –  some of them far-reaching –  for redrawing boundaries but hardly anything came of these public discussions. Territorial reform is sometimes propagated by the richer states as a means to avoid or reduce fiscal transfers. In southwestern Germany, territorial revision seemed to be a top priority since the border between the French and American occupation zones was set along the Autobahn Karlsruhe-Stuttgart-Ulm (today the A8). Article 118 stated ""The division of the territory comprising Baden, Württemberg-Baden and Württemberg-Hohenzollern into Länder may be revised, without regard to the provisions of Article 29, by agreement between the Länder concerned. If no agreement is reached, the revision shall be effected by a federal law, which shall provide for an advisory referendum."" Since no agreement was reached, a referendum was held on 9 December 1951 in four different voting districts, three of which approved the merger (South Baden refused but was overruled as the result of total votes was decisive). On 25 April 1952, the three former states merged to form Baden-Württemberg. Upon its founding in 1949, West Germany had eleven states. These were reduced to nine in 1952 when three south-western states (South Baden, Württemberg-Hohenzollern, and Württemberg-Baden) merged to form Baden-Württemberg. From 1957, when the French-occupied Saar Protectorate was returned and formed into the Saarland, the Federal Republic consisted of ten states, which are referred to as the ""Old States"" today. West Berlin was under the sovereignty of the Western Allies and neither a Western German state nor part of one. However, it was in many ways de facto integrated with West Germany under a special status. A new delimitation of the federal territory keeps being debated in Germany, though ""Some scholars note that there are significant differences among the American states and regional governments in other federations without serious calls for territorial changes ..."", as political scientist Arthur B. Gunlicks remarks. He summarizes the main arguments for boundary reform in Germany: ""... the German system of dual federalism requires strong Länder that have the administrative and fiscal capacity to implement legislation and pay for it from own source revenues. Too many Länder also make coordination among them and with the federation more complicated ..."". But several proposals have failed so far; territorial reform remains a controversial topic in German politics and public perception. The creation of the Federal Republic of Germany in 1949 was through the unification of the western states (which were previously under American, British, and French administration) created in the aftermath of World War II. Initially, in 1949, the states of the Federal Republic were Baden, Bavaria (in German: Bayern), Bremen, Hamburg, Hesse (Hessen), Lower Saxony (Niedersachsen), North Rhine Westphalia (Nordrhein-Westfalen), Rhineland-Palatinate (Rheinland-Pfalz), Schleswig-Holstein, Württemberg-Baden, and Württemberg-Hohenzollern. West Berlin, while officially not part of the Federal Republic, was largely integrated and considered as a de facto state. The governments in Berlin, Bremen and Hamburg are designated by the term Senate. In the three free states of Bavaria, Saxony, and Thuringia the government is referred to as the State Government (Staatsregierung), and in the other ten states the term Land Government (Landesregierung) is used. Before January 1, 2000, Bavaria had a bicameral parliament, with a popularly elected Landtag, and a Senate made up of representatives of the state's major social and economic groups. The Senate was abolished following a referendum in 1998. The states of Berlin, Bremen, and Hamburg are governed slightly differently from the other states. In each of those cities, the executive branch consists of a Senate of approximately eight, selected by the state's parliament; the senators carry out duties equivalent to those of the ministers in the larger states. The equivalent of the Minister-President is the Senatspräsident (President of the Senate) in Bremen, the Erster Bürgermeister (First Mayor) in Hamburg, and the Regierender Bürgermeister (Governing Mayor) in Berlin. The parliament for Berlin is called the Abgeordnetenhaus (House of Representatives), while Bremen and Hamburg both have a Bürgerschaft. The parliaments in the remaining 13 states are referred to as Landtag (State Parliament). The use of the term Länder (Lands) dates back to the Weimar Constitution of 1919. Before this time, the constituent states of the German Empire were called Staaten (States). Today, it is very common to use the term Bundesland (Federal Land). However, this term is not used officially, neither by the constitution of 1919 nor by the Basic Law (Constitution) of 1949. Three Länder call themselves Freistaaten (Free States, which is the old-fashioned German expression for Republic), Bavaria (since 1919), Saxony (originally since 1919 and again since 1990), and Thuringia (since 1994). There is little continuity between the current states and their predecessors of the Weimar Republic with the exception of the three free states, and the two city-states of Hamburg and Bremen. After the Nazi Party seized power in January 1933, the Länder increasingly lost importance. They became administrative regions of a centralised country. Three changes are of particular note: on January 1, 1934, Mecklenburg-Schwerin was united with the neighbouring Mecklenburg-Strelitz; and, by the Greater Hamburg Act (Groß-Hamburg-Gesetz), from April 1, 1937, the area of the city-state was extended, while Lübeck lost its independence and became part of the Prussian province of Schleswig-Holstein. As the premiers did not come to an agreement on this question, the Parliamentary Council was supposed to address this issue. Its provisions are reflected in Article 29. There was a binding provision for a new delimitation of the federal territory: the Federal Territory must be revised ... (paragraph 1). Moreover, in territories or parts of territories whose affiliation with a Land had changed after 8 May 1945 without a referendum, people were allowed to petition for a revision of the current status within a year after the promulgation of the Basic Law (paragraph 2). If at least one tenth of those entitled to vote in Bundestag elections were in favour of a revision, the federal government had to include the proposal into its legislation. Then a referendum was required in each territory or part of a territory whose affiliation was to be changed (paragraph 3). The proposal should not take effect if within any of the affected territories a majority rejected the change. In this case, the bill had to be introduced again and after passing had to be confirmed by referendum in the Federal Republic as a whole (paragraph 4). The reorganization should be completed within three years after the Basic Law had come into force (paragraph 6). In his investiture address, given on 28 October 1969 in Bonn, Chancellor Willy Brandt proposed that the government would consider Article 29 of the Basic Law as a binding order. An expert commission was established, named after its chairman, the former Secretary of State Professor Werner Ernst. After two years of work, the experts delivered their report in 1973. It provided an alternative proposal for both northern Germany and central and southwestern Germany. In the north, either a single new state consisting of Schleswig-Holstein, Hamburg, Bremen and Lower Saxony should be created (solution A) or two new states, one in the northeast consisting of Schleswig-Holstein, Hamburg and the northern part of Lower Saxony (from Cuxhaven to Lüchow-Dannenberg) and one in the northwest consisting of Bremen and the rest of Lower Saxony (solution B). In the Center and South West either Rhineland-Palatinate (with the exception of the Germersheim district but including the Rhine-Neckar region) should be merged with Hesse and the Saarland (solution C), the district of Germersheim would then become part of Baden-Württemberg."
Gramophone_record,"Electric recording which developed during the time that early radio was becoming popular (1925) benefited from the microphones and amplifiers used in radio studios. The early electric recordings were reminiscent tonally of acoustic recordings, except there was more recorded bass and treble as well as delicate sounds and overtones cut on the records. This was in spite of some carbon microphones used, which had resonances that colored the recorded tone. The double button carbon microphone with stretched diaphragm was a marked improvement. Alternatively, the Wente style condenser microphone used with the Western Electric licensed recording method had a brilliant midrange and was prone to overloading from sibilants in speech, but generally it gave more accurate reproduction than carbon microphones. In the 1930s, record companies began issuing collections of 78 rpm records by one performer or of one type of music in specially assembled albums, typically with artwork on the front cover and liner notes on the back or inside cover. Most albums included three or four records, with two sides each, making six or eight tunes per album. When the 12-inch vinyl LP era began in 1949, the single record often had the same or similar number of tunes as a typical album of 78s, and was still often referred to as an ""album"". A further limitation of the gramophone record is that fidelity steadily declines as playback progresses; there is more vinyl per second available for fine reproduction of high frequencies at the large-diameter beginning of the groove than exist at the smaller-diameters close to the end of the side. At the start of a groove on an LP there are 510 mm of vinyl per second traveling past the stylus while the ending of the groove gives 200–210 mm of vinyl per second — less than half the linear resolution. Distortion towards the end of the side is likely to become more apparent as record wear increases.* The newly invented Western Electric moving coil or dynamic microphone was part of the Wide Range System. It had a flatter audio response than the old style Wente condenser type and didn't require electronics installed in the microphone housing. Signals fed to the cutting head were pre-emphasized in the treble region to help override noise in playback. Groove cuts in the vertical plane were employed rather than the usual lateral cuts. The chief advantage claimed was more grooves per inch that could be crowded together, resulting in longer playback time. Additionally, the problem of inner groove distortion, which plagued lateral cuts, could be avoided with the vertical cut system. Wax masters were made by flowing heated wax over a hot metal disc thus avoiding the microscopic irregularities of cast blocks of wax and the necessity of planing and polishing. From the mid-1950s through the 1960s, in the U.S. the common home record player or ""stereo"" (after the introduction of stereo recording) would typically have had these features: a three- or four-speed player (78, 45, 33 1⁄3, and sometimes 16 2⁄3 rpm); with changer, a tall spindle that would hold several records and automatically drop a new record on top of the previous one when it had finished playing, a combination cartridge with both 78 and microgroove styli and a way to flip between the two; and some kind of adapter for playing the 45s with their larger center hole. The adapter could be a small solid circle that fit onto the bottom of the spindle (meaning only one 45 could be played at a time) or a larger adaptor that fit over the entire spindle, permitting a stack of 45s to be played. New or ""virgin"" heavy/heavyweight (180–220 g) vinyl is commonly used for modern audiophile vinyl releases in all genres. Many collectors prefer to have heavyweight vinyl albums, which have been reported to have better sound than normal vinyl because of their higher tolerance against deformation caused by normal play. 180 g vinyl is more expensive to produce only because it uses more vinyl. Manufacturing processes are identical regardless of weight. In fact, pressing lightweight records requires more care. An exception is the propensity of 200 g pressings to be slightly more prone to non-fill, when the vinyl biscuit does not sufficiently fill a deep groove during pressing (percussion or vocal amplitude changes are the usual locations of these artifacts). This flaw causes a grinding or scratching sound at the non-fill point. German record company Odeon is often said to have pioneered the album in 1909 when it released the Nutcracker Suite by Tchaikovsky on 4 double-sided discs in a specially designed package. (It is not indicated what size the records are.) However, Deutsche Grammophon had produced an album for its complete recording of the opera Carmen in the previous year. The practice of issuing albums does not seem to have been widely taken up by other record companies for many years; however, HMV provided an album, with a pictorial cover, for the 1917 recording of The Mikado (Gilbert & Sullivan). In some ways similar to the laser turntable is the IRENE scanning machine for disc records, which images with microphotography in two dimensions, invented by a team of physicists at Lawrence Berkeley Laboratories. IRENE will retrieve the information from a laterally modulated monaural grooved sound source without touching the medium itself, but cannot read vertically modulated information. This excludes grooved recordings such as cylinders and some radio transcriptions that feature a hill-and-dale format of recording, and stereophonic or quadraphonic grooved recordings, which utilize a combination of the two as well as supersonic encoding for quadraphonic. Where old disc recordings are considered to be of artistic or historic interest, from before the era of tape or where no tape master exists, archivists play back the disc on suitable equipment and record the result, typically onto a digital format, which can be copied and manipulated to remove analog flaws without any further damage to the source recording. For example, Nimbus Records uses a specially built horn record player to transfer 78s. Anyone can do this using a standard record player with a suitable pickup, a phono-preamp (pre-amplifier) and a typical personal computer. However, for accurate transfer, professional archivists carefully choose the correct stylus shape and diameter, tracking weight, equalisation curve and other playback parameters and use high-quality analogue-to-digital converters. The complete technical disclosure of the Columbia LP by Peter C. Goldmark, Rene' Snepvangers and William S. Bachman in 1949 made it possible for a great variety of record companies to get into the business of making long playing records. The business grew quickly and interest spread in high fidelity sound and the do-it-yourself market for pickups, turntables, amplifier kits, loudspeaker enclosure plans, and AM/FM radio tuners. The LP record for longer works, 45 rpm for pop music, and FM radio became high fidelity program sources in demand. Radio listeners heard recordings broadcast and this in turn generated more record sales. The industry flourished. In 1877, Thomas Edison invented the phonograph. Unlike the phonautograph, it was capable of both recording and reproducing sound. Despite the similarity of name, there is no documentary evidence that Edison's phonograph was based on Scott's phonautograph. Edison first tried recording sound on a wax-impregnated paper tape, with the idea of creating a ""telephone repeater"" analogous to the telegraph repeater he had been working on. Although the visible results made him confident that sound could be physically recorded and reproduced, his notes do not indicate that he actually reproduced sound before his first experiment in which he used tinfoil as a recording medium several months later. The tinfoil was wrapped around a grooved metal cylinder and a sound-vibrated stylus indented the tinfoil while the cylinder was rotated. The recording could be played back immediately. The Scientific American article that introduced the tinfoil phonograph to the public mentioned Marey, Rosapelly and Barlow as well as Scott as creators of devices for recording but, importantly, not reproducing sound. Edison also invented variations of the phonograph that used tape and disc formats. Numerous applications for the phonograph were envisioned, but although it enjoyed a brief vogue as a startling novelty at public demonstrations, the tinfoil phonograph proved too crude to be put to any practical use. A decade later, Edison developed a greatly improved phonograph that used a hollow wax cylinder instead of a foil sheet. This proved to be both a better-sounding and far more useful and durable device. The wax phonograph cylinder created the recorded sound market at the end of the 1880s and dominated it through the early years of the 20th century. Stereophonic sound recording, which attempts to provide a more natural listening experience by reproducing the spatial locations of sound sources in the horizontal plane, was the natural extension to monophonic recording, and attracted various alternative engineering attempts. The ultimately dominant ""45/45"" stereophonic record system was invented by Alan Blumlein of EMI in 1931 and patented the same year. EMI cut the first stereo test discs using the system in 1933 (see Bell Labs Stereo Experiments of 1933) although the system was not exploited commercially until much later. The 45 rpm discs also came in a variety known as extended play (EP), which achieved up to 10–15 minutes play at the expense of attenuating (and possibly compressing) the sound to reduce the width required by the groove. EP discs were cheaper to produce, and were used in cases where unit sales were likely to be more limited or to reissue LP albums on the smaller format for those people who had only 45 rpm players. LP albums could be purchased 1 EP at a time, with four items per EP, or in a boxed set with 3 EPs or 12 items. The large center hole on 45s allows for easier handling by jukebox mechanisms. EPs were generally discontinued by the late 1950s in the U.S. as three- and four-speed record players replaced the individual 45 players. One indication of the decline of the 45 rpm EP is that the last Columbia Records reissue of Frank Sinatra songs on 45 rpm EP records, called Frank Sinatra (Columbia B-2641) was issued on December 7, 1959. The EP lasted considerably longer in Europe, and was a popular format during the 1960s for recordings by artists such as Serge Gainsbourg and the Beatles. There were important quality advances in recordings specifically made for radio broadcast. In the early 1930s Bell Telephone Laboratories and Western Electric announced the total reinvention of disc recording: the Western Electric Wide Range System, ""The New Voice of Action"". The intent of the new Western Electric system was to improve the overall quality of disc recording and playback. The recording speed was 33 1⁄3 rpm, originally used in the Western Electric/ERPI movie audio disc system implemented in the early Warner Brothers' Vitaphone ""talkies"" of 1927. Ultimately, the New Orthophonic curve was disclosed in a publication by R.C. Moyer of RCA Victor in 1953. He traced RCA Victor characteristics back to the Western Electric ""rubber line"" recorder in 1925 up to the early 1950s laying claim to long-held recording practices and reasons for major changes in the intervening years. The RCA Victor New Orthophonic curve was within the tolerances for the NAB/NARTB, Columbia LP, and AES curves. It eventually became the technical predecessor to the RIAA curve. Contrary to popular belief, if placed properly and prepared-for, drums could be effectively used and heard on even the earliest jazz and military band recordings. The loudest instruments such as the drums and trumpets were positioned the farthest away from the collecting horn. Lillian Hardin Armstrong, a member of King Oliver's Creole Jazz Band, which recorded at Gennett Records in 1923, remembered that at first Oliver and his young second trumpet, Louis Armstrong, stood next to each other and Oliver's horn could not be heard. ""They put Louis about fifteen feet over in the corner, looking all sad."" For fading instrumental parts in and out while recording, some performers were placed on a moveable platform, which could draw the performer(s) nearer or further away as required.[citation needed] Lateral-cut disc records were developed in the United States by Emile Berliner, who named his system the ""gramophone"", distinguishing it from Edison's wax cylinder ""phonograph"" and Columbia's wax cylinder ""graphophone"". Berliner's earliest discs, first marketed in 1889, but only in Europe, were 5 inches (13 cm) in diameter, and were played with a small hand-propelled machine. Both the records and the machine were adequate only for use as a toy or curiosity, due to the limited sound quality. In the United States in 1894, under the Berliner Gramophone trademark, Berliner started marketing records with somewhat more substantial entertainment value, along with somewhat more substantial gramophones to play them. Berliner's records had poor sound quality compared to wax cylinders, but his manufacturing associate Eldridge R. Johnson eventually improved the sound quality. Abandoning Berliner's ""Gramophone"" trademark for legal reasons, in 1901 Johnson's and Berliner's separate companies reorganized to form the Victor Talking Machine Company, whose products would come to dominate the market for many years. Emile Berliner moved his company to Montreal in 1900. The factory which became RCA Victor stills exists. There is a dedicated museum in Montreal for Berliner. The development of quadraphonic records was announced in 1971. These recorded four separate sound signals. This was achieved on the two stereo channels by electronic matrixing, where the additional channels were combined into the main signal. When the records were played, phase-detection circuits in the amplifiers were able to decode the signals into four separate channels. There were two main systems of matrixed quadraphonic records produced, confusingly named SQ (by CBS) and QS (by Sansui). They proved commercially unsuccessful, but were an important precursor to later surround-sound systems, as seen in SACD and home cinema today. Beginning in 1939, Dr. Peter Goldmark and his staff at Columbia Records and at CBS Laboratories undertook efforts to address problems of recording and playing back narrow grooves and developing an inexpensive, reliable consumer playback system. It took about eight years of study, except when it was suspended because of World War II. Finally, the 12-inch (30 cm) Long Play (LP) 33 1⁄3 rpm microgroove record album was introduced by the Columbia Record Company at a New York press conference on June 18, 1948. Eventually the 12-inch (300 mm) 33 1⁄3 rpm LP prevailed as the predominant format for musical albums, and 10-inch LPs were no longer issued. The last Columbia Records reissue of any Frank Sinatra songs on a 10-inch LP record was an album called Hall of Fame, CL 2600, issued on October 26, 1956, containing six songs, one each by Tony Bennett, Rosemary Clooney, Johnnie Ray, Frank Sinatra, Doris Day, and Frankie Laine. The 10-inch LP however had a longer life in the United Kingdom, where important early British rock and roll albums such as Lonnie Donegan's Lonnie Donegan Showcase and Billy Fury's The Sound of Fury were released in that form. The 7-inch (175 mm) 45 rpm disc or ""single"" established a significant niche for shorter duration discs, typically containing one item on each side. The 45 rpm discs typically emulated the playing time of the former 78 rpm discs, while the 12-inch LP discs eventually provided up to one half-hour of recorded material per side. Over the years a variety of record equalization practices emerged and there was no industry standard. For example, in Europe recordings for years required playback with a bass turnover setting of 250–300 Hz and a treble roll-off at 10,000 Hz ranging from 0 to −5 dB or more. In the US there were more varied practices and a tendency to use higher bass turnover frequencies such as 500 Hz as well as a greater treble rolloff like −8.5 dB and even more to record generally higher modulation levels on the record. Tonearm skating forces and other perturbations are also picked up by the stylus. This is a form of frequency multiplexing as the control signal (restoring force) used to keep the stylus in the groove is carried by the same mechanism as the sound itself. Subsonic frequencies below about 20 Hz in the audio signal are dominated by tracking effects, which is one form of unwanted rumble (""tracking noise"") and merges with audible frequencies in the deep bass range up to about 100 Hz. High fidelity sound equipment can reproduce tracking noise and rumble. During a quiet passage, woofer speaker cones can sometimes be seen to vibrate with the subsonic tracking of the stylus, at frequencies as low as just above 0.5 Hz (the frequency at which a 33 1⁄3 rpm record turns on the turntable; 5⁄9 Hz exactly on an ideal turntable). Another reason for very low frequency material can be a warped disk: its undulations produce frequencies of only a few hertz and present day amplifiers have large power bandwidths. For this reason, many stereo receivers contained a switchable subsonic filter. Some subsonic content is directly out of phase in each channel. If played back on a mono subwoofer system, the noise will cancel, significantly reducing the amount of rumble that is reproduced. ELPJ, a Japanese-based company, sells a laser turntable that uses a laser to read vinyl discs optically, without physical contact. The laser turntable eliminates record wear and the possibility of accidental scratches, which degrade the sound, but its expense limits use primarily to digital archiving of analog records, and the laser does not play back colored vinyl or picture discs. Various other laser-based turntables were tried during the 1990s, but while a laser reads the groove very accurately, since it does not touch the record, the dust that vinyl attracts due to static electric charge is not mechanically pushed out of the groove, worsening sound quality in casual use compared to conventional stylus playback. Also in the late 1970s, ""direct-to-disc"" records were produced, aimed at an audiophile niche market. These completely bypassed the use of magnetic tape in favor of a ""purist"" transcription directly to the master lacquer disc. Also during this period, half-speed mastered and ""original master"" records were released, using expensive state-of-the-art technology. A further late 1970s development was the Disco Eye-Cued system used mainly on Motown 12-inch singles released between 1978 and 1980. The introduction, drum-breaks, or choruses of a track were indicated by widely separated grooves, giving a visual cue to DJs mixing the records. The appearance of these records is similar to an LP, but they only contain one track each side. Many electronic dance music and hip hop releases today are still preferred on vinyl; however, digital copies are still widely available. This is because for disc jockeys (""DJs""), vinyl has an advantage over the CD: direct manipulation of the medium. DJ techniques such as slip-cueing, beatmatching, and scratching originated on turntables. With CDs or compact audio cassettes one normally has only indirect manipulation options, e.g., the play, stop, and pause buttons. With a record one can place the stylus a few grooves farther in or out, accelerate or decelerate the turntable, or even reverse its direction, provided the stylus, record player, and record itself are built to withstand it. However, many CDJ and DJ advances, such as DJ software and time-encoded vinyl, now have these capabilities and more. In 1931, RCA Victor launched the first commercially available vinyl long-playing record, marketed as program-transcription discs. These revolutionary discs were designed for playback at 33 1⁄3 rpm and pressed on a 30 cm diameter flexible plastic disc, with a duration of about ten minutes playing time per side. RCA Victor's early introduction of a long-play disc was a commercial failure for several reasons including the lack of affordable, reliable consumer playback equipment and consumer wariness during the Great Depression. Because of financial hardships that plagued the recording industry during that period (and RCA's own parched revenues), Victor's long-playing records were discontinued by early 1933. In January 1938, Milt Gabler started recording for his new label, Commodore Records, and to allow for longer continuous performances, he recorded some 12-inch records. Eddie Condon explained: ""Gabler realized that a jam session needs room for development."" The first two 12-inch recordings did not take advantage of the extra length: ""Carnegie Drag"" was 3:15; ""Carnegie Jump"", 2:41. But at the second session, on April 30, the two 12-inch recordings were longer: ""Embraceable You"" was 4:05; ""Serenade to a Shylock"", 4:32. Another way around the time limitation was to issue a selection on both sides of a single record. Vaudeville stars Gallagher and Shean recorded ""Mr. Gallagher and Mr. Shean"", written by Irving and Jack Kaufman, as two sides of a 10-inch 78 in 1922 for Cameo. An obvious workaround for longer recordings was to release a set of records. An early multi-record release was in 1903, when HMV in England made the first complete recording of an opera, Verdi's Ernani, on 40 single-sided discs. In 1940, Commodore released Eddie Condon and his Band's recording of ""A Good Man Is Hard to Find"" in four parts, issued on both sides of two 12-inch 78s. This limitation on the duration of recordings persisted from 1910 until the invention of the LP record, in 1948. In popular music, this time limitation of about 3:30 on a 10-inch 78 rpm record meant that singers usually did not release long pieces on record. One exception is Frank Sinatra's recording of Rodgers and Hammerstein's ""Soliloquy"", from Carousel, made on May 28, 1946. Because it ran 7:57, longer than both sides of a standard 78 rpm 10-inch record, it was released on Columbia's Masterwork label (the classical division) as two sides of a 12-inch record. The same was true of John Raitt's performance of the song on the original cast album of Carousel, which had been issued on a 78-rpm album set by American Decca in 1945. Vinyl pressings were made with stampers from master cuts that were electroplated in vacuo by means of gold sputtering. Audio response was claimed out to 8,000 Hz, later 13,000 Hz, using light weight pickups employing jeweled styli. Amplifiers and cutters both using negative feedback were employed thereby improving the range of frequencies cut and lowering distortion levels. Radio transcription producers such as World Broadcasting System and Associated Music Publishers (AMP) were the dominant licensees of the Western Electric wide range system and towards the end of the 1930s were responsible for two-thirds of the total radio transcription business. These recordings use a bass turnover of 300 Hz and a 10,000 Hz rolloff of −8.5 dB. In 1931, RCA Victor introduced their vinyl-based Victrolac compound as a material for some unusual-format and special-purpose records. By the end of the 1930s vinyl's advantages of light weight, relative unbreakability and low surface noise had made it the material of choice for prerecorded radio programming and other critical applications. When it came to ordinary 78 rpm records, however, the much higher cost of the raw material, as well as its vulnerability to the heavy pickups and crudely mass-produced steel needles still commonly used in home record players, made its general substitution for shellac impractical at that time. During the Second World War, the United States Armed Forces produced thousands of 12-inch vinyl 78 rpm V-Discs for use by the troops overseas. After the war, the wider use of vinyl became more practical as new record players with relatively lightweight crystal pickups and precision-ground styli made of sapphire or an exotic osmium alloy proliferated. In late 1945, RCA Victor began offering special transparent red vinyl De Luxe pressings of some classical 78s, at a de luxe price. Later, Decca Records introduced vinyl Deccalite 78s, while other record companies came up with vinyl concoctions such as Metrolite, Merco Plastic and Sav-o-flex, but these were mainly used to produce ""unbreakable"" children's records and special thin vinyl DJ pressings for shipment to radio stations. The phonautograph, patented by Léon Scott in 1857, used a vibrating diaphragm and stylus to graphically record sound waves as tracings on sheets of paper, purely for visual analysis and without any intent of playing them back. In the 2000s, these tracings were first scanned by audio engineers and digitally converted into audible sound. Phonautograms of singing and speech made by Scott in 1860 were played back as sound for the first time in 2008. Along with a tuning fork tone and unintelligible snippets recorded as early as 1857, these are the earliest known recordings of sound. In 1901, 10-inch disc records were introduced, followed in 1903 by 12-inch records. These could play for more than three and four minutes respectively, while contemporary cylinders could only play for about two minutes. In an attempt to head off the disc advantage, Edison introduced the Amberol cylinder in 1909, with a maximum playing time of 4½ minutes (at 160 rpm), which in turn were superseded by Blue Amberol Records, which had a playing surface made of celluloid, a plastic, which was far less fragile. Despite these improvements, during the 1910s discs decisively won this early format war, although Edison continued to produce new Blue Amberol cylinders for an ever-dwindling customer base until late in 1929. By 1919 the basic patents for the manufacture of lateral-cut disc records had expired, opening the field for countless companies to produce them. Analog disc records would dominate the home entertainment market until they were outsold by the digital compact disc in the late 1980s (which was in turn supplanted by digital audio recordings distributed via online music stores and Internet file sharing). Over time, fidelity, dynamic and noise levels improved to the point that it was harder to tell the difference between a live performance in the studio and the recorded version. This was especially true after the invention of the variable reluctance magnetic pickup cartridge by General Electric in the 1940s when high quality cuts were played on well-designed audio systems. The Capehart radio/phonographs of the era with large diameter electrodynamic loudspeakers, though not ideal, demonstrated this quite well with ""home recordings"" readily available in the music stores for the public to buy. During the first half of the 1920s, engineers at Western Electric, as well as independent inventors such as Orlando Marsh, developed technology for capturing sound with a microphone, amplifying it with vacuum tubes, then using the amplified signal to drive an electromagnetic recording head. Western Electric's innovations resulted in a greatly expanded and more even frequency response, creating a dramatically fuller, clearer and more natural-sounding recording. Distant or less strong sounds that were impossible to record by the old methods could now be captured. Volume was now limited only by the groove spacing on the record and the limitations of the intended playback device. Victor and Columbia licensed the new electrical system from Western Electric and began issuing electrically recorded discs in 1925. The first classical recording was of Chopin impromptus and Schubert's Litanei by Alfred Cortot for Victor. For collectable or nostalgia purposes, or for the benefit of higher-quality audio playback provided by the 78 rpm speed with newer vinyl records and their lightweight stylus pickups, a small number of 78 rpm records have been released since the major labels ceased production. One of the first attempts at this was in the 1950s, when inventor Ewing Dunbar Nunn founded the label Audiophile Records, which released, in addition to standard 33 1/3 rpm LPs, 78 rpm-mastered albums that were microgroove and pressed on vinyl (as opposed to traditional 78s, with their shellac composition and wider 3-mil sized grooves). This was done by the label mainly to take advantage of the wider audio frequency response that faster speeds like 78 rpm can provide for vinyl microgroove records, hence the label's name (obviously catering to the audiophiles of the 1950s ""hi-fi"" era, when stereo gear could provide a much wider range of audio than before). Also in the late 1950s, Bell Records released a few budget-priced 7"" microgrooved records at 78 rpm. The older 78 format continued to be mass-produced alongside the newer formats using new materials until about 1960 in the U.S., and in a few countries, such as India (where some Beatles recordings were issued on 78), into the 1960s. For example, Columbia Records' last reissue of Frank Sinatra songs on 78 rpm records was an album called Young at Heart, issued November 1, 1954. As late as the 1970s, some children's records were released at the 78 rpm speed. In the United Kingdom, the 78 rpm single lasted longer than in the United States and the 45 rpm took longer to become popular. The 78 rpm was overtaken in popularity by the 45 rpm in the late 1950s, as teenagers became increasingly affluent. The normal commercial disc is engraved with two sound-bearing concentric spiral grooves, one on each side, running from the outside edge towards the center. The last part of the spiral meets an earlier part to form a circle. The sound is encoded by fine variations in the edges of the groove that cause a stylus (needle) placed in it to vibrate at acoustic frequencies when the disc is rotated at the correct speed. Generally, the outer and inner parts of the groove bear no intended sound (an exception is Split Enz's Mental Notes). Towards the center, at the end of the groove, there is another wide-pitched section known as the lead-out. At the very end of this section the groove joins itself to form a complete circle, called the lock groove; when the stylus reaches this point, it circles repeatedly until lifted from the record. On some recordings (for example Sgt. Pepper's Lonely Hearts Club Band by The Beatles, Super Trouper by Abba and Atom Heart Mother by Pink Floyd), the sound continues on the lock groove, which gives a strange repeating effect. Automatic turntables rely on the position or angular velocity of the arm, as it reaches the wider spacing in the groove, to trigger a mechanism that lifts the arm off the record. Precisely because of this mechanism, most automatic turntables are incapable of playing any audio in the lock groove, since they will lift the arm before it reaches that groove. The term ""high fidelity"" was coined in the 1920s by some manufacturers of radio receivers and phonographs to differentiate their better-sounding products claimed as providing ""perfect"" sound reproduction. The term began to be used by some audio engineers and consumers through the 1930s and 1940s. After 1949 a variety of improvements in recording and playback technologies, especially stereo recordings, which became widely available in 1958, gave a boost to the ""hi-fi"" classification of products, leading to sales of individual components for the home such as amplifiers, loudspeakers, phonographs, and tape players. High Fidelity and Audio were two magazines that hi-fi consumers and engineers could read for reviews of playback equipment and recordings. For the first several decades of disc record manufacturing, sound was recorded directly on to the ""master disc"" at the recording studio. From about 1950 on (earlier for some large record companies, later for some small ones) it became usual to have the performance first recorded on audio tape, which could then be processed and/or edited, and then dubbed on to the master disc. A record cutter would engrave the grooves into the master disc. Early versions of these master discs were soft wax, and later a harder lacquer was used. The mastering process was originally something of an art as the operator had to manually allow for the changes in sound which affected how wide the space for the groove needed to be on each rotation. In spite of their flaws, such as the lack of portability, records still have enthusiastic supporters. Vinyl records continue to be manufactured and sold today, especially by independent rock bands and labels, although record sales are considered to be a niche market composed of audiophiles, collectors, and DJs. Old records and out-of-print recordings in particular are in much demand by collectors the world over. (See Record collecting.) Many popular new albums are given releases on vinyl records and older albums are also given reissues, sometimes on audiophile-grade vinyl. Delicate sounds and fine overtones were mostly lost, because it took a lot of sound energy to vibrate the recording horn diaphragm and cutting mechanism. There were acoustic limitations due to mechanical resonances in both the recording and playback system. Some pictures of acoustic recording sessions show horns wrapped with tape to help mute these resonances. Even an acoustic recording played back electrically on modern equipment sounds like it was recorded through a horn, notwithstanding a reduction in distortion because of the modern playback. Toward the end of the acoustic era, there were many fine examples of recordings made with horns. The lateral cut NAB curve was remarkably similar to the NBC Orthacoustic curve that evolved from practices within the National Broadcasting Company since the mid-1930s. Empirically, and not by any formula, it was learned that the bass end of the audio spectrum below 100 Hz could be boosted somewhat to override system hum and turntable rumble noises. Likewise at the treble end beginning at 1,000 Hz, if audio frequencies were boosted by 16 dB at 10,000 Hz the delicate sibilant sounds of speech and high overtones of musical instruments could survive the noise level of cellulose acetate, lacquer/aluminum, and vinyl disc media. When the record was played back using a complementary inverse curve, signal-to-noise ratio was improved and the programming sounded more lifelike. Vinyl records do not break easily, but the soft material is easily scratched. Vinyl readily acquires a static charge, attracting dust that is difficult to remove completely. Dust and scratches cause audio clicks and pops. In extreme cases, they can cause the needle to skip over a series of grooves, or worse yet, cause the needle to skip backwards, creating a ""locked groove"" that repeats over and over. This is the origin of the phrase ""like a broken record"" or ""like a scratched record"", which is often used to describe a person or thing that continually repeats itself. Locked grooves are not uncommon and were even heard occasionally in radio broadcasts. There is a theory that vinyl records can audibly represent higher frequencies than compact discs. According to Red Book specifications, the compact disc has a frequency response of 20 Hz up to 22,050 Hz, and most CD players measure flat within a fraction of a decibel from at least 20 Hz to 20 kHz at full output. Turntable rumble obscures the low-end limit of vinyl but the upper end can be, with some cartridges, reasonably flat within a few decibels to 30 kHz, with gentle roll-off. Carrier signals of Quad LPs popular in the 1970s were at 30 kHz to be out of the range of human hearing. The average human auditory system is sensitive to frequencies from 20 Hz to a maximum of around 20,000 Hz. The upper and lower frequency limits of human hearing vary per person. Original master discs are created by lathe-cutting: a lathe is used to cut a modulated groove into a blank record. The blank records for cutting used to be cooked up, as needed, by the cutting engineer, using what Robert K. Morrison describes as a ""metallic soap,"" containing lead litharge, ozokerite, barium sulfate, montan wax, stearin and paraffin, among other ingredients. Cut ""wax"" sound discs would be placed in a vacuum chamber and gold-sputtered to make them electrically conductive for use as mandrels in an electroforming bath, where pressing stamper parts were made. Later, the French company Pyral invented a ready-made blank disc having a thin nitro-cellulose lacquer coating (approximately 7 mils thickness on both sides) that was applied to an aluminum substrate. Lacquer cuts result in an immediately playable, or processable, master record. If vinyl pressings are wanted, the still-unplayed sound disc is used as a mandrel for electroforming nickel records that are used for manufacturing pressing stampers. The electroformed nickel records are mechanically separated from their respective mandrels. This is done with relative ease because no actual ""plating"" of the mandrel occurs in the type of electrodeposition known as electroforming, unlike with electroplating, in which the adhesion of the new phase of metal is chemical and relatively permanent. The one-molecule-thick coating of silver (that was sprayed onto the processed lacquer sound disc in order to make its surface electrically conductive) reverse-plates onto the nickel record's face. This negative impression disc (having ridges in place of grooves) is known as a nickel master, ""matrix"" or ""father."" The ""father"" is then used as a mandrel to electroform a positive disc known as a ""mother"". Many mothers can be grown on a single ""father"" before ridges deteriorate beyond effective use. The ""mothers"" are then used as mandrels for electroforming more negative discs known as ""sons"". Each ""mother"" can be used to make many ""sons"" before deteriorating. The ""sons"" are then converted into ""stampers"" by center-punching a spindle hole (which was lost from the lacquer sound disc during initial electroforming of the ""father""), and by custom-forming the target pressing profile. This allows them to be placed in the dies of the target (make and model) record press and, by center-roughing, to facilitate the adhesion of the label, which gets stuck onto the vinyl pressing without any glue. In this way, several million vinyl discs can be produced from a single lacquer sound disc. When only a few hundred discs are required, instead of electroforming a ""son"" (for each side), the ""father"" is removed of its silver and converted into a stamper. Production by this latter method, known as the ""two-step-process"" (as it does not entail creation of ""sons"" but does involve creation of ""mothers,"" which are used for test playing and kept as ""safeties"" for electroforming future ""sons"") is limited to a few hundred vinyl pressings. The pressing count can increase if the stamper holds out and the quality of the vinyl is high. The ""sons"" made during a ""three-step"" electroforming make better stampers since they don't require silver removal (which reduces some high fidelity because of etching erasing part of the smallest groove modulations) and also because they have a stronger metal structure than ""fathers"". A gramophone record (phonograph record in American English) or vinyl record, commonly known as a ""record"", is an analogue sound storage medium in the form of a flat polyvinyl chloride (previously shellac) disc with an inscribed, modulated spiral groove. The groove usually starts near the periphery and ends near the center of the disc. Phonograph records are generally described by their diameter in inches (12"", 10"", 7""), the rotational speed in rpm at which they are played (16 2⁄3, 33 1⁄3, 45, 78), and their time capacity resulting from a combination of those parameters (LP – long playing 33 1⁄3 rpm, SP – 78 rpm single, EP – 12-inch single or extended play, 33 or 45 rpm); their reproductive quality or level of fidelity (high-fidelity, orthophonic, full-range, etc.), and the number of audio channels provided (mono, stereo, quad, etc.). A different format, CD-4 (not to be confused with compact disc), by RCA, encoded the front-rear difference information on an ultrasonic carrier, which required a special wideband cartridge to capture it on carefully calibrated pickup arm/turntable combinations. CD-4 was even less successful than the two matrixed formats. (A further problem was that no cutting heads were available that could handle the HF information. That was remedied by cutting at half the speed. Later, the special half-speed cutting heads and equalization techniques were employed to get a wider frequency response in stereo with reduced distortion and greater headroom.) When auto-changing turntables were commonplace, records were typically pressed with a raised (or ridged) outer edge and a raised label area, allowing records to be stacked onto each other without the delicate grooves coming into contact, reducing the risk of damage. Auto-changers included a mechanism to support a stack of several records above the turntable itself, dropping them one at a time onto the active turntable to be played in order. Many longer sound recordings, such as complete operas, were interleaved across several 10-inch or 12-inch discs for use with auto-changing mechanisms, so that the first disk of a three-disk recording would carry sides 1 and 6 of the program, while the second disk would carry sides 2 and 5, and the third, sides 3 and 4, allowing sides 1, 2, and 3 to be played automatically; then the whole stack reversed to play sides 4, 5, and 6. Due to recording mastering and manufacturing limitations, both high and low frequencies were removed from the first recorded signals by various formulae. With low frequencies, the stylus must swing a long way from side to side, requiring the groove to be wide, taking up more space and limiting the playing time of the record. At high frequencies, hiss, pops, and ticks are significant. These problems can be reduced by using equalization to an agreed standard. During recording the amplitude of low frequencies is reduced, thus reducing the groove width required, and the amplitude at high frequencies is increased. The playback equipment boosts bass and cuts treble so as to restore the tonal balance in the original signal; this also reduces the high frequency noise. Thus more music will fit on the record, and noise is reduced. In 1968, Reprise planned to release a series of 78 rpm singles from their artists on their label at the time, called the Reprise Speed Series. Only one disc actually saw release, Randy Newman's I Think It's Going to Rain Today, a track from his self-titled debut album (with The Beehive State on the flipside). Reprise did not proceed further with the series due to a lack of sales for the single, and a lack of general interest in the concept. Guitarist & vocalist Leon Redbone released a promotional 78 rpm record in 1978 featuring two songs (Alabama Jubilee and Please Don't Talk About Me When I'm Gone) from his Champagne Charlie album. In 1980 Stiff Records in the United Kingdom issued a 78 by Joe ""King"" Carrasco containing the songs Buena (Spanish for ""good,"" with the alternate spelling ""Bueno"" on the label) and Tuff Enuff. Underground comic cartoonist and 78 rpm record collector Robert Crumb released three discs with his Cheap Suit Serenaders in the 1980s. The phonograph disc record was the primary medium used for music reproduction until late in the 20th century, replacing the phonograph cylinder record–with which it had co-existed from the late 1880s through to the 1920s–by the late 1920s. Records retained the largest market share even when new formats such as compact cassette were mass-marketed. By the late 1980s, digital media, in the form of the compact disc, had gained a larger market share, and the vinyl record left the mainstream in 1991. From the 1990s to the 2010s, records continued to be manufactured and sold on a much smaller scale, and were especially used by disc jockeys (DJ)s, released by artists in some genres, and listened to by a niche market of audiophiles. The phonograph record has made a niche resurgence in the early 21st century – 9.2 million records were sold in the U.S. in 2014, a 260% increase since 2009. Likewise, in the UK sales have increased five-fold from 2009 to 2014. West in 1930 and later P. G. A. H. Voigt (1940) showed that the early Wente-style condenser microphones contributed to a 4 to 6 dB midrange brilliance or pre-emphasis in the recording chain. This meant that the electrical recording characteristics of Western Electric licensees such as Columbia Records and Victor Talking Machine Company in the 1925 era had a higher amplitude in the midrange region. Brilliance such as this compensated for dullness in many early magnetic pickups having drooping midrange and treble response. As a result, this practice was the empirical beginning of using pre-emphasis above 1,000 Hz in 78 rpm and 33 1⁄3 rpm records. Breakage was very common in the shellac era. In the 1934 John O'Hara novel, Appointment in Samarra, the protagonist ""broke one of his most favorites, Whiteman's Lady of the Evening ... He wanted to cry but could not."" A poignant moment in J. D. Salinger's 1951 novel The Catcher in the Rye occurs after the adolescent protagonist buys a record for his younger sister but drops it and ""it broke into pieces ... I damn-near cried, it made me feel so terrible."" A sequence where a school teacher's collection of 78 rpm jazz records is smashed by a group of rebellious students is a key moment in the film Blackboard Jungle. In 2014 artist Jack White sold 40,000 copies of his second solo release, Lazaretto, on vinyl. The sales of the record beat the largest sales in one week on vinyl since 1991. The sales record was previously held by Pearl Jam's, Vitalogy, which sold 34,000 copies in one week in 1994. In 2014, the sale of vinyl records was the only physical music medium with increasing sales with relation to the previous year. Sales of other mediums including individual digital tracks, digital albums and compact discs have fallen, the latter having the greatest drop-in-sales rate. Vinyl records can be warped by heat, improper storage, exposure to sunlight, or manufacturing defects such as excessively tight plastic shrinkwrap on the album cover. A small degree of warp was common, and allowing for it was part of the art of turntable and tonearm design. ""wow"" (once-per-revolution pitch variation) could result from warp, or from a spindle hole that was not precisely centered. Standard practice for LPs was to place the LP in a paper or plastic inner cover. This, if placed within the outer cardboard cover so that the opening was entirely within the outer cover, was said to reduce ingress of dust onto the record surface. Singles, with rare exceptions, had simple paper covers with no inner cover. Evidence from the early technical literature concerning electrical recording suggests that it wasn't until the 1942–1949 period that there were serious efforts to standardize recording characteristics within an industry. Heretofore, electrical recording technology from company to company was considered a proprietary art all the way back to the 1925 Western Electric licensed method used by Columbia and Victor. For example, what Brunswick-Balke-Collender (Brunswick Corporation) did was different from the practices of Victor. In the 1990s Rhino Records issued a series of boxed sets of 78 rpm reissues of early rock and roll hits, intended for owners of vintage jukeboxes. This was a disaster because Rhino did not warn customers that their records were made of vinyl, and that the vintage 78 RPM juke boxes were designed with heavy tone arms and steel needles to play the hard shellac records of their time. This failure to warn customers gave the Rhino 78 records a bad reputation,[citation needed] as they were destroyed by old juke boxes and old record players but played very well on newer 78-capable turntables with modern lightweight tone arms and jewel needles. Through the 1960s, 1970s, and 1980s, various methods to improve the dynamic range of mass-produced records involved highly advanced disc cutting equipment. These techniques, marketed, to name two, as the CBS DisComputer and Teldec Direct Metal Mastering, were used to reduce inner-groove distortion. RCA Victor introduced another system to reduce dynamic range and achieve a groove with less surface noise under the commercial name of Dynagroove. Two main elements were combined: another disk material with less surface noise in the groove and dynamic compression for masking background noise. Sometimes this was called ""diaphragming"" the source material and not favoured by some music lovers for its unnatural side effects. Both elements were reflected in the brandname of Dynagroove, described elsewhere in more detail. It also used the earlier advanced method of forward-looking control on groove spacing with respect to volume of sound and position on the disk. Lower recorded volume used closer spacing; higher recorded volume used wider spacing, especially with lower frequencies. Also, the higher track density at lower volumes enabled disk recordings to end farther away from the disk center than usual, helping to reduce endtrack distortion even further. One early attempt at lengthening the playing time should be mentioned. At least one manufacturer in the early 1920s, World Records, produced records that played at a constant linear velocity, controlled by Noel Pemberton Billing's patented add-on governor device. As these were played from the outside to the inside, the rotational speed of the records increased as reproduction progressed. This action is similar (although in reverse) to that on the modern compact disc and the CLV version of its predecessor, the Philips Laser Disc. The commercial rivalry between RCA Victor and Columbia Records led to RCA Victor's introduction of what it had intended to be a competing vinyl format, the 7-inch (175 mm) 45 rpm disc. For a two-year period from 1948 to 1950, record companies and consumers faced uncertainty over which of these formats would ultimately prevail in what was known as the ""War of the Speeds"". (See also format war.) In 1949 Capitol and Decca adopted the new LP format and RCA gave in and issued its first LP in January 1950. The 45 rpm size was gaining in popularity, too, and Columbia issued its first 45s in February 1951. By 1954, 200 million 45s had been sold. In March 1949, as RCA released the 45, Columbia released several hundred 7 inch 33 1/3 rpm small spindle hole singles. This format was soon dropped as it became clear that the RCA 45 was the single of choice and the Columbia 12 inch LP would be the 'album' of choice. The first release of the 45 came in seven colors: black 47-xxxx popular series, yellow 47-xxxx juvenile series, green (teal) 48-xxxx country series, deep red 49-xxxx classical series, bright red (cerise) 50-xxxx blues/spiritual series, light blue 51-xxxx international series, dark blue 52-xxxx light classics. All colors were soon dropped in favor of black because of production problems. However, yellow and deep red were continued until about 1952. The first 45 rpm record created for sale was ""PeeWee the Piccolo"" RCA 47-0147 pressed in yellow translucent vinyl at the Sherman Avenue plant, Indianapolis Dec. 7, 1948, R.O. Price, plant manager. Broadcasters were faced with having to adapt daily to the varied recording characteristics of many sources: various makers of ""home recordings"" readily available to the public, European recordings, lateral-cut transcriptions, and vertical-cut transcriptions. Efforts were started in 1942 to standardize within the National Association of Broadcasters (NAB), later known as the National Association of Radio and Television Broadcasters (NARTB). The NAB, among other items, issued recording standards in 1949 for laterally and vertically cut records, principally transcriptions. A number of 78 rpm record producers as well as early LP makers also cut their records to the NAB/NARTB lateral standard. Under the direction of recording engineer C. Robert Fine, Mercury Records initiated a minimalist single microphone monaural recording technique in 1951. The first record, a Chicago Symphony Orchestra performance of Pictures at an Exhibition, conducted by Rafael Kubelik, was described as ""being in the living presence of the orchestra"" by The New York Times music critic. The series of records was then named Mercury Living Presence. In 1955, Mercury began three-channel stereo recordings, still based on the principle of the single microphone. The center (single) microphone was of paramount importance, with the two side mics adding depth and space. Record masters were cut directly from a three-track to two-track mixdown console, with all editing of the master tapes done on the original three-tracks. In 1961, Mercury enhanced this technique with three-microphone stereo recordings using 35 mm magnetic film instead of half-inch tape for recording. The greater thickness and width of 35 mm magnetic film prevented tape layer print-through and pre-echo and gained extended frequency range and transient response. The Mercury Living Presence recordings were remastered to CD in the 1990s by the original producer, Wilma Cozart Fine, using the same method of 3-to-2 mix directly to the master recorder. Groove recordings, first designed in the final quarter of the 19th century, held a predominant position for nearly a century—withstanding competition from reel-to-reel tape, the 8-track cartridge, and the compact cassette. In 1988, the compact disc surpassed the gramophone record in unit sales. Vinyl records experienced a sudden decline in popularity between 1988 and 1991, when the major label distributors restricted their return policies, which retailers had been relying on to maintain and swap out stocks of relatively unpopular titles. First the distributors began charging retailers more for new product if they returned unsold vinyl, and then they stopped providing any credit at all for returns. Retailers, fearing they would be stuck with anything they ordered, only ordered proven, popular titles that they knew would sell, and devoted more shelf space to CDs and cassettes. Record companies also deleted many vinyl titles from production and distribution, further undermining the availability of the format and leading to the closure of pressing plants. This rapid decline in the availability of records accelerated the format's decline in popularity, and is seen by some as a deliberate ploy to make consumers switch to CDs, which were more profitable for the record companies. In 1926 Joseph P. Maxwell and Henry C. Harrison from Bell Telephone Laboratories disclosed that the recording pattern of the Western Electric ""rubber line"" magnetic disc cutter had a constant velocity characteristic. This meant that as frequency increased in the treble, recording amplitude decreased. Conversely, in the bass as frequency decreased, recording amplitude increased. Therefore, it was necessary to attenuate the bass frequencies below about 250 Hz, the bass turnover point, in the amplified microphone signal fed to the recording head. Otherwise, bass modulation became excessive and overcutting took place into the next record groove. When played back electrically with a magnetic pickup having a smooth response in the bass region, a complementary boost in amplitude at the bass turnover point was necessary. G. H. Miller in 1934 reported that when complementary boost at the turnover point was used in radio broadcasts of records, the reproduction was more realistic and many of the musical instruments stood out in their true form. Terms such as ""long-play"" (LP) and ""extended-play"" (EP) describe multi-track records that play much longer than the single-item-per-side records, which typically do not go much past four minutes per side. An LP can play for up to 30 minutes per side, though most played for about 22 minutes per side, bringing the total playing time of a typical LP recording to about forty-five minutes. Many pre-1952 LPs, however, played for about 15 minutes per side. The 7-inch 45 rpm format normally contains one item per side but a 7-inch EP could achieve recording times of 10 to 15 minutes at the expense of attenuating and compressing the sound to reduce the width required by the groove. EP discs were generally used to make available tracks not on singles including tracks on LPs albums in a smaller, less expensive format for those who had only 45 rpm players. The large center hole on 7-inch 45 rpm records allows for easier handling by jukebox mechanisms. The term ""album"", originally used to mean a ""book"" with liner notes, holding several 78 rpm records each in its own ""page"" or sleeve, no longer has any relation to the physical format: a single LP record, or nowadays more typically a compact disc. The earliest disc records (1889–1894) were made of various materials including hard rubber. Around 1895, a shellac-based compound was introduced and became standard. Exact formulas for this compound varied by manufacturer and over the course of time, but it was typically composed of about one-third shellac and about two-thirds mineral filler, which meant finely pulverized rock, usually slate and limestone, with an admixture of cotton fibers to add tensile strength, carbon black for color (without this, it tended to be a ""dirty"" gray or brown color that most record companies considered unattractive), and a very small amount of a lubricant to facilitate mold release during manufacture. Some makers, notably Columbia Records, used a laminated construction with a core disc of coarser material or fiber. The production of shellac records continued until the end of the 78 rpm format (i.e., the late 1950s in most developed countries, but well into the 1960s in some other places), but increasingly less abrasive formulations were used during its declining years and very late examples in truly like-new condition can have as low noise levels as vinyl. In the 1890s, the recording formats of the earliest (toy) discs were mainly 12.5 cm (nominally five inches) in diameter; by the mid-1890s, the discs were usually 7 in (nominally 17.5 cm) in diameter. By 1910 the 10-inch (25.4 cm) record was by far the most popular standard, holding about three minutes of music or other entertainment on a side. From 1903 onwards, 12-inch records (30.5 cm) were also sold commercially, mostly of classical music or operatic selections, with four to five minutes of music per side. Victor, Brunswick and Columbia also issued 12-inch popular medleys, usually spotlighting a Broadway show score. However, other sizes did appear. Eight-inch discs with a 2-inch-diameter (51 mm) label became popular for about a decade in Britain, but they cannot be played in full on most modern record players because the tone arm cannot play far enough in toward the center without modification of the equipment. Some of Elvis Presley's early singles on Sun Records might have sold more copies on 78 than on 45. This is because the majority of those sales in 1954–55 were to the ""hillbilly"" market in the South and Southwestern United States, where replacing the family 78 rpm player with a new 45 rpm player was a luxury few could afford at the time. By the end of 1957, RCA Victor announced that 78s accounted for less than 10% of Presley's singles sales, essentially announcing the death throes of the 78 rpm format. The last Presley single released on 78 in the United States was RCA Victor 20-7410, I Got Stung/One Night (1958), while the last 78 in the UK was RCA 1194, A Mess Of Blues/Girl Of My Best Friend (1960). By about 1910,[note 1] bound collections of empty sleeves with a paperboard or leather cover, similar to a photograph album, were sold as record albums that customers could use to store their records (the term ""record album"" was printed on some covers). These albums came in both 10-inch and 12-inch sizes. The covers of these bound books were wider and taller than the records inside, allowing the record album to be placed on a shelf upright, like a book, suspending the fragile records above the shelf and protecting them. Early recordings were made entirely acoustically, the sound being collected by a horn and piped to a diaphragm, which vibrated the cutting stylus. Sensitivity and frequency range were poor, and frequency response was very irregular, giving acoustic recordings an instantly recognizable tonal quality. A singer practically had to put his or her face in the recording horn. Lower-pitched orchestral instruments such as cellos and double basses were often doubled (or replaced) by louder wind instruments, such as tubas. Standard violins in orchestral ensembles were commonly replaced by Stroh violins, which became popular with recording studios. Eventually, when it was more common for electric recordings to be played back electrically in the 1930s and 1940s, the overall tone was much like listening to a radio of the era. Magnetic pickups became more common and were better designed as time went on, making it possible to improve the damping of spurious resonances. Crystal pickups were also introduced as lower cost alternatives. The dynamic or moving coil microphone was introduced around 1930 and the velocity or ribbon microphone in 1932. Both of these high quality microphones became widespread in motion picture, radio, recording, and public address applications. RCA 45s were also adapted to the smaller spindle of an LP player with a plastic snap-in insert known as a ""spider"". These inserts, commissioned by RCA president David Sarnoff and invented by Thomas Hutchison, were prevalent starting in the 1960s, selling in the tens of millions per year during the 45 rpm heyday. In countries outside the U.S., 45s often had the smaller album-sized holes, e.g., Australia and New Zealand, or as in the United Kingdom, especially before the 1970s, the disc had a small hole within a circular central section held only by three or four lands so that it could be easily punched out if desired (typically for use in jukeboxes). In 1925, 78.26 rpm was chosen as the standard because of the introduction of the electrically powered synchronous turntable motor. This motor ran at 3600 rpm, such that a 46:1 gear ratio would produce 78.26 rpm. In parts of the world that used 50 Hz current, the standard was 77.92 rpm (3,000 rpm with a 77:2 ratio), which was also the speed at which a strobe disc with 77 lines would ""stand still"" in 50 Hz light (92 lines for 60 Hz). After World War II these records were retroactively known as 78s, to distinguish them from other newer disc record formats. Earlier they were just called records, or when there was a need to distinguish them from cylinders, disc records. The playing time of a phonograph record depended on the turntable speed and the groove spacing. At the beginning of the 20th century, the early discs played for two minutes, the same as early cylinder records. The 12-inch disc, introduced by Victor in 1903, increased the playing time to three and a half minutes. Because a 10-inch 78 rpm record could hold about three minutes of sound per side and the 10-inch size was the standard size for popular music, almost all popular recordings were limited to around three minutes in length. For example, when King Oliver's Creole Jazz Band, including Louis Armstrong on his first recordings, recorded 13 sides at Gennett Records in Richmond, Indiana, in 1923, one side was 2:09 and four sides were 2:52–2:59. Some recordings, such as books for the blind, were pressed at 16 2⁄3 rpm. Prestige Records released jazz records in this format in the late 1950s; for example, two of their Miles Davis albums were paired together in this format. Peter Goldmark, the man who developed the 33 1⁄3 rpm record, developed the Highway Hi-Fi 16 2⁄3 rpm record to be played in Chrysler automobiles, but poor performance of the system and weak implementation by Chrysler and Columbia led to the demise of the 16 2⁄3 rpm records. Subsequently, the 16 2⁄3 rpm speed was used for narrated publications for the blind and visually impaired, and were never widely commercially available, although it was common to see new turntable models with a 16 rpm speed setting produced as late as the 1970s. Electrical recording preceded electrical home reproduction because of the initial high cost of the new system. In 1925, the Victor company introduced the Victor Orthophonic Victrola, an acoustical record player that was specifically designed to play electrically recorded discs, as part of a line that also included electrically reproducing Electrolas. The acoustical Orthophonics ranged in price from US$95 to US$300, depending on cabinetry; by comparison, the cheapest Electrola cost US$650, the price of a new Ford automobile in an era when clerical jobs paid about $20 a week. As the playing of gramophone records causes gradual degradation of the recording, they are best preserved by transferring them onto other media and playing the records as rarely as possible. They need to be stored on edge, and do best under environmental conditions that most humans would find comfortable. The medium needs to be kept clean, but alcohol should only be used on PVC or optical media, not on 78s.[citation needed] The equipment for playback of certain formats (e.g., 16 and 78 rpm) is manufactured only in small quantities, leading to increased difficulty in finding equipment to play the recordings. Vinyl's lower surface noise level than shellac was not forgotten, nor was its durability. In the late 1930s, radio commercials and pre-recorded radio programs being sent to disc jockeys started being stamped in vinyl, so they would not break in the mail. In the mid-1940s, special DJ copies of records started being made of vinyl also, for the same reason. These were all 78 rpm. During and after World War II, when shellac supplies were extremely limited, some 78 rpm records were pressed in vinyl instead of shellac, particularly the six-minute 12-inch (30 cm) 78 rpm records produced by V-Disc for distribution to United States troops in World War II. In the 1940s, radio transcriptions, which were usually on 16-inch records, but sometimes 12-inch, were always made of vinyl, but cut at 33 1⁄3 rpm. Shorter transcriptions were often cut at 78 rpm. After World War II, two new competing formats came onto the market and gradually replaced the standard ""78"": the 33 1⁄3 rpm (often just referred to as the 33 rpm), and the 45 rpm (see above). The 33 1⁄3 rpm LP (for ""long-play"") format was developed by Columbia Records and marketed in June 1948. RCA Victor developed the 45 rpm format and marketed it in March 1949, each pursuing their own r&d in secret. Both types of new disc used narrower grooves, intended to be played with smaller stylus—typically 0.001 inches (25 µm) wide, compared to 0.003 inches (76 µm) for a 78—so the new records were sometimes called Microgroove. In the mid-1950s all record companies agreed to a common recording standard called RIAA equalization. Prior to the establishment of the standard each company used its own preferred standard, requiring discriminating listeners to use pre-amplifiers with multiple selectable equalization curves. The mid-1970s saw the introduction of dbx-encoded records, again for the audiophile niche market. These were completely incompatible with standard record playback preamplifiers, relying on the dbx compandor encoding/decoding scheme to greatly increase dynamic range (dbx encoded disks were recorded with the dynamic range compressed by a factor of two in dB: quiet sounds were meant to be played back at low gain and loud sounds were meant to be played back at high gain, via automatic gain control in the playback equipment; this reduced the effect of surface noise on quiet passages). A similar and very short-lived scheme involved using the CBS-developed ""CX"" noise reduction encoding/decoding scheme. Unwilling to accept and license Columbia's system, in February 1949 RCA Victor, in cooperation of its parent, the Radio Corporation of America, released the first 45 rpm single, 7 inches in diameter with a large center hole. The 45 rpm player included a changing mechanism that allowed multiple disks to be stacked, much as a conventional changer handled 78s. The short playing time of a single 45 rpm side meant that long works, such as symphonies, had to be released on multiple 45s instead of a single LP, but RCA claimed that the new high-speed changer rendered side breaks so brief as to be inaudible or inconsequential. Early 45 rpm records were made from either vinyl or polystyrene. They had a playing time of eight minutes. Flexible or so-called ""unbreakable"" records made of unusual materials were introduced by a number of manufacturers at various times during the 78 rpm era. In the UK, Nicole records, made of celluloid or a similar substance coated onto a cardboard core disc, were produced for a few years beginning in 1904, but they suffered from an exceptionally high level of surface noise. In the United States, Columbia Records introduced flexible, fiber-cored ""Marconi Velvet Tone Record"" pressings in 1907, but the advantages and longevity of their relatively noiseless surfaces depended on the scrupulous use of special gold-plated Marconi Needles and the product was not a success. Thin, flexible plastic records such as the German Phonycord and the British Filmophone and Goodson records appeared around 1930 but also did not last long. The contemporary French Pathé Cellodiscs, made of a very thin black plastic, which uncannily resembles the vinyl ""sound sheet"" magazine inserts of the 1965–1985 era, were similarly short-lived. In the US, Hit of the Week records, made of a patented translucent plastic called Durium coated on a heavy brown paper base, were introduced in early 1930. A new issue came out every week and they were sold at newsstands like a weekly magazine. Although inexpensive and commercially successful at first, they soon fell victim to the Great Depression and production in the US ended in 1932. Related Durium records continued to be made somewhat later in the UK and elsewhere, and as remarkably late as 1950 in Italy, where the name ""Durium"" survived far into the LP era as a trademark on ordinary vinyl records. Despite all these attempts at innovation, shellac compounds continued to be used for the overwhelming majority of commercial 78 rpm records during the lifetime of the format. The ""orange peel"" effect on vinyl records is caused by worn molds. Rather than having the proper mirror-like finish, the surface of the record will have a texture that looks like orange peel. This introduces noise into the record, particularly in the lower frequency range. With direct metal mastering (DMM), the master disc is cut on a copper-coated disc, which can also have a minor ""orange peel"" effect on the disc itself. As this ""orange peel"" originates in the master rather than being introduced in the pressing stage, there is no ill effect as there is no physical distortion of the groove. It was not unusual for electric recordings to be played back on acoustic phonographs. The Victor Orthophonic phonograph was a prime example where such playback was expected. In the Orthophonic, which benefited from telephone research, the mechanical pickup head was redesigned with lower resonance than the traditional mica type. Also, a folded horn with an exponential taper was constructed inside the cabinet to provide better impedance matching to the air. As a result, playback of an Orthophonic record sounded like it was coming from a radio."
"Punjab,_Pakistan","Maharaja Ranjit Singh's death in the summer of 1839 brought political chaos and the subsequent battles of succession and the bloody infighting between the factions at court weakened the state. Relationships with neighbouring British territories then broke down, starting the First Anglo-Sikh War; this led to a British official being resident in Lahore and the annexation in 1849 of territory south of the Satluj to British India. After the Second Anglo-Sikh War in 1849, the Sikh Empire became the last territory to be merged into British India. In Jhelum 35 British soldiers of HM XXIV regiment were killed by the local resistance during the Indian Rebellion of 1857.[citation needed] The onset of the southwest monsoon is anticipated to reach Punjab by May, but since the early 1970s the weather pattern has been irregular. The spring monsoon has either skipped over the area or has caused it to rain so hard that floods have resulted. June and July are oppressively hot. Although official estimates rarely place the temperature above 46 °C, newspaper sources claim that it reaches 51 °C and regularly carry reports about people who have succumbed to the heat. Heat records were broken in Multan in June 1993, when the mercury was reported to have risen to 54 °C. In August the oppressive heat is punctuated by the rainy season, referred to as barsat, which brings relief in its wake. The hardest part of the summer is then over, but cooler weather does not come until late October. The Government of Punjab is a provincial government in the federal structure of Pakistan, is based in Lahore, the capital of the Punjab Province. The Chief Minister of Punjab (CM) is elected by the Provincial Assembly of the Punjab to serve as the head of the provincial government in Punjab, Pakistan. The current Chief Minister is Shahbaz Sharif, who became the Chief Minister of Punjab as being restored after Governor's rule starting from 25 February 2009 to 30 March 2009. Thereafter got re-elected as a result of 11 May 2013 elections. The Provincial Assembly of the Punjab is a unicameral legislature of elected representatives of the province of Punjab, which is located in Lahore in eastern Pakistan. The Assembly was established under Article 106 of the Constitution of Pakistan as having a total of 371 seats, with 66 seats reserved for women and eight reserved for non-Muslims. In 1758, the general of the Hindu Maratha Empire, Raghunath Rao conquered Lahore and Attock. Timur Shah Durrani, the son and viceroy of Ahmad Shah Abdali, was driven out of Punjab. Lahore, Multan, Dera Ghazi Khan, Kashmir and other subahs on the south and eastern side of Peshawar were under the Maratha rule for the most part. In Punjab and Kashmir, the Marathas were now major players. The Third Battle of Panipat took place on 1761, Ahmad Shah Abdali invaded the Maratha territory of Punjab and captured remnants of the Maratha Empire in Punjab and Kashmir regions and re-consolidated control over them. Punjab's geography mostly consists of the alluvial plain of the Indus River and its four major tributaries in Pakistan, the Jhelum, Chenab, Ravi, and Sutlej rivers. There are several mountainous regions, including the Sulaiman Mountains in the southwest part of the province, and Margalla Hills, Salt Range, and Pothohar Plateau in the north. Agriculture is the chief source of income and employment in Punjab; wheat and cotton are the principal crops. Since independence, Punjab has become the seat of political and economic power; it remains the most industrialised province of Pakistan. It counts for 39.2% of large scale manufacturing and 70% of small scale manufacturing in the country. Its capital Lahore is a major regional cultural, historical, and economic centre. As of June 2012[update], Pakistan's electricity problems were so severe that violent riots were taking place across Punjab. According to protesters, load shedding was depriving the cities of electricity 20–22 hours a day, causing businesses to go bust and making living extremely hard. Gujranwala, Toba Tek Singh, Faisalabad, Sialkot, Bahawalnagar and communities across Khanewal District saw widespread rioting and violence on Sunday 17 June 2012, with the houses of several members of parliament being attacked as well as the offices of regional energy suppliers Fesco, Gepco and Mepco being ransacked or attacked. The fairs held at the shrines of Sufi saints are called urs. They generally mark the death anniversary of the saint. On these occasions devotees assemble in large numbers and pay homage to the memory of the saint. Soul inspiring music is played and devotees dance in ecstasy. The music on these occasions is essentially folk and appealing. It forms a part of the folk music through mystic messages. The most important urs are: urs of Data Ganj Buksh at Lahore, urs of Hazrat Sultan Bahu at Jhang, urs of Hazrat Shah Jewna at Jhang, urs of Hazrat Mian Mir at Lahore, urs of Baba Farid Ganj Shakar at Pakpattan, urs of Hazrat Bahaudin Zakria at Multan, urs of Sakhi Sarwar Sultan at Dera Ghazi Khan, urs of Shah Hussain at Lahore, urs of Hazrat Bulleh Shah at Kasur, urs of Hazrat Imam Bari (Bari Shah Latif) at Rawalpindi-Islamabad and urs of Shah Inayar Qadri (the murrshad of Bulleh Shah) in Lahore. For the popular taste however, light music, particularly Ghazals and folk songs, which have an appeal of their own, the names of Mehdi Hassan, Ghulam Ali, Nur Jehan, Malika Pukhraj, Farida Khanum, Roshen Ara Begum, and Nusrat Fateh Ali Khan are well-known. Folk songs and dances of the Punjab reflect a wide range of moods: the rains, sowing and harvesting seasons. Luddi, Bhangra and Sammi depict the joy of living. Love legends of Heer Ranjha, Mirza Sahiban, Sohni Mahenwal and Saiful Mulk are sung in different styles. The structure of a mosque is simple and it expresses openness. Calligraphic inscriptions from the Quran decorate mosques and mausoleums in Punjab. The inscriptions on bricks and tiles of the mausoleum of Shah Rukn-e-Alam (1320 AD) at Multan are outstanding specimens of architectural calligraphy. The earliest existing building in South Asia with enamelled tile-work is the tomb of Shah Yusuf Gardezi (1150 AD) at Multan. A specimen of the sixteenth century tile-work at Lahore is the tomb of Sheikh Musa Ahangar, with its brilliant blue dome. The tile-work of Emperor Shah Jahan is of a richer and more elaborate nature. The pictured wall of Lahore Fort is the last line in the tile-work in the entire world. Due to its location, the Punjab region came under constant attacks and influence from the west and witnessed centuries of foreign invasions by the Greeks, Kushans, Scythians, Turks, and Afghans. The city of Taxila, founded by son of Taksh the son Bharat who was the brother of Ram. It was reputed to house the oldest university in the world,[citation needed] Takshashila University. One of the teachers was the great Vedic thinker and politician Chanakya. Taxila was a great centre of learning and intellectual discussion during the Maurya Empire. It is a UN World Heritage site, valued for its archaeological and religious history. Among the Punjabi poets, the names of Sultan Bahu, Bulleh Shah, Mian Muhammad Baksh, and Waris Shah and folk singers like Inayat Hussain Bhatti and Tufail Niazi, Alam Lohar, Sain Marna, Mansoor Malangi, Allah Ditta Lona wala, Talib Hussain Dard, Attaullah Khan Essa Khailwi, Gamoo Tahliwala, Mamzoo Gha-lla, Akbar Jat, Arif Lohar, Ahmad Nawaz Cheena and Hamid Ali Bela are well-known. In the composition of classical ragas, there are such masters as Malika-i-Mauseequi (Queen of Music) Roshan Ara Begum, Ustad Amanat Ali Khan, Salamat Ali Khan and Ustad Fateh Ali Khan. Alam Lohar has made significant contributions to folklore and Punjabi literature, by being a very influential Punjabi folk singer from 1930 until 1979. The capital and largest city is Lahore which was the historical capital of the wider Punjab region. Other important cities include Faisalabad, Rawalpindi, Gujranwala, Sargodha, Multan, Sialkot, Bahawalpur, Gujrat, Sheikhupura, Jhelum and Sahiwal. Undivided Punjab is home to six rivers, of which five flow through Pakistani Punjab. From west to east, these are: the Indus, Jhelum, Beas, Chenab, Ravi and Sutlej. Nearly 60% of Pakistan's population lives in the Punjab. It is the nation's only province that touches every other province; it also surrounds the federal enclave of the national capital city at Islamabad. In the acronym P-A-K-I-S-T-A-N, the P is for Punjab. The province is home to several historical sites, including the Shalimar Gardens, the Lahore Fort, the Badshahi Mosque, the Rohtas Fort and the ruins of the ancient city of Harrapa. The Anarkali Market and Jahangir's Tomb are prominent in the city of Lahore as is the Lahore Museum, while the ancient city of Taxila in the northwest was once a major centre of Buddhist and Hindu influence. Several important Sikh shrines are in the province, including the birthplace of the first Guru, Guru Nanak. (born at Nankana Sahib). There are a few famous hill stations, including Murree, Bhurban, Patriata and Fort Munro. Exhibitions and annual horse shows in all districts and a national horse and cattle show at Lahore are held with the official patronage. The national horse and cattle show at Lahore is the biggest festival where sports, exhibitions, and livestock competitions are held. It not only encourages and patronises agricultural products and livestock through the exhibitions of agricultural products and cattle but is also a colourful documentary on the rich cultural heritage of the province with its strong rural roots. The Punjabis followed a diverse plethora of faiths, mainly comprising Hinduism[citation needed] , when the Muslim Umayyad army led by Muhammad bin Qasim conquered Sindh and Southern Punjab in 712, by defeating Raja Dahir. The Umayyad Caliphate was the second Islamic caliphate established after the death of Muhammad. It was ruled by the Umayyad dynasty, whose name derives from Umayya ibn Abd Shams, the great-grandfather of the first Umayyad caliph. Although the Umayyad family originally came from the city of Mecca, their capital was Damascus. Muhammad bin Qasim was the first to bring message of Islam to the population of Punjab.[citation needed] Punjab was part of different Muslim Empires consisting of Afghans and Turkic peoples in co-operation with local Punjabi tribes and others.[citation needed] In the 11th century, during the reign of Mahmud of Ghazni, the province became an important centre with Lahore as its second capital[citation needed] of the Ghaznavid Empire based out of Afghanistan. The Punjab region became predominantly Muslim due to missionary Sufi saints whose dargahs dot the landscape of Punjab region. Punjab during Mahabharata times was known as Panchanada. Punjab was part of the Indus Valley Civilization, more than 4000 years ago. The main site in Punjab was the city of Harrapa. The Indus Valley Civilization spanned much of what is today Pakistan and eventually evolved into the Indo-Aryan civilisation. The Vedic civilisation flourished along the length of the Indus River. This civilisation shaped subsequent cultures in South Asia and Afghanistan. Although the archaeological site at Harappa was partially damaged in 1857 when engineers constructing the Lahore-Multan railroad used brick from the Harappa ruins for track ballast, an abundance of artefacts have nevertheless been found. Punjab was part of the great ancient empires including the Gandhara Mahajanapadas, Achaemenids, Macedonians, Mauryas, Kushans, Guptas, and Hindu Shahi. It also comprised the Gujar empire for a period of time, otherwise known as the Gurjara-Pratihara empire. Agriculture flourished and trading cities (such as Multan and Lahore) grew in wealth. The northwestern part of the South Asia, including Punjab, was repeatedly invaded or conquered by various foreign empires, such as those of Tamerlane, Alexander the Great and Genghis Khan. Having conquered Drangiana, Arachosia, Gedrosia and Seistan in ten days, Alexander crossed the Hindu Kush and was thus fully informed of the magnificence of the country and its riches in gold, gems and pearls. However, Alexander had to encounter and reduce the tribes on the border of Punjab before entering the luxuriant plains. Having taken a northeasterly direction, he marched against the Aspii (mountaineers), who offered vigorous resistance, but were subdued.[citation needed] Alexander then marched through Ghazni, blockaded Magassa, and then marched to Ora and Bazira. Turning to the northeast, Alexander marched to Pucela, the capital of the district now known as Pakhli. He entered Western Punjab, where the ancient city of Nysa (at the site of modern-day Mong) was situated. A coalition was formed against Alexander by the Cathians, the people of Multan, who were very skilful in war. Alexander invested many troops, eventually killing seventeen thousand Cathians in this battle, and the city of Sagala (present-day Sialkot) was razed to the ground. Alexander left Punjab in 326 B.C. and took his army to the heartlands of his empire.[citation needed] Punjab (Urdu, Punjabi: پنجاب, panj-āb, ""five waters"":  listen (help·info)), also spelled Panjab, is the most populous of the four provinces of Pakistan. It has an area of 205,344 square kilometres (79,284 square miles) and a population of 91.379.615 in 2011, approximately 56% of the country's total population. Its provincial capital and largest city is Lahore. Punjab is bordered by the Indian states of Jammu and Kashmir to the northeast and Punjab and Rajasthan to the east. In Pakistan it is bordered by Sindh to the south, Balochistān and Khyber Pakhtunkhwa to the west, and Islamabad and Azad Kashmir to the north. Punjab witnessed major battles between the armies of India and Pakistan in the wars of 1965 and 1971. Since the 1990s Punjab hosted several key sites of Pakistan's nuclear program such as Kahuta. It also hosts major military bases such as at Sargodha and Rawalpindi. The peace process between India and Pakistan, which began in earnest in 2004, has helped pacify the situation. Trade and people-to-people contacts through the Wagah border are now starting to become common. Indian Sikh pilgrims visit holy sites such as Nankana Sahib. There are 48 departments in Punjab government. Each Department is headed by a Provincial Minister (Politician) and a Provincial Secretary (A civil servant of usually BPS-20 or BPS-21). All Ministers report to the Chief Minister, who is the Chief Executive. All Secretaries report to the Chief Secretary of Punjab, who is usually a BPS-22 Civil Servant. The Chief Secretary in turn reports to the Chief Minister. In addition to these departments, there are several Autonomous Bodies and Attached Departments that report directly to either the Secretaries or the Chief Secretary. Despite its tropical wet and dry climate, extensive irrigation makes it a rich agricultural region. Its canal-irrigation system established by the British is the largest in the world. Wheat and cotton are the largest crops. Other crops include rice, sugarcane, millet, corn, oilseeds, pulses, vegetables, and fruits such as kinoo. Livestock and poultry production are also important. Despite past animosities, the rural masses in Punjab's farms continue to use the Hindu calendar for planting and harvesting. In the mid-fifteenth century, the religion of Sikhism was born. During the Mughal empire, many Hindus increasingly adopted Sikhism. These became a formidable military force against the Mughals and later against the Afghan Empire. After fighting Ahmad Shah Durrani in the later eighteenth century, the Sikhs took control of Punjab and managed to establish the Sikh Empire under Maharaja Ranjit Singh, which lasted from 1799 to 1849. The capital of Ranjit Singh's empire was Lahore, and the empire also extended into Afghanistan and Kashmir. Bhangi Misl was the fist Sikh band to conquer Lahore and other towns of Punjab. Syed Ahmad Barelvi a Muslim, waged jihad and attempted to create an Islamic state with strict enforcement of Islamic law. Syed Ahmad Barelvi in 1821 with many supporters and spent two years organising popular and material support for his Punjab campaign. He carefully developed a network of people through the length and breadth of India to collect funds and encourage volunteers, travelling widely throughout India attracting a following among pious Muslims. In December 1826 Sayyid Ahmad and his followers clashed with Sikh troops at Akora Khattak, but with no decisive result. In a major battle near the town of Balakot in 1831, Sayyid Ahmad and Shah Ismail Shaheed with volunteer Muslims were defeated by the professional Sikh Army. The major and native language spoken in the Punjab is Punjabi (which is written in a Shahmukhi script in Pakistan) and Punjabis comprise the largest ethnic group in country. Punjabi is the provincial language of Punjab. There is not a single district in the province where Punjabi language is mother-tongue of less than 89% of population. The language is not given any official recognition in the Constitution of Pakistan at the national level. Punjabis themselves are a heterogeneous group comprising different tribes, clans (Urdu: برادری‎) and communities. In Pakistani Punjab these tribes have more to do with traditional occupations such as blacksmiths or artisans as opposed to rigid social stratifications. Punjabi dialects spoken in the province include Majhi (Standard), Saraiki and Hindko. Saraiki is mostly spoken in south Punjab, and Pashto, spoken in some parts of north west Punjab, especially in Attock District and Mianwali District. Despite the lack of a coastline, Punjab is the most industrialised province of Pakistan; its manufacturing industries produce textiles, sports goods, heavy machinery, electrical appliances, surgical instruments, vehicles, auto parts, metals, sugar mill plants, aircraft, cement, agricultural machinery, bicycles and rickshaws, floor coverings, and processed foods. In 2003, the province manufactured 90% of the paper and paper boards, 71% of the fertilizers, 69% of the sugar and 40% of the cement of Pakistan. Punjab is Pakistan's second largest province in terms of land area at 205,344 km2 (79,284 sq mi), after Balochistan, and is located at the north western edge of the geologic Indian plate in South Asia. The province is bordered by Kashmir (Azad Kashmir, Pakistan and Jammu and Kashmir, India) to the northeast, the Indian states of Punjab and Rajasthan to the east, the Pakistani province of Sindh to the south, the province of Balochistan to the southwest, the province of Khyber Pakhtunkhwa to the west, and the Islamabad Capital Territory to the north. Punjab has the largest economy in Pakistan, contributing most to the national GDP. The province's economy has quadrupled since 1972. Its share of Pakistan's GDP was 54.7% in 2000 and 59% as of 2010. It is especially dominant in the service and agriculture sectors of Pakistan's economy. With its contribution ranging from 52.1% to 64.5% in the Service Sector and 56.1% to 61.5% in the agriculture sector. It is also major manpower contributor because it has largest pool of professionals and highly skilled (technically trained) manpower in Pakistan. It is also dominant in the manufacturing sector, though the dominance is not as huge, with historical contributions raging from a low of 44% to a high of 52.6%. In 2007, Punjab achieved a growth rate of 7.8% and during the period 2002–03 to 2007–08, its economy grew at a rate of between 7% to 8% per year. and during 2008–09 grew at 6% against the total GDP growth of Pakistan at 4%."
Labour_Party_(UK),"The Labour Party's origins lie in the late 19th century, when it became apparent that there was a need for a new political party to represent the interests and needs of the urban proletariat, a demographic which had increased in number and had recently been given franchise. Some members of the trades union movement became interested in moving into the political field, and after further extensions of the voting franchise in 1867 and 1885, the Liberal Party endorsed some trade-union sponsored candidates. The first Lib–Lab candidate to stand was George Odger in the Southwark by-election of 1870. In addition, several small socialist groups had formed around this time, with the intention of linking the movement to political policies. Among these were the Independent Labour Party, the intellectual and largely middle-class Fabian Society, the Marxist Social Democratic Federation and the Scottish Labour Party. A perceived turning point was when Blair controversially allied himself with US President George W. Bush in supporting the Iraq War, which caused him to lose much of his political support. The UN Secretary-General, among many, considered the war illegal. The Iraq War was deeply unpopular in most western countries, with Western governments divided in their support and under pressure from worldwide popular protests. The decisions that led up to the Iraq war and its subsequent conduct are currently the subject of Sir John Chilcot's Iraq Inquiry. Wilson's government was responsible for a number of sweeping social and educational reforms under the leadership of Home Secretary Roy Jenkins such as the abolishment of the death penalty in 1964, the legalisation of abortion and homosexuality (initially only for men aged 21 or over, and only in England and Wales) in 1967 and the abolition of theatre censorship in 1968. Comprehensive education was expanded and the Open University created. However Wilson's government had inherited a large trade deficit that led to a currency crisis and ultimately a doomed attempt to stave off devaluation of the pound. Labour went on to lose the 1970 general election to the Conservatives under Edward Heath. ""New Labour"" was first termed as an alternative branding for the Labour Party, dating from a conference slogan first used by the Labour Party in 1994, which was later seen in a draft manifesto published by the party in 1996, called New Labour, New Life For Britain. It was a continuation of the trend that had begun under the leadership of Neil Kinnock. ""New Labour"" as a name has no official status, but remains in common use to distinguish modernisers from those holding to more traditional positions, normally referred to as ""Old Labour"". Blair announced in September 2006 that he would quit as leader within the year, though he had been under pressure to quit earlier than May 2007 in order to get a new leader in place before the May elections which were expected to be disastrous for Labour. In the event, the party did lose power in Scotland to a minority Scottish National Party government at the 2007 elections and, shortly after this, Blair resigned as Prime Minister and was replaced by his Chancellor, Gordon Brown. Although the party experienced a brief rise in the polls after this, its popularity soon slumped to its lowest level since the days of Michael Foot. During May 2008, Labour suffered heavy defeats in the London mayoral election, local elections and the loss in the Crewe and Nantwich by-election, culminating in the party registering its worst ever opinion poll result since records began in 1943, of 23%, with many citing Brown's leadership as a key factor. Membership of the party also reached a low ebb, falling to 156,205 by the end of 2009: over 40 per cent of the 405,000 peak reached in 1997 and thought to be the lowest total since the party was founded. Kinnock then resigned as leader and was replaced by John Smith. Smith's leadership once again saw the re-emergence of tension between those on the party's left and those identified as ""modernisers"", both of whom advocated radical revisions of the party's stance albeit in different ways. At the 1993 conference, Smith successfully changed the party rules and lessened the influence of the trade unions on the selection of candidates to stand for Parliament by introducing a one member, one vote system called ""OMOV"" — but only barely, after a barnstorming speech by John Prescott which required Smith to compromise on other individual negotiations. As the economic situation worsened MacDonald agreed to form a ""National Government"" with the Conservatives and the Liberals. On 24 August 1931 MacDonald submitted the resignation of his ministers and led a small number of his senior colleagues in forming the National Government together with the other parties. This caused great anger among those within the Labour Party who felt betrayed by MacDonald's actions: he and his supporters were promptly expelled from the Labour Party and formed a separate National Labour Organisation. The remaining Labour Party MPs (led again by Arthur Henderson) and a few Liberals went into opposition. The ensuing 1931 general election resulted in overwhelming victory for the National Government and disaster for the Labour Party which won only 52 seats, 225 fewer than in 1929. In 1899, a Doncaster member of the Amalgamated Society of Railway Servants, Thomas R. Steels, proposed in his union branch that the Trade Union Congress call a special conference to bring together all left-wing organisations and form them into a single body that would sponsor Parliamentary candidates. The motion was passed at all stages by the TUC, and the proposed conference was held at the Memorial Hall on Farringdon Street on 26 and 27 February 1900. The meeting was attended by a broad spectrum of working-class and left-wing organisations — trades unions represented about one third of the membership of the TUC delegates. Support for the LRC was boosted by the 1901 Taff Vale Case, a dispute between strikers and a railway company that ended with the union being ordered to pay £23,000 damages for a strike. The judgement effectively made strikes illegal since employers could recoup the cost of lost business from the unions. The apparent acquiescence of the Conservative Government of Arthur Balfour to industrial and business interests (traditionally the allies of the Liberal Party in opposition to the Conservative's landed interests) intensified support for the LRC against a government that appeared to have little concern for the industrial proletariat and its problems. The party's performance held up in local elections in 2012 with Labour consolidating its position in the North and Midlands, while also regaining some ground in Southern England. In Wales the party enjoyed good successes, regaining control of most Welsh Councils lost in 2008, including the capital city, Cardiff. In Scotland, Labour's held overall control of Glasgow City Council despite some predictions to the contrary, and also enjoyed a +3.26 swing across Scotland. In London, results were mixed for the party; Ken Livingstone lost the election for Mayor of London, but the party gained its highest ever representation in the Greater London Authority in the concurrent assembly election. The Communist Party of Great Britain was refused affiliation to the Labour Party between 1921 and 1923. Meanwhile, the Liberal Party declined rapidly, and the party also suffered a catastrophic split which allowed the Labour Party to gain much of the Liberals' support. With the Liberals thus in disarray, Labour won 142 seats in 1922, making it the second largest political group in the House of Commons and the official opposition to the Conservative government. After the election the now-rehabilitated Ramsay MacDonald was voted the first official leader of the Labour Party. The 1910 election saw 42 Labour MPs elected to the House of Commons, a significant victory since, a year before the election, the House of Lords had passed the Osborne judgment ruling that Trades Unions in the United Kingdom could no longer donate money to fund the election campaigns and wages of Labour MPs. The governing Liberals were unwilling to repeal this judicial decision with primary legislation. The height of Liberal compromise was to introduce a wage for Members of Parliament to remove the need to involve the Trade Unions. By 1913, faced with the opposition of the largest Trades Unions, the Liberal government passed the Trade Disputes Act to allow Trade Unions to fund Labour MPs once more. The Black Wednesday economic disaster in September 1992 left the Conservative government's reputation for monetary excellence in tatters, and by the end of that year Labour had a comfortable lead over the Tories in the opinion polls. Although the recession was declared over in April 1993 and a period of strong and sustained economic growth followed, coupled with a relatively swift fall in unemployment, the Labour lead in the opinion polls remained strong. However, Smith died from a heart attack in May 1994. After a debate, the 129 delegates passed Hardie's motion to establish ""a distinct Labour group in Parliament, who shall have their own whips, and agree upon their policy, which must embrace a readiness to cooperate with any party which for the time being may be engaged in promoting legislation in the direct interests of labour."" This created an association called the Labour Representation Committee (LRC), meant to coordinate attempts to support MPs sponsored by trade unions and represent the working-class population. It had no single leader, and in the absence of one, the Independent Labour Party nominee Ramsay MacDonald was elected as Secretary. He had the difficult task of keeping the various strands of opinions in the LRC united. The October 1900 ""Khaki election"" came too soon for the new party to campaign effectively; total expenses for the election only came to £33. Only 15 candidatures were sponsored, but two were successful; Keir Hardie in Merthyr Tydfil and Richard Bell in Derby. The nationalist parties, in turn, demanded devolution to their respective constituent countries in return for their supporting the government. When referendums for Scottish and Welsh devolution were held in March 1979 Welsh devolution was rejected outright while the Scottish referendum returned a narrow majority in favour without reaching the required threshold of 40% support. When the Labour government duly refused to push ahead with setting up the proposed Scottish Assembly, the SNP withdrew its support for the government: this finally brought the government down as it triggered a vote of confidence in Callaghan's government that was lost by a single vote on 28 March 1979, necessitating a general election. Labour improved its performance in 1987, gaining 20 seats and so reducing the Conservative majority from 143 to 102. They were now firmly re-established as the second political party in Britain as the Alliance had once again failed to make a breakthrough with seats. A merger of the SDP and Liberals formed the Liberal Democrats. Following the 1987 election, the National Executive Committee resumed disciplinary action against members of Militant, who remained in the party, leading to further expulsions of their activists and the two MPs who supported the group. Fear of advances by the nationalist parties, particularly in Scotland, led to the suppression of a report from Scottish Office economist Gavin McCrone that suggested that an independent Scotland would be 'chronically in surplus'. By 1977 by-election losses and defections to the breakaway Scottish Labour Party left Callaghan heading a minority government, forced to trade with smaller parties in order to govern. An arrangement negotiated in 1977 with Liberal leader David Steel, known as the Lib-Lab Pact, ended after one year. Deals were then forged with various small parties including the Scottish National Party and the Welsh nationalist Plaid Cymru, prolonging the life of the government. The party's decision-making bodies on a national level formally include the National Executive Committee (NEC), Labour Party Conference and National Policy Forum (NPF)—although in practice the Parliamentary leadership has the final say on policy. The 2008 Labour Party Conference was the first at which affiliated trade unions and Constituency Labour Parties did not have the right to submit motions on contemporary issues that would previously have been debated. Labour Party conferences now include more ""keynote"" addresses, guest speakers and question-and-answer sessions, while specific discussion of policy now takes place in the National Policy Forum. On 1 March 2014, at a special conference the party reformed internal Labour election procedures, including replacing the electoral college system for selecting new leaders with a ""one member, one vote"" system following the recommendation of a review by former general-secretary Ray Collins. Mass membership would be encouraged by allowing ""registered supporters"" to join at a low cost, as well as full membership. Members from the trade unions would also have to explicitly ""opt in"" rather than ""opt out"" of paying a political levy to Labour. After its defeat in the 1979 general election the Labour Party underwent a period of internal rivalry between the left represented by Tony Benn, and the right represented by Denis Healey. The election of Michael Foot as leader in 1980, and the leftist policies he espoused, such as unilateral nuclear disarmament, leaving the European Economic Community (EEC) and NATO, closer governmental influence in the banking system, the creation of a national minimum wage and a ban on fox hunting led in 1981 to four former cabinet ministers from the right of the Labour Party (Shirley Williams, William Rodgers, Roy Jenkins and David Owen) forming the Social Democratic Party. Benn was only narrowly defeated by Healey in a bitterly fought deputy leadership election in 1981 after the introduction of an electoral college intended to widen the voting franchise to elect the leader and their deputy. By 1982, the National Executive Committee had concluded that the entryist Militant tendency group were in contravention of the party's constitution. The Militant newspaper's five member editorial board were expelled on 22 February 1983. Historically within the party, differentiation was made between the ""soft left"" and the ""hard left"", with the former embracing more moderately social democratic views while the hard left subscribed to a strongly socialist, even Marxist, ideology. Members on the hard left were often disparaged as the ""loony left,"" particularly in the popular media. The term ""hard left"" was sometimes used in the 1980s to describe Trotskyist groups such as the Militant tendency, Socialist Organiser and Socialist Action. In more recent times, Members of Parliament in the Socialist Campaign Group and the Labour Representation Committee are seen as constituting a hard left in contrast to a soft left represented by organisations such as Compass and the magazine Tribune. Foot resigned and was replaced as leader by Neil Kinnock, with Roy Hattersley as his deputy. The new leadership progressively dropped unpopular policies. The miners strike of 1984–85 over coal mine closures, for which miners' leader Arthur Scargill was blamed, and the Wapping dispute led to clashes with the left of the party, and negative coverage in most of the press. Tabloid vilification of the so-called loony left continued to taint the parliamentary party by association from the activities of 'extra-parliamentary' militants in local government. The 1923 general election was fought on the Conservatives' protectionist proposals but, although they got the most votes and remained the largest party, they lost their majority in parliament, necessitating the formation of a government supporting free trade. Thus, with the acquiescence of Asquith's Liberals, Ramsay MacDonald became the first ever Labour Prime Minister in January 1924, forming the first Labour government, despite Labour only having 191 MPs (less than a third of the House of Commons). Labour runs a minority government in the Welsh Assembly under Carwyn Jones, is the largest opposition party in the Scottish Parliament and has twenty MEPs in the European Parliament, sitting in the Socialists and Democrats Group. The party also organises in Northern Ireland, but does not contest elections to the Northern Ireland Assembly. The Labour Party is a full member of the Party of European Socialists and Progressive Alliance, and holds observer status in the Socialist International. In September 2015, Jeremy Corbyn was elected Leader of the Labour Party. In their first meeting after the election the group's Members of Parliament decided to adopt the name ""The Labour Party"" formally (15 February 1906). Keir Hardie, who had taken a leading role in getting the party established, was elected as Chairman of the Parliamentary Labour Party (in effect, the Leader), although only by one vote over David Shackleton after several ballots. In the party's early years the Independent Labour Party (ILP) provided much of its activist base as the party did not have individual membership until 1918 but operated as a conglomerate of affiliated bodies. The Fabian Society provided much of the intellectual stimulus for the party. One of the first acts of the new Liberal Government was to reverse the Taff Vale judgement. The Labour Party is considered to be left of centre. It was initially formed as a means for the trade union movement to establish political representation for itself at Westminster. It only gained a 'socialist' commitment with the original party constitution of 1918. That 'socialist' element, the original Clause IV, was seen by its strongest advocates as a straightforward commitment to the ""common ownership"", or nationalisation, of the ""means of production, distribution and exchange"". Although about a third of British industry was taken into public ownership after the Second World War, and remained so until the 1980s, the right of the party were questioning the validity of expanding on this objective by the late 1950s. Influenced by Anthony Crosland's book, The Future of Socialism (1956), the circle around party leader Hugh Gaitskell felt that the commitment was no longer necessary. While an attempt to remove Clause IV from the party constitution in 1959 failed, Tony Blair, and the 'modernisers' saw the issue as putting off potential voters, and were successful thirty-five years later, with only limited opposition from senior figures in the party. For many years Labour held to a policy of not allowing residents of Northern Ireland to apply for membership, instead supporting the Social Democratic and Labour Party (SDLP) which informally takes the Labour whip in the House of Commons. The 2003 Labour Party Conference accepted legal advice that the party could not continue to prohibit residents of the province joining, and whilst the National Executive has established a regional constituency party it has not yet agreed to contest elections there. In December 2015 a meeting of the members of the Labour Party in Northern Ireland decided unanimously to contest the elections for the Northern Ireland Assembly held in May 2016. Labour went on to win the 1950 general election, but with a much reduced majority of five seats. Soon afterwards, defence became a divisive issue within the party, especially defence spending (which reached a peak of 14% of GDP in 1951 during the Korean War), straining public finances and forcing savings elsewhere. The Chancellor of the Exchequer, Hugh Gaitskell, introduced charges for NHS dentures and spectacles, causing Bevan, along with Harold Wilson (then President of the Board of Trade), to resign over the dilution of the principle of free treatment on which the NHS had been established. Callaghan had been widely expected to call a general election in the autumn of 1978 when most opinion polls showed Labour to have a narrow lead. However he decided to extend his wage restraint policy for another year hoping that the economy would be in a better shape for a 1979 election. But during the winter of 1978–79 there were widespread strikes among lorry drivers, railway workers, car workers and local government and hospital workers in favour of higher pay-rises that caused significant disruption to everyday life. These events came to be dubbed the ""Winter of Discontent"". Finance proved a major problem for the Labour Party during this period; a ""cash for peerages"" scandal under Blair resulted in the drying up of many major sources of donations. Declining party membership, partially due to the reduction of activists' influence upon policy-making under the reforms of Neil Kinnock and Blair, also contributed to financial problems. Between January and March 2008, the Labour Party received just over £3 million in donations and were £17 million in debt; compared to the Conservatives' £6 million in donations and £12 million in debt. Clement Attlee's proved one of the most radical British governments of the 20th century, enacting Keynesian economic policies, presiding over a policy of nationalising major industries and utilities including the Bank of England, coal mining, the steel industry, electricity, gas, and inland transport (including railways, road haulage and canals). It developed and implemented the ""cradle to grave"" welfare state conceived by the economist William Beveridge. To this day, the party considers the 1948 creation of Britain's publicly funded National Health Service (NHS) under health minister Aneurin Bevan its proudest achievement. Attlee's government also began the process of dismantling the British Empire when it granted independence to India and Pakistan in 1947, followed by Burma (Myanmar) and Ceylon (Sri Lanka) the following year. At a secret meeting in January 1947, Attlee and six cabinet ministers, including Foreign Secretary Ernest Bevin, decided to proceed with the development of Britain's nuclear weapons programme, in opposition to the pacifist and anti-nuclear stances of a large element inside the Labour Party. The party was a member of the Labour and Socialist International between 1923 and 1940. Since 1951 the party has been a member of the Socialist International, which was founded thanks to the efforts of the Clement Attlee leadership. However, in February 2013, the Labour Party NEC decided to downgrade participation to observer membership status, ""in view of ethical concerns, and to develop international co-operation through new networks"". Labour was a founding member of the Progressive Alliance international founded in co-operation with the Social Democratic Party of Germany and other social-democratic parties on 22 May 2013. The ""yo yo"" in the opinion polls continued into 1992, though after November 1990 any Labour lead in the polls was rarely sufficient for a majority. Major resisted Kinnock's calls for a general election throughout 1991. Kinnock campaigned on the theme ""It's Time for a Change"", urging voters to elect a new government after more than a decade of unbroken Conservative rule. However, the Conservatives themselves had undergone a dramatic change in the change of leader from Thatcher to Major, at least in terms of style if not substance. From the outset, it was clearly a well-received change, as Labour's 14-point lead in the November 1990 ""Poll of Polls"" was replaced by an 8% Tory lead a month later. The 2015 General Election resulted in a net loss of seats throughout Great Britain, with Labour representation falling to 232 seats in the House of Commons. The Party lost 40 of its 41 seats in Scotland in the face of record breaking swings to the Scottish National Party. The scale of the decline in Labour's support was much greater than what had occurred at the 2011 elections for the Scottish parliament. Though Labour gained more than 20 seats in England and Wales, mostly from the Liberal Democrats but also from the Conservative Party, it lost more seats to Conservative challengers, including that of Ed Balls, for net losses overall. The government collapsed after only nine months when the Liberals voted for a Select Committee inquiry into the Campbell Case, a vote which MacDonald had declared to be a vote of confidence. The ensuing 1924 general election saw the publication, four days before polling day, of the Zinoviev letter, in which Moscow talked about a Communist revolution in Britain. The letter had little impact on the Labour vote—which held up. It was the collapse of the Liberal party that led to the Conservative landslide. The Conservatives were returned to power although Labour increased its vote from 30.7% to a third of the popular vote, most Conservative gains being at the expense of the Liberals. However many Labourites for years blamed their defeat on foul play (the Zinoviev Letter), thereby according to A. J. P. Taylor misunderstanding the political forces at work and delaying needed reforms in the party. After losing the 1970 general election, Labour returned to opposition, but retained Harold Wilson as Leader. Heath's government soon ran into trouble over Northern Ireland and a dispute with miners in 1973 which led to the ""three-day week"". The 1970s proved a difficult time to be in government for both the Conservatives and Labour due to the 1973 oil crisis which caused high inflation and a global recession. The Labour Party was returned to power again under Wilson a few weeks after the February 1974 general election, forming a minority government with the support of the Ulster Unionists. The Conservatives were unable to form a government alone as they had fewer seats despite receiving more votes numerically. It was the first general election since 1924 in which both main parties had received less than 40% of the popular vote and the first of six successive general elections in which Labour failed to reach 40% of the popular vote. In a bid to gain a majority, a second election was soon called for October 1974 in which Labour, still with Harold Wilson as leader, won a majority of three, gaining just 18 seats taking its total to 319. Labour has long been identified with red, a political colour traditionally affiliated with socialism and the labour movement. The party conference in 1931 passed a motion ""That this conference adopts Party Colours, which should be uniform throughout the country, colours to be red and gold"". Since the party's inception, the red flag has been Labour's official symbol; the flag has been associated with socialism and revolution ever since the 1789 French Revolution and the revolutions of 1848. The red rose, a symbol of social democracy, was adopted as the party symbol in 1986 as part of a rebranding exercise and is now incorporated into the party logo. As it was founded by the unions to represent the interests of working-class people, Labour's link with the unions has always been a defining characteristic of the party. In recent years this link has come under increasing strain, with the RMT being expelled from the party in 2004 for allowing its branches in Scotland to affiliate to the left-wing Scottish Socialist Party. Other unions have also faced calls from members to reduce financial support for the Party and seek more effective political representation for their views on privatisation, public spending cuts and the anti-trade union laws. Unison and GMB have both threatened to withdraw funding from constituency MPs and Dave Prentis of UNISON has warned that the union will write ""no more blank cheques"" and is dissatisfied with ""feeding the hand that bites us"". Union funding was redesigned in 2013 after the Falkirk candidate-selection controversy. From the late-1980s onwards, the party adopted free market policies, leading many observers to describe the Labour Party as social democratic or the Third Way, rather than democratic socialist. Other commentators go further and argue that traditional social democratic parties across Europe, including the British Labour Party, have been so deeply transformed in recent years that it is no longer possible to describe them ideologically as 'social democratic', and claim that this ideological shift has put new strains on the party's traditional relationship with the trade unions. In the 2010 general election on 6 May that year, Labour with 29.0% of the vote won the second largest number of seats (258). The Conservatives with 36.5% of the vote won the largest number of seats (307), but no party had an overall majority, meaning that Labour could still remain in power if they managed to form a coalition with at least one smaller party. However, the Labour Party would have had to form a coalition with more than one other smaller party to gain an overall majority; anything less would result in a minority government. On 10 May 2010, after talks to form a coalition with the Liberal Democrats broke down, Brown announced his intention to stand down as Leader before the Labour Party Conference but a day later resigned as both Prime Minister and party leader. Harriet Harman became the Leader of the Opposition and acting Leader of the Labour Party following the resignation of Gordon Brown on 11 May 2010, pending a leadership election subsequently won by Ed Miliband. Miliband emphasised ""responsible capitalism"" and greater state intervention to change the balance of the UK economy away from financial services. Tackling vested interests and opening up closed circles in British society were also themes he returned to a number of times. Miliband also argued for greater regulation on banks and the energy companies."
Geology,"In the laboratory, stratigraphers analyze samples of stratigraphic sections that can be returned from the field, such as those from drill cores. Stratigraphers also analyze data from geophysical surveys that show the locations of stratigraphic units in the subsurface. Geophysical data and well logs can be combined to produce a better view of the subsurface, and stratigraphers often use computer programs to do this in three dimensions. Stratigraphers can then use these data to reconstruct ancient processes occurring on the surface of the Earth, interpret past environments, and locate areas for water, coal, and hydrocarbon extraction. The following four timelines show the geologic time scale. The first shows the entire time from the formation of the Earth to the present, but this compresses the most recent eon. Therefore, the second scale shows the most recent eon with an expanded scale. The second scale compresses the most recent era, so the most recent era is expanded in the third scale. Since the Quaternary is a very short period with short epochs, it is further expanded in the fourth scale. The second, third, and fourth timelines are therefore each subsections of their preceding timeline as indicated by asterisks. The Holocene (the latest epoch) is too small to be shown clearly on the third timeline on the right, another reason for expanding the fourth scale. The Pleistocene (P) epoch. Q stands for the Quaternary period. The principle of cross-cutting relationships pertains to the formation of faults and the age of the sequences through which they cut. Faults are younger than the rocks they cut; accordingly, if a fault is found that penetrates some formations but not those on top of it, then the formations that were cut are older than the fault, and the ones that are not cut must be younger than the fault. Finding the key bed in these situations may help determine whether the fault is a normal fault or a thrust fault. Some modern scholars, such as Fielding H. Garrison, are of the opinion that the origin of the science of geology can be traced to Persia after the Muslim conquests had come to an end. Abu al-Rayhan al-Biruni (973–1048 CE) was one of the earliest Persian geologists, whose works included the earliest writings on the geology of India, hypothesizing that the Indian subcontinent was once a sea. Drawing from Greek and Indian scientific literature that were not destroyed by the Muslim conquests, the Persian scholar Ibn Sina (Avicenna, 981–1037) proposed detailed explanations for the formation of mountains, the origin of earthquakes, and other topics central to modern geology, which provided an essential foundation for the later development of the science. In China, the polymath Shen Kuo (1031–1095) formulated a hypothesis for the process of land formation: based on his observation of fossil animal shells in a geological stratum in a mountain hundreds of miles from the ocean, he inferred that the land was formed by erosion of the mountains and by deposition of silt. Petrologists can also use fluid inclusion data and perform high temperature and pressure physical experiments to understand the temperatures and pressures at which different mineral phases appear, and how they change through igneous and metamorphic processes. This research can be extrapolated to the field to understand metamorphic processes and the conditions of crystallization of igneous rocks. This work can also help to explain processes that occur within the Earth, such as subduction and magma chamber evolution. All of these processes do not necessarily occur in a single environment, and do not necessarily occur in a single order. The Hawaiian Islands, for example, consist almost entirely of layered basaltic lava flows. The sedimentary sequences of the mid-continental United States and the Grand Canyon in the southwestern United States contain almost-undeformed stacks of sedimentary rocks that have remained in place since Cambrian time. Other areas are much more geologically complex. In the southwestern United States, sedimentary, volcanic, and intrusive rocks have been metamorphosed, faulted, foliated, and folded. Even older rocks, such as the Acasta gneiss of the Slave craton in northwestern Canada, the oldest known rock in the world have been metamorphosed to the point where their origin is undiscernable without laboratory analysis. In addition, these processes can occur in stages. In many places, the Grand Canyon in the southwestern United States being a very visible example, the lower rock units were metamorphosed and deformed, and then deformation ended and the upper, undeformed units were deposited. Although any amount of rock emplacement and rock deformation can occur, and they can occur any number of times, these concepts provide a guide to understanding the geological history of an area. In the laboratory, biostratigraphers analyze rock samples from outcrop and drill cores for the fossils found in them. These fossils help scientists to date the core and to understand the depositional environment in which the rock units formed. Geochronologists precisely date rocks within the stratigraphic section in order to provide better absolute bounds on the timing and rates of deposition. Magnetic stratigraphers look for signs of magnetic reversals in igneous rock units within the drill cores. Other scientists perform stable isotope studies on the rocks to gain information about past climate. James Hutton is often viewed as the first modern geologist. In 1785 he presented a paper entitled Theory of the Earth to the Royal Society of Edinburgh. In his paper, he explained his theory that the Earth must be much older than had previously been supposed in order to allow enough time for mountains to be eroded and for sediments to form new rocks at the bottom of the sea, which in turn were raised up to become dry land. Hutton published a two-volume version of his ideas in 1795 (Vol. 1, Vol. 2). The first geological map of the U.S. was produced in 1809 by William Maclure. In 1807, Maclure commenced the self-imposed task of making a geological survey of the United States. Almost every state in the Union was traversed and mapped by him, the Allegheny Mountains being crossed and recrossed some 50 times. The results of his unaided labours were submitted to the American Philosophical Society in a memoir entitled Observations on the Geology of the United States explanatory of a Geological Map, and published in the Society's Transactions, together with the nation's first geological map. This antedates William Smith's geological map of England by six years, although it was constructed using a different classification of rocks. The development of plate tectonics provided a physical basis for many observations of the solid Earth. Long linear regions of geologic features could be explained as plate boundaries. Mid-ocean ridges, high regions on the seafloor where hydrothermal vents and volcanoes exist, were explained as divergent boundaries, where two plates move apart. Arcs of volcanoes and earthquakes were explained as convergent boundaries, where one plate subducts under another. Transform boundaries, such as the San Andreas fault system, resulted in widespread powerful earthquakes. Plate tectonics also provided a mechanism for Alfred Wegener's theory of continental drift, in which the continents move across the surface of the Earth over geologic time. They also provided a driving force for crustal deformation, and a new setting for the observations of structural geology. The power of the theory of plate tectonics lies in its ability to combine all of these observations into a single theory of how the lithosphere moves over the convecting mantle. The principle of inclusions and components states that, with sedimentary rocks, if inclusions (or clasts) are found in a formation, then the inclusions must be older than the formation that contains them. For example, in sedimentary rocks, it is common for gravel from an older formation to be ripped up and included in a newer layer. A similar situation with igneous rocks occurs when xenoliths are found. These foreign bodies are picked up as magma or lava flows, and are incorporated, later to cool in the matrix. As a result, xenoliths are older than the rock which contains them. When rock units are placed under horizontal compression, they shorten and become thicker. Because rock units, other than muds, do not significantly change in volume, this is accomplished in two primary ways: through faulting and folding. In the shallow crust, where brittle deformation can occur, thrust faults form, which cause deeper rock to move on top of shallower rock. Because deeper rock is often older, as noted by the principle of superposition, this can result in older rocks moving on top of younger ones. Movement along faults can result in folding, either because the faults are not planar or because rock layers are dragged along, forming drag folds as slip occurs along the fault. Deeper in the Earth, rocks behave plastically, and fold instead of faulting. These folds can either be those where the material in the center of the fold buckles upwards, creating ""antiforms"", or where it buckles downwards, creating ""synforms"". If the tops of the rock units within the folds remain pointing upwards, they are called anticlines and synclines, respectively. If some of the units in the fold are facing downward, the structure is called an overturned anticline or syncline, and if all of the rock units are overturned or the correct up-direction is unknown, they are simply called by the most general terms, antiforms and synforms. In the 1960s, a series of discoveries, the most important of which was seafloor spreading, showed that the Earth's lithosphere, which includes the crust and rigid uppermost portion of the upper mantle, is separated into a number of tectonic plates that move across the plastically deforming, solid, upper mantle, which is called the asthenosphere. There is an intimate coupling between the movement of the plates on the surface and the convection of the mantle: oceanic plate motions and mantle convection currents always move in the same direction, because the oceanic lithosphere is the rigid upper thermal boundary layer of the convecting mantle. This coupling between rigid plates moving on the surface of the Earth and the convecting mantle is called plate tectonics. There are three major types of rock: igneous, sedimentary, and metamorphic. The rock cycle is an important concept in geology which illustrates the relationships between these three types of rock, and magma. When a rock crystallizes from melt (magma and/or lava), it is an igneous rock. This rock can be weathered and eroded, and then redeposited and lithified into a sedimentary rock, or be turned into a metamorphic rock due to heat and pressure that change the mineral content of the rock which gives it a characteristic fabric. The sedimentary rock can then be subsequently turned into a metamorphic rock due to heat and pressure and is then weathered, eroded, deposited, and lithified, ultimately becoming a sedimentary rock. Sedimentary rock may also be re-eroded and redeposited, and metamorphic rock may also undergo additional metamorphism. All three types of rocks may be re-melted; when this happens, a new magma is formed, from which an igneous rock may once again crystallize. Among the most well-known experiments in structural geology are those involving orogenic wedges, which are zones in which mountains are built along convergent tectonic plate boundaries. In the analog versions of these experiments, horizontal layers of sand are pulled along a lower surface into a back stop, which results in realistic-looking patterns of faulting and the growth of a critically tapered (all angles remain the same) orogenic wedge. Numerical models work in the same way as these analog models, though they are often more sophisticated and can include patterns of erosion and uplift in the mountain belt. This helps to show the relationship between erosion and the shape of the mountain range. These studies can also give useful information about pathways for metamorphism through pressure, temperature, space, and time. The addition of new rock units, both depositionally and intrusively, often occurs during deformation. Faulting and other deformational processes result in the creation of topographic gradients, causing material on the rock unit that is increasing in elevation to be eroded by hillslopes and channels. These sediments are deposited on the rock unit that is going down. Continual motion along the fault maintains the topographic gradient in spite of the movement of sediment, and continues to create accommodation space for the material to deposit. Deformational events are often also associated with volcanism and igneous activity. Volcanic ashes and lavas accumulate on the surface, and igneous intrusions enter from below. Dikes, long, planar igneous intrusions, enter along cracks, and therefore often form in large numbers in areas that are being actively deformed. This can result in the emplacement of dike swarms, such as those that are observable across the Canadian shield, or rings of dikes around the lava tube of a volcano. At the beginning of the 20th century, important advancement in geological science was facilitated by the ability to obtain accurate absolute dates to geologic events using radioactive isotopes and other methods. This changed the understanding of geologic time. Previously, geologists could only use fossils and stratigraphic correlation to date sections of rock relative to one another. With isotopic dates it became possible to assign absolute ages to rock units, and these absolute dates could be applied to fossil sequences in which there was datable material, converting the old relative ages into new absolute ages. Seismologists can use the arrival times of seismic waves in reverse to image the interior of the Earth. Early advances in this field showed the existence of a liquid outer core (where shear waves were not able to propagate) and a dense solid inner core. These advances led to the development of a layered model of the Earth, with a crust and lithosphere on top, the mantle below (separated within itself by seismic discontinuities at 410 and 660 kilometers), and the outer core and inner core below that. More recently, seismologists have been able to create detailed images of wave speeds inside the earth in the same way a doctor images a body in a CT scan. These images have led to a much more detailed view of the interior of the Earth, and have replaced the simplified layered model with a much more dynamic model. Sir Charles Lyell first published his famous book, Principles of Geology, in 1830. This book, which influenced the thought of Charles Darwin, successfully promoted the doctrine of uniformitarianism. This theory states that slow geological processes have occurred throughout the Earth's history and are still occurring today. In contrast, catastrophism is the theory that Earth's features formed in single, catastrophic events and remained unchanged thereafter. Though Hutton believed in uniformitarianism, the idea was not widely accepted at the time. The principle of faunal succession is based on the appearance of fossils in sedimentary rocks. As organisms exist at the same time period throughout the world, their presence or (sometimes) absence may be used to provide a relative age of the formations in which they are found. Based on principles laid out by William Smith almost a hundred years before the publication of Charles Darwin's theory of evolution, the principles of succession were developed independently of evolutionary thought. The principle becomes quite complex, however, given the uncertainties of fossilization, the localization of fossil types due to lateral changes in habitat (facies change in sedimentary strata), and that not all fossils may be found globally at the same time. In addition to identifying rocks in the field, petrologists identify rock samples in the laboratory. Two of the primary methods for identifying rocks in the laboratory are through optical microscopy and by using an electron microprobe. In an optical mineralogy analysis, thin sections of rock samples are analyzed through a petrographic microscope, where the minerals can be identified through their different properties in plane-polarized and cross-polarized light, including their birefringence, pleochroism, twinning, and interference properties with a conoscopic lens. In the electron microprobe, individual locations are analyzed for their exact chemical compositions and variation in composition within individual crystals. Stable and radioactive isotope studies provide insight into the geochemical evolution of rock units. Structural geologists use microscopic analysis of oriented thin sections of geologic samples to observe the fabric within the rocks which gives information about strain within the crystalline structure of the rocks. They also plot and combine measurements of geological structures in order to better understand the orientations of faults and folds in order to reconstruct the history of rock deformation in the area. In addition, they perform analog and numerical experiments of rock deformation in large and small settings. For many geologic applications, isotope ratios of radioactive elements are measured in minerals that give the amount of time that has passed since a rock passed through its particular closure temperature, the point at which different radiometric isotopes stop diffusing into and out of the crystal lattice. These are used in geochronologic and thermochronologic studies. Common methods include uranium-lead dating, potassium-argon dating, argon-argon dating and uranium-thorium dating. These methods are used for a variety of applications. Dating of lava and volcanic ash layers found within a stratigraphic sequence can provide absolute age data for sedimentary rock units which do not contain radioactive isotopes and calibrate relative dating techniques. These methods can also be used to determine ages of pluton emplacement. Thermochemical techniques can be used to determine temperature profiles within the crust, the uplift of mountain ranges, and paleotopography. Geologists use a number of field, laboratory, and numerical modeling methods to decipher Earth history and understand the processes that occur on and inside the Earth. In typical geological investigations, geologists use primary information related to petrology (the study of rocks), stratigraphy (the study of sedimentary layers), and structural geology (the study of positions of rock units and their deformation). In many cases, geologists also study modern soils, rivers, landscapes, and glaciers; investigate past and current life and biogeochemical pathways, and use geophysical methods to investigate the subsurface. Extension causes the rock units as a whole to become longer and thinner. This is primarily accomplished through normal faulting and through the ductile stretching and thinning. Normal faults drop rock units that are higher below those that are lower. This typically results in younger units being placed below older units. Stretching of units can result in their thinning; in fact, there is a location within the Maria Fold and Thrust Belt in which the entire sedimentary sequence of the Grand Canyon can be seen over a length of less than a meter. Rocks at the depth to be ductilely stretched are often also metamorphosed. These stretched rocks can also pinch into lenses, known as boudins, after the French word for ""sausage"", because of their visual similarity."
Royal_assent,"When the act is assented to by the sovereign in person, or by empowered Royal Commissioners, royal assent is considered given at the moment when the assent is declared in the presence of both houses jointly assembled. When the procedure created by the Royal Assent Act 1967 is followed, assent is considered granted when the presiding officers of both houses, having received the letters patent from the king or queen signifying the assent, have notified their respective house of the grant of royal assent. Thus, if each presiding officer makes the announcement at a different time (for instance because one house is not sitting on a certain date), assent is regarded as effective when the second announcement is made. This is important because, under British Law, unless there is any provision to the contrary, an act takes effect on the date on which it receives royal assent and that date is not regarded as being the date when the letters patent are signed, or when they are delivered to the presiding officers of each house, but the date on which both houses have been formally acquainted of the assent. Royal assent is the method by which a country's constitutional monarch (possibly through a delegated official) formally approves an act of that nation's parliament, thus making it a law or letting it be promulgated as law. In the vast majority of contemporary monarchies, this act is considered to be little more than a formality; even in those nations which still permit their ruler to withhold the royal assent (such as the United Kingdom, Norway, and Liechtenstein), the monarch almost never does so, save in a dire political emergency or upon the advice of their government. While the power to withhold royal assent was once exercised often in European monarchies, it is exceedingly rare in the modern, democratic political atmosphere that has developed there since the 18th century. Originally, legislative power was exercised by the sovereign acting on the advice of the Curia Regis, or Royal Council, in which important magnates and clerics participated and which evolved into parliament. The so-called Model Parliament included bishops, abbots, earls, barons, and two knights from each shire and two burgesses from each borough among its members. In 1265, the Earl of Leicester irregularly called a full parliament without royal authorisation. The body eventually came to be divided into two branches: bishops, abbots, earls, and barons formed the House of Lords, while the shire and borough representatives formed the House of Commons. The King would seek the advice and consent of both houses before making any law. During Henry VI's reign, it became regular practice for the two houses to originate legislation in the form of bills, which would not become law unless the sovereign's assent was obtained, as the sovereign was, and still remains, the enactor of laws. Hence, all acts include the clause ""Be it enacted by the Queen's (King's) most Excellent Majesty, by and with the advice and consent of the Lords Spiritual and Temporal, and Commons, in this present Parliament assembled, and by the authority of the same, as follows..."". The Parliament Acts 1911 and 1949 provide a second potential preamble if the House of Lords were to be excluded from the process. In Commonwealth realms other than the UK, royal assent is granted or withheld either by the realm's sovereign or, more frequently, by the representative of the sovereign, the governor-general. In federated realms, assent in each state, province, or territory is granted or withheld by the representatives of the sovereign. In Australia, this is the governors of the states, administrators of the territories, or the governor-general in the Australian Capital Territory. For Canada, this is the lieutenant governors of the provinces. A lieutenant governor may defer assent to the governor general, and the governor general may defer assent to federal bills to the sovereign. During the rule of the succeeding Hanoverian dynasty, power was gradually exercised more by parliament and the government. The first Hanoverian monarch, George I, relied on his ministers to a greater extent than did previous monarchs. Later Hanoverian monarchs attempted to restore royal control over legislation: George III and George IV both openly opposed Catholic Emancipation and asserted that to grant assent to a Catholic emancipation bill would violate the Coronation Oath, which required the sovereign to preserve and protect the established Church of England from Papal domination and would grant rights to individuals who were in league with a foreign power which did not recognise their legitimacy. However, George IV reluctantly granted his assent upon the advice of his ministers. Thus, as the concept of ministerial responsibility has evolved, the power to withhold royal assent has fallen into disuse, both in the United Kingdom and in the other Commonwealth realms. Instead, the monarch directly grants royal assent by Order in Council. Assent is granted or refused on the advice of the Lord Chancellor. A recent example when assent was refused (or, more correctly, when the Lord Chancellor declined to present the law for assent) was in 2007, concerning reforms to the constitution of the Chief Pleas of Sark. (A revised version of the proposed reforms was subsequently given assent.) In 2011, campaigners against a law that sought to reduce the number of senators in the states of Jersey petitioned the Privy Council to advise the Queen to refuse royal assent. An Order in Council of 13 July 2011 established new rules for the consideration of petitions against granting royal assent. In the United Kingdom, a bill is presented for royal assent after it has passed all the required stages in both the House of Commons and the House of Lords. Under the Parliament Acts 1911 and 1949, the House of Commons may, under certain circumstances, direct that a bill be presented for assent despite lack of passage by the House of Lords. Officially, assent is granted by the sovereign or by Lords Commissioners authorised to act by letters patent. It may be granted in parliament or outside parliament; in the latter case, each house must be separately notified before the bill takes effect. When granting assent by commission, the sovereign authorises three or more (normally five) lords who are Privy Counsellors to grant assent in his or her name. The Lords Commissioners, as the monarch's representatives are known, wear scarlet parliamentary robes and sit on a bench between the throne and the Woolsack. The Lords Reading Clerk reads the commission aloud; the senior commissioner then states, ""My Lords, in obedience to Her Majesty's Commands, and by virtue of the Commission which has been now read, We do declare and notify to you, the Lords Spiritual and Temporal and Commons in Parliament assembled, that Her Majesty has given Her Royal Assent to the several Acts in the Commission mentioned."" The government, consisting of the monarch and the ministers, will then usually approve the proposal and the sovereign and one of the ministers signs the proposal with the addition of an enacting clause, thereafter notifying the States General that ""The King assents to the proposal."" It has happened in exceptional circumstances that the government does not approve a law that has been passed in parliament. In such a case, neither the monarch nor a minister will sign the bill, notifying the States General that ""The King will keep the proposal under advisement."" A law that has received royal assent will be published in the State Magazine, with the original being kept in the archives of the King's Offices. No provision within the constitution grants the monarch an ability to veto legislation directly; however, no provision prohibits the sovereign from withholding royal assent, which effectively constitutes a veto. When the Spanish media asked King Juan Carlos if he would endorse the bill legalising same-sex marriages, he answered ""Soy el Rey de España y no el de Bélgica"" (""I am the King of Spain and not that of Belgium"")—a reference to King Baudouin I of Belgium, who had refused to sign the Belgian law legalising abortion. The King gave royal assent to Law 13/2005 on 1 July 2005; the law was gazetted in the Boletín Oficial del Estado on 2 July and came into effect on 3 July 2005. Likewise, in 2010, King Juan Carlos gave royal assent to a law permitting abortion on demand. Royal assent is sometimes associated with elaborate ceremonies. In the United Kingdom, for instance, the sovereign may appear personally in the House of Lords or may appoint Lords Commissioners, who announce that royal assent has been granted at a ceremony held at the Palace of Westminster. However, royal assent is usually granted less ceremonially by letters patent. In other nations, such as Australia, the governor-general merely signs the bill. In Canada, the governor general may give assent either in person at a ceremony held in the Senate or by a written declaration notifying parliament of his or her agreement to the bill. The power of parliament to pass bills was often thwarted by monarchs. Charles I dissolved parliament in 1629, after it passed motions critical of and bills seeking to restrict his arbitrary exercise of power. During the eleven years of personal rule that followed, Charles performed legally dubious actions, such as raising taxes without parliament's approval. After the English Civil War, it was accepted that parliament should be summoned to meet regularly, but it was still commonplace for monarchs to refuse royal assent to bills. In 1678, Charles II withheld his assent from a bill ""for preserving the Peace of the Kingdom by raising the Militia, and continuing them in Duty for Two and Forty Days,"" suggesting that he, not parliament, should control the militia. The last Stuart monarch, Anne, similarly withheld on 11 March 1708, on the advice of her ministers, her assent from a bill for the settling of Militia in Scotland. No monarch has since withheld royal assent on a bill passed by the British parliament. Measures, which were the means by which the National Assembly for Wales passed legislation between 2006 and 2011, were assented to by the Queen by means of an Order in Council. Section 102 of the Government of Wales Act 2006 required the Clerk to the Assembly to present measures passed by the assembly after a four-week period during which the Counsel General for Wales or the Attorney General could refer the proposed measure to the Supreme Court for a decision as to whether the measure was within the assembly's legislative competence. If the Spanish monarch ever refused in conscience to grant royal assent, a procedure similar to the Belgian handling of King Baudouin's objection would not be possible under the current constitution. If the sovereign were ever declared incapable of discharging royal authority, his or her powers would not be transferred to the Cabinet, pending the parliamentary appointment of a regency. Instead, the constitution mandates the next person of age in the line of succession would immediately become regent. Therefore, had Juan Carlos followed the Belgian example in 2005 or 2010, a declaration of incapacity would have transferred power to Felipe, then the heir apparent. The constitution of Jordan grants its monarch the right to withhold assent to laws passed by its parliament. Article 93 of that document gives the Jordanian sovereign six months to sign or veto any legislation sent to him from the National Assembly; if he vetoes it within that timeframe, the assembly may override his veto by a two-thirds vote of both houses; otherwise, the law does not go into effect (but it may be reconsidered in the next session of the assembly). If the monarch fails to act within six months of the bill being presented to him, it becomes law without his signature. After the House of Representatives has debated the law, it either approves it and sends it to the Senate with the text ""The Second Chamber of the States General sends the following approved proposal of law to the First Chamber"", or it rejects it and returns it to the government with the text ""The Second Chamber of the States General has rejected the accompanying proposal of law."" If the upper house then approves the law, it sends it back to the government with the text ""To the King, The States General have accepted the proposal of law as it is offered here."" In Belgium, the sanction royale has the same legal effect as royal assent; the Belgian constitution requires a theoretically possible refusal of royal sanction to be countersigned—as any other act of the monarch—by a minister responsible before the House of Representatives. The monarch promulgates the law, meaning that he or she formally orders that the law be officially published and executed. In 1990, when King Baudouin advised his cabinet he could not, in conscience, sign a bill decriminalising abortion (a refusal patently not covered by a responsible minister), the Council of Ministers, at the King's own request, declared Baudouin incapable of exercising his powers. In accordance with the Belgian constitution, upon the declaration of the sovereign's incapacity, the Council of Ministers assumed the powers of the head of state until parliament could rule on the King's incapacity and appoint a regent. The bill was then assented to by all members of the Council of Ministers ""on behalf of the Belgian People"". In a joint meeting, both houses of parliament declared the King capable of exercising his powers again the next day. Under the Royal Assent Act 1967, royal assent can be granted by the sovereign in writing, by means of letters patent, that are presented to the presiding officer of each house of parliament. Then, the presiding officer makes a formal, but simple statement to the house, acquainting each house that royal assent has been granted to the acts mentioned. Thus, unlike the granting of royal assent by the monarch in person or by Royal Commissioners, the method created by the Royal Assent Act 1967 does not require both houses to meet jointly for the purpose of receiving the notice of royal assent. The standard text of the letters patent is set out in The Crown Office (Forms and Proclamations Rules) Order 1992, with minor amendments in 2000. In practice this remains the standard method, a fact that is belied by the wording of the letters patent for the appointment of the Royal Commissioners and by the wording of the letters patent for the granting of royal assent in writing under the 1967 Act (""... And forasmuch as We cannot at this time be present in the Higher House of Our said Parliament being the accustomed place for giving Our Royal Assent...""). In Canada, the traditional ceremony for granting assent in parliament was regularly used until the 21st century, long after it had been discontinued in the United Kingdom and other Commonwealth realms. One result, conceived as part of a string of royal duties intended to demonstrate Canada's status as an independent kingdom, was that King George VI personally assented to nine bills of the Canadian parliament during the 1939 royal tour of Canada—85 years after his great-grandmother, Queen Victoria, had last granted royal assent personally in the United Kingdom. Under the Royal Assent Act 2002, however, the alternative practice of granting assent in writing, with each house being notified separately ( the Speaker of the Senate or a representative reads to the senators the letters from the governor general regarding the written declaration of Royal Assent), was brought into force. As the act also provides, royal assent is to be signified—by the governor general, or, more often, by a deputy, usually a Justice of the Supreme Court, at least twice each calendar year: for the first appropriation measure and for at least one other act, usually the first non-appropriation measure passed. However, the act provides that a grant of royal assent is not rendered invalid by a failure to employ the traditional ceremony where required. Royal assent is the final stage in the legislative process for acts of the Scottish parliament. The process is governed by sections 28, 32, and 33 of the Scotland Act 1998. After a bill has been passed, the Presiding Officer of the Scottish Parliament submits it to the monarch for royal assent after a four-week period, during which the Advocate General for Scotland, the Lord Advocate, the Attorney General or the Secretary of State for Scotland may refer the bill to the Supreme Court of the United Kingdom (prior to 1 October 2009, the Judicial Committee of the Privy Council) for review of its legality. Royal assent is signified by letters patent under the Great Seal of Scotland in the following form which is set out in The Scottish Parliament (Letters Patent and Proclamations) Order 1999 (SI 1999/737) and of which notice is published in the London, Edinburgh, and Belfast Gazettes: While royal assent has not been withheld in the United Kingdom since 1708, it has often been withheld in British colonies and former colonies by governors acting on royal instructions. In the United States Declaration of Independence, colonists complained that George III ""has refused his Assent to Laws, the most wholesome and necessary for the public good [and] has forbidden his Governors to pass Laws of immediate and pressing importance, unless suspended in their operation till his Assent should be obtained; and when so suspended, he has utterly neglected to attend to them."" Even after colonies such as Canada, Australia, New Zealand, the Union of South Africa, and Newfoundland were granted responsible government, the British government continued to sometimes advise governors-general on the granting of assent; assent was also occasionally reserved to allow the British government to examine a bill before advising the governor-general. Since 1993, the Sodor and Man Diocesan Synod has had power to enact measures making provision ""with respect to any matter concerning the Church of England in the Island"". If approved by Tynwald, a measure ""shall have the force and effect of an Act of Tynwald upon the Royal Assent thereto being announced to Tynwald"". Between 1979 and 1993, the Synod had similar powers, but limited to the extension to the Isle of Man of measures of the General Synod. Before 1994, royal assent was granted by Order in Council, as for a bill, but the power to grant royal assent to measures has now been delegated to the lieutenant governor. A Measure does not require promulgation. During the 1960s, the ceremony of assenting by commission was discontinued and is now only employed once a year, at the end of the annual parliamentary session. In 1960, the Gentleman Usher of the Black Rod arrived to summon the House of Commons during a heated debate and several members protested against the disruption by refusing to attend the ceremony. The debacle was repeated in 1965; this time, when the Speaker left the chair to go to the House of Lords, some members continued to make speeches. As a result, the Royal Assent Act 1967 was passed, creating an additional form for the granting of royal assent. As the attorney-general explained, ""there has been a good deal of resentment not only at the loss of Parliamentary time that has been involved but at the breaking of the thread of a possibly eloquent speech and the disruption of a debate that may be caused."" The granting of assent by the monarch in person, or by commission, is still possible, but this third form is used on a day-to-day basis. In Australia, a technical issue arose with the royal assent in both 1976 and 2001. In 1976, a bill originating in the House of Representatives was mistakenly submitted to the Governor-General and assented to. However, it was later discovered that it had not been passed by each house. The error arose because two bills of the same title had originated from the house. The Governor-General revoked the first assent, before assenting to the bill which had actually passed. The same procedure was followed to correct a similar error which arose in 2001. Since the Balfour Declaration of 1926 and the Statute of Westminster 1931, the all Commonwealth realms have been sovereign kingdoms, the monarch and governors-general acting solely on the advice of the local ministers who generally maintain the support of the legislature and are the ones who secure the passage of bills. They, therefore, are unlikely to advise the sovereign or his or her representative to withhold assent. The power to withhold the royal assent was exercised by Alberta's lieutenant governor, John C. Bowen, in 1937, in respect of three bills passed in the legislature dominated by William Aberhart's Social Credit party. Two bills sought to put banks under the authority of the province, thereby interfering with the federal government's powers. The third, the Accurate News and Information Bill, purported to force newspapers to print government rebuttals to stories to which the provincial cabinet objected. The unconstitutionality of all three bills was later confirmed by the Supreme Court of Canada and by the Judicial Committee of the Privy Council. Independently of the method used to signify royal assent, it is the responsibility of the Clerk of the Parliaments, once the assent has been duly notified to both houses, not only to endorse the act in the name of the monarch with the formal Norman French formula, but to certify that assent has been granted. The clerk signs one authentic copy of the bill and inserts the date (in English) on which the assent was notified to the two houses after the title of the act. When an act is published, the signature of the clerk is omitted, as is the Norman French formula, should the endorsement have been made in writing. However, the date on which the assent was notified is printed in brackets. Articles 41 and 68 of the constitution empower the sovereign to withhold royal assent from bills adopted by the Legislative Assembly. In 2010, the kingdom moved towards greater democracy, with King George Tupou V saying that he would be guided by his prime minister in the exercising of his powers. Nonetheless, this does not preclude an independent royal decision to exercise a right of veto. In November 2011, the assembly adopted an Arms and Ammunitions (Amendment) Bill, which reduced the possible criminal sentences for the illicit possession of firearms. The bill was adopted by ten votes to eight. Two members of the assembly had recently been charged with the illicit possession of firearms. The Prime Minister, Lord Tuʻivakanō, voted in favour of the amendment. Members of the opposition denounced the bill and asked the King to veto it, which he did in December. If the Governor General of Canada is unable to give assent, it can be done by either the Deputy of the Governor General of Canada—the Chief Justice of Canada—or another justice of the Supreme Court of Canada. It is not actually necessary for the governor general to sign a bill passed by a legislature, the signature being merely an attestation. In each case, the parliament must be apprised of the granting of assent before the bill is considered to have become law. Two methods are available: the sovereign's representatives may grant assent in the presence of both houses of parliament; alternatively, each house may be notified separately, usually by the speaker of that house. However, though both houses must be notified on the same day, notice to the House of Commons while it is not in session may be given by way of publishing a special issue of the Journals of the House of Commons, whereas the Senate must be sitting and the governor general's letter read aloud by the speaker. Special procedures apply to legislation passed by Tynwald, the legislature of the Isle of Man. Before the lordship of the Island was purchased by the British Crown in 1765 (the Revestment), the assent of the Lord of Mann to a bill was signified by letter to the governor. After 1765, royal assent was at first signified by letter from the Secretary of State to the governor; but, during the British Regency, the practice began of granting royal assent by Order in Council, which continues to this day, though limited to exceptional cases since 1981. A new device for granting assent was created during the reign of King Henry VIII. In 1542, Henry sought to execute his fifth wife, Catherine Howard, whom he accused of committing adultery; the execution was to be authorised not after a trial but by a bill of attainder, to which he would have to personally assent after listening to the entire text. Henry decided that ""the repetition of so grievous a Story and the recital of so infamous a crime"" in his presence ""might reopen a Wound already closing in the Royal Bosom"". Therefore, parliament inserted a clause into the Act of Attainder, providing that assent granted by Commissioners ""is and ever was and ever shall be, as good"" as assent granted by the sovereign personally. The procedure was used only five times during the 16th century, but more often during the 17th and 18th centuries, especially when George III's health began to deteriorate. Queen Victoria became the last monarch to personally grant assent in 1854. Royal assent is not sufficient to give legal effect to an Act of Tynwald. By ancient custom, an act did not come into force until it had been promulgated at an open-air sitting of Tynwald, usually held on Tynwald Hill at St John's on St John's Day (24 June), but, since the adoption of the Gregorian calendar in 1753, on 5 July (or on the following Monday if 5 July is a Saturday or Sunday). Promulgation originally consisted of the reading of the Act in English and Manx; but, after 1865 the reading of the title of the act and a summary of each section were sufficient. This was reduced in 1895 to the titles and a memorandum of the object and purport of the act, and, since 1988, only the short title and a summary of the long title have been read. Before the reign of Henry VIII, the sovereign always granted his or her assent in person. The sovereign, wearing the Imperial State Crown, would be seated on the throne in the Lords chamber, surrounded by heralds and members of the royal court—a scene that nowadays is repeated only at the annual State Opening of Parliament. The Commons, led by their speaker, would listen from the Bar of the Lords, just outside the chamber. The Clerk of the Parliaments presented the bills awaiting assent to the monarch, save that supply bills were traditionally brought up by the speaker. The Clerk of the Crown, standing on the sovereign's right, then read aloud the titles of the bills (in earlier times, the entire text of the bills). The Clerk of the Parliaments, standing on the sovereign's left, responded by stating the appropriate Norman French formula. The Royal Assent ceremony takes place in the Senate, as the sovereign is traditionally barred from the House of Commons. On the day of the event, the Speaker of the Senate will read to the chamber a notice from the secretary to the governor general indicating when the viceroy or a deputy thereof will arrive. The Senate thereafter cannot adjourn until after the ceremony. The speaker moves to sit beside the throne, the Mace Bearer, with mace in hand, stands adjacent to him or her, and the governor general enters to take the speaker's chair. The Usher of the Black Rod is then commanded by the speaker to summon the Members of Parliament, who follow Black Rod back to the Senate, the Sergeant-at-Arms carrying the mace of the House of Commons. In the Senate, those from the commons stand behind the bar, while Black Rod proceeds to stand next to the governor general, who then nods his or her head to signify Royal Assent to the presented bills (which do not include appropriations bills). Once the list of bills is complete, the Clerk of the Senate states: ""in Her Majesty's name, His [or Her] Excellency the Governor General [or the deputy] doth assent to these bills."" If there are any appropriation bills to receive Royal Assent, the Speaker of the House of Commons will read their titles and the Senate clerk repeats them to the governor general, who nods his or her head to communicate Royal Assent. When these bills have all been assented to, the Clerk of the Senate recites ""in Her Majesty's name, His [or Her] Excellency the Governor General [or the deputy] thanks her loyal subjects, accepts their benevolence and assents to these bills. The governor general or his or her deputy then depart parliament. In Australia, the formal ceremony of granting assent in parliament has not been regularly used since the early 20th century. Now, the bill is sent to the governor-general's residence by the house in which it originated. The governor-general then signs the bill, sending messages to the President of the Senate and the Speaker of the House of Representatives, who notify their respective houses of the governor-general's action. A similar practice is followed in New Zealand, where the governor-general has not personally granted the Royal Assent in parliament since 1875. Under modern constitutional conventions, the sovereign acts on the advice of his or her ministers. Since these ministers most often maintain the support of parliament and are the ones who obtain the passage of bills, it is highly improbable that they would advise the sovereign to withhold assent. An exception is sometimes stated to be if bills are not passed in good faith, though it is difficult to make an interpretation on what this circumstance might constitute. Hence, in modern practice, royal assent is always granted; a refusal to do so would be appropriate only in an emergency requiring the use of the monarch's reserve powers. Title IV of the 1978 Spanish constitution invests the Consentimiento Real (Royal Assent) and promulgation (publication) of laws with the monarch of Spain, while Title III, The Cortes Generales, Chapter 2, Drafting of Bills, outlines the method by which bills are passed. According to Article 91, within fifteen days of passage of a bill by the Cortes Generales, the sovereign shall give his or her assent and publish the new law. Article 92 invests the monarch with the right to call for a referendum, on the advice of the president of the government (commonly referred to in English as the prime minister) and the authorisation of the cortes. Articles 77–79 of the Norwegian Constitution specifically grant the monarch of Norway the right to withhold royal assent from any bill passed by the Storting. Should the sovereign ever choose to exercise this privilege, Article 79 provides a means by which his veto may be over-ridden: ""If a Bill has been passed unaltered by two sessions of the Storting, constituted after two separate successive elections and separated from each other by at least two intervening sessions of the Storting, without a divergent Bill having been passed by any Storting in the period between the first and last adoption, and it is then submitted to the King with a petition that His Majesty shall not refuse his assent to a Bill which, after the most mature deliberation, the Storting considers to be beneficial, it shall become law even if the Royal Assent is not accorded before the Storting goes into recess."" The Clerk of the Parliaments, an official of the House of Lords, traditionally states a formula in Anglo-Norman Law French, indicating the sovereign's decision. The granting of royal assent to a supply bill is indicated with the words ""La Reyne remercie ses bons sujets, accepte leur benevolence, et ainsi le veult"", translated as ""The Queen thanks her good subjects, accepts their bounty, and wills it so."" For other public or private bills, the formula is simply ""La Reyne le veult"" (""the Queen wills it""). For personal bills, the phrase is ""Soit fait comme il est désiré"" (""let it be as it is desired""). The appropriate formula for withholding assent is the euphemistic ""La Reyne s'avisera"" (""the Queen will consider it""). When the sovereign is male, Le Roy is substituted for La Reyne."
1973_oil_crisis,"In 2004, declassified documents revealed that the U.S. was so distraught by the rise in oil prices and being challenged by under-developed countries that they briefly considered military action to forcibly seize Middle Eastern oilfields in late 1973. Although no explicit plan was mentioned, a conversation between U.S. Secretary of Defense James Schlesinger and British Ambassador to the United States Lord Cromer revealed Schlesinger had told him that ""it was no longer obvious to him that the U.S. could not use force."" British Prime Minister Edward Heath was so worried by this prospect that he ordered a British intelligence estimate of U.S. intentions, which concluded America ""might consider it could not tolerate a situation in which the U.S. and its allies were at the mercy of a small group of unreasonable countries,"" and that they would prefer a rapid operation to seize oilfields in Saudi Arabia and Kuwait, and possibly Abu Dhabi in military action was decided upon. Although the Soviet response to such an act would likely not involve force, intelligence warned ""the American occupation would need to last 10 years as the West developed alternative energy sources, and would result in the ‘total alienation’ of the Arabs and much of the rest of the Third World."" The embargo was not uniform across Europe. Of the nine members of the European Economic Community (EEC), the Netherlands faced a complete embargo, the UK and France received almost uninterrupted supplies (having refused to allow America to use their airfields and embargoed arms and supplies to both the Arabs and the Israelis), while the other six faced partial cutbacks. The UK had traditionally been an ally of Israel, and Harold Wilson's government supported the Israelis during the Six-Day War. His successor, Ted Heath, reversed this policy in 1970, calling for Israel to withdraw to its pre-1967 borders. OPEC soon lost its preeminent position, and in 1981, its production was surpassed by that of other countries. Additionally, its own member nations were divided. Saudi Arabia, trying to recover market share, increased production, pushing prices down, shrinking or eliminating profits for high-cost producers. The world price, which had peaked during the 1979 energy crisis at nearly $40 per barrel, decreased during the 1980s to less than $10 per barrel. Adjusted for inflation, oil briefly fell back to pre-1973 levels. This ""sale"" price was a windfall for oil-importing nations, both developing and developed. To help reduce consumption, in 1974 a national maximum speed limit of 55 mph (about 88 km/h) was imposed through the Emergency Highway Energy Conservation Act. Development of the Strategic Petroleum Reserve began in 1975, and in 1977 the cabinet-level Department of Energy was created, followed by the National Energy Act of 1978.[citation needed] On November 28, 1995, Bill Clinton signed the National Highway Designation Act, ending the federal 55 mph (89 km/h) speed limit, allowing states to restore their prior maximum speed limit. In 1973, Nixon named William E. Simon as the first Administrator of the Federal Energy Office, a short-term organization created to coordinate the response to the embargo. Simon allocated states the same amount of domestic oil for 1974 that each had consumed in 1972, which worked for states whose populations were not increasing. In other states, lines at gasoline stations were common. The American Automobile Association reported that in the last week of February 1974, 20% of American gasoline stations had no fuel. This contributed to the ""Oil Shock"". After 1971, OPEC was slow to readjust prices to reflect this depreciation. From 1947 to 1967, the dollar price of oil had risen by less than two percent per year. Until the oil shock, the price had also remained fairly stable versus other currencies and commodities. OPEC ministers had not developed institutional mechanisms to update prices in sync with changing market conditions, so their real incomes lagged. The substantial price increases of 1973–1974 largely returned their prices and corresponding incomes to Bretton Woods levels in terms of commodities such as gold. Despite being relatively unaffected by the embargo, the UK nonetheless faced an oil crisis of its own - a series of strikes by coal miners and railroad workers over the winter of 1973–74 became a major factor in the change of government. Heath asked the British to heat only one room in their houses over the winter. The UK, Germany, Italy, Switzerland and Norway banned flying, driving and boating on Sundays. Sweden rationed gasoline and heating oil. The Netherlands imposed prison sentences for those who used more than their ration of electricity. An increase in imported cars into North America forced General Motors, Ford and Chrysler to introduce smaller and fuel-efficient models for domestic sales. The Dodge Omni / Plymouth Horizon from Chrysler, the Ford Fiesta and the Chevrolet Chevette all had four-cylinder engines and room for at least four passengers by the late 1970s. By 1985, the average American vehicle moved 17.4 miles per gallon, compared to 13.5 in 1970. The improvements stayed even though the price of a barrel of oil remained constant at $12 from 1974 to 1979. Sales of large sedans for most makes (except Chrysler products) recovered within two model years of the 1973 crisis. The Cadillac DeVille and Fleetwood, Buick Electra, Oldsmobile 98, Lincoln Continental, Mercury Marquis, and various other luxury oriented sedans became popular again in the mid-1970s. The only full-size models that did not recover were lower price models such as the Chevrolet Bel Air, and Ford Galaxie 500. Slightly smaller, mid-size models such as the Oldsmobile Cutlass, Chevrolet Monte Carlo, Ford Thunderbird and various other models sold well. In response to American aid to Israel, on October 16, 1973, OPEC raised the posted price of oil by 70%, to $5.11 a barrel. The following day, oil ministers agreed to the embargo, a cut in production by five percent from September's output and to continue to cut production in five percent monthly increments until their economic and political objectives were met. On October 19, Nixon requested Congress to appropriate $2.2 billion in emergency aid to Israel, including $1.5 billion in outright grants. George Lenczowski notes, ""Military supplies did not exhaust Nixon's eagerness to prevent Israel's collapse...This [$2.2 billion] decision triggered a collective OPEC response."" Libya immediately announced it would embargo oil shipments to the United States. Saudi Arabia and the other Arab oil-producing states joined the embargo on October 20, 1973. At their Kuwait meeting, OAPEC proclaimed the embargo that curbed exports to various countries and blocked all oil deliveries to the US as a ""principal hostile country"". The USSR's invasion of Afghanistan was only one sign of insecurity in the region, also marked by increased American weapons sales, technology, and outright military presence. Saudi Arabia and Iran became increasingly dependent on American security assurances to manage both external and internal threats, including increased military competition between them over increased oil revenues. Both states were competing for preeminence in the Persian Gulf and using increased revenues to fund expanded militaries. By 1979, Saudi arms purchases from the US exceeded five times Israel's. Another motive for the large scale purchase of arms from the US by Saudi Arabia was the failure of the Shah during January 1979 to maintain control of Iran, a non-Arabic but largely Shiite Muslim nation, which fell to a theocratic Islamist government under the Ayatollah Ruhollah Khomeini in the wake of the 1979 Iranian Revolution. Saudi Arabia, on the other hand, is an Arab, largely Sunni Muslim nation headed by a near absolutist monarchy. In the wake of the Iranian revolution the Saudis were forced to deal with the prospect of internal destabilization via the radicalism of Islamism, a reality which would quickly be revealed in the seizure of the Grand Mosque in Mecca by Wahhabi extremists during November 1979 and a Shiite revolt in the oil rich Al-Hasa region of Saudi Arabia in December of the same year. In November 2010, Wikileaks leaked confidential diplomatic cables pertaining to the United States and its allies which revealed that the late Saudi King Abdullah urged the United States to attack Iran in order to destroy its potential nuclear weapons program, describing Iran as ""a snake whose head should be cut off without any procrastination."" Although lacking historical connections to the Middle East, Japan was the country most dependent on Arab oil. 71% of its imported oil came from the Middle East in 1970. On November 7, 1973, the Saudi and Kuwaiti governments declared Japan a ""nonfriendly"" country to encourage it to change its noninvolvement policy. It received a 5% production cut in December, causing a panic. On November 22, Japan issued a statement ""asserting that Israel should withdraw from all of the 1967 territories, advocating Palestinian self-determination, and threatening to reconsider its policy toward Israel if Israel refused to accept these preconditions"". By December 25, Japan was considered an Arab-friendly state. Federal safety standards, such as NHTSA Federal Motor Vehicle Safety Standard 215 (pertaining to safety bumpers), and compacts like the 1974 Mustang I were a prelude to the DOT ""downsize"" revision of vehicle categories. By 1977, GM's full-sized cars reflected the crisis. By 1979, virtually all ""full-size"" American cars had shrunk, featuring smaller engines and smaller outside dimensions. Chrysler ended production of their full-sized luxury sedans at the end of the 1981 model year, moving instead to a full front-wheel drive lineup for 1982 (except for the M-body Dodge Diplomat/Plymouth Gran Fury and Chrysler New Yorker Fifth Avenue sedans). The crisis reduced the demand for large cars. Japanese imports, primarily the Toyota Corona, the Toyota Corolla, the Datsun B210, the Datsun 510, the Honda Civic, the Mitsubishi Galant (a captive import from Chrysler sold as the Dodge Colt), the Subaru DL, and later the Honda Accord all had four cylinder engines that were more fuel efficient than the typical American V8 and six cylinder engines. Japanese imports became mass-market leaders with unibody construction and front-wheel drive, which became de facto standards. The energy crisis led to greater interest in renewable energy, nuclear power and domestic fossil fuels. There is criticism that American energy policies since the crisis have been dominated by crisis-mentality thinking, promoting expensive quick fixes and single-shot solutions that ignore market and technology realities. Instead of providing stable rules that support basic research while leaving plenty of scope for entrepreneurship and innovation, congresses and presidents have repeatedly backed policies which promise solutions that are politically expedient, but whose prospects are doubtful. Compact trucks were introduced, such as the Toyota Hilux and the Datsun Truck, followed by the Mazda Truck (sold as the Ford Courier), and the Isuzu-built Chevrolet LUV. Mitsubishi rebranded its Forte as the Dodge D-50 a few years after the oil crisis. Mazda, Mitsubishi and Isuzu had joint partnerships with Ford, Chrysler, and GM, respectively. Later the American makers introduced their domestic replacements (Ford Ranger, Dodge Dakota and the Chevrolet S10/GMC S-15), ending their captive import policy. On October 6, 1973, Syria and Egypt, with support from other Arab nations, launched a surprise attack on Israel, on Yom Kippur. This renewal of hostilities in the Arab–Israeli conflict released the underlying economic pressure on oil prices. At the time, Iran was the world's second-largest oil exporter and a close US ally. Weeks later, the Shah of Iran said in an interview: ""Of course [the price of oil] is going to rise... Certainly! And how!... You've [Western nations] increased the price of the wheat you sell us by 300 percent, and the same for sugar and cement... You buy our crude oil and sell it back to us, refined as petrochemicals, at a hundred times the price you've paid us... It's only fair that, from now on, you should pay more for oil. Let's say ten times more."" The 1973 oil crisis began in October 1973 when the members of the Organization of Arab Petroleum Exporting Countries (OAPEC, consisting of the Arab members of OPEC plus Egypt and Syria) proclaimed an oil embargo. By the end of the embargo in March 1974, the price of oil had risen from US$3 per barrel to nearly $12 globally; US prices were significantly higher. The embargo caused an oil crisis, or ""shock"", with many short- and long-term effects on global politics and the global economy. It was later called the ""first oil shock"", followed by the 1979 oil crisis, termed the ""second oil shock."" The crisis had a major impact on international relations and created a rift within NATO. Some European nations and Japan sought to disassociate themselves from United States foreign policy in the Middle East to avoid being targeted by the boycott. Arab oil producers linked any future policy changes to peace between the belligerents. To address this, the Nixon Administration began multilateral negotiations with the combatants. They arranged for Israel to pull back from the Sinai Peninsula and the Golan Heights. By January 18, 1974, US Secretary of State Henry Kissinger had negotiated an Israeli troop withdrawal from parts of the Sinai Peninsula. The promise of a negotiated settlement between Israel and Syria was enough to convince Arab oil producers to lift the embargo in March 1974. Price controls exacerbated the crisis in the US. The system limited the price of ""old oil"" (that which had already been discovered) while allowing newly discovered oil to be sold at a higher price to encourage investment. Predictably, old oil was withdrawn from the market, creating greater scarcity. The rule also discouraged development of alternative energies. The rule had been intended to promote oil exploration. Scarcity was addressed by rationing (as in many countries). Motorists faced long lines at gas stations beginning in summer 1972 and increasing by summer 1973. On August 15, 1971, the United States unilaterally pulled out of the Bretton Woods Accord. The US abandoned the Gold Exchange Standard whereby the value of the dollar had been pegged to the price of gold and all other currencies were pegged to the dollar, whose value was left to ""float"" (rise and fall according to market demand). Shortly thereafter, Britain followed, floating the pound sterling. The other industrialized nations followed suit with their respective currencies. Anticipating that currency values would fluctuate unpredictably for a time, the industrialized nations increased their reserves (by expanding their money supplies) in amounts far greater than before. The result was a depreciation of the dollar and other industrialized nations' currencies. Because oil was priced in dollars, oil producers' real income decreased. In September 1971, OPEC issued a joint communiqué stating that, from then on, they would price oil in terms of a fixed amount of gold. Some buyers lamented the small size of the first Japanese compacts, and both Toyota and Nissan (then known as Datsun) introduced larger cars such as the Toyota Corona Mark II, the Toyota Cressida, the Mazda 616 and Datsun 810, which added passenger space and amenities such as air conditioning, power steering, AM-FM radios, and even power windows and central locking without increasing the price of the vehicle. A decade after the 1973 oil crisis, Honda, Toyota and Nissan, affected by the 1981 voluntary export restraints, opened US assembly plants and established their luxury divisions (Acura, Lexus and Infiniti, respectively) to distinguish themselves from their mass-market brands. Some of the income was dispensed in the form of aid to other underdeveloped nations whose economies had been caught between higher oil prices and lower prices for their own export commodities, amid shrinking Western demand. Much went for arms purchases that exacerbated political tensions, particularly in the Middle East. Saudi Arabia spent over 100 billion dollars in the ensuing decades for helping spread its fundamentalist interpretation of Islam, known as Wahhabism, throughout the world, via religious charities such al-Haramain Foundation, which often also distributed funds to violent Sunni extremist groups such as Al-Qaeda and the Taliban. In the United States, scholars argue that there already existed a negotiated settlement based on equality between both parties prior to 1973. The possibility that the Middle East could become another superpower confrontation with the USSR was of more concern to the US than oil. Further, interest groups and government agencies more worried about energy were no match for Kissinger's dominance. In the US production, distribution and price disruptions ""have been held responsible for recessions, periods of excessive inflation, reduced productivity, and lower economic growth."" The embargo had a negative influence on the US economy by causing immediate demands to address the threats to U.S. energy security. On an international level, the price increases changed competitive positions in many industries, such as automobiles. Macroeconomic problems consisted of both inflationary and deflationary impacts. The embargo left oil companies searching for new ways to increase oil supplies, even in rugged terrain such as the Arctic. Finding oil and developing new fields usually required five to ten years before significant production."
Premier_League,"The Premier League sends representatives to UEFA's European Club Association, the number of clubs and the clubs themselves chosen according to UEFA coefficients. For the 2012–13 season the Premier League has 10 representatives in the Association: Arsenal, Aston Villa, Chelsea, Everton, Fulham, Liverpool, Manchester City, Manchester United, Newcastle United and Tottenham Hotspur. The European Club Association is responsible for electing three members to UEFA's Club Competitions Committee, which is involved in the operations of UEFA competitions such as the Champions League and UEFA Europa League. One significant feature of the Premier League in the mid-2000s was the dominance of the so-called ""Big Four"" clubs: Arsenal, Chelsea, Liverpool and Manchester United. During this decade, and particularly from 2002 to 2009, they dominated the top four spots, which came with UEFA Champions League qualification, taking all top four places in 5 out of 6 seasons from 2003–04 to 2008–09 inclusive, with Arsenal going as far as winning the league without losing a single game in 2003–04, the only time it has ever happened in the Premier League. In May 2008 Kevin Keegan stated that ""Big Four"" dominance threatened the division, ""This league is in danger of becoming one of the most boring but great leagues in the world."" Premier League chief executive Richard Scudamore said in defence: ""There are a lot of different tussles that go on in the Premier League depending on whether you're at the top, in the middle or at the bottom that make it interesting."" The TV rights agreement between the Premier League and Sky has faced accusations of being a cartel, and a number of court cases have arisen as a result. An investigation by the Office of Fair Trading in 2002 found BSkyB to be dominant within the pay TV sports market, but concluded that there were insufficient grounds for the claim that BSkyB had abused its dominant position. In July 1999 the Premier League's method of selling rights collectively for all member clubs was investigated by the UK Restrictive Practices Court, who concluded that the agreement was not contrary to the public interest. Due to insistence by the International Federation of Association Football (FIFA), the international governing body of football, that domestic leagues reduce the number of games clubs played, the number of clubs was reduced to 20 in 1995 when four teams were relegated from the league and only two teams promoted. On 8 June 2006, FIFA requested that all major European leagues, including Italy's Serie A and Spain's La Liga be reduced to 18 teams by the start of the 2007–08 season. The Premier League responded by announcing their intention to resist such a reduction. Ultimately, the 2007–08 season kicked off again with 20 teams. Participation in the Premier League by some Scottish or Irish clubs has sometimes been discussed, but without result. The idea came closest to reality in 1998, when Wimbledon received Premier League approval to relocate to Dublin, Ireland, but the move was blocked by the Football Association of Ireland. Additionally, the media occasionally discusses the idea that Scotland's two biggest teams, Celtic and Rangers, should or will take part in the Premier League, but nothing has come of these discussions. Despite significant European success during the 1970s and early 1980s, the late '80s had marked a low point for English football. Stadiums were crumbling, supporters endured poor facilities, hooliganism was rife, and English clubs were banned from European competition for five years following the Heysel Stadium disaster in 1985. The Football League First Division, which had been the top level of English football since 1888, was well behind leagues such as Italy's Serie A and Spain's La Liga in attendances and revenues, and several top English players had moved abroad. As of the 2015–16 season, Premier League football has been played in 53 stadiums since the formation of the Premier League in 1992. The Hillsborough disaster in 1989 and the subsequent Taylor Report saw a recommendation that standing terraces should be abolished; as a result all stadiums in the Premier League are all-seater. Since the formation of the Premier League, football grounds in England have seen constant improvements to capacity and facilities, with some clubs moving to new-build stadiums. Nine stadiums that have seen Premier League football have now been demolished. The stadiums for the 2010–11 season show a large disparity in capacity: Old Trafford, the home of Manchester United has a capacity of 75,957 with Bloomfield Road, the home of Blackpool, having a capacity of 16,220. The combined total capacity of the Premier League in the 2010–11 season is 770,477 with an average capacity of 38,523. There are 20 clubs in the Premier League. During the course of a season (from August to May) each club plays the others twice (a double round-robin system), once at their home stadium and once at that of their opponents, for a total of 38 games. Teams receive three points for a win and one point for a draw. No points are awarded for a loss. Teams are ranked by total points, then goal difference, and then goals scored. If still equal, teams are deemed to occupy the same position. If there is a tie for the championship, for relegation, or for qualification to other competitions, a play-off match at a neutral venue decides rank. The three lowest placed teams are relegated into the Football League Championship, and the top two teams from the Championship, together with the winner of play-offs involving the third to sixth placed Championship clubs, are promoted in their place. Managers in the Premier League are involved in the day-to-day running of the team, including the training, team selection, and player acquisition. Their influence varies from club-to-club and is related to the ownership of the club and the relationship of the manager with fans. Managers are required to have a UEFA Pro Licence which is the final coaching qualification available, and follows the completion of the UEFA 'B' and 'A' Licences. The UEFA Pro Licence is required by every person who wishes to manage a club in the Premier League on a permanent basis (i.e. more than 12 weeks – the amount of time an unqualified caretaker manager is allowed to take control). Caretaker appointments are managers that fill the gap between a managerial departure and a new appointment. Several caretaker managers have gone on to secure a permanent managerial post after performing well as a caretaker; examples include Paul Hart at Portsmouth and David Pleat at Tottenham Hotspur. Its main body is solid sterling silver and silver gilt, while its plinth is made of malachite, a semi-precious stone. The plinth has a silver band around its circumference, upon which the names of the title-winning clubs are listed. Malachite's green colour is also representative of the green field of play. The design of the trophy is based on the heraldry of Three Lions that is associated with English football. Two of the lions are found above the handles on either side of the trophy – the third is symbolised by the captain of the title winning team as he raises the trophy, and its gold crown, above his head at the end of the season. The ribbons that drape the handles are presented in the team colours of the league champions that year. However, by the turn of the 1990s the downward trend was starting to reverse; England had been successful in the 1990 FIFA World Cup, reaching the semi-finals. UEFA, European football's governing body, lifted the five-year ban on English clubs playing in European competitions in 1990 (resulting in Manchester United lifting the UEFA Cup Winners' Cup in 1991) and the Taylor Report on stadium safety standards, which proposed expensive upgrades to create all-seater stadiums in the aftermath of the Hillsborough disaster, was published in January of that year. At the close of the 1991 season, a proposal was tabled for the establishment of a new league that would bring more money into the game overall. The Founder Members Agreement, signed on 17 July 1991 by the game's top-flight clubs, established the basic principles for setting up the FA Premier League. The newly formed top division would have commercial independence from The Football Association and the Football League, giving the FA Premier League licence to negotiate its own broadcast and sponsorship agreements. The argument given at the time was that the extra income would allow English clubs to compete with teams across Europe. The Premier League has the highest revenue of any football league in the world, with total club revenues of €2.48 billion in 2009–10. In 2013–14, due to improved television revenues and cost controls, the Premier League had net profits in excess of £78 million, exceeding all other football leagues. In 2010 the Premier League was awarded the Queen's Award for Enterprise in the International Trade category for its outstanding contribution to international trade and the value it brings to English football and the United Kingdom's broadcasting industry. The first Sky television rights agreement was worth £304 million over five seasons. The next contract, negotiated to start from the 1997–98 season, rose to £670 million over four seasons. The third contract was a £1.024 billion deal with BSkyB for the three seasons from 2001–02 to 2003–04. The league brought in £320 million from the sale of its international rights for the three-year period from 2004–05 to 2006–07. It sold the rights itself on a territory-by-territory basis. Sky's monopoly was broken from August 2006 when Setanta Sports was awarded rights to show two out of the six packages of matches available. This occurred following an insistence by the European Commission that exclusive rights should not be sold to one television company. Sky and Setanta paid a total of £1.7 billion, a two-thirds increase which took many commentators by surprise as it had been widely assumed that the value of the rights had levelled off following many years of rapid growth. Setanta also hold rights to a live 3 pm match solely for Irish viewers. The BBC has retained the rights to show highlights for the same three seasons (on Match of the Day) for £171.6 million, a 63 per cent increase on the £105 million it paid for the previous three-year period. Sky and BT have agreed to jointly pay £84.3 million for delayed television rights to 242 games (that is the right to broadcast them in full on television and over the internet) in most cases for a period of 50 hours after 10 pm on matchday. Overseas television rights fetched £625 million, nearly double the previous contract. The total raised from these deals is more than £2.7 billion, giving Premier League clubs an average media income from league games of around £40 million-a-year from 2007 to 2010. Stadium attendances are a significant source of regular income for Premier League clubs. For the 2009–10 season, average attendances across the league clubs were 34,215 for Premier League matches with a total aggregate attendance figure of 13,001,616. This represents an increase of 13,089 from the average attendance of 21,126 recorded in the league's first season (1992–93). However, during the 1992–93 season the capacities of most stadiums were reduced as clubs replaced terraces with seats in order to meet the Taylor Report's 1994–95 deadline for all-seater stadiums. The Premier League's record average attendance of 36,144 was set during the 2007–08 season. This record was then beaten in the 2013–14 season recording an average attendance of 36,695 with a total attendance of just under 14 million, the highest average in England's top flight since 1950. The Golden Boot is awarded to the top Premier League scorer at the end of each season. Former Blackburn Rovers and Newcastle United striker Alan Shearer holds the record for most Premier League goals with 260. Twenty-four players have reached the 100-goal mark. Since the first Premier League season in 1992–93, 14 different players from 10 different clubs have won or shared the top scorers title. Thierry Henry won his fourth overall scoring title by scoring 27 goals in the 2005–06 season. Andrew Cole and Alan Shearer hold the record for most goals in a season (34) – for Newcastle and Blackburn respectively. Ryan Giggs of Manchester United holds the record for scoring goals in consecutive seasons, having scored in the first 21 seasons of the league. Players may only be transferred during transfer windows that are set by the Football Association. The two transfer windows run from the last day of the season to 31 August and from 31 December to 31 January. Player registrations cannot be exchanged outside these windows except under specific licence from the FA, usually on an emergency basis. As of the 2010–11 season, the Premier League introduced new rules mandating that each club must register a maximum 25-man squad of players aged over 21, with the squad list only allowed to be changed in transfer windows or in exceptional circumstances. This was to enable the 'home grown' rule to be enacted, whereby the League would also from 2010 require at least 8 of the named 25 man squad to be made up of 'home-grown players'. The years following 2009 marked a shift in the structure of the ""Big Four"" with Tottenham Hotspur and Manchester City both breaking into the top four. In the 2009–10 season, Tottenham finished fourth and became the first team to break the top four since Everton in 2005. Criticism of the gap between an elite group of ""super clubs"" and the majority of the Premier League has continued, nevertheless, due to their increasing ability to spend more than the other Premier League clubs. Manchester City won the title in the 2011–12 season, becoming the first club outside the ""Big Four"" to win since 1994–95. That season also saw two of the Big Four (Chelsea and Liverpool) finish outside the top four places for the first time since 1994–95. In response to concerns that clubs were increasingly passing over young English players in favour of foreign players, in 1999, the Home Office tightened its rules for granting work permits to players from countries outside of the European Union. A non-EU player applying for the permit must have played for his country in at least 75 per cent of its competitive 'A' team matches for which he was available for selection during the previous two years, and his country must have averaged at least 70th place in the official FIFA world rankings over the previous two years. If a player does not meet those criteria, the club wishing to sign him may appeal. Television has played a major role in the history of the Premier League. The League's decision to assign broadcasting rights to BSkyB in 1992 was at the time a radical decision, but one that has paid off. At the time pay television was an almost untested proposition in the UK market, as was charging fans to watch live televised football. However, a combination of Sky's strategy, the quality of Premier League football and the public's appetite for the game has seen the value of the Premier League's TV rights soar. At the inception of the Premier League in 1992–93, just eleven players named in the starting line-ups for the first round of matches hailed from outside of the United Kingdom or Ireland. By 2000–01, the number of foreign players participating in the Premier League was 36 per cent of the total. In the 2004–05 season the figure had increased to 45 per cent. On 26 December 1999, Chelsea became the first Premier League side to field an entirely foreign starting line-up, and on 14 February 2005 Arsenal were the first to name a completely foreign 16-man squad for a match. By 2009, under 40% of the players in the Premier League were English. The competition formed as the FA Premier League on 20 February 1992 following the decision of clubs in the Football League First Division to break away from the Football League, which was originally founded in 1888, and take advantage of a lucrative television rights deal. The deal was worth £1 billion a year domestically as of 2013–14, with BSkyB and BT Group securing the domestic rights to broadcast 116 and 38 games respectively. The league generates €2.2 billion per year in domestic and international television rights. In 2014/15, teams were apportioned revenues of £1.6 billion. The Premier League is the most-watched football league in the world, broadcast in 212 territories to 643 million homes and a potential TV audience of 4.7 billion people. In the 2014–15 season, the average Premier League match attendance exceeded 36,000, second highest of any professional football league behind the Bundesliga's 43,500. Most stadium occupancies are near capacity. The Premier League rank second in the UEFA coefficients of leagues based on performances in European competitions over the past five seasons. The record transfer fee for a Premier League player has risen steadily over the lifetime of the competition. Prior to the start of the first Premier League season Alan Shearer became the first British player to command a transfer fee of more than £3 million. The record rose steadily in the Premier League's first few seasons, until Alan Shearer made a record breaking £15 million move to Newcastle United in 1996. The three highest transfer in the sport's history had a Premier League club on the selling end, with Tottenham Hotspur selling Gareth Bale to Real Madrid for £85 million in 2013, Manchester United's sale of Cristiano Ronaldo to Real Madrid for £80 million in 2009, and Liverpool selling Luis Suárez to Barcelona for £75 million in 2014. The Football Association Premier League Ltd (FAPL) is operated as a corporation and is owned by the 20 member clubs. Each club is a shareholder, with one vote each on issues such as rule changes and contracts. The clubs elect a chairman, chief executive, and board of directors to oversee the daily operations of the league. The current chairman is Sir Dave Richards, who was appointed in April 1999, and the chief executive is Richard Scudamore, appointed in November 1999. The former chairman and chief executive, John Quinton and Peter Leaver, were forced to resign in March 1999 after awarding consultancy contracts to former Sky executives Sam Chisholm and David Chance. The Football Association is not directly involved in the day-to-day operations of the Premier League, but has veto power as a special shareholder during the election of the chairman and chief executive and when new rules are adopted by the league. There has been an increasing gulf between the Premier League and the Football League. Since its split with the Football League, many established clubs in the Premier League have managed to distance themselves from their counterparts in lower leagues. Owing in large part to the disparity in revenue from television rights between the leagues, many newly promoted teams have found it difficult to avoid relegation in their first season in the Premier League. In every season except 2001–02 and 2011–12, at least one Premier League newcomer has been relegated back to the Football League. In 1997–98 all three promoted clubs were relegated at the end of the season. The team placed fifth in the Premier League automatically qualifies for the UEFA Europa League, and the sixth and seventh-placed teams can also qualify, depending on the winners of the two domestic cup competitions i.e. the FA Cup and the Capital One Cup (League Cup). Two Europa League places are reserved for the winners of each tournament; if the winner of either the FA Cup or League Cup qualifies for the Champions League, then that place will go to the next-best placed finisher in the Premier League. A further place in the UEFA Europa League is also available via the Fair Play initiative. If the Premier League has one of the three highest Fair Play rankings in Europe, the highest ranked team in the Premier League Fair Play standings which has not already qualified for Europe will automatically qualify for the UEFA Europa League first qualifying round. An exception to the usual European qualification system happened in 2005, after Liverpool won the Champions League the year before, but did not finish in a Champions League qualification place in the Premier League that season. UEFA gave special dispensation for Liverpool to enter the Champions League, giving England five qualifiers. UEFA subsequently ruled that the defending champions qualify for the competition the following year regardless of their domestic league placing. However, for those leagues with four entrants in the Champions League, this meant that if the Champions League winner finished outside the top four in its domestic league, it would qualify at the expense of the fourth-placed team in the league. No association can have more than four entrants in the Champions League. This occurred in 2012, when Chelsea – who had won the Champions League the previous year, but finished sixth in the league – qualified for the Champions League in place of Tottenham Hotspur, who went into the Europa League. Television money had also become much more important; the Football League received £6.3 million for a two-year agreement in 1986, but when that deal was renewed in 1988, the price rose to £44 million over four years. The 1988 negotiations were the first signs of a breakaway league; ten clubs threatened to leave and form a ""super league"", but were eventually persuaded to stay. As stadiums improved and match attendance and revenues rose, the country's top teams again considered leaving the Football League in order to capitalise on the growing influx of money being pumped into the sport. Between the 1992–93 season and the 2012–13 season, Premier League clubs had won the UEFA Champions League four times (as well as supplying five of the runners-up), behind Spain's La Liga with six wins, and Italy's Serie A with five wins, and ahead of, among others, Germany's Bundesliga with three wins (see table here). The FIFA Club World Cup (or the FIFA Club World Championship, as it was originally called) has been won by Premier league clubs once (Manchester United in 2008), and they have also been runners-up twice, behind Brazil's Brasileirão with four wins, and Spain's La Liga and Italy's Serie A with two wins each (see table here). The league held its first season in 1992–93 and was originally composed of 22 clubs. The first ever Premier League goal was scored by Brian Deane of Sheffield United in a 2–1 win against Manchester United. The 22 inaugural members of the new Premier League were Arsenal, Aston Villa, Blackburn Rovers, Chelsea, Coventry City, Crystal Palace, Everton, Ipswich Town, Leeds United, Liverpool, Manchester City, Manchester United, Middlesbrough, Norwich City, Nottingham Forest, Oldham Athletic, Queens Park Rangers, Sheffield United, Sheffield Wednesday, Southampton, Tottenham Hotspur, and Wimbledon. Luton Town, Notts County and West Ham United were the three teams relegated from the old first division at the end of the 1991–92 season, and did not take part in the inaugural Premier League season. In 2011, a Welsh club participated in the Premier League for the first time after Swansea City gained promotion. The first Premier League match to be played outside England was Swansea City's home match at the Liberty Stadium against Wigan Athletic on 20 August 2011. In 2012–13, Swansea qualified for the Europa League by winning the League Cup. The number of Welsh clubs in the Premier League increased to two for the first time in 2013–14, as Cardiff City gained promotion, but Cardiff City was relegated after its maiden season. The Premier League is a corporation in which the 20 member clubs act as shareholders. Seasons run from August to May. Teams play 38 matches each (playing each team in the league twice, home and away), totalling 380 matches in the season. Most games are played on Saturday and Sunday afternoons; others during weekday evenings. It is currently sponsored by Barclays Bank and thus officially known as the Barclays Premier League and is colloquially known as the Premiership. Outside the UK it is commonly referred to as the English Premier League (EPL). The BBC's highlights package on Saturday and Sunday nights, as well as other evenings when fixtures justify, will run until 2016. Television rights alone for the period 2010 to 2013 have been purchased for £1.782 billion. On 22 June 2009, due to troubles encountered by Setanta Sports after it failed to meet a final deadline over a £30 million payment to the Premier League, ESPN was awarded two packages of UK rights containing a total of 46 matches that were available for the 2009–10 season as well as a package of 23 matches per season from 2010–11 to 2012–13. On 13 June 2012, the Premier League announced that BT had been awarded 38 games a season for the 2013–14 through 2015–16 seasons at £246 million-a-year. The remaining 116 games were retained by Sky who paid £760 million-a-year. The total domestic rights have raised £3.018 billion, an increase of 70.2% over the 2010–11 to 2012–13 rights. The value of the licensing deal rose by another 70.2% in 2015, when Sky and BT paid a total of £5.136 billion to renew their contracts with the Premier League for another three years up to the 2018–19 season. The Premier League sells its television rights on a collective basis. This is in contrast to some other European Leagues, including La Liga, in which each club sells its rights individually, leading to a much higher share of the total income going to the top few clubs. The money is divided into three parts: half is divided equally between the clubs; one quarter is awarded on a merit basis based on final league position, the top club getting twenty times as much as the bottom club, and equal steps all the way down the table; the final quarter is paid out as facilities fees for games that are shown on television, with the top clubs generally receiving the largest shares of this. The income from overseas rights is divided equally between the twenty clubs. The Premier League is broadcast in the United States through NBC Sports. Premier League viewership has increased rapidly, with NBC and NBCSN averaging a record 479,000 viewers in the 2014–15 season, up 118% from 2012–13 when coverage still aired on Fox Soccer and ESPN/ESPN2 (220,000 viewers), and NBC Sports has been widely praised for its coverage. NBC Sports reached a six-year extension with the Premier League in 2015 to broadcast the league through the 2021–22 season in a deal valued at $1 billion (£640 million). In 1992, the First Division clubs resigned from the Football League en masse and on 27 May 1992 the FA Premier League was formed as a limited company working out of an office at the Football Association's then headquarters in Lancaster Gate. This meant a break-up of the 104-year-old Football League that had operated until then with four divisions; the Premier League would operate with a single division and the Football League with three. There was no change in competition format; the same number of teams competed in the top flight, and promotion and relegation between the Premier League and the new First Division remained the same as the old First and Second Divisions with three teams relegated from the league and three promoted. The Premier League is particularly popular in Asia, where it is the most widely distributed sports programme. In Australia, Fox Sports broadcasts almost all of the season's 380 matches live, and Foxtel gives subscribers the option of selecting which Saturday 3pm match to watch. In India, the matches are broadcast live on STAR Sports. In China, the broadcast rights were awarded to Super Sports in a six-year agreement that began in the 2013–14 season. As of the 2013–14 season, Canadian broadcast rights to the Premier League are jointly owned by Sportsnet and TSN, with both rival networks holding rights to 190 matches per season. The managing director of London Weekend Television (LWT), Greg Dyke, met with the representatives of the ""big five"" football clubs in England in 1990. The meeting was to pave the way for a break away from The Football League. Dyke believed that it would be more lucrative for LWT if only the larger clubs in the country were featured on national television and wanted to establish whether the clubs would be interested in a larger share of television rights money. The five clubs decided it was a good idea and decided to press ahead with it; however, the league would have no credibility without the backing of The Football Association and so David Dein of Arsenal held talks to see whether the FA were receptive to the idea. The FA did not enjoy an amicable relationship with the Football League at the time and considered it as a way to weaken the Football League's position. The Premier League distributes a portion of its television revenue to clubs that are relegated from the league in the form of ""parachute payments"". Starting with the 2013–14 season, these payments are in excess of £60 million over four seasons. Though designed to help teams adjust to the loss of television revenues (the average Premier League team receives £55 million while the average Football League Championship club receives £2 million), critics maintain that the payments actually widen the gap between teams that have reached the Premier League and those that have not, leading to the common occurrence of teams ""bouncing back"" soon after their relegation. For some clubs who have failed to win immediate promotion back to the Premier League, financial problems, including in some cases administration or even liquidation have followed. Further relegations down the footballing ladder have ensued for several clubs unable to cope with the gap."
Airport,"On runways, green lights indicate the beginning of the runway for landing, while red lights indicate the end of the runway. Runway edge lighting consists of white lights spaced out on both sides of the runway, indicating the edge. Some airports have more complicated lighting on the runways including lights that run down the centerline of the runway and lights that help indicate the approach (an approach lighting system, or ALS). Low-traffic airports may use pilot controlled lighting to save electricity and staffing costs. Airports may also contain premium and VIP services. The premium and VIP services may include express check-in and dedicated check-in counters. These services are usually reserved for First and Business class passengers, premium frequent flyers, and members of the airline's clubs. Premium services may sometimes be open to passengers who are members of a different airline's frequent flyer program. This can sometimes be part of a reciprocal deal, as when multiple airlines are part of the same alliance, or as a ploy to attract premium customers away from rival airlines. Most major airports provide commercial outlets for products and services. Most of these companies, many of which are internationally known brands, are located within the departure areas. These include clothing boutiques and restaurants. Prices charged for items sold at these outlets are generally higher than those outside the airport. However, some airports now regulate costs to keep them comparable to ""street prices"". This term is misleading as prices often match the manufacturers' suggested retail price (MSRP) but are almost never discounted.[citation needed] The first lighting used on an airport was during the latter part of the 1920s; in the 1930s approach lighting came into use. These indicated the proper direction and angle of descent. The colours and flash intervals of these lights became standardized under the International Civil Aviation Organization (ICAO). In the 1940s, the slope-line approach system was introduced. This consisted of two rows of lights that formed a funnel indicating an aircraft's position on the glideslope. Additional lights indicated incorrect altitude and direction. The distances passengers need to move within a large airport can be substantial. It is common for airports to provide moving walkways and buses. The Hartsfield–Jackson Atlanta International Airport has a tram that takes people through the concourses and baggage claim. Major airports with more than one terminal offer inter-terminal transportation, such as Mexico City International Airport, where the domestic building of Terminal 1 is connected by Aerotrén to Terminal 2, on the other side of the airport. Most of the world's airports are owned by local, regional, or national government bodies who then lease the airport to private corporations who oversee the airport's operation. For example, in the United Kingdom the state-owned British Airports Authority originally operated eight of the nation's major commercial airports - it was subsequently privatized in the late 1980s, and following its takeover by the Spanish Ferrovial consortium in 2006, has been further divested and downsized to operating just five. Germany's Frankfurt Airport is managed by the quasi-private firm Fraport. While in India GMR Group operates, through joint ventures, Indira Gandhi International Airport and Rajiv Gandhi International Airport. Bengaluru International Airport and Chhatrapati Shivaji International Airport are controlled by GVK Group. The rest of India's airports are managed by the Airports Authority of India. At extremely large airports, a circuit is in place but not usually used. Rather, aircraft (usually only commercial with long routes) request approach clearance while they are still hours away from the airport, often before they even take off from their departure point. Large airports have a frequency called Clearance Delivery which is used by departing aircraft specifically for this purpose. This then allows aircraft to take the most direct approach path to the runway and land without worrying about interference from other aircraft. While this system keeps the airspace free and is simpler for pilots, it requires detailed knowledge of how aircraft are planning to use the airport ahead of time and is therefore only possible with large commercial airliners on pre-scheduled flights. The system has recently become so advanced that controllers can predict whether an aircraft will be delayed on landing before it even takes off; that aircraft can then be delayed on the ground, rather than wasting expensive fuel waiting in the air. Most airports welcome filming on site, although it must be agreed in advance and may be subject to a fee. Landside, filming can take place in all public areas. However airside, filming is heavily restricted, the only airside locations where filming is permitted are the Departure Lounge and some outside areas. To film in an airside location, all visitors must go through security, the same as passengers, and be accompanied by a full airside pass holder and have their passport with them at all times. Filming can not be undertaken in Security, at Immigration/Customs, or in Baggage Reclaim. Tower Control controls aircraft on the runway and in the controlled airspace immediately surrounding the airport. Tower controllers may use radar to locate an aircraft's position in three-dimensional space, or they may rely on pilot position reports and visual observation. They coordinate the sequencing of aircraft in the traffic pattern and direct aircraft on how to safely join and leave the circuit. Aircraft which are only passing through the airspace must also contact Tower Control in order to be sure that they remain clear of other traffic. At all airports the use of a traffic pattern (often called a traffic circuit outside the U.S.) is possible. They may help to assure smooth traffic flow between departing and arriving aircraft. There is no technical need within modern aviation for performing this pattern, provided there is no queue. And due to the so-called SLOT-times, the overall traffic planning tend to assure landing queues are avoided. If for instance an aircraft approaches runway 17 (which has a heading of approx. 170 degrees) from the north (coming from 360/0 degrees heading towards 180 degrees), the aircraft will land as fast as possible by just turning 10 degrees and follow the glidepath, without orbit the runway for visual reasons, whenever this is possible. For smaller piston engined airplanes at smaller airfields without ILS equipment, things are very differently though. Ground Control is responsible for directing all ground traffic in designated ""movement areas"", except the traffic on runways. This includes planes, baggage trains, snowplows, grass cutters, fuel trucks, stair trucks, airline food trucks, conveyor belt vehicles and other vehicles. Ground Control will instruct these vehicles on which taxiways to use, which runway they will use (in the case of planes), where they will park, and when it is safe to cross runways. When a plane is ready to takeoff it will stop short of the runway, at which point it will be turned over to Tower Control. After a plane has landed, it will depart the runway and be returned to Ground Control. Generally, this pattern is a circuit consisting of five ""legs"" that form a rectangle (two legs and the runway form one side, with the remaining legs forming three more sides). Each leg is named (see diagram), and ATC directs pilots on how to join and leave the circuit. Traffic patterns are flown at one specific altitude, usually 800 or 1,000 ft (244 or 305 m) above ground level (AGL). Standard traffic patterns are left-handed, meaning all turns are made to the left. One of the main reason for this is that pilots sit on the left side of the airplane, and a Left-hand patterns improves their visibility of the airport and pattern. Right-handed patterns do exist, usually because of obstacles such as a mountain, or to reduce noise for local residents. The predetermined circuit helps traffic flow smoothly because all pilots know what to expect, and helps reduce the chance of a mid-air collision. An airport is an aerodrome with facilities for flights to take off and land. Airports often have facilities to store and maintain aircraft, and a control tower. An airport consists of a landing area, which comprises an aerially accessible open space including at least one operationally active surface such as a runway for a plane to take off or a helipad, and often includes adjacent utility buildings such as control towers, hangars  and terminals. Larger airports may have fixed base operator services, airport aprons, air traffic control centres, passenger facilities such as restaurants and lounges, and emergency services. Airport construction boomed during the 1960s with the increase in jet aircraft traffic. Runways were extended out to 3,000 m (9,800 ft). The fields were constructed out of reinforced concrete using a slip-form machine that produces a continual slab with no disruptions along the length. The early 1960s also saw the introduction of jet bridge systems to modern airport terminals, an innovation which eliminated outdoor passenger boarding. These systems became commonplace in the United States by the 1970s. Following the war, some of these military airfields added civil facilities for handling passenger traffic. One of the earliest such fields was Paris – Le Bourget Airport at Le Bourget, near Paris. The first airport to operate scheduled international commercial services was Hounslow Heath Aerodrome in August 1919, but it was closed and supplanted by Croydon Airport in March 1920. In 1922, the first permanent airport and commercial terminal solely for commercial aviation was opened at Flughafen Devau near what was then Königsberg, East Prussia. The airports of this era used a paved ""apron"", which permitted night flying as well as landing heavier aircraft. There are a number of aids available to pilots, though not all airports are equipped with them. A visual approach slope indicator (VASI) helps pilots fly the approach for landing. Some airports are equipped with a VHF omnidirectional range (VOR) to help pilots find the direction to the airport. VORs are often accompanied by a distance measuring equipment (DME) to determine the distance to the VOR. VORs are also located off airports, where they serve to provide airways for aircraft to navigate upon. In poor weather, pilots will use an instrument landing system (ILS) to find the runway and fly the correct approach, even if they cannot see the ground. The number of instrument approaches based on the use of the Global Positioning System (GPS) is rapidly increasing and may eventually be the primary means for instrument landings. An airbase, sometimes referred to as an air station or airfield, provides basing and support of military aircraft. Some airbases, known as military airports, provide facilities similar to their civilian counterparts. For example, RAF Brize Norton in the UK has a terminal which caters to passengers for the Royal Air Force's scheduled TriStar flights to the Falkland Islands. Some airbases are co-located with civilian airports, sharing the same ATC facilities, runways, taxiways and emergency services, but with separate terminals, parking areas and hangars. Bardufoss Airport , Bardufoss Air Station in Norway and Pune Airport in India are examples of this. The majority of the world's airports are non-towered, with no air traffic control presence. However, at particularly busy airports, or airports with other special requirements, there is an air traffic control (ATC) system whereby controllers (usually ground-based) direct aircraft movements via radio or other communications links. This coordinated oversight facilitates safety and speed in complex operations where traffic moves in all three dimensions. Air traffic control responsibilities at airports are usually divided into at least two main areas: ground and tower, though a single controller may work both stations. The busiest airports also have clearance delivery, apron control, and other specialized ATC stations. Airports have played major roles in films and television programs due to their very nature as a transport and international hub, and sometimes because of distinctive architectural features of particular airports. One such example of this is The Terminal, a film about a man who becomes permanently grounded in an airport terminal and must survive only on the food and shelter provided by the airport. They are also one of the major elements in movies such as The V.I.P.s, Airplane!, Airport (1970), Die Hard 2, Soul Plane, Jackie Brown, Get Shorty, Home Alone, Liar Liar, Passenger 57, Final Destination (2000), Unaccompanied Minors, Catch Me If You Can, Rendition and The Langoliers. They have also played important parts in television series like Lost, The Amazing Race, America's Next Top Model, Cycle 10 which have significant parts of their story set within airports. In other programmes and films, airports are merely indicative of journeys, e.g. Good Will Hunting. Airports are divided into landside and airside areas. Landside areas include parking lots, public transportation train stations and access roads. Airside areas include all areas accessible to aircraft, including runways, taxiways and aprons. Access from landside areas to airside areas is tightly controlled at most airports. Passengers on commercial flights access airside areas through terminals, where they can purchase tickets, clear security check, or claim luggage and board aircraft through gates. The waiting areas which provide passenger access to aircraft are typically called concourses, although this term is often used interchangeably with terminal. The majority of the world's airports are non-towered, with no air traffic control presence. Busy airports have air traffic control (ATC) system. All airports use a traffic pattern to assure smooth traffic flow between departing and arriving aircraft. There are a number of aids available to pilots, though not all airports are equipped with them. Many airports have lighting that help guide planes using the runways and taxiways at night or in rain, snow, or fog. In the U.S. and Canada, the vast majority of airports, large and small, will either have some form of automated airport weather station, a human observer or a combination of the two. Air safety is an important concern in the operation of an airport, and airports often have their own safety services. The title of ""world's oldest airport"" is disputed, but College Park Airport in Maryland, US, established in 1909 by Wilbur Wright, is generally agreed to be the world's oldest continually operating airfield, although it serves only general aviation traffic. Bisbee-Douglas International Airport in Arizona was declared ""the first international airport of the Americas"" by US president Franklin D. Roosevelt in 1943. Pearson Field Airport in Vancouver, Washington had a dirigible land in 1905 and planes in 1911 and is still in use. Bremen Airport opened in 1913 and remains in use, although it served as an American military field between 1945 and 1949. Amsterdam Airport Schiphol opened on September 16, 1916 as a military airfield, but only accepted civil aircraft from December 17, 1920, allowing Sydney Airport in Sydney, Australia—which started operations in January 1920—to claim to be one of the world's oldest continually operating commercial airports. Minneapolis-Saint Paul International Airport in Minneapolis-Saint Paul, Minnesota, opened in 1920 and has been in continuous commercial service since. It serves about 35,000,000 passengers each year and continues to expand, recently opening a new 11,000 foot (3,355 meter) runway. Of the airports constructed during this early period in aviation, it is one of the largest and busiest that is still currently operating. Rome Ciampino Airport, opened 1916, is also a contender, as well as the Don Mueang International Airport near Bangkok,Thailand, which opened in 1914. Increased aircraft traffic during World War I led to the construction of landing fields. Aircraft had to approach these from certain directions and this led to the development of aids for directing the approach and landing slope. Many large airports are located near railway trunk routes for seamless connection of multimodal transport, for instance Frankfurt Airport, Amsterdam Airport Schiphol, London Heathrow Airport, London Gatwick Airport and London Stansted Airport. It is also common to connect an airport and a city with rapid transit, light rail lines or other non-road public transport systems. Some examples of this would include the AirTrain JFK at John F. Kennedy International Airport in New York, Link Light Rail that runs from the heart of downtown Seattle to Seattle–Tacoma International Airport, and the Silver Line T at Boston's Logan International Airport by the Massachusetts Bay Transportation Authority (MBTA). Such a connection lowers risk of missed flights due to traffic congestion. Large airports usually have access also through controlled-access highways ('freeways' or 'motorways') from which motor vehicles enter either the departure loop or the arrival loop. Hazards to aircraft include debris, nesting birds, and reduced friction levels due to environmental conditions such as ice, snow, or rain. Part of runway maintenance is airfield rubber removal which helps maintain friction levels. The fields must be kept clear of debris using cleaning equipment so that loose material does not become a projectile and enter an engine duct (see foreign object damage). In adverse weather conditions, ice and snow clearing equipment can be used to improve traction on the landing strip. For waiting aircraft, equipment is used to spray special deicing fluids on the wings. Many ground crew at the airport work at the aircraft. A tow tractor pulls the aircraft to one of the airbridges, The ground power unit is plugged in. It keeps the electricity running in the plane when it stands at the terminal. The engines are not working, therefore they do not generate the electricity, as they do in flight. The passengers disembark using the airbridge. Mobile stairs can give the ground crew more access to the aircraft's cabin. There is a cleaning service to clean the aircraft after the aircraft lands. Flight catering provides the food and drinks on flights. A toilet waste truck removes the human waste from the tank which holds the waste from the toilets in the aircraft. A water truck fills the water tanks of the aircraft. A fuel transfer vehicle transfers aviation fuel from fuel tanks underground, to the aircraft tanks. A tractor and its dollies bring in luggage from the terminal to the aircraft. They also carry luggage to the terminal if the aircraft has landed, and is being unloaded. Hi-loaders lift the heavy luggage containers to the gate of the cargo hold. The ground crew push the luggage containers into the hold. If it has landed, they rise, the ground crew push the luggage container on the hi-loader, which carries it down. The luggage container is then pushed on one of the tractors dollies. The conveyor, which is a conveyor belt on a truck, brings in the awkwardly shaped, or late luggage. The airbridge is used again by the new passengers to embark the aircraft. The tow tractor pushes the aircraft away from the terminal to a taxi area. The aircraft should be off of the airport and in the air in 90 minutes. The airport charges the airline for the time the aircraft spends at the airport."
Comprehensive_school,"There is some controversy about comprehensive schools. As a rule of thumb those supporting The Left Party, the Social Democratic Party of Germany and Alliance '90/The Greens are in favour of comprehensive schools, while those supporting the Christian Democratic Union and the Free Democratic Party are opposed to them. In these schools children could be selected on the basis of curriculum aptitude related to the school's specialism even though the schools do take quotas from each quartile of the attainment range to ensure they were not selective by attainment. A problem with this is whether the quotas should be taken from a normal distribution or from the specific distribution of attainment in the immediate catchment area. In the selective school system, which survives in several parts of the United Kingdom, admission is dependent on selection criteria, most commonly a cognitive test or tests. Although comprehensive schools were introduced to England and Wales in 1965, there are 164 selective grammar schools that are still in operation.[citation needed] (though this is a small number compared to approximately 3500 state secondary schools in England). Most comprehensives are secondary schools for children between the ages of 11 to 16, but in a few areas there are comprehensive middle schools, and in some places the secondary level is divided into two, for students aged 11 to 14 and those aged 14 to 18, roughly corresponding to the US middle school (or junior high school) and high school, respectively. With the advent of key stages in the National Curriculum some local authorities reverted from the Middle School system to 11–16 and 11–18 schools so that the transition between schools corresponds to the end of one key stage and the start of another. Education in Northern Ireland differs slightly from systems used elsewhere in the United Kingdom, but it is more similar to that used in England and Wales than it is to Scotland. A comprehensive school is a state school that does not select its intake on the basis of academic achievement or aptitude. This is in contrast to the selective school system, where admission is restricted on the basis of selection criteria. The term is commonly used in relation to England and Wales, where comprehensive schools were introduced on an experimental basis in the 1940s and became more widespread from 1965. About 90% of British secondary school pupils now attend comprehensive schools. They correspond broadly to the public high school in the United States and Canada and to the German Gesamtschule.[citation needed] Comprehensive schools are primarily about providing an entitlement curriculum to all children, without selection whether due to financial considerations or attainment. A consequence of that is a wider ranging curriculum, including practical subjects such as design and technology and vocational learning, which were less common or non-existent in grammar schools. Providing post-16 education cost-effectively becomes more challenging for smaller comprehensive schools, because of the number of courses needed to cover a broader curriculum with comparatively fewer students. This is why schools have tended to get larger and also why many local authorities have organised secondary education into 11–16 schools, with the post-16 provision provided by Sixth Form colleges and Further Education Colleges. Comprehensive schools do not select their intake on the basis of academic achievement or aptitude, but there are demographic reasons why the attainment profiles of different schools vary considerably. In addition, government initiatives such as the City Technology Colleges and Specialist schools programmes have made the comprehensive ideal less certain. Germany has a comprehensive school known as the Gesamtschule. While some German schools such as the Gymnasium and the Realschule have rather strict entrance requirements, the Gesamtschule does not have such requirements. They offer college preparatory classes for the students who are doing well, general education classes for average students, and remedial courses for those who aren't doing that well. In most cases students attending a Gesamtschule may graduate with the Hauptschulabschluss, the Realschulabschluss or the Abitur depending on how well they did in school. Scotland has a very different educational system from England and Wales, though also based on comprehensive education. It has different ages of transfer, different examinations and a different philosophy of choice and provision. All publicly funded primary and secondary schools are comprehensive. The Scottish Government has rejected plans for specialist schools as of 2005. By 1975 the majority of local authorities in England and Wales had abandoned the 11-plus examination and moved to a comprehensive system. Over that 10-year period many secondary modern schools and grammar schools were amalgamated to form large neighbourhood comprehensives, whilst a number of new schools were built to accommodate a growing school population. By the mid-1970s the system had been almost fully implemented, with virtually no secondary modern schools remaining. Many grammar schools were either closed or changed to comprehensive status. Some local authorities, including Sandwell and Dudley in the West Midlands, changed all of its state secondary schools to comprehensive schools during the 1970s. Since the 1988 Education Reform Act, parents have a right to choose which school their child should go to or whether to not send them to school at all and to home educate them instead. The concept of ""school choice"" introduces the idea of competition between state schools, a fundamental change to the original ""neighbourhood comprehensive"" model, and is partly intended as a means by which schools that are perceived to be inferior are forced either to improve or, if hardly anyone wants to go there, to close down. Government policy is currently promoting 'specialisation' whereby parents choose a secondary school appropriate for their child's interests and skills. Most initiatives focus on parental choice and information, implementing a pseudo-market incentive to encourage better schools. This logic has underpinned the controversial league tables of school performance. Comprehensive schools have been accused of grade inflation after a study revealed that Gymnasium senior students of average mathematical ability found themselves at the very bottom of their class and had an average grade of ""Five"", which means ""Failed"". Gesamtschule senior students of average mathematical ability found themselves in the upper half of their class and had an average grade of ""Three Plus"". When a central Abitur examination was established in the State of North Rhine-Westphalia, it was revealed that Gesamtschule students did worse than could be predicted by their grades or class rank. Barbara Sommer (Christian Democratic Union), Education Minister of North Rhine-Westphalia, commented that: Looking at the performance gap between comprehensives and the Gymnasium [at the Abitur central examination] [...] it is difficult to understand why the Social Democratic Party of Germany wants to do away with the Gymnasium. [...] The comprehensives do not help students achieve [...] I am sick and tired of the comprehensive schools blaming their problems on the social class origins of their students. What kind of attitude is this to blame their own students? She also called the Abitur awarded by the Gymnasium the true Abitur and the Abitur awarded by the Gesamtschule ""Abitur light"". As a reaction, Sigrid Beer (Alliance '90/The Greens) stated that comprehensives were structurally discriminated against by the government, which favoured the Gymnasiums. She also said that many of the students awarded the Abitur by the comprehensives came from ""underprivileged groups"" and sneering at their performance was a ""piece of impudence"". Gesamtschulen might put bright working class students at risk according to several studies. It could be shown that an achievement gap opens between working class students attending a comprehensive and their middle class peers. Also working class students attending a Gymnasium or a Realschule outperform students from similar backgrounds attending a comprehensive. However it is not students attending a comprehensive, but students attending a Hauptschule, who perform the poorest. The ""Mittelschule"" is a school in some States of Germany that offers regular classes and remedial classes but no college preparatory classes. In some States of Germany, the Hauptschule does not exist, and any student who has not been accepted by another school has to attend the Mittelschule. Students may be awarded the Hauptschulabschluss or the Mittlere Reife but not the Abitur. The first comprehensives were set up after the Second World War. In 1946, for example, Walworth School was one of five 'experimental' comprehensive schools set up by the London County Council Another early comprehensive school was Holyhead County School in Anglesey in 1949. Other early examples of comprehensive schools included Woodlands Boys School in Coventry (opened in 1954) and Tividale Comprehensive School in Tipton. Gibraltar opened its first comprehensive school in 1972. Between the ages of 12 and 16 two comprehensive schools cater for girls and boys separately. Students may also continue into the sixth form to complete their A-levels. The largest expansion of comprehensive schools in 1965 resulted from a policy decision taken in 1965 by Anthony Crosland, Secretary of State for Education in the 1964–1970 Labour government. The policy decision was implemented by Circular 10/65, an instruction to local education authorities to plan for conversion. Students sat the 11+ examination in their last year of primary education and were sent to one of a secondary modern, secondary technical or grammar school depending on their perceived ability. Secondary technical schools were never widely implemented and for 20 years there was a virtual bipartite system which saw fierce competition for the available grammar school places, which varied between 15% and 25% of total secondary places, depending on location.[citation needed] In 1976 the future Labour prime minister James Callaghan launched what became known as the 'great debate' on the education system. He went on to list the areas he felt needed closest scrutiny: the case for a core curriculum, the validity and use of informal teaching methods, the role of school inspection and the future of the examination system. Comprehensive school remains the most common type of state secondary school in England, and the only type in Wales. They account for around 90% of pupils, or 64% if one does not count schools with low-level selection. This figure varies by region. Comprehensive schools were introduced into Ireland in 1966 by an initiative by Patrick Hillery, Minister for Education, to give a broader range of education compared to that of the vocational school system, which was then the only system of schools completely controlled by the state. Until then, education in Ireland was largely dominated by religious persuasion, particularly the voluntary secondary school system was a particular realisation of this. The comprehensive school system is still relatively small and to an extent has been superseded by the community school concept. The Irish word for a comprehensive school is a 'scoil chuimsitheach.' In principle, comprehensive schools were conceived as ""neighbourhood"" schools for all students in a specified catchment area. Current education reforms with Academies Programme, Free Schools and University Technical Colleges will no doubt have some impact on the comprehensive ideal but it is too early to say to what degree. Finland has used comprehensive schools since the 1970s, in the sense that everyone is expected to complete the nine grades of peruskoulu, from the age 7 to 16. The division to lower comprehensive school (grades 1–6, ala-aste, alakoulu) and upper comprehensive school (grades 7–9, yläaste, yläkoulu) has been discontinued. The introduction of the community school model in the 1970s controversially removed the denominational basis of the schools, but religious interests were invited to be represented on the Boards of Management. Community schools are divided into two models, the community school vested in the Minister for Education and the community college vested in the local Education and Training Board. Community colleges tended to be amalgamations of unviable local schools under the umbrella of a new community school model, but community schools have tended to be entirely new foundations. In Ireland comprehensive schools were an earlier model of state schools, introduced in the late 1960s and largely replaced by the secular community model of the 1970s. The comprehensive model generally incorporated older schools that were under Roman Catholic or Protestant ownership, and the various denominations still manage the school as patrons or trustees. The state owns the school property, which is vested in the trustees in perpetuity. The model was adopted to make state schools more acceptable to a largely conservative society of the time. Starting in 2010/2011, Hauptschulen were merged with Realschulen and Gesamtschulen to form a new type of comprehensive school in the German States of Berlin and Hamburg, called Stadtteilschule in Hamburg and Sekundarschule in Berlin (see: Education in Berlin, Education in Hamburg). The percentage of students attending a Gesamtschule varies by Bundesland. In the State of Brandenburg more than 50% of all students attended a Gesamtschule in 2007, while in the State of Bavaria less than 1% did. According to a study done by Helmut Fend (who had always been a fierce proponent of comprehensive schools) revealed that comprehensive schools do not help working class students. He compared alumni of the tripartite system to alumni of comprehensive schools. While working class alumni of comprehensive schools were awarded better school diplomas at age 35, they held similar occupational positions as working class alumni of the tripartite system and were as unlikely to graduate from college. In 1970 Margaret Thatcher became Secretary of State for Education of the new Conservative government. She ended the compulsion on local authorities to convert, however, many local authorities were so far down the path that it would have been prohibitively expensive to attempt to reverse the process, and more comprehensive schools were established under Mrs Thatcher than any other education secretary."
United_States_Army,"The Vietnam War is often regarded as a low point for the U.S. Army due to the use of drafted personnel, the unpopularity of the war with the American public, and frustrating restrictions placed on the military by American political leaders. While American forces had been stationed in the Republic of Vietnam since 1959, in intelligence & advising/training roles, they did not deploy in large numbers until 1965, after the Gulf of Tonkin Incident. American forces effectively established and maintained control of the ""traditional"" battlefield, however they struggled to counter the guerrilla hit and run tactics of the communist Viet Cong and the North Vietnamese Army. On a tactical level, American soldiers (and the U.S. military as a whole) did not lose a sizable battle. The army has relied heavily on tents to provide the various facilities needed while on deployment. The most common tent uses for the military are as temporary barracks (sleeping quarters), DFAC buildings (dining facilities), forward operating bases (FOBs), after action review (AAR), tactical operations center (TOC), morale, welfare, and recreation (MWR) facilities, and security checkpoints. Furthermore, most of these tents are set up and operated through the support of Natick Soldier Systems Center. During the Cold War, American troops and their allies fought Communist forces in Korea and Vietnam. The Korean War began in 1950, when the Soviets walked out of a U.N. Security meeting, removing their possible veto. Under a United Nations umbrella, hundreds of thousands of U.S. troops fought to prevent the takeover of South Korea by North Korea, and later, to invade the northern nation. After repeated advances and retreats by both sides, and the PRC People's Volunteer Army's entry into the war, the Korean Armistice Agreement returned the peninsula to the status quo in 1953. The War of 1812, the second and last American war against the United Kingdom, was less successful for the U.S. than the Revolution and Northwest Indian War against natives had been, though it ended on a high note for Americans as well. After the taking control of Lake Erie in 1813, the Americans were able to seize parts of western Upper Canada, burn York and defeat Tecumseh, which caused his Indian Confederacy to collapse. Following ending victories in the province of Upper Canada, which dubbed the U.S. Army ""Regulars, by God!"", British troops were able to capture and burn Washington. The regular army, however, proved they were professional and capable of defeating the British army during the invasions of Plattsburgh and Baltimore, prompting British agreement on the previously rejected terms of a status quo ante bellum. Two weeks after a treaty was signed (but not ratified), Andrew Jackson defeated the British in the Battle of New Orleans and became a national hero. Per the treaty both sides returned to the status quo with no victor. Collective training at the unit level takes place at the unit's assigned station, but the most intensive training at higher echelons is conducted at the three combat training centers (CTC); the National Training Center (NTC) at Fort Irwin, California, the Joint Readiness Training Center (JRTC) at Fort Polk, Louisiana, and the Joint Multinational Training Center (JMRC) at the Hohenfels Training Area in Hohenfels, Germany. ARFORGEN is the Army Force Generation process approved in 2006 to meet the need to continuously replenish forces for deployment, at unit level, and for other echelons as required by the mission. Individual-level replenishment still requires training at a unit level, which is conducted at the continental US (CONUS) replacement center at Fort Bliss, in New Mexico and Texas, before their individual deployment. The United States Army is made up of three components: the active component, the Regular Army; and two reserve components, the Army National Guard and the Army Reserve. Both reserve components are primarily composed of part-time soldiers who train once a month, known as battle assemblies or unit training assemblies (UTAs), and conduct two to three weeks of annual training each year. Both the Regular Army and the Army Reserve are organized under Title 10 of the United States Code, while the National Guard is organized under Title 32. While the Army National Guard is organized, trained and equipped as a component of the U.S. Army, when it is not in federal service it is under the command of individual state and territorial governors; the District of Columbia National Guard, however, reports to the U.S. President, not the district's mayor, even when not federalized. Any or all of the National Guard can be federalized by presidential order and against the governor's wishes. The Total Force Policy was adopted by Chief of Staff of the Army General Creighton Abrams in the aftermath of the Vietnam War and involves treating the three components of the army – the Regular Army, the Army National Guard and the Army Reserve as a single force. Believing that no U.S. president should be able to take the United States (and more specifically the U.S. Army) to war without the support of the American people, General Abrams intertwined the structure of the three components of the army in such a way as to make extended operations impossible, without the involvement of both the Army National Guard and the Army Reserve. The U.S. Army black beret (having been permanently replaced with the patrol cap) is no longer worn with the new ACU for garrison duty. After years of complaints that it wasn't suited well for most work conditions, Army Chief of Staff General Martin Dempsey eliminated it for wear with the ACU in June 2011. Soldiers still wear berets who are currently in a unit in jump status, whether the wearer is parachute-qualified, or not (maroon beret), Members of the 75th Ranger Regiment and the Airborne and Ranger Training Brigade (tan beret), and Special Forces (rifle green beret) and may wear it with the Army Service Uniform for non-ceremonial functions. Unit commanders may still direct the wear of patrol caps in these units in training environments or motor pools. The army is led by a civilian Secretary of the Army, who has the statutory authority to conduct all the affairs of the army under the authority, direction and control of the Secretary of Defense. The Chief of Staff of the Army, who is the highest-ranked military officer in the army, serves as the principal military adviser and executive agent for the Secretary of the Army, i.e., its service chief; and as a member of the Joint Chiefs of Staff, a body composed of the service chiefs from each of the four military services belonging to the Department of Defense who advise the President of the United States, the Secretary of Defense, and the National Security Council on operational military matters, under the guidance of the Chairman and Vice Chairman of the Joint Chiefs of Staff. In 1986, the Goldwater–Nichols Act mandated that operational control of the services follows a chain of command from the President to the Secretary of Defense directly to the unified combatant commanders, who have control of all armed forces units in their geographic or function area of responsibility. Thus, the secretaries of the military departments (and their respective service chiefs underneath them) only have the responsibility to organize, train and equip their service components. The army provides trained forces to the combatant commanders for use as directed by the Secretary of Defense. For the first two years Confederate forces did well in set battles but lost control of the border states. The Confederates had the advantage of defending a very large country in an area where disease caused twice as many deaths as combat. The Union pursued a strategy of seizing the coastline, blockading the ports, and taking control of the river systems. By 1863 the Confederacy was being strangled. Its eastern armies fought well, but the western armies were defeated one after another until the Union forces captured New Orleans in 1862 along with the Tennessee River. In the famous Vicksburg Campaign of 1862–63, Ulysses Grant seized the Mississippi River and cut off the Southwest. Grant took command of Union forces in 1864 and after a series of battles with very heavy casualties, he had Lee under siege in Richmond as William T. Sherman captured Atlanta and marched through Georgia and the Carolinas. The Confederate capital was abandoned in April 1865 and Lee subsequently surrendered his army at Appomattox Court House; all other Confederate armies surrendered within a few months. The end of World War II set the stage for the East–West confrontation known as the Cold War. With the outbreak of the Korean War, concerns over the defense of Western Europe rose. Two corps, V and VII, were reactivated under Seventh United States Army in 1950 and American strength in Europe rose from one division to four. Hundreds of thousands of U.S. troops remained stationed in West Germany, with others in Belgium, the Netherlands and the United Kingdom, until the 1990s in anticipation of a possible Soviet attack. The American Civil War was the costliest war for the U.S. in terms of casualties. After most slave states, located in the southern U.S., formed the Confederate States, C.S. troops led by former U.S. Army officers, mobilized a very large fraction of Southern white manpower. Forces of the United States (the ""Union"" or ""the North"") formed the Union Army consisting of a small body of regular army units and a large body of volunteer units raised from every state, north and south, except South Carolina.[citation needed] As a uniformed military service, the Army is part of the Department of the Army, which is one of the three military departments of the Department of Defense. The U.S. Army is headed by a civilian senior appointed civil servant, the Secretary of the Army (SECARMY), and by a chief military officer, the Chief of Staff of the Army (CSA) who is also a member of the Joint Chiefs of Staff. In the fiscal year 2016, the projected end strength for the Regular Army (USA) was 475,000 soldiers; the Army National Guard (ARNG) had 342,000 soldiers, and the United States Army Reserve (USAR) had 198,000 soldiers; the combined-component strength of the U.S. Army was 1,015,000 soldiers. As a branch of the armed forces, the mission of the U.S. Army is ""to fight and win our Nation's wars, by providing prompt, sustained, land dominance, across the full range of military operations and the spectrum of conflict, in support of combatant commanders."" The service participates in conflicts worldwide and is the major ground-based offensive and defensive force. The Pentagon bought 25,000 MRAP vehicles since 2007 in 25 variants through rapid acquisition with no long-term plans for the platforms. The Army plans to divest 7,456 vehicles and retain 8,585. Of the total number of vehicles the Army will keep, 5,036 will be put in storage, 1,073 will be used for training, and the remainder will be spread across the active force. The Oshkosh M-ATV will be kept the most at 5,681 vehicles, as it is smaller and lighter than other MRAPs for off-road mobility. The other most retained vehicle will be the Navistar MaxxPro Dash with 2,633 vehicles, plus 301 Maxxpro ambulances. Thousands of other MRAPs like the Cougar, BAE Caiman, and larger MaxxPros will be disposed of. The army is also changing its base unit from divisions to brigades. Division lineage will be retained, but the divisional headquarters will be able to command any brigade, not just brigades that carry their divisional lineage. The central part of this plan is that each brigade will be modular, i.e., all brigades of the same type will be exactly the same, and thus any brigade can be commanded by any division. As specified before the 2013 end-strength re-definitions, the three major types of ground combat brigades are: Many units are supplemented with a variety of specialized weapons, including the M249 SAW (Squad Automatic Weapon), to provide suppressive fire at the fire-team level. Indirect fire is provided by the M203 grenade launcher. The M1014 Joint Service Combat Shotgun or the Mossberg 590 Shotgun are used for door breaching and close-quarters combat. The M14EBR is used by designated marksmen. Snipers use the M107 Long Range Sniper Rifle, the M2010 Enhanced Sniper Rifle, and the M110 Semi-Automatic Sniper Rifle. The army's major campaign against the Indians was fought in Florida against Seminoles. It took long wars (1818–58) to finally defeat the Seminoles and move them to Oklahoma. The usual strategy in Indian wars was to seize control of the Indians winter food supply, but that was no use in Florida where there was no winter. The second strategy was to form alliances with other Indian tribes, but that too was useless because the Seminoles had destroyed all the other Indians when they entered Florida in the late eighteenth century. By 1989 Germany was nearing reunification and the Cold War was coming to a close. Army leadership reacted by starting to plan for a reduction in strength. By November 1989 Pentagon briefers were laying out plans to reduce army end strength by 23%, from 750,000 to 580,000. A number of incentives such as early retirement were used. In 1990 Iraq invaded its smaller neighbor, Kuwait, and U.S. land forces, quickly deployed to assure the protection of Saudi Arabia. In January 1991 Operation Desert Storm commenced, a U.S.-led coalition which deployed over 500,000 troops, the bulk of them from U.S. Army formations, to drive out Iraqi forces. The campaign ended in total victory, as Western coalition forces routed the Iraqi Army, organized along Soviet lines, in just one hundred hours. Following their basic and advanced training at the individual-level, soldiers may choose to continue their training and apply for an ""additional skill identifier"" (ASI). The ASI allows the army to take a wide ranging MOS and focus it into a more specific MOS. For example, a combat medic, whose duties are to provide pre-hospital emergency treatment, may receive ASI training to become a cardiovascular specialist, a dialysis specialist, or even a licensed practical nurse. For commissioned officers, ASI training includes pre-commissioning training either at USMA, or via ROTC, or by completing OCS. After commissioning, officers undergo branch specific training at the Basic Officer Leaders Course, (formerly called Officer Basic Course), which varies in time and location according their future assignments. Further career development is available through the Army Correspondence Course Program. The task of organizing the U.S. Army commenced in 1775. In the first one hundred years of its existence, the United States Army was maintained as a small peacetime force to man permanent forts and perform other non-wartime duties such as engineering and construction works. During times of war, the U.S. Army was augmented by the much larger United States Volunteers which were raised independently by various state governments. States also maintained full-time militias which could also be called into the service of the army. By the twentieth century, the U.S. Army had mobilized the U.S. Volunteers on four separate occasions during each of the major wars of the nineteenth century. During World War I, the ""National Army"" was organized to fight the conflict, replacing the concept of U.S. Volunteers. It was demobilized at the end of World War I, and was replaced by the Regular Army, the Organized Reserve Corps, and the State Militias. In the 1920s and 1930s, the ""career"" soldiers were known as the ""Regular Army"" with the ""Enlisted Reserve Corps"" and ""Officer Reserve Corps"" augmented to fill vacancies when needed. During the 1960s the Department of Defense continued to scrutinize the reserve forces and to question the number of divisions and brigades as well as the redundancy of maintaining two reserve components, the Army National Guard and the Army Reserve. In 1967 Secretary of Defense Robert McNamara decided that 15 combat divisions in the Army National Guard were unnecessary and cut the number to 8 divisions (1 mechanized infantry, 2 armored, and 5 infantry), but increased the number of brigades from 7 to 18 (1 airborne, 1 armored, 2 mechanized infantry, and 14 infantry). The loss of the divisions did not set well with the states. Their objections included the inadequate maneuver element mix for those that remained and the end to the practice of rotating divisional commands among the states that supported them. Under the proposal, the remaining division commanders were to reside in the state of the division base. No reduction, however, in total Army National Guard strength was to take place, which convinced the governors to accept the plan. The states reorganized their forces accordingly between 1 December 1967 and 1 May 1968. The United States Army (USA) is the largest branch of the United States Armed Forces and performs land-based military operations. It is one of the seven uniformed services of the United States and is designated as the Army of the United States in the United States Constitution, Article 2, Section 2, Clause 1 and United States Code, Title 10, Subtitle B, Chapter 301, Section 3001. As the largest and senior branch of the U.S. military, the modern U.S. Army has its roots in the Continental Army, which was formed (14 June 1775) to fight the American Revolutionary War (1775–83)—before the U.S. was established as a country. After the Revolutionary War, the Congress of the Confederation created the United States Army on 3 June 1784, to replace the disbanded Continental Army. The United States Army considers itself descended from the Continental Army, and dates its institutional inception from the origin of that armed force in 1775. The Continental Army was created on 14 June 1775 by the Continental Congress as a unified army for the colonies to fight Great Britain, with George Washington appointed as its commander. The army was initially led by men who had served in the British Army or colonial militias and who brought much of British military heritage with them. As the Revolutionary War progressed, French aid, resources, and military thinking influenced the new army. A number of European soldiers came on their own to help, such as Friedrich Wilhelm von Steuben, who taught the army Prussian tactics and organizational skills. On September 11, 2001, 53 Army civilians (47 employees and six contractors) and 22 soldiers were among the 125 victims killed in the Pentagon in a terrorist attack when American Airlines Flight 77 commandeered by five Al-Qaeda hijackers slammed into the western side of the building, as part of the September 11 attacks. Lieutenant General Timothy Maude was the highest-ranking military official killed at the Pentagon, and the most senior U.S. Army officer killed by foreign action since the death of Lieutenant General Simon B. Buckner, Jr. on June 18, 1945, in the Battle of Okinawa during World War II. The army employs various individual weapons to provide light firepower at short ranges. The most common weapons used by the army are the compact variant of the M16 rifle, the M4 carbine, as well as the 7.62×51mm variant of the FN SCAR for Army Rangers. The primary sidearm in the U.S. Army is the 9 mm M9 pistol; the M11 pistol is also used. Both handguns are to be replaced through the Modular Handgun System program. Soldiers are also equiped with various hand grenades, such as the M67 fragmentation grenade and M18 smoke grenade. Currently, the army is divided into the Regular Army, the Army Reserve, and the Army National Guard. The army is also divided into major branches such as Air Defense Artillery, Infantry, Aviation, Signal Corps, Corps of Engineers, and Armor. Before 1903 members of the National Guard were considered state soldiers unless federalized (i.e., activated) by the President. Since the Militia Act of 1903 all National Guard soldiers have held dual status: as National Guardsmen under the authority of the governor of their state or territory and, when activated, as a reserve of the U.S. Army under the authority of the President. In response to the September 11 attacks, and as part of the Global War on Terror, U.S. and NATO forces invaded Afghanistan in October 2001, displacing the Taliban government. The U.S. Army also led the combined U.S. and allied invasion of Iraq in 2003. It served as the primary source for ground forces with its ability to sustain short and long-term deployment operations. In the following years the mission changed from conflict between regular militaries to counterinsurgency, resulting in the deaths of more than 4,000 U.S service members (as of March 2008) and injuries to thousands more. 23,813 insurgents were killed in Iraq between 2003–2011. The army's most common vehicle is the High Mobility Multipurpose Wheeled Vehicle (HMMWV), commonly called the Humvee, which is capable of serving as a cargo/troop carrier, weapons platform, and ambulance, among many other roles. While they operate a wide variety of combat support vehicles, one of the most common types centers on the family of HEMTT vehicles. The M1A2 Abrams is the army's main battle tank, while the M2A3 Bradley is the standard infantry fighting vehicle. Other vehicles include the Stryker, and the M113 armored personnel carrier, and multiple types of Mine Resistant Ambush Protected (MRAP) vehicles. The U.S. Army currently consists of 10 active divisions as well as several independent units. The force is in the process of contracting after several years of growth. In June 2013, the Army announced plans to downsize to 32 active combat brigade teams by 2015 to match a reduction in active duty strength to 490,000 soldiers. Army Chief of Staff Raymond Odierno has projected that by 2018 the Army will eventually shrink to ""450,000 in the active component, 335,000 in the National Guard and 195,000 in U.S. Army Reserve."" The United States joined World War II in December 1941 after the Japanese attack on Pearl Harbor. On the European front, U.S. Army troops formed a significant portion of the forces that captured North Africa and Sicily, and later fought in Italy. On D-Day, June 6, 1944, and in the subsequent liberation of Europe and defeat of Nazi Germany, millions of U.S. Army troops played a central role. In the Pacific War, U.S. Army soldiers participated alongside the United States Marine Corps in capturing the Pacific Islands from Japanese control. Following the Axis surrenders in May (Germany) and August (Japan) of 1945, army troops were deployed to Japan and Germany to occupy the two defeated nations. Two years after World War II, the Army Air Forces separated from the army to become the United States Air Force in September 1947 after decades of attempting to separate. Also, in 1948, the army was desegregated by order of President Harry S. Truman. Starting in 1910, the army began acquiring fixed-wing aircraft. In 1910, Mexico was having a civil war, peasant rebels fighting government soldiers. The army was deployed to American towns near the border to ensure safety to lives and property. In 1916, Pancho Villa, a major rebel leader, attacked Columbus, New Mexico, prompting a U.S. intervention in Mexico until 7 February 1917. They fought the rebels and the Mexican federal troops until 1918. The United States joined World War I in 1917 on the side of Britain, France, Russia, Italy and other allies. U.S. troops were sent to the Western Front and were involved in the last offensives that ended the war. With the armistice in November 1918, the army once again decreased its forces. After the war, though, the Continental Army was quickly given land certificates and disbanded in a reflection of the republican distrust of standing armies. State militias became the new nation's sole ground army, with the exception of a regiment to guard the Western Frontier and one battery of artillery guarding West Point's arsenal. However, because of continuing conflict with Native Americans, it was soon realized that it was necessary to field a trained standing army. The Regular Army was at first very small, and after General St. Clair's defeat at the Battle of the Wabash, the Regular Army was reorganized as the Legion of the United States, which was established in 1791 and renamed the ""United States Army"" in 1796. Training in the U.S. Army is generally divided into two categories – individual and collective. Basic training consists of 10 weeks for most recruits followed by Advanced Individualized Training (AIT) where they receive training for their military occupational specialties (MOS). Some individuals MOSs range anywhere from 14–20 weeks of One Station Unit Training (OSUT), which combines Basic Training and AIT. The length of AIT school varies by the MOS The length of time spent in AIT depends on the MOS of the soldier, and some highly technical MOS training may require many months (e.g., foreign language translators). Depending on the needs of the army, Basic Combat Training for combat arms soldiers is conducted at a number of locations, but two of the longest-running are the Armor School and the Infantry School, both at Fort Benning, Georgia."
Harvard_University,"Harvard's 209-acre (85 ha) main campus is centered on Harvard Yard in Cambridge, about 3 miles (5 km) west-northwest of the State House in downtown Boston, and extends into the surrounding Harvard Square neighborhood. Harvard Yard itself contains the central administrative offices and main libraries of the university, academic buildings including Sever Hall and University Hall, Memorial Church, and the majority of the freshman dormitories. Sophomore, junior, and senior undergraduates live in twelve residential Houses, nine of which are south of Harvard Yard along or near the Charles River. The other three are located in a residential neighborhood half a mile northwest of the Yard at the Quadrangle (commonly referred to as the Quad), which formerly housed Radcliffe College students until Radcliffe merged its residential system with Harvard. Each residential house contains rooms for undergraduates, House masters, and resident tutors, as well as a dining hall and library. The facilities were made possible by a gift from Yale University alumnus Edward Harkness. For the 2012–13 school year annual tuition was $38,000, with a total cost of attendance of $57,000. Beginning 2007, families with incomes below $60,000 pay nothing for their children to attend, including room and board. Families with incomes between $60,000 to $80,000 pay only a few thousand dollars per year, and families earning between $120,000 and $180,000 pay no more than 10% of their annual incomes. In 2009, Harvard offered grants totaling $414 million across all eleven divisions;[further explanation needed] $340 million came from institutional funds, $35 million from federal support, and $39 million from other outside support. Grants total 88% of Harvard's aid for undergraduate students, with aid also provided by loans (8%) and work-study (4%). The four-year, full-time undergraduate program comprises a minority of enrollments at the university and emphasizes instruction with an ""arts and sciences focus"". Between 1978 and 2008, entering students were required to complete a core curriculum of seven classes outside of their concentration. Since 2008, undergraduate students have been required to complete courses in eight General Education categories: Aesthetic and Interpretive Understanding, Culture and Belief, Empirical and Mathematical Reasoning, Ethical Reasoning, Science of Living Systems, Science of the Physical Universe, Societies of the World, and United States in the World. Harvard offers a comprehensive doctoral graduate program and there is a high level of coexistence between graduate and undergraduate degrees. The Carnegie Foundation for the Advancement of Teaching, The New York Times, and some students have criticized Harvard for its reliance on teaching fellows for some aspects of undergraduate education; they consider this to adversely affect the quality of education. Other: Civil rights leader W. E. B. Du Bois; philosopher Henry David Thoreau; authors Ralph Waldo Emerson and William S. Burroughs; educators Werner Baer, Harlan Hanson; poets Wallace Stevens, T. S. Eliot and E. E. Cummings; conductor Leonard Bernstein; cellist Yo Yo Ma; pianist and composer Charlie Albright; composer John Alden Carpenter; comedian, television show host and writer Conan O'Brien; actors Tatyana Ali, Nestor Carbonell, Matt Damon, Fred Gwynne, Hill Harper, Rashida Jones, Tommy Lee Jones, Ashley Judd, Jack Lemmon, Natalie Portman, Mira Sorvino, Elisabeth Shue, and Scottie Thompson; film directors Darren Aronofsky, Terrence Malick, Mira Nair, and Whit Stillman; architect Philip Johnson; musicians Rivers Cuomo, Tom Morello, and Gram Parsons; musician, producer and composer Ryan Leslie; serial killer Ted Kaczynski; programmer and activist Richard Stallman; NFL quarterback Ryan Fitzpatrick; NFL center Matt Birk; NBA player Jeremy Lin; US Ski Team skier Ryan Max Riley; physician Sachin H. Jain; physicist J. Robert Oppenheimer; computer pioneer and inventor An Wang; Tibetologist George de Roerich; and Marshall Admiral Isoroku Yamamoto. Charles W. Eliot, president 1869–1909, eliminated the favored position of Christianity from the curriculum while opening it to student self-direction. While Eliot was the most crucial figure in the secularization of American higher education, he was motivated not by a desire to secularize education, but by Transcendentalist Unitarian convictions. Derived from William Ellery Channing and Ralph Waldo Emerson, these convictions were focused on the dignity and worth of human nature, the right and ability of each person to perceive truth, and the indwelling God in each person. Harvard's 2,400 professors, lecturers, and instructors instruct 7,200 undergraduates and 14,000 graduate students. The school color is crimson, which is also the name of the Harvard sports teams and the daily newspaper, The Harvard Crimson. The color was unofficially adopted (in preference to magenta) by an 1875 vote of the student body, although the association with some form of red can be traced back to 1858, when Charles William Eliot, a young graduate student who would later become Harvard's 21st and longest-serving president (1869–1909), bought red bandanas for his crew so they could more easily be distinguished by spectators at a regatta. Older than The Game by 23 years, the Harvard-Yale Regatta was the original source of the athletic rivalry between the two schools. It is held annually in June on the Thames River in eastern Connecticut. The Harvard crew is typically considered to be one of the top teams in the country in rowing. Today, Harvard fields top teams in several other sports, such as the Harvard Crimson men's ice hockey team (with a strong rivalry against Cornell), squash, and even recently won NCAA titles in Men's and Women's Fencing. Harvard also won the Intercollegiate Sailing Association National Championships in 2003. Throughout the 18th century, Enlightenment ideas of the power of reason and free will became widespread among Congregationalist ministers, putting those ministers and their congregations in tension with more traditionalist, Calvinist parties.:1–4 When the Hollis Professor of Divinity David Tappan died in 1803 and the president of Harvard Joseph Willard died a year later, in 1804, a struggle broke out over their replacements. Henry Ware was elected to the chair in 1805, and the liberal Samuel Webber was appointed to the presidency of Harvard two years later, which signaled the changing of the tide from the dominance of traditional ideas at Harvard to the dominance of liberal, Arminian ideas (defined by traditionalists as Unitarian ideas).:4–5:24 Women remained segregated at Radcliffe, though more and more took Harvard classes. Nonetheless, Harvard's undergraduate population remained predominantly male, with about four men attending Harvard College for every woman studying at Radcliffe. Following the merger of Harvard and Radcliffe admissions in 1977, the proportion of female undergraduates steadily increased, mirroring a trend throughout higher education in the United States. Harvard's graduate schools, which had accepted females and other groups in greater numbers even before the college, also became more diverse in the post-World War II period. Undergraduate admission to Harvard is characterized by the Carnegie Foundation as ""more selective, lower transfer-in"". Harvard College accepted 5.3% of applicants for the class of 2019, a record low and the second lowest acceptance rate among all national universities. Harvard College ended its early admissions program in 2007 as the program was believed to disadvantage low-income and under-represented minority applicants applying to selective universities, yet for the class of 2016 an Early Action program was reintroduced. During the divestment from South Africa movement in the late 1980s, student activists erected a symbolic ""shantytown"" on Harvard Yard and blockaded a speech given by South African Vice Consul Duke Kent-Brown. The Harvard Management Company repeatedly refused to divest, stating that ""operating expenses must not be subject to financially unrealistic strictures or carping by the unsophisticated or by special interest groups."" However, the university did eventually reduce its South African holdings by $230 million (out of $400 million) in response to the pressure. Established originally by the Massachusetts legislature and soon thereafter named for John Harvard (its first benefactor), Harvard is the United States' oldest institution of higher learning, and the Harvard Corporation (formally, the President and Fellows of Harvard College) is its first chartered corporation. Although never formally affiliated with any denomination, the early College primarily trained Congregationalist and Unitarian clergy. Its curriculum and student body were gradually secularized during the 18th century, and by the 19th century Harvard had emerged as the central cultural establishment among Boston elites. Following the American Civil War, President Charles W. Eliot's long tenure (1869–1909) transformed the college and affiliated professional schools into a modern research university; Harvard was a founding member of the Association of American Universities in 1900. James Bryant Conant led the university through the Great Depression and World War II and began to reform the curriculum and liberalize admissions after the war. The undergraduate college became coeducational after its 1977 merger with Radcliffe College. In the early years the College trained many Puritan ministers.[citation needed] (A 1643 publication said the school's purpose was ""to advance learning and perpetuate it to posterity, dreading to leave an illiterate ministry to the churches when our present ministers shall lie in the dust"".) It offered a classic curriculum on the English university model—​​many leaders in the colony had attended the University of Cambridge—​​but conformed Puritanism. It was never affiliated with any particular denomination, but many of its earliest graduates went on to become clergymen in Congregational and Unitarian churches. Harvard has several athletic facilities, such as the Lavietes Pavilion, a multi-purpose arena and home to the Harvard basketball teams. The Malkin Athletic Center, known as the ""MAC"", serves both as the university's primary recreation facility and as a satellite location for several varsity sports. The five-story building includes two cardio rooms, an Olympic-size swimming pool, a smaller pool for aquaerobics and other activities, a mezzanine, where all types of classes are held, an indoor cycling studio, three weight rooms, and a three-court gym floor to play basketball. The MAC offers personal trainers and specialty classes. It is home to Harvard volleyball, fencing and wrestling. The offices of several of the school's varsity coaches are also in the MAC. The University is organized into eleven separate academic units—ten faculties and the Radcliffe Institute for Advanced Study—with campuses throughout the Boston metropolitan area: its 209-acre (85 ha) main campus is centered on Harvard Yard in Cambridge, approximately 3 miles (5 km) northwest of Boston; the business school and athletics facilities, including Harvard Stadium, are located across the Charles River in the Allston neighborhood of Boston and the medical, dental, and public health schools are in the Longwood Medical Area. Harvard's $37.6 billion financial endowment is the largest of any academic institution. James Bryant Conant (president, 1933–1953) reinvigorated creative scholarship to guarantee its preeminence among research institutions. He saw higher education as a vehicle of opportunity for the talented rather than an entitlement for the wealthy, so Conant devised programs to identify, recruit, and support talented youth. In 1943, he asked the faculty make a definitive statement about what general education ought to be, at the secondary as well as the college level. The resulting Report, published in 1945, was one of the most influential manifestos in the history of American education in the 20th century. The Harvard Business School and many of the university's athletics facilities, including Harvard Stadium, are located on a 358-acre (145 ha) campus opposite the Cambridge campus in Allston. The John W. Weeks Bridge is a pedestrian bridge over the Charles River connecting both campuses. The Harvard Medical School, Harvard School of Dental Medicine, and the Harvard School of Public Health are located on a 21-acre (8.5 ha) campus in the Longwood Medical and Academic Area approximately 3.3 miles (5.3 km) southwest of downtown Boston and 3.3 miles (5.3 km) south of the Cambridge campus. Politics: U.N. Secretary General Ban Ki-moon; American political leaders John Hancock, John Adams, John Quincy Adams, Rutherford B. Hayes, Theodore Roosevelt, Franklin D. Roosevelt, John F. Kennedy, Al Gore, George W. Bush and Barack Obama; Chilean President Sebastián Piñera; Colombian President Juan Manuel Santos; Costa Rican President José María Figueres; Mexican Presidents Felipe Calderón, Carlos Salinas de Gortari and Miguel de la Madrid; Mongolian President Tsakhiagiin Elbegdorj; Peruvian President Alejandro Toledo; Taiwanese President Ma Ying-jeou; Canadian Governor General David Lloyd Johnston; Indian Member of Parliament Jayant Sinha; Albanian Prime Minister Fan S. Noli; Canadian Prime Ministers Mackenzie King and Pierre Trudeau; Greek Prime Minister Antonis Samaras; Israeli Prime Minister Benjamin Netanyahu; former Pakistani Prime Minister Benazir Bhutto; U. S. Secretary of Housing and Urban Development Shaun Donovan; Canadian political leader Michael Ignatieff; Pakistani Members of Provincial Assembly Murtaza Bhutto and Sanam Bhutto; Bangladesh Minister of Finance Abul Maal Abdul Muhith; President of Puntland Abdiweli Mohamed Ali; U.S. Ambassador to the European Union Anthony Luzzatto Gardner. The Harvard University Library System is centered in Widener Library in Harvard Yard and comprises nearly 80 individual libraries holding over 18 million volumes. According to the American Library Association, this makes it the largest academic library in the United States, and one of the largest in the world. Cabot Science Library, Lamont Library, and Widener Library are three of the most popular libraries for undergraduates to use, with easy access and central locations. There are rare books, manuscripts and other special collections throughout Harvard's libraries; Houghton Library, the Arthur and Elizabeth Schlesinger Library on the History of Women in America, and the Harvard University Archives consist principally of rare and unique materials. America's oldest collection of maps, gazetteers, and atlases both old and new is stored in Pusey Library and open to the public. The largest collection of East-Asian language material outside of East Asia is held in the Harvard-Yenching Library. Harvard has purchased tracts of land in Allston, a walk across the Charles River from Cambridge, with the intent of major expansion southward. The university now owns approximately fifty percent more land in Allston than in Cambridge. Proposals to connect the Cambridge campus with the new Allston campus include new and enlarged bridges, a shuttle service and/or a tram. Plans also call for sinking part of Storrow Drive (at Harvard's expense) for replacement with park land and pedestrian access to the Charles River, as well as the construction of bike paths, and buildings throughout the Allston campus. The institution asserts that such expansion will benefit not only the school, but surrounding community, pointing to such features as the enhanced transit infrastructure, possible shuttles open to the public, and park space which will also be publicly accessible. The Harvard Crimson competes in 42 intercollegiate sports in the NCAA Division I Ivy League. Harvard has an intense athletic rivalry with Yale University culminating in The Game, although the Harvard–Yale Regatta predates the football game. This rivalry, though, is put aside every two years when the Harvard and Yale Track and Field teams come together to compete against a combined Oxford University and Cambridge University team, a competition that is the oldest continuous international amateur competition in the world. Harvard's faculty includes scholars such as biologist E. O. Wilson, cognitive scientist Steven Pinker, physicists Lisa Randall and Roy Glauber, chemists Elias Corey, Dudley R. Herschbach and George M. Whitesides, computer scientists Michael O. Rabin and Leslie Valiant, Shakespeare scholar Stephen Greenblatt, writer Louis Menand, critic Helen Vendler, historians Henry Louis Gates, Jr. and Niall Ferguson, economists Amartya Sen, N. Gregory Mankiw, Robert Barro, Stephen A. Marglin, Don M. Wilson III and Martin Feldstein, political philosophers Harvey Mansfield, Baroness Shirley Williams and Michael Sandel, Fields Medalist mathematician Shing-Tung Yau, political scientists Robert Putnam, Joseph Nye, and Stanley Hoffmann, scholar/composers Robert Levin and Bernard Rands, astrophysicist Alyssa A. Goodman, and legal scholars Alan Dershowitz and Lawrence Lessig. Harvard was formed in 1636 by vote of the Great and General Court of the Massachusetts Bay Colony. It was initially called ""New College"" or ""the college at New Towne"". In 1638, the college became home for North America's first known printing press, carried by the ship John of London. In 1639, the college was renamed Harvard College after deceased clergyman John Harvard, who was an alumnus of the University of Cambridge. He had left the school £779 and his library of some 400 books. The charter creating the Harvard Corporation was granted in 1650. Harvard is a large, highly residential research university. The nominal cost of attendance is high, but the University's large endowment allows it to offer generous financial aid packages. It operates several arts, cultural, and scientific museums, alongside the Harvard Library, which is the world's largest academic and private library system, comprising 79 individual libraries with over 18 million volumes. Harvard's alumni include eight U.S. presidents, several foreign heads of state, 62 living billionaires, 335 Rhodes Scholars, and 242 Marshall Scholars. To date, some 150 Nobel laureates, 18 Fields Medalists and 13 Turing Award winners have been affiliated as students, faculty, or staff. Harvard's academic programs operate on a semester calendar beginning in early September and ending in mid-May. Undergraduates typically take four half-courses per term and must maintain a four-course rate average to be considered full-time. In many concentrations, students can elect to pursue a basic program or an honors-eligible program requiring a senior thesis and/or advanced course work. Students graduating in the top 4–5% of the class are awarded degrees summa cum laude, students in the next 15% of the class are awarded magna cum laude, and the next 30% of the class are awarded cum laude. Harvard has chapters of academic honor societies such as Phi Beta Kappa and various committees and departments also award several hundred named prizes annually. Harvard, along with other universities, has been accused of grade inflation, although there is evidence that the quality of the student body and its motivation have also increased. Harvard College reduced the number of students who receive Latin honors from 90% in 2004 to 60% in 2005. Moreover, the honors of ""John Harvard Scholar"" and ""Harvard College Scholar"" will now be given only to the top 5 percent and the next 5 percent of each class. In 1846, the natural history lectures of Louis Agassiz were acclaimed both in New York and on the campus at Harvard College. Agassiz's approach was distinctly idealist and posited Americans' ""participation in the Divine Nature"" and the possibility of understanding ""intellectual existences"". Agassiz's perspective on science combined observation with intuition and the assumption that a person can grasp the ""divine plan"" in all phenomena. When it came to explaining life-forms, Agassiz resorted to matters of shape based on a presumed archetype for his evidence. This dual view of knowledge was in concert with the teachings of Common Sense Realism derived from Scottish philosophers Thomas Reid and Dugald Stewart, whose works were part of the Harvard curriculum at the time. The popularity of Agassiz's efforts to ""soar with Plato"" probably also derived from other writings to which Harvard students were exposed, including Platonic treatises by Ralph Cudworth, John Norrisand, in a Romantic vein, Samuel Coleridge. The library records at Harvard reveal that the writings of Plato and his early modern and Romantic followers were almost as regularly read during the 19th century as those of the ""official philosophy"" of the more empirical and more deistic Scottish school. Harvard has been highly ranked by many university rankings. In particular, it has consistently topped the Academic Ranking of World Universities (ARWU) since 2003, and the THE World Reputation Rankings since 2011, when the first time such league tables were published. When the QS and Times were published in partnership as the THE-QS World University Rankings during 2004-2009, Harvard had also been regarded the first in every year. The University's undergraduate program has been continuously among the top two in the U.S. News & World Report. In 2014, Harvard topped the University Ranking by Academic Performance (URAP). It was ranked 8th on the 2013-2014 PayScale College Salary Report and 14th on the 2013 PayScale College Education Value Rankings. From a poll done by The Princeton Review, Harvard is the second most commonly named ""dream college"", both for students and parents in 2013, and was the first nominated by parents in 2009. In 2011, the Mines ParisTech : Professional Ranking World Universities ranked Harvard 1st university in the world in terms of number of alumni holding CEO position in Fortune Global 500 companies. Harvard operates several arts, cultural, and scientific museums. The Harvard Art Museums comprises three museums. The Arthur M. Sackler Museum includes collections of ancient, Asian, Islamic and later Indian art, the Busch-Reisinger Museum, formerly the Germanic Museum, covers central and northern European art, and the Fogg Museum of Art, covers Western art from the Middle Ages to the present emphasizing Italian early Renaissance, British pre-Raphaelite, and 19th-century French art. The Harvard Museum of Natural History includes the Harvard Mineralogical Museum, Harvard University Herbaria featuring the Blaschka Glass Flowers exhibit, and the Museum of Comparative Zoology. Other museums include the Carpenter Center for the Visual Arts, designed by Le Corbusier, housing the film archive, the Peabody Museum of Archaeology and Ethnology, specializing in the cultural history and civilizations of the Western Hemisphere, and the Semitic Museum featuring artifacts from excavations in the Middle East. Harvard's athletic rivalry with Yale is intense in every sport in which they meet, coming to a climax each fall in the annual football meeting, which dates back to 1875 and is usually called simply ""The Game"". While Harvard's football team is no longer one of the country's best as it often was a century ago during football's early days (it won the Rose Bowl in 1920), both it and Yale have influenced the way the game is played. In 1903, Harvard Stadium introduced a new era into football with the first-ever permanent reinforced concrete stadium of its kind in the country. The stadium's structure actually played a role in the evolution of the college game. Seeking to reduce the alarming number of deaths and serious injuries in the sport, Walter Camp (former captain of the Yale football team), suggested widening the field to open up the game. But the stadium was too narrow to accommodate a wider playing surface. So, other steps had to be taken. Camp would instead support revolutionary new rules for the 1906 season. These included legalizing the forward pass, perhaps the most significant rule change in the sport's history. Harvard has the largest university endowment in the world. As of September 2011[update], it had nearly regained the loss suffered during the 2008 recession. It was worth $32 billion in 2011, up from $28 billion in September 2010 and $26 billion in 2009. It suffered about 30% loss in 2008-09. In December 2008, Harvard announced that its endowment had lost 22% (approximately $8 billion) from July to October 2008, necessitating budget cuts. Later reports suggest the loss was actually more than double that figure, a reduction of nearly 50% of its endowment in the first four months alone. Forbes in March 2009 estimated the loss to be in the range of $12 billion. One of the most visible results of Harvard's attempt to re-balance its budget was their halting of construction of the $1.2 billion Allston Science Complex that had been scheduled to be completed by 2011, resulting in protests from local residents. As of 2012[update], Harvard University had a total financial aid reserve of $159 million for students, and a Pell Grant reserve of $4.093 million available for disbursement."
Madrasa,"Much of the study in the madrasah college centred on examining whether certain opinions of law were orthodox. This scholarly process of ""determining orthodoxy began with a question which the Muslim layman, called in that capacity mustaftī, presented to a jurisconsult, called mufti, soliciting from him a response, called fatwa, a legal opinion (the religious law of Islam covers civil as well as religious matters). The mufti (professor of legal opinions) took this question, studied it, researched it intensively in the sacred scriptures, in order to find a solution to it. This process of scholarly research was called ijtihād, literally, the exertion of one's efforts to the utmost limit."" The first institute of madrasa education was at the estate of Hazrat Zaid bin Arkam near a hill called Safa, where Hazrat Muhammad was the teacher and the students were some of his followers.[citation needed] After Hijrah (migration) the madrasa of ""Suffa"" was established in Madina on the east side of the Al-Masjid an-Nabawi mosque. Hazrat 'Ubada bin Samit was appointed there by Hazrat Muhammad as teacher and among the students.[citation needed] In the curriculum of the madrasa, there were teachings of The Qur'an,The Hadith, fara'iz, tajweed, genealogy, treatises of first aid, etc. There were also trainings of horse-riding, art of war, handwriting and calligraphy, athletics and martial arts. The first part of madrasa based education is estimated from the first day of ""nabuwwat"" to the first portion of the ""Umaiya"" caliphate.[citation needed] The Arabic term ijāzat al-tadrīs was awarded to Islamic scholars who were qualified to teach. According to Makdisi, the Latin title licentia docendi 'licence to teach' in the European university may have been a translation of the Arabic, but the underlying concept was very different. A significant difference between the ijāzat al-tadrīs and the licentia docendi was that the former was awarded by the individual scholar-teacher, while the latter was awarded by the chief official of the university, who represented the collective faculty, rather than the individual scholar-teacher. The first Madressa established in North America, Al-Rashid Islamic Institute, was established in Cornwall, Ontario in 1983 and has graduates who are Hafiz (Quran) and Ulama. The seminary was established by Mazhar Alam under the direction of his teacher the leading Indian Tablighi scholar Muhammad Zakariya Kandhlawi and focuses on the traditional Hanafi school of thought and shuns Salafist / Wahabi teachings. Due to its proximity to the US border city of Messina the school has historically had a high ratio of US students. Their most prominent graduate Shaykh Muhammad Alshareef completed his Hifz in the early 1990s then went on to deviate from his traditional roots and form the Salafist organization the AlMaghrib Institute. However, the classification of madaris as ""universities"" is disputed on the question of understanding of each institution on its own terms. In madaris, the ijāzahs were only issued in one field, the Islamic religious law of sharīʻah, and in no other field of learning. Other academic subjects, including the natural sciences, philosophy and literary studies, were only treated ""ancillary"" to the study of the Sharia. For example, a natural science like astronomy was only studied (if at all) to supply religious needs, like the time for prayer. This is why Ptolemaic astronomy was considered adequate, and is still taught in some modern day madaris. The Islamic law undergraduate degree from al-Azhar, the most prestigious madrasa, was traditionally granted without final examinations, but on the basis of the students' attentive attendance to courses. In contrast to the medieval doctorate which was granted by the collective authority of the faculty, the Islamic degree was not granted by the teacher to the pupil based on any formal criteria, but remained a ""personal matter, the sole prerogative of the person bestowing it; no one could force him to give one"". ""The first Ottoman Medrese was created in İznik in 1331 and most Ottoman medreses followed the traditions of Sunni Islam."" ""When an Ottoman sultan established a new medrese, he would invite scholars from the Islamic world—for example, Murad II brought scholars from Persia, such as ʻAlāʼ al-Dīn and Fakhr al-Dīn who helped enhance the reputation of the Ottoman medrese"". This reveals that the Islamic world was interconnected in the early modern period as they travelled around to other Islamic states exchanging knowledge. This sense that the Ottoman Empire was becoming modernised through globalization is also recognised by Hamadeh who says: ""Change in the eighteenth century as the beginning of a long and unilinear march toward westernisation reflects the two centuries of reformation in sovereign identity."" İnalcık also mentions that while scholars from for example Persia travelled to the Ottomans in order to share their knowledge, Ottomans travelled as well to receive education from scholars of these Islamic lands, such as Egypt, Persia and Turkestan. Hence, this reveals that similar to today's modern world, individuals from the early modern society travelled abroad to receive education and share knowledge and that the world was more interconnected than it seems. Also, it reveals how the system of ""schooling"" was also similar to today's modern world where students travel abroad to different countries for studies. Examples of Ottoman madaris are the ones built by Mehmed the Conqueror. He built eight madaris that were built ""on either side of the mosque where there were eight higher madaris for specialised studies and eight lower medreses, which prepared students for these."" The fact that they were built around, or near mosques reveals the religious impulses behind madrasa building and it reveals the interconnectedness between institutions of learning and religion. The students who completed their education in the lower medreses became known as danismends. This reveals that similar to the education system today, the Ottomans' educational system involved different kinds of schools attached to different kinds of levels. For example, there were lower madaris and specialised ones, and for one to get into the specialised area meant that he had to complete the classes in the lower one in order to adequately prepare himself for higher learning. al-Qarawīyīn University in Fez, Morocco is recognised by many historians as the oldest degree-granting university in the world, having been founded in 859 by Fatima al-Fihri. While the madrasa college could also issue degrees at all levels, the jāmiʻahs (such as al-Qarawīyīn and al-Azhar University) differed in the sense that they were larger institutions, more universal in terms of their complete source of studies, had individual faculties for different subjects, and could house a number of mosques, madaris, and other institutions within them. Such an institution has thus been described as an ""Islamic university"". Madaris were largely centred on the study of fiqh (Islamic jurisprudence). The ijāzat al-tadrīs wa-al-iftāʼ (""licence to teach and issue legal opinions"") in the medieval Islamic legal education system had its origins in the 9th century after the formation of the madhāhib (schools of jurisprudence). George Makdisi considers the ijāzah to be the origin of the European doctorate. However, in an earlier article, he considered the ijāzah to be of ""fundamental difference"" to the medieval doctorate, since the former was awarded by an individual teacher-scholar not obliged to follow any formal criteria, whereas the latter was conferred on the student by the collective authority of the faculty. To obtain an ijāzah, a student ""had to study in a guild school of law, usually four years for the basic undergraduate course"" and ten or more years for a post-graduate course. The ""doctorate was obtained after an oral examination to determine the originality of the candidate's theses"", and to test the student's ""ability to defend them against all objections, in disputations set up for the purpose."" These were scholarly exercises practised throughout the student's ""career as a graduate student of law."" After students completed their post-graduate education, they were awarded ijazas giving them the status of faqīh 'scholar of jurisprudence', muftī 'scholar competent in issuing fatwās', and mudarris 'teacher'. During the rule of the Fatimid and Mamluk dynasties and their successor states in the medieval Middle East, many of the ruling elite founded madaris through a religious endowment known as the waqf. Not only was the madrasa a potent symbol of status but it was an effective means of transmitting wealth and status to their descendants. Especially during the Mamlūk period, when only former slaves could assume power, the sons of the ruling Mamlūk elite were unable to inherit. Guaranteed positions within the new madaris thus allowed them to maintain status. Madaris built in this period include the Mosque-Madrasah of Sultan Ḥasan in Cairo. Madrasa (Arabic: مدرسة‎, madrasah, pl. مدارس, madāris, Turkish: Medrese) is the Arabic word for any type of educational institution, whether secular or religious (of any religion). The word is variously transliterated madrasah, madarasaa, medresa, madrassa, madraza, medrese, etc. In the West, the word usually refers to a specific type of religious school or college for the study of the Islamic religion, though this may not be the only subject studied. Not all students in madaris are Muslims; there is also a modern curriculum. People of all ages attend, and many often move on to becoming imams.[citation needed] The certificate of an ʻālim, for example, requires approximately twelve years of study.[citation needed] A good number of the ḥuffāẓ (plural of ḥāfiẓ) are the product of the madaris. The madaris also resemble colleges, where people take evening classes and reside in dormitories. An important function of the madaris is to admit orphans and poor children in order to provide them with education and training. Madaris may enroll female students; however, they study separately from the men.[citation needed] At the beginning of the Caliphate or Islamic Empire, the reliance on courts initially confined sponsorship and scholarly activities to major centres. Within several centuries, the development of Muslim educational institutions such as the madrasah and masjid eventually introduced such activities to provincial towns and dispersed them across the Islamic legal schools and Sufi orders. In addition to religious subjects, they also taught the ""rational sciences,"" as varied as mathematics, astronomy, astrology, geography, alchemy, philosophy, magic, and occultism, depending on the curriculum of the specific institution in question. The madaris, however, were not centres of advanced scientific study; scientific advances in Islam were usually carried out by scholars working under the patronage of royal courts. During this time,[when?] the Caliphate experienced a growth in literacy, having the highest literacy rate of the Middle Ages, comparable to classical Athens' literacy in antiquity but on a much larger scale. The emergence of the maktab and madrasa institutions played a fundamental role in the relatively high literacy rates of the medieval Islamic world. During its formative period, the term madrasah referred to a higher education institution, whose curriculum initially included only the ""religious sciences"", whilst philosophy and the secular sciences were often excluded. The curriculum slowly began to diversify, with many later madaris teaching both the religious and the ""secular sciences"", such as logic, mathematics and philosophy. Some madaris further extended their curriculum to history, politics, ethics, music, metaphysics, medicine, astronomy and chemistry. The curriculum of a madrasah was usually set by its founder, but most generally taught both the religious sciences and the physical sciences. Madaris were established throughout the Islamic world, examples being the 9th century University of al-Qarawiyyin, the 10th century al-Azhar University (the most famous), the 11th century Niẓāmīyah, as well as 75 madaris in Cairo, 51 in Damascus and up to 44 in Aleppo between 1155 and 1260. Many more were also established in the Andalusian cities of Córdoba, Seville, Toledo, Granada (Madrasah of Granada), Murcia, Almería, Valencia and Cádiz during the Caliphate of Córdoba. In the medieval Islamic world, an elementary school was known as a maktab, which dates back to at least the 10th century. Like madaris (which referred to higher education), a maktab was often attached to an endowed mosque. In the 11th century, the famous Persian Islamic philosopher and teacher Ibn Sīnā (known as Avicenna in the West), in one of his books, wrote a chapter about the maktab entitled ""The Role of the Teacher in the Training and Upbringing of Children,"" as a guide to teachers working at maktab schools. He wrote that children can learn better if taught in classes instead of individual tuition from private tutors, and he gave a number of reasons for why this is the case, citing the value of competition and emulation among pupils, as well as the usefulness of group discussions and debates. Ibn Sīnā described the curriculum of a maktab school in some detail, describing the curricula for two stages of education in a maktab school. In India the majority of these schools follow the Hanafi school of thought. The religious establishment forms part of the mainly two large divisions within the country, namely the Deobandis, who dominate in numbers (of whom the Darul Uloom Deoband constitutes one of the biggest madaris) and the Barelvis, who also make up a sizeable portion (Sufi-oriented). Some notable establishments include: Al Jamiatul Ashrafia, Mubarakpur, Manzar Islam Bareilly, Jamia Nizamdina New Delhi, Jamia Nayeemia Muradabad which is one of the largest learning centres for the Barelvis. The HR[clarification needed] ministry of the government of India has recently[when?] declared that a Central Madrasa Board would be set up. This will enhance the education system of madaris in India. Though the madaris impart Quranic education mainly, efforts are on to include Mathematics, Computers and science in the curriculum. In July 2015, the state government of Maharashtra created a stir de-recognised madrasa education, receiving critisicm from several political parties with the NCP accusing the ruling BJP of creating Hindu-Muslim friction in the state, and Kamal Farooqui of the All India Muslim Personal Law Board saying it was ""ill-designed""  Prior to the 12th century, women accounted for less than one percent of the world’s Islamic scholars. However, al-Sakhawi and Mohammad Akram Nadwi have since found evidence of over 8,000 female scholars since the 15th century. al-Sakhawi devotes an entire volume of his 12-volume biographical dictionary al-Ḍawʾ al-lāmiʻ to female scholars, giving information on 1,075 of them. More recently, the scholar Mohammad Akram Nadwi, currently a researcher from the Oxford Centre for Islamic Studies, has written 40 volumes on the muḥaddithāt (the women scholars of ḥadīth), and found at least 8,000 of them. According to the Sunni scholar Ibn ʻAsākir in the 12th century, there were opportunities for female education in the medieval Islamic world, writing that women could study, earn ijazahs (academic degrees), and qualify as scholars and teachers. This was especially the case for learned and scholarly families, who wanted to ensure the highest possible education for both their sons and daughters. Ibn ʻAsakir had himself studied under 80 different female teachers in his time. Female education in the Islamic world was inspired by Muhammad's wives, such as Khadijah, a successful businesswoman. According to a hadith attributed to Muhammad, he praised the women of Medina because of their desire for religious knowledge: Western commentators post-9/11 often perceive madaris as places of radical revivalism with a connotation of anti-Americanism and radical extremism, frequently associated in the Western press with Wahhabi attitudes toward non-Muslims. In Arabic the word madrasa simply means ""school"" and does not imply a political or religious affiliation, radical or otherwise. Madaris have varied curricula, and are not all religious. Some madaris in India, for example, have a secularised identity. Although early madaris were founded primarily to gain ""knowledge of God"" they also taught subjects such as mathematics and poetry. For example, in the Ottoman Empire, ""Madrasahs had seven categories of sciences that were taught, such as: styles of writing, oral sciences like the Arabic language, grammar, rhetoric, and history and intellectual sciences, such as logic."" This is similar to the Western world, in which universities began as institutions of the Catholic church. Al-Azhar University, founded in Cairo, Egypt in 975 by the Ismaʻīlī Shīʻī Fatimid dynasty as a jāmiʻah, had individual faculties for a theological seminary, Islamic law and jurisprudence, Arabic grammar, Islamic astronomy, early Islamic philosophy and logic in Islamic philosophy. The postgraduate doctorate in law was only obtained after ""an oral examination to determine the originality of the candidate's theses"", and to test the student's ""ability to defend them against all objections, in disputations set up for the purpose."" ‘Abd al-Laṭīf al-Baghdādī also delivered lectures on Islamic medicine at al-Azhar, while Maimonides delivered lectures on medicine and astronomy there during the time of Saladin. Another early jāmiʻah was the Niẓāmīyah of Baghdād (founded 1091), which has been called the ""largest university of the Medieval world."" Mustansiriya University, established by the ʻAbbāsid caliph al-Mustanṣir in 1233, in addition to teaching the religious subjects, offered courses dealing with philosophy, mathematics and the natural sciences. From around 750, during the Abbasid Caliphate, women “became renowned for their brains as well as their beauty”. In particular, many well known women of the time were trained from childhood in music, dancing and poetry. Mahbuba was one of these. Another feminine figure to be remembered for her achievements was Tawaddud, ""a slave girl who was said to have been bought at great cost by Hārūn al-Rashīd because she had passed her examinations by the most eminent scholars in astronomy, medicine, law, philosophy, music, history, Arabic grammar, literature, theology and chess"". Moreover, among the most prominent feminine figures was Shuhda who was known as ""the Scholar"" or ""the Pride of Women"" during the 12th century in Baghdad. Despite the recognition of women's aptitudes during the Abbasid dynasty, all these came to an end in Iraq with the sack of Baghdad in 1258. There is disagreement whether madaris ever became universities. Scholars like Arnold H. Green and Seyyed Hossein Nasr have argued that starting in the 10th century, some medieval Islamic madaris indeed became universities. George Makdisi and others, however, argue that the European university has no parallel in the medieval Islamic world. Darleen Pryds questions this view, pointing out that madaris and European universities in the Mediterranean region shared similar foundations by princely patrons and were intended to provide loyal administrators to further the rulers' agenda. Other scholars regard the university as uniquely European in origin and characteristics. In 2004, madaris were mainstreamed in 16 Regions nationwide, primarily in Muslim-majority areas in Mindanao under the auspices of the Department of Education (DepEd). The DepEd adopted Department Order No. 51, which instituted Arabic-language and Islamic Values instruction for Muslim children in state schools, and authorised implementation of the Standard Madrasa Curriculum (SMC) in private-run madaris. While there are state-recognised Islamic schools, such as Ibn Siena Integrated School in the Islamic City of Marawi, Sarang Bangun LC in Zamboanga and SMIE in Jolo, their Islamic studies programmes initially varied in application and content. In Singapore, madrasahs are private schools which are overseen by Majlis Ugama Islam Singapura (MUIS, English: Islamic Religious Council of Singapore). There are six Madrasahs in Singapore, catering to students from Primary 1 to Secondary 4. Four Madrasahs are coeducational and two are for girls. Students take a range of Islamic Studies subjects in addition to mainstream MOE curriculum subjects and sit for the PSLE and GCE 'O' Levels like their peers. In 2009, MUIS introduced the ""Joint Madrasah System"" (JMS), a joint collaboration of Madrasah Al-Irsyad Al-Islamiah primary school and secondary schools Madrasah Aljunied Al-Islamiah (offering the ukhrawi, or religious stream) and Madrasah Al-Arabiah Al-Islamiah (offering the academic stream). The JMS aims to introduce the International Baccalaureate (IB) programme into the Madrasah Al-Arabiah Al-Islamiah by 2019. Students attending a madrasah are required to wear the traditional Malay attire, including the songkok for boys and tudong for girls, in contrast to mainstream government schools which ban religious headgear as Singapore is officially a secular state. For students who wish to attend a mainstream school, they may opt to take classes on weekends at the madrasah instead of enrolling full-time. Although Ottoman madaris had a number of different branches of study, such as calligraphic sciences, oral sciences, and intellectual sciences, they primarily served the function of an Islamic centre for spiritual learning. ""The goal of all knowledge and in particular, of the spiritual sciences is knowledge of God."" Religion, for the most part, determines the significance and importance of each science. As İnalcık mentions: ""Those which aid religion are good and sciences like astrology are bad."" However, even though mathematics, or studies in logic were part of the madrasa's curriculum, they were all centred around religion. Even mathematics had a religious impulse behind its teachings. ""The Ulema of the Ottoman medreses held the view that hostility to logic and mathematics was futile since these accustomed the mind to correct thinking and thus helped to reveal divine truths"" – key word being ""divine"". İnalcık also mentions that even philosophy was only allowed to be studied so that it helped to confirm the doctrines of Islam."" Hence, madaris – schools were basically religious centres for religious teachings and learning in the Ottoman world. Although scholars such as Goffman have argued that the Ottomans were highly tolerant and lived in a pluralistic society, it seems that schools that were the main centres for learning were in fact heftily religious and were not religiously pluralistic, but centred around Islam. Similarly, in Europe ""Jewish children learned the Hebrew letters and texts of basic prayers at home, and then attended a school organised by the synagogue to study the Torah."" Wiesner-Hanks also says that Protestants also wanted to teach ""proper religious values."" This shows that in the early modern period, Ottomans and Europeans were similar in their ideas about how schools should be managed and what they should be primarily focused on. Thus, Ottoman madaris were very similar to present day schools in the sense that they offered a wide range of studies; however, these studies, in their ultimate objective, aimed to further solidify and consolidate Islamic practices and theories. As with any other country during the Early Modern Period, such as Italy and Spain in Europe, the Ottoman social life was interconnected with the medrese. Medreses were built in as part of a Mosque complex where many programmes, such as aid to the poor through soup kitchens, were held under the infrastructure of a mosque, which reveals the interconnectedness of religion and social life during this period. ""The mosques to which medreses were attached, dominated the social life in Ottoman cities."" Social life was not dominated by religion only in the Muslim world of the Ottoman Empire; it was also quite similar to the social life of Europe during this period. As Goffman says: ""Just as mosques dominated social life for the Ottomans, churches and synagogues dominated life for the Christians and Jews as well."" Hence, social life and the medrese were closely linked, since medreses taught many curricula, such as religion, which highly governed social life in terms of establishing orthodoxy. ""They tried moving their developing state toward Islamic orthodoxy."" Overall, the fact that mosques contained medreses comes to show the relevance of education to religion in the sense that education took place within the framework of religion and religion established social life by trying to create a common religious orthodoxy. Hence, medreses were simply part of the social life of society as students came to learn the fundamentals of their societal values and beliefs. In Southeast Asia, Muslim students have a choice of attending a secular government or an Islamic school. Madaris or Islamic schools are known as Sekolah Agama (Malay: religious school) in Malaysia and Indonesia, โรงเรียนศาสนาอิสลาม (Thai: school of Islam) in Thailand and madaris in the Philippines. In countries where Islam is not the majority or state religion, Islamic schools are found in regions such as southern Thailand (near the Thai-Malaysian border) and the southern Philippines in Mindanao, where a significant Muslim population can be found. Nevertheless, Makdisi has asserted that the European university borrowed many of its features from the Islamic madrasa, including the concepts of a degree and doctorate. Makdisi and Hugh Goddard have also highlighted other terms and concepts now used in modern universities which most likely have Islamic origins, including ""the fact that we still talk of professors holding the 'Chair' of their subject"" being based on the ""traditional Islamic pattern of teaching where the professor sits on a chair and the students sit around him"", the term 'academic circles' being derived from the way in which Islamic students ""sat in a circle around their professor"", and terms such as ""having 'fellows', 'reading' a subject, and obtaining 'degrees', can all be traced back"" to the Islamic concepts of aṣḥāb ('companions, as of Muhammad'), qirāʼah ('reading aloud the Qur'an') and ijāzah ('licence [to teach]') respectively. Makdisi has listed eighteen such parallels in terminology which can be traced back to their roots in Islamic education. Some of the practices now common in modern universities which Makdisi and Goddard trace back to an Islamic root include ""practices such as delivering inaugural lectures, wearing academic robes, obtaining doctorates by defending a thesis, and even the idea of academic freedom are also modelled on Islamic custom."" The Islamic scholarly system of fatwá and ijmāʻ, meaning opinion and consensus respectively, formed the basis of the ""scholarly system the West has practised in university scholarship from the Middle Ages down to the present day."" According to Makdisi and Goddard, ""the idea of academic freedom"" in universities was also ""modelled on Islamic custom"" as practised in the medieval Madrasa system from the 9th century. Islamic influence was ""certainly discernible in the foundation of the first deliberately planned university"" in Europe, the University of Naples Federico II founded by Frederick II, Holy Roman Emperor in 1224. However, all of these facets of medieval university life are considered by standard scholarship to be independent medieval European developments with no tracable Islamic influence. Generally, some reviewers have pointed out the strong inclination of Makdisi of overstating his case by simply resting on ""the accumulation of close parallels"", but all the while failing to point to convincing channels of transmission between the Muslim and Christian world. Norman Daniel points out that the Arab equivalent of the Latin disputation, the taliqa, was reserved for the ruler's court, not the madrasa, and that the actual differences between Islamic fiqh and medieval European civil law were profound. The taliqa only reached Islamic Spain, the only likely point of transmission, after the establishment of the first medieval universities. In fact, there is no Latin translation of the taliqa and, most importantly, no evidence of Latin scholars ever showing awareness of Arab influence on the Latin method of disputation, something they would have certainly found noteworthy. Rather, it was the medieval reception of the Greek Organon which set the scholastic sic et non in motion. Daniel concludes that resemblances in method had more to with the two religions having ""common problems: to reconcile the conflicting statements of their own authorities, and to safeguard the data of revelation from the impact of Greek philosophy""; thus Christian scholasticism and similar Arab concepts should be viewed in terms of a parallel occurrence, not of the transmission of ideas from one to the other, a view shared by Hugh Kennedy. As Muslim institutions of higher learning, the madrasa had the legal designation of waqf. In central and eastern Islamic lands, the view that the madrasa, as a charitable endowment, will remain under the control of the donor (and their descendent), resulted in a ""spurt"" of establishment of madaris in the 11th and 12th centuries. However, in Western Islamic lands, where the Maliki views prohibited donors from controlling their endowment, madaris were not as popular. Unlike the corporate designation of Western institutions of higher learning, the waqf designation seemed to have led to the exclusion of non-orthodox religious subjects such a philosophy and natural science from the curricula. The madrasa of al-Qarawīyīn, one of the two surviving madaris that predate the founding of the earliest medieval universities and are thus claimed to be the ""first universities"" by some authors, has acquired official university status as late as 1947. The other, al-Azhar, did acquire this status in name and essence only in the course of numerous reforms during the 19th and 20th century, notably the one of 1961 which introduced non-religious subjects to its curriculum, such as economics, engineering, medicine, and agriculture. It should also be noted that many medieval universities were run for centuries as Christian cathedral schools or monastic schools prior to their formal establishment as universitas scholarium; evidence of these immediate forerunners of the university dates back to the 6th century AD, thus well preceding the earliest madaris. George Makdisi, who has published most extensively on the topic concludes in his comparison between the two institutions: The term ""Islamic education"" means education in the light of Islam itself, which is rooted in the teachings of the Quran - holy book of Muslims. Islamic education and Muslim education are not the same. Because Islamic education has epistemological integration which is founded on Tawhid - Oneness or monotheism. For details Read ""A Qur’anic Methodology for Integrating Knowledge and Education: Implications for Malaysia’s Islamic Education Strategy"" written Tareq M Zayed  and ""Knowledge of Shariah and Knowledge to Manage ‘Self’ and ‘System’: Integration of Islamic Epistemology with the Knowledge and Education"" authored by Tareq M Zayed Ibn Sīnā refers to the secondary education stage of maktab schooling as a period of specialisation when pupils should begin to acquire manual skills, regardless of their social status. He writes that children after the age of 14 should be allowed to choose and specialise in subjects they have an interest in, whether it was reading, manual skills, literature, preaching, medicine, geometry, trade and commerce, craftsmanship, or any other subject or profession they would be interested in pursuing for a future career. He wrote that this was a transitional stage and that there needs to be flexibility regarding the age in which pupils graduate, as the student's emotional development and chosen subjects need to be taken into account. Today, the system of Arabic and Islamic education has grown and further integrated with Kerala government administration. In 2005, an estimated 6,000 Muslim Arabic teachers taught in Kerala government schools, with over 500,000 Muslim students. State-appointed committees, not private mosques or religious scholars outside the government, determine the curriculum and accreditation of new schools and colleges. Primary education in Arabic and Islamic studies is available to Kerala Muslims almost entirely in after-school madrasa programs - sharply unlike full-time madaris common in north India, which may replace formal schooling. Arabic colleges (over eleven of which exist within the state-run University of Calicut and the Kannur University) provide B.A. and Masters' level degrees. At all levels, instruction is co-educational, with many women instructors and professors. Islamic education boards are independently run by the following organizations, accredited by the Kerala state government: Samastha Kerala Islamic Education Board, Kerala Nadvathul Mujahideen, Jamaat-e-Islami Hind, and Jamiat Ulema-e-Hind. However, in English, the term madrasah usually refers to the specifically Islamic institutions. A typical Islamic school usually offers two courses of study: a ḥifẓ course teaching memorization of the Qur'an (the person who commits the entire Qurʼan to memory is called a ḥāfiẓ); and an ʻālim course leading the candidate to become an accepted scholar in the community. A regular curriculum includes courses in Arabic, tafsir (Qur'anic interpretation), sharīʻah (Islamic law), hadiths (recorded sayings and deeds of Muhammad), mantiq (logic), and Muslim history. In the Ottoman Empire, during the Early Modern Period, the study of hadiths was introduced by Süleyman I. Depending on the educational demands, some madaris also offer additional advanced courses in Arabic literature, English and other foreign languages, as well as science and world history. Ottoman madaris along with religious teachings also taught ""styles of writing, grammary, syntax, poetry, composition, natural sciences, political sciences, and etiquette."" Medievalist specialists who define the university as a legally autonomous corporation disagree with the term ""university"" for the Islamic madaris and jāmi‘ahs because the medieval university (from Latin universitas) was structurally different, being a legally autonomous corporation rather than a waqf institution like the madrasa and jāmiʻah. Despite the many similarities, medieval specialists have coined the term ""Islamic college"" for madrasa and jāmiʻah to differentiate them from the legally autonomous corporations that the medieval European universities were. In a sense, the madrasa resembles a university college in that it has most of the features of a university, but lacks the corporate element. Toby Huff summarises the difference as follows: The word madrasah derives from the triconsonantal Semitic root د-ر-س D-R-S 'to learn, study', through the wazn (form/stem) مفعل(ة)‎; mafʻal(ah), meaning ""a place where something is done"". Therefore, madrasah literally means ""a place where learning and studying take place"". The word is also present as a loanword with the same innocuous meaning in many Arabic-influenced languages, such as: Urdu, Bengali, Hindi, Persian, Turkish, Azeri, Kurdish, Indonesian, Malay and Bosnian / Croatian. In the Arabic language, the word مدرسة madrasah simply means the same as school does in the English language, whether that is private, public or parochial school, as well as for any primary or secondary school whether Muslim, non-Muslim, or secular. Unlike the use of the word school in British English, the word madrasah more closely resembles the term school in American English, in that it can refer to a university-level or post-graduate school as well as to a primary or secondary school. For example, in the Ottoman Empire during the Early Modern Period, madaris had lower schools and specialised schools where the students became known as danişmends. The usual Arabic word for a university, however, is جامعة (jāmiʻah). The Hebrew cognate midrasha also connotes the meaning of a place of learning; the related term midrash literally refers to study or learning, but has acquired mystical and religious connotations."
Great_Plains,"To allow for agricultural development of the Great Plains and house a growing population, the US passed the Homestead Acts of 1862: it allowed a settler to claim up to 160 acres (65 ha) of land, provided that he lived on it for a period of five years and cultivated it. The provisions were expanded under the Kinkaid Act of 1904 to include a homestead of an entire section. Hundreds of thousands of people claimed such homesteads, sometimes building sod houses out of the very turf of their land. Many of them were not skilled dryland farmers and failures were frequent. Much of the Plains were settled during relatively wet years. Government experts did not understand how farmers should cultivate the prairies and gave advice counter to what would have worked[citation needed]. Germans from Russia who had previously farmed, under similar circumstances, in what is now Ukraine were marginally more successful than other homesteaders. The Dominion Lands Act of 1871 served a similar function for establishing homesteads on the prairies in Canada. The North American Environmental Atlas, produced by the Commission for Environmental Cooperation, a NAFTA agency composed of the geographical agencies of the Mexican, American, and Canadian governments uses the ""Great Plains"" as an ecoregion synonymous with predominant prairies and grasslands rather than as physiographic region defined by topography. The Great Plains ecoregion includes five sub-regions: Temperate Prairies, West-Central Semi-Arid Prairies, South-Central Semi-Arid Prairies, Texas Louisiana Coastal Plains, and Tamaulipus-Texas Semi-Arid Plain, which overlap or expand upon other Great Plains designations. Much of the Great Plains became open range, or rangeland where cattle roamed free, hosting ranching operations where anyone was theoretically free to run cattle. In the spring and fall, ranchers held roundups where their cowboys branded new calves, treated animals and sorted the cattle for sale. Such ranching began in Texas and gradually moved northward. In 1866-95, cowboys herded 10 million cattle north to rail heads such as Dodge City, Kansas and Ogallala, Nebraska; from there, cattle were shipped eastward. The 100th meridian roughly corresponds with the line that divides the Great Plains into an area that receive 20 inches (510 millimetres) or more of rainfall per year and an area that receives less than 20 in (510 mm). In this context, the High Plains, as well as Southern Alberta, south-western Saskatchewan and Eastern Montana are mainly semi hot steppe land and are generally characterised by rangeland or marginal farmland. The region (especially the High Plains) is periodically subjected to extended periods of drought; high winds in the region may then generate devastating dust storms. The eastern Great Plains near the eastern boundary falls in the humid subtropical climate zone in the southern areas, and the northern and central areas fall in the humid continental climate. From the 1950s on, many areas of the Great Plains have become productive crop-growing areas because of extensive irrigation on large landholdings. The United States is a major exporter of agricultural products. The southern portion of the Great Plains lies over the Ogallala Aquifer, a huge underground layer of water-bearing strata dating from the last ice age. Center pivot irrigation is used extensively in drier sections of the Great Plains, resulting in aquifer depletion at a rate that is greater than the ground's ability to recharge. The railroads opened up the Great Plains for settlement, for now it was possible to ship wheat and other crops at low cost to the urban markets in the East, and Europe. Homestead land was free for American settlers. Railroads sold their land at cheap rates to immigrants in expectation they would generate traffic as soon as farms were established. Immigrants poured in, especially from Germany and Scandinavia. On the plains, very few single men attempted to operate a farm or ranch by themselves; they clearly understood the need for a hard-working wife, and numerous children, to handle the many chores, including child-rearing, feeding and clothing the family, managing the housework, feeding the hired hands, and, especially after the 1930s, handling paperwork and financial details. During the early years of settlement, farm women played an integral role in assuring family survival by working outdoors. After approximately one generation, women increasingly left the fields, thus redefining their roles within the family. New technology including sewing and washing machines encouraged women to turn to domestic roles. The scientific housekeeping movement, promoted across the land by the media and government extension agents, as well as county fairs which featured achievements in home cookery and canning, advice columns for women regarding farm bookkeeping, and home economics courses in the schools. The Great Plains is the broad expanse of flat land (a plain), much of it covered in prairie, steppe and grassland, that lies west of the Mississippi River tallgrass prairie states and east of the Rocky Mountains in the United States and Canada. This area covers parts, but not all, of the states of Colorado, Kansas, Montana, Nebraska, New Mexico, North Dakota, Oklahoma, South Dakota, Texas, and Wyoming, and the Canadian provinces of Alberta, Manitoba and Saskatchewan. The region is known for supporting extensive cattle ranching and dry farming. The term ""Great Plains"", for the region west of about the 96th or 98th meridian and east of the Rocky Mountains, was not generally used before the early 20th century. Nevin Fenneman's 1916 study, Physiographic Subdivision of the United States, brought the term Great Plains into more widespread usage. Before that the region was almost invariably called the High Plains, in contrast to the lower Prairie Plains of the Midwestern states. Today the term ""High Plains"" is used for a subregion of the Great Plains. With the arrival of Francisco Vázquez de Coronado, a Spanish conquistador, the first recorded history of encounter between Europeans and Native Americans in the Great Plains occurred in Texas, Kansas and Nebraska from 1540-1542. In that same time period, Hernando de Soto crossed a west-northwest direction in what is now Oklahoma and Texas. Today this is known as the De Soto Trail. The Spanish thought the Great Plains were the location of the mythological Quivira and Cíbola, a place said to be rich in gold. During the Cenozoic era, specifically about 25 million years ago during the Miocene and Pliocene epochs, the continental climate became favorable to the evolution of grasslands. Existing forest biomes declined and grasslands became much more widespread. The grasslands provided a new niche for mammals, including many ungulates and glires, that switched from browsing diets to grazing diets. Traditionally, the spread of grasslands and the development of grazers have been strongly linked. However, an examination of mammalian teeth suggests that it is the open, gritty habitat and not the grass itself which is linked to diet changes in mammals, giving rise to the ""grit, not grass"" hypothesis. The rural Plains have lost a third of their population since 1920. Several hundred thousand square miles (several hundred thousand square kilometers) of the Great Plains have fewer than 6 inhabitants per square mile (2.3 inhabitants per square kilometer)—the density standard Frederick Jackson Turner used to declare the American frontier ""closed"" in 1893. Many have fewer than 2 inhabitants per square mile (0.77 inhabitants per square kilometer). There are more than 6,000 ghost towns in the state of Kansas alone, according to Kansas historian Daniel Fitzgerald. This problem is often exacerbated by the consolidation of farms and the difficulty of attracting modern industry to the region. In addition, the smaller school-age population has forced the consolidation of school districts and the closure of high schools in some communities. The continuing population loss has led some to suggest that the current use of the drier parts of the Great Plains is not sustainable, and there has been a proposal - the ""Buffalo Commons"" - to return approximately 139,000 square miles (360,000 km2) of these drier parts to native prairie land. After 1870, the new railroads across the Plains brought hunters who killed off almost all the bison for their hides. The railroads offered attractive packages of land and transportation to European farmers, who rushed to settle the land. They (and Americans as well) also took advantage of the homestead laws to obtain free farms. Land speculators and local boosters identified many potential towns, and those reached by the railroad had a chance, while the others became ghost towns. In Kansas, for example, nearly 5000 towns were mapped out, but by 1970 only 617 were actually operating. In the mid-20th century, closeness to an interstate exchange determined whether a town would flourish or struggle for business. Although the eastern image of farm life in the prairies emphasized the isolation of the lonely farmer and wife, plains residents created busy social lives for themselves. They often sponsored activities that combined work, food and entertainment such as barn raisings, corn huskings, quilting bees, Grange meetings, church activities and school functions. Women organized shared meals and potluck events, as well as extended visits between families. The Grange was a nationwide farmers' organization, they reserved high offices for women, and gave them a voice in public affairs."
Symbiosis,"Amensalism is the type of relationship that exists where one species is inhibited or completely obliterated and one is unaffected. This type of symbiosis is relatively uncommon in rudimentary reference texts, but is omnipresent in the natural world.[citation needed] There are two types of amensalism, competition and antibiosis. Competition is where a larger or stronger organisms deprives a smaller or weaker one from a resource. Antibiosis occurs when one organism is damaged or killed by another through a chemical secretion. An example of competition is a sapling growing under the shadow of a mature tree. The mature tree can begin to rob the sapling of necessary sunlight and, if the mature tree is very large, it can take up rainwater and deplete soil nutrients. Throughout the process the mature tree is unaffected. Indeed, if the sapling dies, the mature tree gains nutrients from the decaying sapling. Note that these nutrients become available because of the sapling's decomposition, rather than from the living sapling, which would be a case of parasitism.[citation needed] An example of antibiosis is Juglans nigra (black walnut), secreting juglone, a substance which destroys many herbaceous plants within its root zone. Endosymbiosis is any symbiotic relationship in which one symbiont lives within the tissues of the other, either within the cells or extracellularly. Examples include diverse microbiomes, rhizobia, nitrogen-fixing bacteria that live in root nodules on legume roots; actinomycete nitrogen-fixing bacteria called Frankia, which live in alder tree root nodules; single-celled algae inside reef-building corals; and bacterial endosymbionts that provide essential nutrients to about 10%–15% of insects. Adaptation of the endosymbiont to the host's lifestyle leads to many changes in the endosymbiont–the foremost being drastic reduction in its genome size. This is due to many genes being lost during the process of metabolism, and DNA repair and recombination. While important genes participating in the DNA to RNA transcription, protein translation and DNA/RNA replication are retained. That is, a decrease in genome size is due to loss of protein coding genes and not due to lessening of inter-genic regions or open reading frame (ORF) size. Thus, species that are naturally evolving and contain reduced sizes of genes can be accounted for an increased number of noticeable differences between them, thereby leading to changes in their evolutionary rates. As the endosymbiotic bacteria related with these insects are passed on to the offspring strictly via vertical genetic transmission, intracellular bacteria goes through many hurdles during the process, resulting in the decrease in effective population sizes when compared to the free living bacteria. This incapability of the endosymbiotic bacteria to reinstate its wild type phenotype via a recombination process is called as Muller's ratchet phenomenon. Muller's ratchet phenomenon together with less effective population sizes has led to an accretion of deleterious mutations in the non-essential genes of the intracellular bacteria. This could have been due to lack of selection mechanisms prevailing in the rich environment of the host. During mutualistic symbioses, the host cell lacks some of the nutrients, which are provided by the endosymbiont. As a result, the host favors endosymbiont's growth processes within itself by producing some specialized cells. These cells affect the genetic composition of the host in order to regulate the increasing population of the endosymbionts and ensuring that these genetic changes are passed onto the offspring via vertical transmission (heredity). Commensal relationships may involve one organism using another for transportation (phoresy) or for housing (inquilinism), or it may also involve one organism using something another created, after its death (metabiosis). Examples of metabiosis are hermit crabs using gastropod shells to protect their bodies and spiders building their webs on plants. Amensalism is an interaction where an organism inflicts harm to another organism without any costs or benefits received by the other. A clear case of amensalism is where sheep or cattle trample grass. Whilst the presence of the grass causes negligible detrimental effects to the animal's hoof, the grass suffers from being crushed. Amensalism is often used to describe strongly asymmetrical competitive interactions, such as has been observed between the Spanish ibex and weevils of the genus Timarcha which feed upon the same type of shrub. Whilst the presence of the weevil has almost no influence on food availability, the presence of ibex has an enormous detrimental effect on weevil numbers, as they consume significant quantities of plant matter and incidentally ingest the weevils upon it. The definition of symbiosis has varied among scientists. Some believe symbiosis should only refer to persistent mutualisms, while others believe it should apply to any type of persistent biological interaction (in other words mutualistic, commensalistic, or parasitic). After 130 years of debate, current biology and ecology textbooks now use the latter ""de Bary"" definition or an even broader definition (where symbiosis means all species interactions), with the restrictive definition no longer used (in other words, symbiosis means mutualism). Symbiosis (from Greek σύν ""together"" and βίωσις ""living"") is close and often long-term interaction between two different biological species. In 1877 Albert Bernhard Frank used the word symbiosis (which previously had been used to depict people living together in community) to describe the mutualistic relationship in lichens. In 1879, the German mycologist Heinrich Anton de Bary defined it as ""the living together of unlike organisms."" Symbiosis played a major role in the co-evolution of flowering plants and the animals that pollinate them. Many plants that are pollinated by insects, bats, or birds have highly specialized flowers modified to promote pollination by a specific pollinator that is also correspondingly adapted. The first flowering plants in the fossil record had relatively simple flowers. Adaptive speciation quickly gave rise to many diverse groups of plants, and, at the same time, corresponding speciation occurred in certain insect groups. Some groups of plants developed nectar and large sticky pollen, while insects evolved more specialized morphologies to access and collect these rich food sources. In some taxa of plants and insects the relationship has become dependent, where the plant species can only be pollinated by one species of insect. Synnecrosis is a rare type of symbiosis in which the interaction between species is detrimental to both organisms involved. It is a short-lived condition, as the interaction eventually causes death. Because of this, evolution selects against synnecrosis and it is uncommon in nature. An example of this is the relationship between some species of bees and victims of the bee sting. Species of bees who die after stinging their prey inflict pain on themselves (albeit to protect the hive) as well as on the victim. This term is rarely used. Commensalism describes a relationship between two living organisms where one benefits and the other is not significantly harmed or helped. It is derived from the English word commensal used of human social interaction. The word derives from the medieval Latin word, formed from com- and mensa, meaning ""sharing a table"". An example of mutual symbiosis is the relationship between the ocellaris clownfish that dwell among the tentacles of Ritteri sea anemones. The territorial fish protects the anemone from anemone-eating fish, and in turn the stinging tentacles of the anemone protect the clownfish from its predators. A special mucus on the clownfish protects it from the stinging tentacles. A large percentage of herbivores have mutualistic gut flora that help them digest plant matter, which is more difficult to digest than animal prey. This gut flora is made up of cellulose-digesting protozoans or bacteria living in the herbivores' intestines. Coral reefs are the result of mutualisms between coral organisms and various types of algae that live inside them. Most land plants and land ecosystems rely on mutualisms between the plants, which fix carbon from the air, and mycorrhyzal fungi, which help in extracting water and minerals from the ground. One of the most spectacular examples of obligate mutualism is between the siboglinid tube worms and symbiotic bacteria that live at hydrothermal vents and cold seeps. The worm has no digestive tract and is wholly reliant on its internal symbionts for nutrition. The bacteria oxidize either hydrogen sulfide or methane, which the host supplies to them. These worms were discovered in the late 1980s at the hydrothermal vents near the Galapagos Islands and have since been found at deep-sea hydrothermal vents and cold seeps in all of the world's oceans. While historically, symbiosis has received less attention than other interactions such as predation or competition, it is increasingly recognized as an important selective force behind evolution, with many species having a long history of interdependent co-evolution. In fact, the evolution of all eukaryotes (plants, animals, fungi, and protists) is believed under the endosymbiotic theory to have resulted from a symbiosis between various sorts of bacteria. This theory is supported by certain organelles dividing independently of the cell, and the observation that some organelles seem to have their own nucleic acid. A further example is the goby fish, which sometimes lives together with a shrimp. The shrimp digs and cleans up a burrow in the sand in which both the shrimp and the goby fish live. The shrimp is almost blind, leaving it vulnerable to predators when outside its burrow. In case of danger the goby fish touches the shrimp with its tail to warn it. When that happens both the shrimp and goby fish quickly retreat into the burrow. Different species of gobies (Elacatinus spp.) also exhibit mutualistic behavior through cleaning up ectoparasites in other fish. Symbiotic relationships include those associations in which one organism lives on another (ectosymbiosis, such as mistletoe), or where one partner lives inside the other (endosymbiosis, such as lactobacilli and other bacteria in humans or Symbiodinium in corals). Symbiosis is also classified by physical attachment of the organisms; symbiosis in which the organisms have bodily union is called conjunctive symbiosis, and symbiosis in which they are not in union is called disjunctive symbiosis. Another non-obligate symbiosis is known from encrusting bryozoans and hermit crabs that live in a close relationship. The bryozoan colony (Acanthodesia commensale) develops a cirumrotatory growth and offers the crab (Pseudopagurus granulimanus) a helicospiral-tubular extension of its living chamber that initially was situated within a gastropod shell. A parasitic relationship is one in which one member of the association benefits while the other is harmed. This is also known as antagonistic or antipathetic symbiosis. Parasitic symbioses take many forms, from endoparasites that live within the host's body to ectoparasites that live on its surface. In addition, parasites may be necrotrophic, which is to say they kill their host, or biotrophic, meaning they rely on their host's surviving. Biotrophic parasitism is an extremely successful mode of life. Depending on the definition used, as many as half of all animals have at least one parasitic phase in their life cycles, and it is also frequent in plants and fungi. Moreover, almost all free-living animals are host to one or more parasite taxa. An example of a biotrophic relationship would be a tick feeding on the blood of its host. Some symbiotic relationships are obligate, meaning that both symbionts entirely depend on each other for survival. For example, many lichens consist of fungal and photosynthetic symbionts that cannot live on their own. Others are facultative (optional): they can, but do not have to live with the other organism. The biologist Lynn Margulis, famous for her work on endosymbiosis, contends that symbiosis is a major driving force behind evolution. She considers Darwin's notion of evolution, driven by competition, to be incomplete and claims that evolution is strongly based on co-operation, interaction, and mutual dependence among organisms. According to Margulis and Dorion Sagan, ""Life did not take over the globe by combat, but by networking."" Mutualism or interspecies reciprocal altruism is a relationship between individuals of different species where both individuals benefit. In general, only lifelong interactions involving close physical and biochemical contact can properly be considered symbiotic. Mutualistic relationships may be either obligate for both species, obligate for one but facultative for the other, or facultative for both. Many biologists restrict the definition of symbiosis to close mutualist relationships. Ectosymbiosis, also referred to as exosymbiosis, is any symbiotic relationship in which the symbiont lives on the body surface of the host, including the inner surface of the digestive tract or the ducts of exocrine glands. Examples of this include ectoparasites such as lice, commensal ectosymbionts such as the barnacles that attach themselves to the jaw of baleen whales, and mutualist ectosymbionts such as cleaner fish."
Humanism,"Religious humanism is an integration of humanist ethical philosophy with religious rituals and beliefs that centre on human needs, interests, and abilities. Though practitioners of religious humanism did not officially organise under the name of ""humanism"" until the late 19th and early 20th centuries, non-theistic religions paired with human-centred ethical philosophy have a long history. The Cult of Reason (French: Culte de la Raison) was a religion based on deism devised during the French Revolution by Jacques Hébert, Pierre Gaspard Chaumette and their supporters. In 1793 during the French Revolution, the cathedral Notre Dame de Paris was turned into a ""Temple to Reason"" and for a time Lady Liberty replaced the Virgin Mary on several altars. In the 1850s, Auguste Comte, the Father of Sociology, founded Positivism, a ""religion of humanity"". One of the earliest forerunners of contemporary chartered humanist organisations was the Humanistic Religious Association formed in 1853 in London. This early group was democratically organised, with male and female members participating in the election of the leadership and promoted knowledge of the sciences, philosophy, and the arts. The Ethical Culture movement was founded in 1876. The movement's founder, Felix Adler, a former member of the Free Religious Association, conceived of Ethical Culture as a new religion that would retain the ethical message at the heart of all religions. Ethical Culture was religious in the sense of playing a defining role in people's lives and addressing issues of ultimate concern. In the 6th century BCE, Taoist teacher Lao Tzu espoused a series of naturalistic concepts with some elements of humanistic philosophy. The Silver Rule of Confucianism from Analects XV.24, is an example of ethical philosophy based on human values rather than the supernatural. Humanistic thought is also contained in other Confucian classics, e.g., as recorded in Zuo Zhuan, Ji Liang says, ""People is the zhu (master, lord, dominance, owner or origin) of gods. So, to sage kings, people first, gods second""; Neishi Guo says, ""Gods, clever, righteous and wholehearted, comply with human."" Taoist and Confucian secularism contain elements of moral thought devoid of religious authority or deism however they only partly resembled our modern concept of secularism. In the high Renaissance, in fact, there was a hope that more direct knowledge of the wisdom of antiquity, including the writings of the Church fathers, the earliest known Greek texts of the Christian Gospels, and in some cases even the Jewish Kabbalah, would initiate a harmonious new era of universal agreement. With this end in view, Renaissance Church authorities afforded humanists what in retrospect appears a remarkable degree of freedom of thought. One humanist, the Greek Orthodox Platonist Gemistus Pletho (1355–1452), based in Mystras, Greece (but in contact with humanists in Florence, Venice, and Rome) taught a Christianised version of pagan polytheism. The words of the comic playwright P. Terentius Afer reverberated across the Roman world of the mid-2nd century BCE and beyond. Terence, an African and a former slave, was well placed to preach the message of universalism, of the essential unity of the human race, that had come down in philosophical form from the Greeks, but needed the pragmatic muscles of Rome in order to become a practical reality. The influence of Terence's felicitous phrase on Roman thinking about human rights can hardly be overestimated. Two hundred years later Seneca ended his seminal exposition of the unity of humankind with a clarion-call: The ad fontes principle also had many applications. The re-discovery of ancient manuscripts brought a more profound and accurate knowledge of ancient philosophical schools such as Epicureanism, and Neoplatonism, whose Pagan wisdom the humanists, like the Church fathers of old, tended, at least initially, to consider as deriving from divine revelation and thus adaptable to a life of Christian virtue. The line from a drama of Terence, Homo sum, humani nihil a me alienum puto (or with nil for nihil), meaning ""I am a human being, I think nothing human alien to me"", known since antiquity through the endorsement of Saint Augustine, gained renewed currency as epitomising the humanist attitude. The statement, in a play modeled or borrowed from a (now lost) Greek comedy by Menander, may have originated in a lighthearted vein – as a comic rationale for an old man's meddling – but it quickly became a proverb and throughout the ages was quoted with a deeper meaning, by Cicero and Saint Augustine, to name a few, and most notably by Seneca. Richard Bauman writes: Early humanists saw no conflict between reason and their Christian faith (see Christian Humanism). They inveighed against the abuses of the Church, but not against the Church itself, much less against religion. For them, the word ""secular"" carried no connotations of disbelief – that would come later, in the nineteenth century. In the Renaissance to be secular meant simply to be in the world rather than in a monastery. Petrarch frequently admitted that his brother Gherardo's life as a Carthusian monk was superior to his own (although Petrarch himself was in Minor Orders and was employed by the Church all his life). He hoped that he could do some good by winning earthly glory and praising virtue, inferior though that might be to a life devoted solely to prayer. By embracing a non-theistic philosophic base, however, the methods of the humanists, combined with their eloquence, would ultimately have a corrosive effect on established authority. In 1808 Bavarian educational commissioner Friedrich Immanuel Niethammer coined the term Humanismus to describe the new classical curriculum he planned to offer in German secondary schools, and by 1836 the word ""humanism"" had been absorbed into the English language in this sense. The coinage gained universal acceptance in 1856, when German historian and philologist Georg Voigt used humanism to describe Renaissance humanism, the movement that flourished in the Italian Renaissance to revive classical learning, a use which won wide acceptance among historians in many nations, especially Italy. Polemics about humanism have sometimes assumed paradoxical twists and turns. Early 20th century critics such as Ezra Pound, T. E. Hulme, and T. S. Eliot considered humanism to be sentimental ""slop"" (Hulme)[citation needed] or ""an old bitch gone in the teeth"" (Pound) and wanted to go back to a more manly, authoritarian society such as (they believed) existed in the Middle Ages. Postmodern critics who are self-described anti-humanists, such as Jean-François Lyotard and Michel Foucault, have asserted that humanism posits an overarching and excessively abstract notion of humanity or universal human nature, which can then be used as a pretext for imperialism and domination of those deemed somehow less than human. ""Humanism fabricates the human as much as it fabricates the nonhuman animal"", suggests Timothy Laurie, turning the human into what he calls ""a placeholder for a range of attributes that have been considered most virtuous among humans (e.g. rationality, altruism), rather than most commonplace (e.g. hunger, anger)"". Nevertheless, philosopher Kate Soper notes that by faulting humanism for falling short of its own benevolent ideals, anti-humanism thus frequently ""secretes a humanist rhetoric"". Raymond B. Bragg, the associate editor of The New Humanist, sought to consolidate the input of Leon Milton Birkhead, Charles Francis Potter, and several members of the Western Unitarian Conference. Bragg asked Roy Wood Sellars to draft a document based on this information which resulted in the publication of the Humanist Manifesto in 1933. Potter's book and the Manifesto became the cornerstones of modern humanism, the latter declaring a new religion by saying, ""any religion that can hope to be a synthesising and dynamic force for today must be shaped for the needs of this age. To establish such a religion is a major necessity of the present."" It then presented 15 theses of humanism as foundational principles for this new religion. After 1517, when the new invention of printing made these texts widely available, the Dutch humanist Erasmus, who had studied Greek at the Venetian printing house of Aldus Manutius, began a philological analysis of the Gospels in the spirit of Valla, comparing the Greek originals with their Latin translations with a view to correcting errors and discrepancies in the latter. Erasmus, along with the French humanist Jacques Lefèvre d'Étaples, began issuing new translations, laying the groundwork for the Protestant Reformation. Henceforth Renaissance humanism, particularly in the German North, became concerned with religion, while Italian and French humanism concentrated increasingly on scholarship and philology addressed to a narrow audience of specialists, studiously avoiding topics that might offend despotic rulers or which might be seen as corrosive of faith. After the Reformation, critical examination of the Bible did not resume until the advent of the so-called Higher criticism of the 19th-century German Tübingen school. Better acquaintance with Greek and Roman technical writings also influenced the development of European science (see the history of science in the Renaissance). This was despite what A. C. Crombie (viewing the Renaissance in the 19th-century manner as a chapter in the heroic March of Progress) calls ""a backwards-looking admiration for antiquity"", in which Platonism stood in opposition to the Aristotelian concentration on the observable properties of the physical world. But Renaissance humanists, who considered themselves as restoring the glory and nobility of antiquity, had no interest in scientific innovation. However, by the mid-to-late 16th century, even the universities, though still dominated by Scholasticism, began to demand that Aristotle be read in accurate texts edited according to the principles of Renaissance philology, thus setting the stage for Galileo's quarrels with the outmoded habits of Scholasticism. At about the same time, the word ""humanism"" as a philosophy centred on humankind (as opposed to institutionalised religion) was also being used in Germany by the so-called Left Hegelians, Arnold Ruge, and Karl Marx, who were critical of the close involvement of the church in the repressive German government. There has been a persistent confusion between the several uses of the terms: philanthropic humanists look to what they consider their antecedents in critical thinking and human-centered philosophy among the Greek philosophers and the great figures of Renaissance history; and scholarly humanists stress the linguistic and cultural disciplines needed to understand and interpret these philosophers and artists. The humanists' close study of Latin literary texts soon enabled them to discern historical differences in the writing styles of different periods. By analogy with what they saw as decline of Latin, they applied the principle of ad fontes, or back to the sources, across broad areas of learning, seeking out manuscripts of Patristic literature as well as pagan authors. In 1439, while employed in Naples at the court of Alfonso V of Aragon (at the time engaged in a dispute with the Papal States) the humanist Lorenzo Valla used stylistic textual analysis, now called philology, to prove that the Donation of Constantine, which purported to confer temporal powers on the Pope of Rome, was an 8th-century forgery. For the next 70 years, however, neither Valla nor any of his contemporaries thought to apply the techniques of philology to other controversial manuscripts in this way. Instead, after the fall of the Byzantine Empire to the Turks in 1453, which brought a flood of Greek Orthodox refugees to Italy, humanist scholars increasingly turned to the study of Neoplatonism and Hermeticism, hoping to bridge the differences between the Greek and Roman Churches, and even between Christianity itself and the non-Christian world. The refugees brought with them Greek manuscripts, not only of Plato and Aristotle, but also of the Christian Gospels, previously unavailable in the Latin West. Gellius says that in his day humanitas is commonly used as a synonym for philanthropy – or kindness and benevolence toward one's fellow human being. Gellius maintains that this common usage is wrong, and that model writers of Latin, such as Cicero and others, used the word only to mean what we might call ""humane"" or ""polite"" learning, or the Greek equivalent Paideia. Gellius became a favorite author in the Italian Renaissance, and, in fifteenth-century Italy, teachers and scholars of philosophy, poetry, and rhetoric were called and called themselves ""humanists"". Modern scholars, however, point out that Cicero (106 – 43 BCE), who was most responsible for defining and popularizing the term humanitas, in fact frequently used the word in both senses, as did his near contemporaries. For Cicero, a lawyer, what most distinguished humans from brutes was speech, which, allied to reason, could (and should) enable them to settle disputes and live together in concord and harmony under the rule of law. Thus humanitas included two meanings from the outset and these continue in the modern derivative, humanism, which even today can refer to both humanitarian benevolence and to scholarship. During the French Revolution, and soon after, in Germany (by the Left Hegelians), humanism began to refer to an ethical philosophy centered on humankind, without attention to the transcendent or supernatural. The designation Religious Humanism refers to organized groups that sprang up during the late-nineteenth and early twentieth centuries. It is similar to Protestantism, although centered on human needs, interests, and abilities rather than the supernatural. In the Anglophone world, such modern, organized forms of humanism, which are rooted in the 18th-century Enlightenment, have to a considerable extent more or less detached themselves from the historic connection of humanism with classical learning and the liberal arts. In his book, Humanism (1997), Tony Davies calls these critics ""humanist anti-humanists"". Critics of antihumanism, most notably Jürgen Habermas, counter that while antihumanists may highlight humanism's failure to fulfil its emancipatory ideal, they do not offer an alternative emancipatory project of their own. Others, like the German philosopher Heidegger considered themselves humanists on the model of the ancient Greeks, but thought humanism applied only to the German ""race"" and specifically to the Nazis and thus, in Davies' words, were anti-humanist humanists. Such a reading of Heidegger's thought is itself deeply controversial; Heidegger includes his own views and critique of Humanism in Letter On Humanism. Davies acknowledges that after the horrific experiences of the wars of the 20th century ""it should no longer be possible to formulate phrases like 'the destiny of man' or the 'triumph of human reason' without an instant consciousness of the folly and brutality they drag behind them"". For ""it is almost impossible to think of a crime that has not been committed in the name of human reason"". Yet, he continues, ""it would be unwise to simply abandon the ground occupied by the historical humanisms. For one thing humanism remains on many occasions the only available alternative to bigotry and persecution. The freedom to speak and write, to organise and campaign in defence of individual or collective interests, to protest and disobey: all these can only be articulated in humanist terms."" Humanists reacted against this utilitarian approach and the narrow pedantry associated with it. They sought to create a citizenry (frequently including women) able to speak and write with eloquence and clarity and thus capable of engaging the civic life of their communities and persuading others to virtuous and prudent actions. This was to be accomplished through the study of the studia humanitatis, today known as the humanities: grammar, rhetoric, history, poetry and moral philosophy. As a program to revive the cultural – and particularly the literary – legacy and moral philosophy of classical antiquity, Humanism was a pervasive cultural mode and not the program of a few isolated geniuses like Rabelais or Erasmus as is still sometimes popularly believed. But in the mid-18th century, during the French Enlightenment, a more ideological use of the term had come into use. In 1765, the author of an anonymous article in a French Enlightenment periodical spoke of ""The general love of humanity ... a virtue hitherto quite nameless among us, and which we will venture to call 'humanism', for the time has come to create a word for such a beautiful and necessary thing"". The latter part of the 18th and the early 19th centuries saw the creation of numerous grass-roots ""philanthropic"" and benevolent societies dedicated to human betterment and the spreading of knowledge (some Christian, some not). After the French Revolution, the idea that human virtue could be created by human reason alone independently from traditional religious institutions, attributed by opponents of the Revolution to Enlightenment philosophes such as Rousseau, was violently attacked by influential religious and political conservatives, such as Edmund Burke and Joseph de Maistre, as a deification or idolatry of humanity. Humanism began to acquire a negative sense. The Oxford English Dictionary records the use of the word ""humanism"" by an English clergyman in 1812 to indicate those who believe in the ""mere humanity"" (as opposed to the divine nature) of Christ, i.e., Unitarians and Deists. In this polarised atmosphere, in which established ecclesiastical bodies tended to circle the wagons and reflexively oppose political and social reforms like extending the franchise, universal schooling, and the like, liberal reformers and radicals embraced the idea of Humanism as an alternative religion of humanity. The anarchist Proudhon (best known for declaring that ""property is theft"") used the word ""humanism"" to describe a ""culte, déification de l’humanité"" (""worship, deification of humanity"") and Ernest Renan in L’avenir de la science: pensées de 1848 (""The Future of Knowledge: Thoughts on 1848"") (1848–49), states: ""It is my deep conviction that pure humanism will be the religion of the future, that is, the cult of all that pertains to humanity—all of life, sanctified and raised to the level of a moral value."" Renaissance humanism was an intellectual movement in Europe of the later Middle Ages and the Early Modern period. The 19th-century German historian Georg Voigt (1827–91) identified Petrarch as the first Renaissance humanist. Paul Johnson agrees that Petrarch was ""the first to put into words the notion that the centuries between the fall of Rome and the present had been the age of Darkness"". According to Petrarch, what was needed to remedy this situation was the careful study and imitation of the great classical authors. For Petrarch and Boccaccio, the greatest master was Cicero, whose prose became the model for both learned (Latin) and vernacular (Italian) prose. Just as artist and inventor Leonardo da Vinci – partaking of the zeitgeist though not himself a humanist – advocated study of human anatomy, nature, and weather to enrich Renaissance works of art, so Spanish-born humanist Juan Luis Vives (c. 1493–1540) advocated observation, craft, and practical techniques to improve the formal teaching of Aristotelian philosophy at the universities, helping to free them from the grip of Medieval Scholasticism. Thus, the stage was set for the adoption of an approach to natural philosophy, based on empirical observations and experimentation of the physical universe, making possible the advent of the age of scientific inquiry that followed the Renaissance. 6th-century BCE pre-Socratic Greek philosophers Thales of Miletus and Xenophanes of Colophon were the first in the region to attempt to explain the world in terms of human reason rather than myth and tradition, thus can be said to be the first Greek humanists. Thales questioned the notion of anthropomorphic gods and Xenophanes refused to recognise the gods of his time and reserved the divine for the principle of unity in the universe. These Ionian Greeks were the first thinkers to assert that nature is available to be studied separately from the supernatural realm. Anaxagoras brought philosophy and the spirit of rational inquiry from Ionia to Athens. Pericles, the leader of Athens during the period of its greatest glory was an admirer of Anaxagoras. Other influential pre-Socratics or rational philosophers include Protagoras (like Anaxagoras a friend of Pericles), known for his famous dictum ""man is the measure of all things"" and Democritus, who proposed that matter was composed of atoms. Little of the written work of these early philosophers survives and they are known mainly from fragments and quotations in other writers, principally Plato and Aristotle. The historian Thucydides, noted for his scientific and rational approach to history, is also much admired by later humanists. In the 3rd century BCE, Epicurus became known for his concise phrasing of the problem of evil, lack of belief in the afterlife, and human-centred approaches to achieving eudaimonia. He was also the first Greek philosopher to admit women to his school as a rule. Contemporary humanism entails a qualified optimism about the capacity of people, but it does not involve believing that human nature is purely good or that all people can live up to the Humanist ideals without help. If anything, there is recognition that living up to one's potential is hard work and requires the help of others. The ultimate goal is human flourishing; making life better for all humans, and as the most conscious species, also promoting concern for the welfare of other sentient beings and the planet as a whole. The focus is on doing good and living well in the here and now, and leaving the world a better place for those who come after. In 1925, the English mathematician and philosopher Alfred North Whitehead cautioned: ""The prophecy of Francis Bacon has now been fulfilled; and man, who at times dreamt of himself as a little lower than the angels, has submitted to become the servant and the minister of nature. It still remains to be seen whether the same actor can play both parts"". Humanistic psychology is a psychological perspective which rose to prominence in the mid-20th century in response to Sigmund Freud's psychoanalytic theory and B. F. Skinner's Behaviorism. The approach emphasizes an individual's inherent drive towards self-actualization and creativity. Psychologists Carl Rogers and Abraham Maslow introduced a positive, humanistic psychology in response to what they viewed as the overly pessimistic view of psychoanalysis in the early 1960s. Other sources include the philosophies of existentialism and phenomenology. Active in the early 1920s, F.C.S. Schiller labelled his work ""humanism"" but for Schiller the term referred to the pragmatist philosophy he shared with William James. In 1929, Charles Francis Potter founded the First Humanist Society of New York whose advisory board included Julian Huxley, John Dewey, Albert Einstein and Thomas Mann. Potter was a minister from the Unitarian tradition and in 1930 he and his wife, Clara Cook Potter, published Humanism: A New Religion. Throughout the 1930s, Potter was an advocate of such liberal causes as, women’s rights, access to birth control, ""civil divorce laws"", and an end to capital punishment. Renaissance humanism was an activity of cultural and educational reform engaged in by civic and ecclesiastical chancellors, book collectors, educators, and writers, who by the late fifteenth century began to be referred to as umanisti – ""humanists"". It developed during the fourteenth and the beginning of the fifteenth centuries, and was a response to the challenge of scholastic university education, which was then dominated by Aristotelian philosophy and logic. Scholasticism focused on preparing men to be doctors, lawyers or professional theologians, and was taught from approved textbooks in logic, natural philosophy, medicine, law and theology. There were important centres of humanism at Florence, Naples, Rome, Venice, Mantua, Ferrara, and Urbino. In China, Yellow Emperor is regarded as the humanistic primogenitor.[citation needed] Sage kings such as Yao and Shun are humanistic figures as recorded.[citation needed] King Wu of Zhou has the famous saying: ""Humanity is the Ling (efficacious essence) of the world (among all)."" Among them Duke of Zhou, respected as a founder of Rujia (Confucianism), is especially prominent and pioneering in humanistic thought. His words were recorded in the Book of History as follows (translation):[citation needed] Humanism is a philosophical and ethical stance that emphasizes the value and agency of human beings, individually and collectively, and generally prefers critical thinking and evidence (rationalism, empiricism) over acceptance of dogma or superstition. The meaning of the term humanism has fluctuated according to the successive intellectual movements which have identified with it. Generally, however, humanism refers to a perspective that affirms some notion of human freedom and progress. In modern times, humanist movements are typically aligned with secularism, and today humanism typically refers to a non-theistic life stance centred on human agency and looking to science rather than revelation from a supernatural source to understand the world. Eliot and her circle, who included her companion George Henry Lewes (the biographer of Goethe) and the abolitionist and social theorist Harriet Martineau, were much influenced by the positivism of Auguste Comte, whom Martineau had translated. Comte had proposed an atheistic culte founded on human principles – a secular Religion of Humanity (which worshiped the dead, since most humans who have ever lived are dead), complete with holidays and liturgy, modeled on the rituals of what was seen as a discredited and dilapidated Catholicism. Although Comte's English followers, like Eliot and Martineau, for the most part rejected the full gloomy panoply of his system, they liked the idea of a religion of humanity. Comte's austere vision of the universe, his injunction to ""vivre pour altrui"" (""live for others"", from which comes the word ""altruism""), and his idealisation of women inform the works of Victorian novelists and poets from George Eliot and Matthew Arnold to Thomas Hardy. Another instance of ancient humanism as an organised system of thought is found in the Gathas of Zarathustra, composed between 1,000 BCE – 600 BCE in Greater Iran. Zarathustra's philosophy in the Gathas lays out a conception of humankind as thinking beings dignified with choice and agency according to the intellect which each receives from Ahura Mazda (God in the form of supreme wisdom). The idea of Ahura Mazda as a non-intervening deistic divine God/Grand Architect of the universe tied with a unique eschatology and ethical system implying that each person is held morally responsible for their choices, made freely in this present life, in the afterlife. The importance placed on thought, action, responsibility, and a non-intervening creator was appealed to by, and inspired a number of, Enlightenment humanist thinkers in Europe such as Voltaire and Montesquieu. Davies identifies Paine's The Age of Reason as ""the link between the two major narratives of what Jean-François Lyotard calls the narrative of legitimation"": the rationalism of the 18th-century Philosophes and the radical, historically based German 19th-century Biblical criticism of the Hegelians David Friedrich Strauss and Ludwig Feuerbach. ""The first is political, largely French in inspiration, and projects 'humanity as the hero of liberty'. The second is philosophical, German, seeks the totality and autonomy of knowledge, and stresses understanding rather than freedom as the key to human fulfilment and emancipation. The two themes converged and competed in complex ways in the 19th century and beyond, and between them set the boundaries of its various humanisms. Homo homini deus est (""The human being is a god to humanity"" or ""god is nothing [other than] the human being to himself""), Feuerbach had written."
Bird_migration,"Many, if not most, birds migrate in flocks. For larger birds, flying in flocks reduces the energy cost. Geese in a V-formation may conserve 12–20% of the energy they would need to fly alone. Red knots Calidris canutus and dunlins Calidris alpina were found in radar studies to fly 5 km/h (3.1 mph) faster in flocks than when they were flying alone. Some bar-tailed godwits Limosa lapponica have the longest known non-stop flight of any migrant, flying 11,000 km from Alaska to their New Zealand non-breeding areas. Prior to migration, 55 percent of their bodyweight is stored as fat to fuel this uninterrupted journey. Navigation is based on a variety of senses. Many birds have been shown to use a sun compass. Using the sun for direction involves the need for making compensation based on the time. Navigation has also been shown to be based on a combination of other abilities including the ability to detect magnetic fields (magnetoception), use visual landmarks as well as olfactory cues. Aristotle however suggested that swallows and other birds hibernated. This belief persisted as late as 1878, when Elliott Coues listed the titles of no less than 182 papers dealing with the hibernation of swallows. Even the ""highly observant"" Gilbert White, in his posthumously published 1789 The Natural History of Selborne, quoted a man's story about swallows being found in a chalk cliff collapse ""while he was a schoolboy at Brighthelmstone"", though the man denied being an eyewitness. However, he also writes that ""as to swallows being found in a torpid state during the winter in the Isle of Wight or any part of this country, I never heard any such account worth attending to"", and that if early swallows ""happen to find frost and snow they immediately withdraw for a time—a circumstance this much more in favour of hiding than migration"", since he doubts they would ""return for a week or two to warmer latitudes"". Some large broad-winged birds rely on thermal columns of rising hot air to enable them to soar. These include many birds of prey such as vultures, eagles, and buzzards, but also storks. These birds migrate in the daytime. Migratory species in these groups have great difficulty crossing large bodies of water, since thermals only form over land, and these birds cannot maintain active flight for long distances. Mediterranean and other seas present a major obstacle to soaring birds, which must cross at the narrowest points. Massive numbers of large raptors and storks pass through areas such as the Strait of Messina, Gibraltar, Falsterbo, and the Bosphorus at migration times. More common species, such as the European honey buzzard Pernis apivorus, can be counted in hundreds of thousands in autumn. Other barriers, such as mountain ranges, can also cause funnelling, particularly of large diurnal migrants. This is a notable factor in the Central American migratory bottleneck. Batumi bottleneck in the Caucasus is one of the heaviest migratory funnels on earth. Avoiding flying over the Black Sea surface and across high mountains, hundreds of thousands of soaring birds funnel through an area around the city of Batumi, Georgia. Birds of prey such as honey buzzards which migrate using thermals lose only 10 to 20% of their weight during migration, which may explain why they forage less during migration than do smaller birds of prey with more active flight such as falcons, hawks and harriers. Bird migration routes have been studied by a variety of techniques including the oldest, marking. Swans have been marked with a nick on the beak since about 1560 in England. Scientific ringing was pioneered by Hans Christian Cornelius Mortensen in 1899. Other techniques include radar and satellite tracking. The control of migration, its timing and response are genetically controlled and appear to be a primitive trait that is present even in non-migratory species of birds. The ability to navigate and orient themselves during migration is a much more complex phenomenon that may include both endogenous programs as well as learning. Records of bird migration were made as much as 3,000 years ago by the Ancient Greek writers Hesiod, Homer, Herodotus and Aristotle. The Bible also notes migrations, as in the Book of Job (39:26), where the inquiry is made: ""Is it by your insight that the hawk hovers, spreads its wings southward?"" The author of Jeremiah (8:7) wrote: ""Even the stork in the heavens knows its seasons, and the turtle dove, the swift and the crane keep the time of their arrival."" Birds need to alter their metabolism in order to meet the demands of migration. The storage of energy through the accumulation of fat and the control of sleep in nocturnal migrants require special physiological adaptations. In addition, the feathers of a bird suffer from wear-and-tear and require to be molted. The timing of this molt - usually once a year but sometimes twice - varies with some species molting prior to moving to their winter grounds and others molting prior to returning to their breeding grounds. Apart from physiological adaptations, migration sometimes requires behavioural changes such as flying in flocks to reduce the energy used in migration or the risk of predation. Hunting along migration routes threatens some bird species. The populations of Siberian cranes (Leucogeranus leucogeranus) that wintered in India declined due to hunting along the route, particularly in Afghanistan and Central Asia. Birds were last seen in their favourite wintering grounds in Keoladeo National Park in 2002. Structures such as power lines, wind farms and offshore oil-rigs have also been known to affect migratory birds. Other migration hazards include pollution, storms, wildfires, and habitat destruction along migration routes, denying migrants food at stopover points. For example, in the East Asian–Australasian Flyway, up to 65% of key intertidal habitat at the Yellow Sea migration bottleneck has been destroyed since the 1950s. Sometimes circumstances such as a good breeding season followed by a food source failure the following year lead to irruptions in which large numbers of a species move far beyond the normal range. Bohemian waxwings Bombycilla garrulus well show this unpredictable variation in annual numbers, with five major arrivals in Britain during the nineteenth century, but 18 between the years 1937 and 2000. Red crossbills Loxia curvirostra too are irruptive, with widespread invasions across England noted in 1251, 1593, 1757, and 1791. Birds fly at varying altitudes during migration. An expedition to Mt. Everest found skeletons of northern pintail Anas acuta and black-tailed godwit Limosa limosa at 5,000 m (16,000 ft) on the Khumbu Glacier. Bar-headed geese Anser indicus have been recorded by GPS flying at up to 6,540 metres (21,460 ft) while crossing the Himalayas, at the same time engaging in the highest rates of climb to altitude for any bird. Anecdotal reports of them flying much higher have yet to be corroborated with any direct evidence. Seabirds fly low over water but gain altitude when crossing land, and the reverse pattern is seen in landbirds. However most bird migration is in the range of 150 to 600 m (490 to 1,970 ft). Bird strike aviation records from the United States show most collisions occur below 600 m (2,000 ft) and almost none above 1,800 m (5,900 ft). It was not until the end of the eighteenth century that migration as an explanation for the winter disappearance of birds from northern climes was accepted. Thomas Bewick's A History of British Birds (Volume 1, 1797) mentions a report from ""a very intelligent master of a vessel"" who, ""between the islands of Minorca and Majorca, saw great numbers of Swallows flying northward"", and states the situation in Britain as follows: Bewick then describes an experiment which succeeded in keeping swallows alive in Britain for several years, where they remained warm and dry through the winters. He concludes: The typical image of migration is of northern landbirds, such as swallows (Hirundinidae) and birds of prey, making long flights to the tropics. However, many Holarctic wildfowl and finch (Fringillidae) species winter in the North Temperate Zone, in regions with milder winters than their summer breeding grounds. For example, the pink-footed goose Anser brachyrhynchus migrates from Iceland to Britain and neighbouring countries, whilst the dark-eyed junco Junco hyemalis migrates from subarctic and arctic climates to the contiguous United States and the American goldfinch from taiga to wintering grounds extending from the American South northwestward to Western Oregon. Migratory routes and wintering grounds are traditional and learned by young during their first migration with their parents. Some ducks, such as the garganey Anas querquedula, move completely or partially into the tropics. The European pied flycatcher Ficedula hypoleuca also follows this migratory trend, breeding in Asia and Europe and wintering in Africa. It has been possible to teach a migration route to a flock of birds, for example in re-introduction schemes. After a trial with Canada geese Branta canadensis, microlight aircraft were used in the US to teach safe migration routes to reintroduced whooping cranes Grus americana. Often, the migration route of a long-distance migrator bird doesn't follow a straight line between breeding and wintering grounds. Rather, it could follow an hooked or arched line, with detours around geographical barriers. For most land-birds, such barriers could consist in seas, large water bodies or high mountain ranges, because of the lack of stopover or feeding sites, or the lack of thermal columns for broad-winged birds. The Arctic tern holds the long-distance migration record for birds, travelling between Arctic breeding grounds and the Antarctic each year. Some species of tubenoses (Procellariiformes) such as albatrosses circle the earth, flying over the southern oceans, while others such as Manx shearwaters migrate 14,000 km (8,700 mi) between their northern breeding grounds and the southern ocean. Shorter migrations are common, including altitudinal migrations on mountains such as the Andes and Himalayas. Migration is the regular seasonal movement, often north and south, undertaken by many species of birds. Bird movements include those made in response to changes in food availability, habitat, or weather. Sometimes, journeys are not termed ""true migration"" because they are irregular (nomadism, invasions, irruptions) or in only one direction (dispersal, movement of young away from natal area). Migration is marked by its annual seasonality. Non-migratory birds are said to be resident or sedentary. Approximately 1800 of the world's 10,000 bird species are long-distance migrants. The migration of birds also aids the movement of other species, including those of ectoparasites such as ticks and lice, which in turn may carry micro-organisms including those of concern to human health. Due to the global spread of avian influenza, bird migration has been studied as a possible mechanism of disease transmission, but it has been found not to present a special risk; import of pet and domestic birds is a greater threat. Some viruses that are maintained in birds without lethal effects, such as the West Nile Virus may however be spread by migrating birds. Birds may also have a role in the dispersal of propagules of plants and plankton. Migratory birds may use two electromagnetic tools to find their destinations: one that is entirely innate and another that relies on experience. A young bird on its first migration flies in the correct direction according to the Earth's magnetic field, but does not know how far the journey will be. It does this through a radical pair mechanism whereby chemical reactions in special photo pigments sensitive to long wavelengths are affected by the field. Although this only works during daylight hours, it does not use the position of the sun in any way. At this stage the bird is in the position of a boy scout with a compass but no map, until it grows accustomed to the journey and can put its other capabilities to use. With experience it learns various landmarks and this ""mapping"" is done by magnetites in the trigeminal system, which tell the bird how strong the field is. Because birds migrate between northern and southern regions, the magnetic field strengths at different latitudes let it interpret the radical pair mechanism more accurately and let it know when it has reached its destination. There is a neural connection between the eye and ""Cluster N"", the part of the forebrain that is active during migrational orientation, suggesting that birds may actually be able to see the magnetic field of the earth. A similar situation occurs with waders (called shorebirds in North America). Many species, such as dunlin Calidris alpina and western sandpiper Calidris mauri, undertake long movements from their Arctic breeding grounds to warmer locations in the same hemisphere, but others such as semipalmated sandpiper C. pusilla travel longer distances to the tropics in the Southern Hemisphere. Seabird migration is similar in pattern to those of the waders and waterfowl. Some, such as the black guillemot Cepphus grylle and some gulls, are quite sedentary; others, such as most terns and auks breeding in the temperate northern hemisphere, move varying distances south in the northern winter. The Arctic tern Sterna paradisaea has the longest-distance migration of any bird, and sees more daylight than any other, moving from its Arctic breeding grounds to the Antarctic non-breeding areas. One Arctic tern, ringed (banded) as a chick on the Farne Islands off the British east coast, reached Melbourne, Australia in just three months from fledging, a sea journey of over 22,000 km (14,000 mi). Many tubenosed birds breed in the southern hemisphere and migrate north in the southern winter. Theoretical analyses show that detours that increase flight distance by up to 20% will often be adaptive on aerodynamic grounds - a bird that loads itself with food to cross a long barrier flies less efficiently. However some species show circuitous migratory routes that reflect historical range expansions and are far from optimal in ecological terms. An example is the migration of continental populations of Swainson's thrush Catharus ustulatus, which fly far east across North America before turning south via Florida to reach northern South America; this route is believed to be the consequence of a range expansion that occurred about 10,000 years ago. Detours may also be caused by differential wind conditions, predation risk, or other factors. The primary physiological cue for migration are the changes in the day length. These changes are also related to hormonal changes in the birds. In the period before migration, many birds display higher activity or Zugunruhe (German: migratory restlessness), first described by Johann Friedrich Naumann in 1795, as well as physiological changes such as increased fat deposition. The occurrence of Zugunruhe even in cage-raised birds with no environmental cues (e.g. shortening of day and falling temperature) has pointed to the role of circannual endogenous programs in controlling bird migrations. Caged birds display a preferential flight direction that corresponds with the migratory direction they would take in nature, changing their preferential direction at roughly the same time their wild conspecifics change course. These advantages offset the high stress, physical exertion costs, and other risks of the migration. Predation can be heightened during migration: Eleonora's falcon Falco eleonorae, which breeds on Mediterranean islands, has a very late breeding season, coordinated with the autumn passage of southbound passerine migrants, which it feeds to its young. A similar strategy is adopted by the greater noctule bat, which preys on nocturnal passerine migrants. The higher concentrations of migrating birds at stopover sites make them prone to parasites and pathogens, which require a heightened immune response. Many bird populations migrate long distances along a flyway. The most common pattern involves flying north in the spring to breed in the temperate or Arctic summer and returning in the autumn to wintering grounds in warmer regions to the south. Of course, in the southern hemisphere the directions are reversed, but there is less land area in the far south to support long-distance migration. Large scale climatic changes, as have been experienced in the past, are expected to have an effect on the timing of migration. Studies have shown a variety of effects including timing changes in migration, breeding as well as population variations. In the tropics there is little variation in the length of day throughout the year, and it is always warm enough for a food supply, but altitudinal migration occurs in some tropical birds. There is evidence that this enables the migrants to obtain more of their preferred foods such as fruits. For some species of waders, migration success depends on the availability of certain key food resources at stopover points along the migration route. This gives the migrants an opportunity to refuel for the next leg of the voyage. Some examples of important stopover locations are the Bay of Fundy and Delaware Bay. Within a population, it is common for different ages and/or sexes to have different patterns of timing and distance. Female chaffinches Fringilla coelebs in Eastern Fennoscandia migrate earlier in the autumn than males do. The primary motivation for migration appears to be food; for example, some hummingbirds choose not to migrate if fed through the winter. Also, the longer days of the northern summer provide extended time for breeding birds to feed their young. This helps diurnal birds to produce larger clutches than related non-migratory species that remain in the tropics. As the days shorten in autumn, the birds return to warmer regions where the available food supply varies little with the season. A related phenomenon called ""abmigration"" involves birds from one region joining similar birds from a different breeding region in the common winter grounds and then migrating back along with the new population. This is especially common in some waterfowl, which shift from one flyway to another. Bird migration is the regular seasonal movement, often north and south along a flyway, between breeding and wintering grounds. Many species of bird migrate. Migration carries high costs in predation and mortality, including from hunting by humans, and is driven primarily by availability of food. It occurs mainly in the northern hemisphere, where birds are funnelled on to specific routes by natural barriers such as the Mediterranean Sea or the Caribbean Sea. Short-distance passerine migrants have two evolutionary origins. Those that have long-distance migrants in the same family, such as the common chiffchaff Phylloscopus collybita, are species of southern hemisphere origins that have progressively shortened their return migration to stay in the northern hemisphere. Reverse migration, where the genetic programming of young birds fails to work properly, can lead to rarities turning up as vagrants thousands of kilometres out of range. The most pelagic species, mainly in the 'tubenose' order Procellariiformes, are great wanderers, and the albatrosses of the southern oceans may circle the globe as they ride the ""roaring forties"" outside the breeding season. The tubenoses spread widely over large areas of open ocean, but congregate when food becomes available. Many are also among the longest-distance migrants; sooty shearwaters Puffinus griseus nesting on the Falkland Islands migrate 14,000 km (8,700 mi) between the breeding colony and the North Atlantic Ocean off Norway. Some Manx shearwaters Puffinus puffinus do this same journey in reverse. As they are long-lived birds, they may cover enormous distances during their lives; one record-breaking Manx shearwater is calculated to have flown 8 million km (5 million miles) during its over-50 year lifespan. Some predators take advantage of the concentration of birds during migration. Greater noctule bats feed on nocturnal migrating passerines. Some birds of prey specialize on migrating waders. The same considerations about barriers and detours that apply to long-distance land-bird migration apply to water birds, but in reverse: a large area of land without bodies of water that offer feeding sites may also be a barrier to a bird that feeds in coastal waters. Detours avoiding such barriers are observed: for example, brent geese Branta bernicla migrating from the Taymyr Peninsula to the Wadden Sea travel via the White Sea coast and the Baltic Sea rather than directly across the Arctic Ocean and northern Scandinavia. Nocturnal migrants minimize predation, avoid overheating, and can feed during the day. One cost of nocturnal migration is the loss of sleep. Migrants may be able to alter their quality of sleep to compensate for the loss. Long distance migrants are believed to disperse as young birds and form attachments to potential breeding sites and to favourite wintering sites. Once the site attachment is made they show high site-fidelity, visiting the same wintering sites year after year. Historically, migration has been recorded as much as 3,000 years ago by Ancient Greek authors including Homer and Aristotle, and in the Book of Job, for species such as storks, turtle doves, and swallows. More recently, Johannes Leche began recording dates of arrivals of spring migrants in Finland in 1749, and scientific studies have used techniques including bird ringing and satellite tracking. Threats to migratory birds have grown with habitat destruction especially of stopover and wintering sites, as well as structures such as power lines and wind farms. Migration in birds is highly labile and is believed to have developed independently in many avian lineages. While it is agreed that the behavioral and physiological adaptations necessary for migration are under genetic control, some authors have argued that no genetic change is necessary for migratory behavior to develop in a sedentary species because the genetic framework for migratory behavior exists in nearly all avian lineages. This explains the rapid appearance of migratory behavior after the most recent glacial maximum. Bird migration is primarily, but not entirely, a Northern Hemisphere phenomenon. This is because land birds in high northern latitudes, where food becomes scarce in winter, leave for areas further south (including the Southern Hemisphere) to overwinter, and because the continental landmass is much larger in the Northern Hemisphere. In contrast, among (pelagic) seabirds, species of the Southern Hemisphere are more likely to migrate. This is because there is a large area of ocean in the Southern Hemisphere, and more islands suitable for seabirds to nest. Aristotle noted that cranes traveled from the steppes of Scythia to marshes at the headwaters of the Nile. Pliny the Elder, in his Historia Naturalis, repeats Aristotle's observations. In polygynous species with considerable sexual dimorphism, males tend to return earlier to the breeding sites than their females. This is termed protandry. Bird migration is not limited to birds that can fly. Most species of penguin (Spheniscidae) migrate by swimming. These routes can cover over 1,000 km (620 mi). Dusky grouse Dendragapus obscurus perform altitudinal migration mostly by walking. Emus Dromaius novaehollandiae in Australia have been observed to undertake long-distance movements on foot during droughts. Migrating birds can lose their way and appear outside their normal ranges. This can be due to flying past their destinations as in the ""spring overshoot"" in which birds returning to their breeding areas overshoot and end up further north than intended. Certain areas, because of their location, have become famous as watchpoints for such birds. Examples are the Point Pelee National Park in Canada, and Spurn in England. Many of the smaller insectivorous birds including the warblers, hummingbirds and flycatchers migrate large distances, usually at night. They land in the morning and may feed for a few days before resuming their migration. The birds are referred to as passage migrants in the regions where they occur for short durations between the origin and destination. Many long-distance migrants appear to be genetically programmed to respond to changing day length. Species that move short distances, however, may not need such a timing mechanism, instead moving in response to local weather conditions. Thus mountain and moorland breeders, such as wallcreeper Tichodroma muraria and white-throated dipper Cinclus cinclus, may move only altitudinally to escape the cold higher ground. Other species such as merlin Falco columbarius and Eurasian skylark Alauda arvensis move further, to the coast or towards the south. Species like the chaffinch are much less migratory in Britain than those of continental Europe, mostly not moving more than 5 km in their lives. Species that have no long-distance migratory relatives, such as the waxwings Bombycilla, are effectively moving in response to winter weather and the loss of their usual winter food, rather than enhanced breeding opportunities. The ability of birds to navigate during migrations cannot be fully explained by endogenous programming, even with the help of responses to environmental cues. The ability to successfully perform long-distance migrations can probably only be fully explained with an accounting for the cognitive ability of the birds to recognize habitats and form mental maps. Satellite tracking of day migrating raptors such as ospreys and honey buzzards has shown that older individuals are better at making corrections for wind drift. Within a species not all populations may be migratory; this is known as ""partial migration"". Partial migration is very common in the southern continents; in Australia, 44% of non-passerine birds and 32% of passerine species are partially migratory. In some species, the population at higher latitudes tends to be migratory and will often winter at lower latitude. The migrating birds bypass the latitudes where other populations may be sedentary, where suitable wintering habitats may already be occupied. This is an example of leap-frog migration. Many fully migratory species show leap-frog migration (birds that nest at higher latitudes spend the winter at lower latitudes), and many show the alternative, chain migration, where populations 'slide' more evenly north and south without reversing order. Orientation behaviour studies have been traditionally carried out using variants of a setup known as the Emlen funnel, which consists of a circular cage with the top covered by glass or wire-screen so that either the sky is visible or the setup is placed in a planetarium or with other controls on environmental cues. The orientation behaviour of the bird inside the cage is studied quantitatively using the distribution of marks that the bird leaves on the walls of the cage. Other approaches used in pigeon homing studies make use of the direction in which the bird vanishes on the horizon. The timing of migration seems to be controlled primarily by changes in day length. Migrating birds navigate using celestial cues from the sun and stars, the earth's magnetic field, and probably also mental maps. Most migrations begin with the birds starting off in a broad front. Often, this front narrows into one or more preferred routes termed flyways. These routes typically follow mountain ranges or coastlines, sometimes rivers, and may take advantage of updrafts and other wind patterns or avoid geographical barriers such as large stretches of open water. The specific routes may be genetically programmed or learned to varying degrees. The routes taken on forward and return migration are often different. A common pattern in North America is clockwise migration, where birds flying North tend to be further West, and flying South tend to shift Eastwards."
Infrared,"In the semiconductor industry, infrared light can be used to characterize materials such as thin films and periodic trench structures. By measuring the reflectance of light from the surface of a semiconductor wafer, the index of refraction (n) and the extinction Coefficient (k) can be determined via the Forouhi-Bloomer dispersion equations. The reflectance from the infrared light can also be used to determine the critical dimension, depth, and sidewall angle of high aspect ratio trench structures. Near-infrared is the region closest in wavelength to the radiation detectable by the human eye, mid- and far-infrared are progressively further from the visible spectrum. Other definitions follow different physical mechanisms (emission peaks, vs. bands, water absorption) and the newest follow technical reasons (the common silicon detectors are sensitive to about 1,050 nm, while InGaAs's sensitivity starts around 950 nm and ends between 1,700 and 2,600 nm, depending on the specific configuration). Unfortunately, international standards for these specifications are not currently available. The infrared portion of the spectrum has several useful benefits for astronomers. Cold, dark molecular clouds of gas and dust in our galaxy will glow with radiated heat as they are irradiated by imbedded stars. Infrared can also be used to detect protostars before they begin to emit visible light. Stars emit a smaller portion of their energy in the infrared spectrum, so nearby cool objects such as planets can be more readily detected. (In the visible light spectrum, the glare from the star will drown out the reflected light from a planet.) Thermographic cameras detect radiation in the infrared range of the electromagnetic spectrum (roughly 900–14,000 nanometers or 0.9–14 μm) and produce images of that radiation. Since infrared radiation is emitted by all objects based on their temperatures, according to the black body radiation law, thermography makes it possible to ""see"" one's environment with or without visible illumination. The amount of radiation emitted by an object increases with temperature, therefore thermography allows one to see variations in temperature (hence the name). IR data transmission is also employed in short-range communication among computer peripherals and personal digital assistants. These devices usually conform to standards published by IrDA, the Infrared Data Association. Remote controls and IrDA devices use infrared light-emitting diodes (LEDs) to emit infrared radiation that is focused by a plastic lens into a narrow beam. The beam is modulated, i.e. switched on and off, to encode the data. The receiver uses a silicon photodiode to convert the infrared radiation to an electric current. It responds only to the rapidly pulsing signal created by the transmitter, and filters out slowly changing infrared radiation from ambient light. Infrared communications are useful for indoor use in areas of high population density. IR does not penetrate walls and so does not interfere with other devices in adjoining rooms. Infrared is the most common way for remote controls to command appliances. Infrared remote control protocols like RC-5, SIRC, are used to communicate with infrared. Infrared vibrational spectroscopy (see also near-infrared spectroscopy) is a technique that can be used to identify molecules by analysis of their constituent bonds. Each chemical bond in a molecule vibrates at a frequency characteristic of that bond. A group of atoms in a molecule (e.g., CH2) may have multiple modes of oscillation caused by the stretching and bending motions of the group as a whole. If an oscillation leads to a change in dipole in the molecule then it will absorb a photon that has the same frequency. The vibrational frequencies of most molecules correspond to the frequencies of infrared light. Typically, the technique is used to study organic compounds using light radiation from 4000–400 cm−1, the mid-infrared. A spectrum of all the frequencies of absorption in a sample is recorded. This can be used to gain information about the sample composition in terms of chemical groups present and also its purity (for example, a wet sample will show a broad O-H absorption around 3200 cm−1). Infrared tracking, also known as infrared homing, refers to a passive missile guidance system, which uses the emission from a target of electromagnetic radiation in the infrared part of the spectrum to track it. Missiles that use infrared seeking are often referred to as ""heat-seekers"", since infrared (IR) is just below the visible spectrum of light in frequency and is radiated strongly by hot bodies. Many objects such as people, vehicle engines, and aircraft generate and retain heat, and as such, are especially visible in the infrared wavelengths of light compared to objects in the background. The concept of emissivity is important in understanding the infrared emissions of objects. This is a property of a surface that describes how its thermal emissions deviate from the ideal of a black body. To further explain, two objects at the same physical temperature will not show the same infrared image if they have differing emissivity. For example, for any pre-set emissivity value, objects with higher emissivity will appear hotter, and those with a lower emissivity will appear cooler. For that reason, incorrect selection of emissivity will give inaccurate results when using infrared cameras and pyrometers. The sensitivity of Earth-based infrared telescopes is significantly limited by water vapor in the atmosphere, which absorbs a portion of the infrared radiation arriving from space outside of selected atmospheric windows. This limitation can be partially alleviated by placing the telescope observatory at a high altitude, or by carrying the telescope aloft with a balloon or an aircraft. Space telescopes do not suffer from this handicap, and so outer space is considered the ideal location for infrared astronomy. Infrared radiation is used in industrial, scientific, and medical applications. Night-vision devices using active near-infrared illumination allow people or animals to be observed without the observer being detected. Infrared astronomy uses sensor-equipped telescopes to penetrate dusty regions of space, such as molecular clouds; detect objects such as planets, and to view highly red-shifted objects from the early days of the universe. Infrared thermal-imaging cameras are used to detect heat loss in insulated systems, to observe changing blood flow in the skin, and to detect overheating of electrical apparatus. Heat is energy in transit that flows due to temperature difference. Unlike heat transmitted by thermal conduction or thermal convection, thermal radiation can propagate through a vacuum. Thermal radiation is characterized by a particular spectrum of many wavelengths that is associated with emission from an object, due to the vibration of its molecules at a given temperature. Thermal radiation can be emitted from objects at any wavelength, and at very high temperatures such radiations are associated with spectra far above the infrared, extending into visible, ultraviolet, and even X-ray regions (i.e., the solar corona). Thus, the popular association of infrared radiation with thermal radiation is only a coincidence based on typical (comparatively low) temperatures often found near the surface of planet Earth. In infrared photography, infrared filters are used to capture the near-infrared spectrum. Digital cameras often use infrared blockers. Cheaper digital cameras and camera phones have less effective filters and can ""see"" intense near-infrared, appearing as a bright purple-white color. This is especially pronounced when taking pictures of subjects near IR-bright areas (such as near a lamp), where the resulting infrared interference can wash out the image. There is also a technique called 'T-ray' imaging, which is imaging using far-infrared or terahertz radiation. Lack of bright sources can make terahertz photography more challenging than most other infrared imaging techniques. Recently T-ray imaging has been of considerable interest due to a number of new developments such as terahertz time-domain spectroscopy. Infrared cleaning is a technique used by some Motion picture film scanner, film scanners and flatbed scanners to reduce or remove the effect of dust and scratches upon the finished scan. It works by collecting an additional infrared channel from the scan at the same position and resolution as the three visible color channels (red, green, and blue). The infrared channel, in combination with the other channels, is used to detect the location of scratches and dust. Once located, those defects can be corrected by scaling or replaced by inpainting. High, cold ice clouds such as Cirrus or Cumulonimbus show up bright white, lower warmer clouds such as Stratus or Stratocumulus show up as grey with intermediate clouds shaded accordingly. Hot land surfaces will show up as dark-grey or black. One disadvantage of infrared imagery is that low cloud such as stratus or fog can be a similar temperature to the surrounding land or sea surface and does not show up. However, using the difference in brightness of the IR4 channel (10.3–11.5 µm) and the near-infrared channel (1.58–1.64 µm), low cloud can be distinguished, producing a fog satellite picture. The main advantage of infrared is that images can be produced at night, allowing a continuous sequence of weather to be studied. Infrared reflectography (fr; it; es), as called by art conservators, can be applied to paintings to reveal underlying layers in a completely non-destructive manner, in particular the underdrawing or outline drawn by the artist as a guide. This often reveals the artist's use of carbon black, which shows up well in reflectograms, as long as it has not also been used in the ground underlying the whole painting. Art conservators are looking to see whether the visible layers of paint differ from the underdrawing or layers in between – such alterations are called pentimenti when made by the original artist. This is very useful information in deciding whether a painting is the prime version by the original artist or a copy, and whether it has been altered by over-enthusiastic restoration work. In general, the more pentimenti the more likely a painting is to be the prime version. It also gives useful insights into working practices. Earth's surface and the clouds absorb visible and invisible radiation from the sun and re-emit much of the energy as infrared back to atmosphere. Certain substances in the atmosphere, chiefly cloud droplets and water vapor, but also carbon dioxide, methane, nitrous oxide, sulfur hexafluoride, and chlorofluorocarbons, absorb this infrared, and re-radiate it in all directions including back to Earth. Thus, the greenhouse effect keeps the atmosphere and surface much warmer than if the infrared absorbers were absent from the atmosphere. Infrared radiation is popularly known as ""heat radiation""[citation needed], but light and electromagnetic waves of any frequency will heat surfaces that absorb them. Infrared light from the Sun accounts for 49% of the heating of Earth, with the rest being caused by visible light that is absorbed then re-radiated at longer wavelengths. Visible light or ultraviolet-emitting lasers can char paper and incandescently hot objects emit visible radiation. Objects at room temperature will emit radiation concentrated mostly in the 8 to 25 µm band, but this is not distinct from the emission of visible light by incandescent objects and ultraviolet by even hotter objects (see black body and Wien's displacement law). The onset of infrared is defined (according to different standards) at various values typically between 700 nm and 800 nm, but the boundary between visible and infrared light is not precisely defined. The human eye is markedly less sensitive to light above 700 nm wavelength, so longer wavelengths make insignificant contributions to scenes illuminated by common light sources. However, particularly intense near-IR light (e.g., from IR lasers, IR LED sources, or from bright daylight with the visible light removed by colored gels) can be detected up to approximately 780 nm, and will be perceived as red light. Sources providing wavelengths as long as 1050 nm can be seen as a dull red glow in intense sources, causing some difficulty in near-IR illumination of scenes in the dark (usually this practical problem is solved by indirect illumination). Leaves are particularly bright in the near IR, and if all visible light leaks from around an IR-filter are blocked, and the eye is given a moment to adjust to the extremely dim image coming through a visually opaque IR-passing photographic filter, it is possible to see the Wood effect that consists of IR-glowing foliage. Infrared is used in night vision equipment when there is insufficient visible light to see. Night vision devices operate through a process involving the conversion of ambient light photons into electrons that are then amplified by a chemical and electrical process and then converted back into visible light. Infrared light sources can be used to augment the available ambient light for conversion by night vision devices, increasing in-the-dark visibility without actually using a visible light source. The discovery of infrared radiation is ascribed to William Herschel, the astronomer, in the early 19th century. Herschel published his results in 1800 before the Royal Society of London. Herschel used a prism to refract light from the sun and detected the infrared, beyond the red part of the spectrum, through an increase in the temperature recorded on a thermometer. He was surprised at the result and called them ""Calorific Rays"". The term 'Infrared' did not appear until late in the 19th century."
Amazon_rainforest,"The region is home to about 2.5 million insect species, tens of thousands of plants, and some 2,000 birds and mammals. To date, at least 40,000 plant species, 2,200 fishes, 1,294 birds, 427 mammals, 428 amphibians, and 378 reptiles have been scientifically classified in the region. One in five of all the bird species in the world live in the rainforests of the Amazon, and one in five of the fish species live in Amazonian rivers and streams. Scientists have described between 96,660 and 128,843 invertebrate species in Brazil alone. As indigenous territories continue to be destroyed by deforestation and ecocide, such as in the Peruvian Amazon indigenous peoples' rainforest communities continue to disappear, while others, like the Urarina continue to struggle to fight for their cultural survival and the fate of their forested territories. Meanwhile, the relationship between non-human primates in the subsistence and symbolism of indigenous lowland South American peoples has gained increased attention, as have ethno-biology and community-based conservation efforts. Deforestation is the conversion of forested areas to non-forested areas. The main sources of deforestation in the Amazon are human settlement and development of the land. Prior to the early 1960s, access to the forest's interior was highly restricted, and the forest remained basically intact. Farms established during the 1960s were based on crop cultivation and the slash and burn method. However, the colonists were unable to manage their fields and the crops because of the loss of soil fertility and weed invasion. The soils in the Amazon are productive for just a short period of time, so farmers are constantly moving to new areas and clearing more land. These farming practices led to deforestation and caused extensive environmental damage. Deforestation is considerable, and areas cleared of forest are visible to the naked eye from outer space. The rainforest contains several species that can pose a hazard. Among the largest predatory creatures are the black caiman, jaguar, cougar, and anaconda. In the river, electric eels can produce an electric shock that can stun or kill, while piranha are known to bite and injure humans. Various species of poison dart frogs secrete lipophilic alkaloid toxins through their flesh. There are also numerous parasites and disease vectors. Vampire bats dwell in the rainforest and can spread the rabies virus. Malaria, yellow fever and Dengue fever can also be contracted in the Amazon region. In 2010 the Amazon rainforest experienced another severe drought, in some ways more extreme than the 2005 drought. The affected region was approximate 1,160,000 square miles (3,000,000 km2) of rainforest, compared to 734,000 square miles (1,900,000 km2) in 2005. The 2010 drought had three epicenters where vegetation died off, whereas in 2005 the drought was focused on the southwestern part. The findings were published in the journal Science. In a typical year the Amazon absorbs 1.5 gigatons of carbon dioxide; during 2005 instead 5 gigatons were released and in 2010 8 gigatons were released. The biodiversity of plant species is the highest on Earth with one 2001 study finding a quarter square kilometer (62 acres) of Ecuadorian rainforest supports more than 1,100 tree species. A study in 1999 found one square kilometer (247 acres) of Amazon rainforest can contain about 90,790 tonnes of living plants. The average plant biomass is estimated at 356 ± 47 tonnes per hectare. To date, an estimated 438,000 species of plants of economic and social interest have been registered in the region with many more remaining to be discovered or catalogued. The total number of tree species in the region is estimated at 16,000. For a long time, it was thought that the Amazon rainforest was only ever sparsely populated, as it was impossible to sustain a large population through agriculture given the poor soil. Archeologist Betty Meggers was a prominent proponent of this idea, as described in her book Amazonia: Man and Culture in a Counterfeit Paradise. She claimed that a population density of 0.2 inhabitants per square kilometre (0.52/sq mi) is the maximum that can be sustained in the rainforest through hunting, with agriculture needed to host a larger population. However, recent anthropological findings have suggested that the region was actually densely populated. Some 5 million people may have lived in the Amazon region in AD 1500, divided between dense coastal settlements, such as that at Marajó, and inland dwellers. By 1900 the population had fallen to 1 million and by the early 1980s it was less than 200,000. During the mid-Eocene, it is believed that the drainage basin of the Amazon was split along the middle of the continent by the Purus Arch. Water on the eastern side flowed toward the Atlantic, while to the west water flowed toward the Pacific across the Amazonas Basin. As the Andes Mountains rose, however, a large basin was created that enclosed a lake; now known as the Solimões Basin. Within the last 5–10 million years, this accumulating water broke through the Purus Arch, joining the easterly flow toward the Atlantic. The first European to travel the length of the Amazon River was Francisco de Orellana in 1542. The BBC's Unnatural Histories presents evidence that Orellana, rather than exaggerating his claims as previously thought, was correct in his observations that a complex civilization was flourishing along the Amazon in the 1540s. It is believed that the civilization was later devastated by the spread of diseases from Europe, such as smallpox. Since the 1970s, numerous geoglyphs have been discovered on deforested land dating between AD 0–1250, furthering claims about Pre-Columbian civilizations. Ondemar Dias is accredited with first discovering the geoglyphs in 1977 and Alceu Ranzi with furthering their discovery after flying over Acre. The BBC's Unnatural Histories presented evidence that the Amazon rainforest, rather than being a pristine wilderness, has been shaped by man for at least 11,000 years through practices such as forest gardening and terra preta. Environmentalists are concerned about loss of biodiversity that will result from destruction of the forest, and also about the release of the carbon contained within the vegetation, which could accelerate global warming. Amazonian evergreen forests account for about 10% of the world's terrestrial primary productivity and 10% of the carbon stores in ecosystems—of the order of 1.1 × 1011 metric tonnes of carbon. Amazonian forests are estimated to have accumulated 0.62 ± 0.37 tons of carbon per hectare per year between 1975 and 1996. Between 1991 and 2000, the total area of forest lost in the Amazon rose from 415,000 to 587,000 square kilometres (160,000 to 227,000 sq mi), with most of the lost forest becoming pasture for cattle. Seventy percent of formerly forested land in the Amazon, and 91% of land deforested since 1970, is used for livestock pasture. Currently, Brazil is the second-largest global producer of soybeans after the United States. New research however, conducted by Leydimere Oliveira et al., has shown that the more rainforest is logged in the Amazon, the less precipitation reaches the area and so the lower the yield per hectare becomes. So despite the popular perception, there has been no economical advantage for Brazil from logging rainforest zones and converting these to pastoral fields. The use of remote sensing for the conservation of the Amazon is also being used by the indigenous tribes of the basin to protect their tribal lands from commercial interests. Using handheld GPS devices and programs like Google Earth, members of the Trio Tribe, who live in the rainforests of southern Suriname, map out their ancestral lands to help strengthen their territorial claims. Currently, most tribes in the Amazon do not have clearly defined boundaries, making it easier for commercial ventures to target their territories. To accurately map the Amazon's biomass and subsequent carbon related emissions, the classification of tree growth stages within different parts of the forest is crucial. In 2006 Tatiana Kuplich organized the trees of the Amazon into four categories: (1) mature forest, (2) regenerating forest [less than three years], (3) regenerating forest [between three and five years of regrowth], and (4) regenerating forest [eleven to eighteen years of continued development]. The researcher used a combination of Synthetic aperture radar (SAR) and Thematic Mapper (TM) to accurately place the different portions of the Amazon into one of the four classifications. There is evidence that there have been significant changes in Amazon rainforest vegetation over the last 21,000 years through the Last Glacial Maximum (LGM) and subsequent deglaciation. Analyses of sediment deposits from Amazon basin paleolakes and from the Amazon Fan indicate that rainfall in the basin during the LGM was lower than for the present, and this was almost certainly associated with reduced moist tropical vegetation cover in the basin. There is debate, however, over how extensive this reduction was. Some scientists argue that the rainforest was reduced to small, isolated refugia separated by open forest and grassland; other scientists argue that the rainforest remained largely intact but extended less far to the north, south, and east than is seen today. This debate has proved difficult to resolve because the practical limitations of working in the rainforest mean that data sampling is biased away from the center of the Amazon basin, and both explanations are reasonably well supported by the available data. Following the Cretaceous–Paleogene extinction event, the extinction of the dinosaurs and the wetter climate may have allowed the tropical rainforest to spread out across the continent. From 66–34 Mya, the rainforest extended as far south as 45°. Climate fluctuations during the last 34 million years have allowed savanna regions to expand into the tropics. During the Oligocene, for example, the rainforest spanned a relatively narrow band. It expanded again during the Middle Miocene, then retracted to a mostly inland formation at the last glacial maximum. However, the rainforest still managed to thrive during these glacial periods, allowing for the survival and evolution of a broad diversity of species. NASA's CALIPSO satellite has measured the amount of dust transported by wind from the Sahara to the Amazon: an average 182 million tons of dust are windblown out of the Sahara each year, at 15 degrees west longitude, across 1,600 miles (2,600 km) over the Atlantic Ocean (some dust falls into the Atlantic), then at 35 degrees West longitude at the eastern coast of South America, 27.7 million tons (15%) of dust fall over the Amazon basin, 132 million tons of dust remain in the air, 43 million tons of dust are windblown and falls on the Caribbean Sea, past 75 degrees west longitude. The needs of soy farmers have been used to justify many of the controversial transportation projects that are currently developing in the Amazon. The first two highways successfully opened up the rainforest and led to increased settlement and deforestation. The mean annual deforestation rate from 2000 to 2005 (22,392 km2 or 8,646 sq mi per year) was 18% higher than in the previous five years (19,018 km2 or 7,343 sq mi per year). Although deforestation has declined significantly in the Brazilian Amazon between 2004 and 2014, there has been an increase to the present day. In 2005, parts of the Amazon basin experienced the worst drought in one hundred years, and there were indications that 2006 could have been a second successive year of drought. A July 23, 2006 article in the UK newspaper The Independent reported Woods Hole Research Center results showing that the forest in its present form could survive only three years of drought. Scientists at the Brazilian National Institute of Amazonian Research argue in the article that this drought response, coupled with the effects of deforestation on regional climate, are pushing the rainforest towards a ""tipping point"" where it would irreversibly start to die. It concludes that the forest is on the brink of being turned into savanna or desert, with catastrophic consequences for the world's climate. Terra preta (black earth), which is distributed over large areas in the Amazon forest, is now widely accepted as a product of indigenous soil management. The development of this fertile soil allowed agriculture and silviculture in the previously hostile environment; meaning that large portions of the Amazon rainforest are probably the result of centuries of human management, rather than naturally occurring as has previously been supposed. In the region of the Xingu tribe, remains of some of these large settlements in the middle of the Amazon forest were found in 2003 by Michael Heckenberger and colleagues of the University of Florida. Among those were evidence of roads, bridges and large plazas. The Amazon rainforest (Portuguese: Floresta Amazônica or Amazônia; Spanish: Selva Amazónica, Amazonía or usually Amazonia; French: Forêt amazonienne; Dutch: Amazoneregenwoud), also known in English as Amazonia or the Amazon Jungle, is a moist broadleaf forest that covers most of the Amazon basin of South America. This basin encompasses 7,000,000 square kilometres (2,700,000 sq mi), of which 5,500,000 square kilometres (2,100,000 sq mi) are covered by the rainforest. This region includes territory belonging to nine nations. The majority of the forest is contained within Brazil, with 60% of the rainforest, followed by Peru with 13%, Colombia with 10%, and with minor amounts in Venezuela, Ecuador, Bolivia, Guyana, Suriname and French Guiana. States or departments in four nations contain ""Amazonas"" in their names. The Amazon represents over half of the planet's remaining rainforests, and comprises the largest and most biodiverse tract of tropical rainforest in the world, with an estimated 390 billion individual trees divided into 16,000 species. One computer model of future climate change caused by greenhouse gas emissions shows that the Amazon rainforest could become unsustainable under conditions of severely reduced rainfall and increased temperatures, leading to an almost complete loss of rainforest cover in the basin by 2100. However, simulations of Amazon basin climate change across many different models are not consistent in their estimation of any rainfall response, ranging from weak increases to strong decreases. The result indicates that the rainforest could be threatened though the 21st century by climate change in addition to deforestation."
Tibet,"The main crops grown are barley, wheat, buckwheat, rye, potatoes, and assorted fruits and vegetables. Tibet is ranked the lowest among China’s 31 provinces on the Human Development Index according to UN Development Programme data. In recent years, due to increased interest in Tibetan Buddhism, tourism has become an increasingly important sector, and is actively promoted by the authorities. Tourism brings in the most income from the sale of handicrafts. These include Tibetan hats, jewelry (silver and gold), wooden items, clothing, quilts, fabrics, Tibetan rugs and carpets. The Central People's Government exempts Tibet from all taxation and provides 90% of Tibet's government expenditures. However most of this investment goes to pay migrant workers who do not settle in Tibet and send much of their income home to other provinces. In 1980, General Secretary and reformist Hu Yaobang visited Tibet and ushered in a period of social, political, and economic liberalization. At the end of the decade, however, analogously to the Tiananmen Square protests of 1989, monks in the Drepung and Sera monasteries started protesting for independence, and so the government halted reforms and started an anti-separatist campaign. Human rights organisations have been critical of the Beijing and Lhasa governments' approach to human rights in the region when cracking down on separatist convulsions that have occurred around monasteries and cities, most recently in the 2008 Tibetan unrest. Qing dynasty rule in Tibet began with their 1720 expedition to the country when they expelled the invading Dzungars. Amdo came under Qing control in 1724, and eastern Kham was incorporated into neighbouring Chinese provinces in 1728. Meanwhile, the Qing government sent resident commissioners called Ambans to Lhasa. In 1750 the Ambans and the majority of the Han Chinese and Manchus living in Lhasa were killed in a riot, and Qing troops arrived quickly and suppressed the rebels in the next year. Like the preceding Yuan dynasty, the Manchus of the Qing dynasty exerted military and administrative control of the region, while granting it a degree of political autonomy. The Qing commander publicly executed a number of supporters of the rebels and, as in 1723 and 1728, made changes in the political structure and drew up a formal organization plan. The Qing now restored the Dalai Lama as ruler, leading the governing council called Kashag, but elevated the role of Ambans to include more direct involvement in Tibetan internal affairs. At the same time the Qing took steps to counterbalance the power of the aristocracy by adding officials recruited from the clergy to key posts. The Indus and Brahmaputra rivers originate from a lake (Tib: Tso Mapham) in Western Tibet, near Mount Kailash. The mountain is a holy pilgrimage site for both Hindus and Tibetans. The Hindus consider the mountain to be the abode of Lord Shiva. The Tibetan name for Mt. Kailash is Khang Rinpoche. Tibet has numerous high-altitude lakes referred to in Tibetan as tso or co. These include Qinghai Lake, Lake Manasarovar, Namtso, Pangong Tso, Yamdrok Lake, Siling Co, Lhamo La-tso, Lumajangdong Co, Lake Puma Yumco, Lake Paiku, Lake Rakshastal, Dagze Co and Dong Co. The Qinghai Lake (Koko Nor) is the largest lake in the People's Republic of China. For several decades, peace reigned in Tibet, but in 1792 the Qing Qianlong Emperor sent a large Chinese army into Tibet to push the invading Nepalese out. This prompted yet another Qing reorganization of the Tibetan government, this time through a written plan called the ""Twenty-Nine Regulations for Better Government in Tibet"". Qing military garrisons staffed with Qing troops were now also established near the Nepalese border. Tibet was dominated by the Manchus in various stages in the 18th century, and the years immediately following the 1792 regulations were the peak of the Qing imperial commissioners' authority; but there was no attempt to make Tibet a Chinese province. The economy of Tibet is dominated by subsistence agriculture, though tourism has become a growing industry in recent decades. The dominant religion in Tibet is Tibetan Buddhism; in addition there is Bön, which is similar to Tibetan Buddhism, and there are also Tibetan Muslims and Christian minorities. Tibetan Buddhism is a primary influence on the art, music, and festivals of the region. Tibetan architecture reflects Chinese and Indian influences. Staple foods in Tibet are roasted barley, yak meat, and butter tea. After the Xinhai Revolution (1911–12) toppled the Qing dynasty and the last Qing troops were escorted out of Tibet, the new Republic of China apologized for the actions of the Qing and offered to restore the Dalai Lama's title. The Dalai Lama refused any Chinese title and declared himself ruler of an independent Tibet. In 1913, Tibet and Mongolia concluded a treaty of mutual recognition. For the next 36 years, the 13th Dalai Lama and the regents who succeeded him governed Tibet. During this time, Tibet fought Chinese warlords for control of the ethnically Tibetan areas in Xikang and Qinghai (parts of Kham and Amdo) along the upper reaches of the Yangtze River. In 1914 the Tibetan government signed the Simla Accord with Britain, ceding the South Tibet region to British India. The Chinese government denounced the agreement as illegal. Following the Xinhai Revolution against the Qing dynasty in 1912, Qing soldiers were disarmed and escorted out of Tibet Area (Ü-Tsang). The region subsequently declared its independence in 1913 without recognition by the subsequent Chinese Republican government. Later, Lhasa took control of the western part of Xikang, China. The region maintained its autonomy until 1951 when, following the Battle of Chamdo, Tibet became incorporated into the People's Republic of China, and the previous Tibetan government was abolished in 1959 after a failed uprising. Today, China governs western and central Tibet as the Tibet Autonomous Region while the eastern areas are now mostly ethnic autonomous prefectures within Sichuan, Qinghai and other neighbouring provinces. There are tensions regarding Tibet's political status and dissident groups that are active in exile. It is also said that Tibetan activists in Tibet have been arrested or tortured. The earliest Tibetan historical texts identify the Zhang Zhung culture as a people who migrated from the Amdo region into what is now the region of Guge in western Tibet. Zhang Zhung is considered to be the original home of the Bön religion. By the 1st century BCE, a neighboring kingdom arose in the Yarlung valley, and the Yarlung king, Drigum Tsenpo, attempted to remove the influence of the Zhang Zhung by expelling the Zhang's Bön priests from Yarlung. He was assassinated and Zhang Zhung continued its dominance of the region until it was annexed by Songtsen Gampo in the 7th century. Prior to Songtsän Gampo, the kings of Tibet were more mythological than factual, and there is insufficient evidence of their existence. After the Dalai Lama's government fled to Dharamsala, India, during the 1959 Tibetan Rebellion, it established a rival government-in-exile. Afterwards, the Central People's Government in Beijing renounced the agreement and began implementation of the halted social and political reforms. During the Great Leap Forward, between 200,000 and 1,000,000 Tibetans died, and approximately 6,000 monasteries were destroyed during the Cultural Revolution. In 1962 China and India fought a brief war over the disputed South Tibet and Aksai Chin regions. Although China won the war, Chinese troops withdrew north of the McMahon Line, effectively ceding South Tibet to India. From January 18–20, 2010 a national conference on Tibet and areas inhabited by Tibetans in Sichuan, Yunnan, Gansu and Qinghai was held in China and a substantial plan to improve development of the areas was announced. The conference was attended by General secretary Hu Jintao, Wu Bangguo, Wen Jiabao, Jia Qinglin, Li Changchun, Xi Jinping, Li Keqiang, He Guoqiang and Zhou Yongkang, all members of CPC Politburo Standing Committee signaling the commitment of senior Chinese leaders to development of Tibet and ethnic Tibetan areas. The plan calls for improvement of rural Tibetan income to national standards by 2020 and free education for all rural Tibetan children. China has invested 310 billion yuan (about 45.6 billion U.S. dollars) in Tibet since 2001. ""Tibet's GDP was expected to reach 43.7 billion yuan in 2009, up 170 percent from that in 2000 and posting an annual growth of 12.3 percent over the past nine years."" In 1904, a British expedition to Tibet, spurred in part by a fear that Russia was extending its power into Tibet as part of The Great Game, invaded the country, hoping that negotiations with the 13th Dalai Lama would be more effective than with Chinese representatives. When the British-led invasion reached Tibet on December 12, 1903, an armed confrontation with the ethnic Tibetans resulted in the Massacre of Chumik Shenko, which resulted in 600 fatalities amongst the Tibetan forces, compared to only 12 on the British side. Afterwards, in 1904 Francis Younghusband imposed a treaty known as the Treaty of Lhasa, which was subsequently repudiated and was succeeded by a 1906 treaty signed between Britain and China. This period also saw some contacts with Jesuits and Capuchins from Europe, and in 1774 a Scottish nobleman, George Bogle, came to Shigatse to investigate prospects of trade for the British East India Company. However, in the 19th century the situation of foreigners in Tibet grew more tenuous. The British Empire was encroaching from northern India into the Himalayas, the Emirate of Afghanistan and the Russian Empire were expanding into Central Asia and each power became suspicious of the others' intentions in Tibet. Muslims have been living in Tibet since as early as the 8th or 9th century. In Tibetan cities, there are small communities of Muslims, known as Kachee (Kache), who trace their origin to immigrants from three main regions: Kashmir (Kachee Yul in ancient Tibetan), Ladakh and the Central Asian Turkic countries. Islamic influence in Tibet also came from Persia. After 1959 a group of Tibetan Muslims made a case for Indian nationality based on their historic roots to Kashmir and the Indian government declared all Tibetan Muslims Indian citizens later on that year. Other Muslim ethnic groups who have long inhabited Tibet include Hui, Salar, Dongxiang and Bonan. There is also a well established Chinese Muslim community (gya kachee), which traces its ancestry back to the Hui ethnic group of China. In 1877, the Protestant James Cameron from the China Inland Mission walked from Chongqing to Batang in Garzê Tibetan Autonomous Prefecture, Sichuan province, and ""brought the Gospel to the Tibetan people."" Beginning in the 20th century, in Diqing Tibetan Autonomous Prefecture in Yunnan, a large number of Lisu people and some Yi and Nu people converted to Christianity. Famous earlier missionaries include James O. Fraser, Alfred James Broomhall and Isobel Kuhn of the China Inland Mission, among others who were active in this area. Other pre-modern Chinese names for Tibet include Wusiguo (Chinese: 烏斯國; pinyin: Wūsīguó; cf. Tibetan dbus, Ü, [wyʔ˨˧˨]), Wusizang (Chinese: 烏斯藏; pinyin: wūsīzàng, cf. Tibetan dbus-gtsang, Ü-Tsang), Tubote (Chinese: 圖伯特; pinyin: Túbótè), and Tanggute (Chinese: 唐古忒; pinyin: Tánggǔtè, cf. Tangut). American Tibetologist Elliot Sperling has argued in favor of a recent tendency by some authors writing in Chinese to revive the term Tubote (simplified Chinese: 图伯特; traditional Chinese: 圖伯特; pinyin: Túbótè) for modern use in place of Xizang, on the grounds that Tubote more clearly includes the entire Tibetan plateau rather than simply the Tibet Autonomous Region.[citation needed] The atmosphere is severely dry nine months of the year, and average annual snowfall is only 18 inches (46 cm), due to the rain shadow effect. Western passes receive small amounts of fresh snow each year but remain traversible all year round. Low temperatures are prevalent throughout these western regions, where bleak desolation is unrelieved by any vegetation bigger than a low bush, and where wind sweeps unchecked across vast expanses of arid plain. The Indian monsoon exerts some influence on eastern Tibet. Northern Tibet is subject to high temperatures in the summer and intense cold in the winter. Tibetan music often involves chanting in Tibetan or Sanskrit, as an integral part of the religion. These chants are complex, often recitations of sacred texts or in celebration of various festivals. Yang chanting, performed without metrical timing, is accompanied by resonant drums and low, sustained syllables. Other styles include those unique to the various schools of Tibetan Buddhism, such as the classical music of the popular Gelugpa school, and the romantic music of the Nyingmapa, Sakyapa and Kagyupa schools. Tibet has various festivals that are commonly performed to worship the Buddha[citation needed] throughout the year. Losar is the Tibetan New Year Festival. Preparations for the festive event are manifested by special offerings to family shrine deities, painted doors with religious symbols, and other painstaking jobs done to prepare for the event. Tibetans eat Guthuk (barley noodle soup with filling) on New Year's Eve with their families. The Monlam Prayer Festival follows it in the first month of the Tibetan calendar, falling between the fourth and the eleventh days of the first Tibetan month. It involves dancing and participating in sports events, as well as sharing picnics. The event was established in 1049 by Tsong Khapa, the founder of the Dalai Lama and the Panchen Lama's order. In 1661 another Jesuit, Johann Grueber, crossed Tibet from Sining to Lhasa (where he spent a month), before heading on to Nepal. He was followed by others who actually built a church in Lhasa. These included the Jesuit Father Ippolito Desideri, 1716–1721, who gained a deep knowledge of Tibetan culture, language and Buddhism, and various Capuchins in 1707–1711, 1716–1733 and 1741–1745, Christianity was used by some Tibetan monarchs and their courts and the Karmapa sect lamas to counterbalance the influence of the Gelugpa sect in the 17th century until in 1745 when all the missionaries were expelled at the lama's insistence. Historically, the population of Tibet consisted of primarily ethnic Tibetans and some other ethnic groups. According to tradition the original ancestors of the Tibetan people, as represented by the six red bands in the Tibetan flag, are: the Se, Mu, Dong, Tong, Dru and Ra. Other traditional ethnic groups with significant population or with the majority of the ethnic group residing in Tibet (excluding a disputed area with India) include Bai people, Blang, Bonan, Dongxiang, Han, Hui people, Lhoba, Lisu people, Miao, Mongols, Monguor (Tu people), Menba (Monpa), Mosuo, Nakhi, Qiang, Nu people, Pumi, Salar, and Yi people. The language has numerous regional dialects which are generally not mutually intelligible. It is employed throughout the Tibetan plateau and Bhutan and is also spoken in parts of Nepal and northern India, such as Sikkim. In general, the dialects of central Tibet (including Lhasa), Kham, Amdo and some smaller nearby areas are considered Tibetan dialects. Other forms, particularly Dzongkha, Sikkimese, Sherpa, and Ladakhi, are considered by their speakers, largely for political reasons, to be separate languages. However, if the latter group of Tibetan-type languages are included in the calculation, then 'greater Tibetan' is spoken by approximately 6 million people across the Tibetan Plateau. Tibetan is also spoken by approximately 150,000 exile speakers who have fled from modern-day Tibet to India and other countries. The Tibetan Empire emerged in the 7th century, but with the fall of the empire the region soon divided into a variety of territories. The bulk of western and central Tibet (Ü-Tsang) was often at least nominally unified under a series of Tibetan governments in Lhasa, Shigatse, or nearby locations; these governments were at various times under Mongol and Chinese overlordship. The eastern regions of Kham and Amdo often maintained a more decentralized indigenous political structure, being divided among a number of small principalities and tribal groups, while also often falling more directly under Chinese rule after the Battle of Chamdo; most of this area was eventually incorporated into the Chinese provinces of Sichuan and Qinghai. The current borders of Tibet were generally established in the 18th century. Standing at 117 metres (384 feet) in height and 360 metres (1,180 feet) in width, the Potala Palace is the most important example of Tibetan architecture. Formerly the residence of the Dalai Lama, it contains over one thousand rooms within thirteen stories, and houses portraits of the past Dalai Lamas and statues of the Buddha. It is divided between the outer White Palace, which serves as the administrative quarters, and the inner Red Quarters, which houses the assembly hall of the Lamas, chapels, 10,000 shrines, and a vast library of Buddhist scriptures. The Potala Palace is a World Heritage Site, as is Norbulingka, the former summer residence of the Dalai Lama. The modern Standard Chinese exonym for the ethnic Tibetan region is Zangqu (Chinese: 藏区; pinyin: Zàngqū), which derives by metonymy from the Tsang region around Shigatse plus the addition of a Chinese suffix, 区 qū, which means ""area, district, region, ward"". Tibetan people, language, and culture, regardless of where they are from, are referred to as Zang (Chinese: 藏; pinyin: Zàng) although the geographical term Xīzàng is often limited to the Tibet Autonomous Region. The term Xīzàng was coined during the Qing dynasty in the reign of the Jiaqing Emperor (1796–1820) through the addition of a prefix meaning ""west"" (西 xī) to Zang. Roman Catholic Jesuits and Capuchins arrived from Europe in the 17th and 18th centuries. Portuguese missionaries Jesuit Father António de Andrade and Brother Manuel Marques first reached the kingdom of Gelu in western Tibet in 1624 and was welcomed by the royal family who allowed them to build a church later on. By 1627, there were about a hundred local converts in the Guge kingdom. Later on, Christianity was introduced to Rudok, Ladakh and Tsang and was welcomed by the ruler of the Tsang kingdom, where Andrade and his fellows established a Jesuit outpost at Shigatse in 1626. Although spoken Tibetan varies according to the region, the written language, based on Classical Tibetan, is consistent throughout. This is probably due to the long-standing influence of the Tibetan empire, whose rule embraced (and extended at times far beyond) the present Tibetan linguistic area, which runs from northern Pakistan in the west to Yunnan and Sichuan in the east, and from north of Qinghai Lake south as far as Bhutan. The Tibetan language has its own script which it shares with Ladakhi and Dzongkha, and which is derived from the ancient Indian Brāhmī script. The most important crop in Tibet is barley, and dough made from barley flour—called tsampa—is the staple food of Tibet. This is either rolled into noodles or made into steamed dumplings called momos. Meat dishes are likely to be yak, goat, or mutton, often dried, or cooked into a spicy stew with potatoes. Mustard seed is cultivated in Tibet, and therefore features heavily in its cuisine. Yak yogurt, butter and cheese are frequently eaten, and well-prepared yogurt is considered something of a prestige item. Butter tea is very popular to drink. The 5th Dalai Lama is known for unifying the Tibetan heartland under the control of the Gelug school of Tibetan Buddhism, after defeating the rival Kagyu and Jonang sects and the secular ruler, the Tsangpa prince, in a prolonged civil war. His efforts were successful in part because of aid from Güshi Khan, the Oirat leader of the Khoshut Khanate. With Güshi Khan as a largely uninvolved overlord, the 5th Dalai Lama and his intimates established a civil administration which is referred to by historians as the Lhasa state. This Tibetan regime or government is also referred to as the Ganden Phodrang. Religion is extremely important to the Tibetans and has a strong influence over all aspects of their lives. Bön is the ancient religion of Tibet, but has been almost eclipsed by Tibetan Buddhism, a distinctive form of Mahayana and Vajrayana, which was introduced into Tibet from the Sanskrit Buddhist tradition of northern India. Tibetan Buddhism is practiced not only in Tibet but also in Mongolia, parts of northern India, the Buryat Republic, the Tuva Republic, and in the Republic of Kalmykia and some other parts of China. During China's Cultural Revolution, nearly all Tibet's monasteries were ransacked and destroyed by the Red Guards. A few monasteries have begun to rebuild since the 1980s (with limited support from the Chinese government) and greater religious freedom has been granted – although it is still limited. Monks returned to monasteries across Tibet and monastic education resumed even though the number of monks imposed is strictly limited. Before the 1950s, between 10 and 20% of males in Tibet were monks. Tibet retained nominal power over religious and regional political affairs, while the Mongols managed a structural and administrative rule over the region, reinforced by the rare military intervention. This existed as a ""diarchic structure"" under the Yuan emperor, with power primarily in favor of the Mongols. Mongolian prince Khuden gained temporal power in Tibet in the 1240s and sponsored Sakya Pandita, whose seat became the capital of Tibet. Drogön Chögyal Phagpa, Sakya Pandita's nephew became Imperial Preceptor of Kublai Khan, founder of the Yuan dynasty. Between 1346 and 1354, Tai Situ Changchub Gyaltsen toppled the Sakya and founded the Phagmodrupa Dynasty. The following 80 years saw the founding of the Gelug school (also known as Yellow Hats) by the disciples of Je Tsongkhapa, and the founding of the important Ganden, Drepung and Sera monasteries near Lhasa. However, internal strife within the dynasty and the strong localism of the various fiefs and political-religious factions led to a long series of internal conflicts. The minister family Rinpungpa, based in Tsang (West Central Tibet), dominated politics after 1435. In 1565 they were overthrown by the Tsangpa Dynasty of Shigatse which expanded its power in different directions of Tibet in the following decades and favoured the Karma Kagyu sect. Tibet has some of the world's tallest mountains, with several of them making the top ten list. Mount Everest, located on the border with Nepal, is, at 8,848 metres (29,029 ft), the highest mountain on earth. Several major rivers have their source in the Tibetan Plateau (mostly in present-day Qinghai Province). These include the Yangtze, Yellow River, Indus River, Mekong, Ganges, Salween and the Yarlung Tsangpo River (Brahmaputra River). The Yarlung Tsangpo Grand Canyon, along the Yarlung Tsangpo River, is among the deepest and longest canyons in the world. The best-known medieval Chinese name for Tibet is Tubo (Chinese: 吐蕃 also written as 土蕃 or 土番; pinyin: Tǔbō or Tǔfān). This name first appears in Chinese characters as 土番 in the 7th century (Li Tai) and as 吐蕃 in the 10th-century (Old Book of Tang describing 608–609 emissaries from Tibetan King Namri Songtsen to Emperor Yang of Sui). In the Middle Chinese spoken during that period, as reconstructed by William H. Baxter, 土番 was pronounced thux-phjon and 吐蕃 was pronounced thux-pjon (with the x representing tone). In 821/822 CE Tibet and China signed a peace treaty. A bilingual account of this treaty, including details of the borders between the two countries, is inscribed on a stone pillar which stands outside the Jokhang temple in Lhasa. Tibet continued as a Central Asian empire until the mid-9th century, when a civil war over succession led to the collapse of imperial Tibet. The period that followed is known traditionally as the Era of Fragmentation, when political control over Tibet became divided between regional warlords and tribes with no dominant centralized authority. The Tibetan name for their land, Bod བོད་, means ""Tibet"" or ""Tibetan Plateau"", although it originally meant the central region around Lhasa, now known in Tibetan as Ü. The Standard Tibetan pronunciation of Bod, [pʰøʔ˨˧˨], is transcribed Bhö in Tournadre Phonetic Transcription, Bö in the THL Simplified Phonetic Transcription and Poi in Tibetan pinyin. Some scholars believe the first written reference to Bod ""Tibet"" was the ancient Bautai people recorded in the Egyptian Greek works Periplus of the Erythraean Sea (1st century CE) and Geographia (Ptolemy, 2nd century CE), itself from the Sanskrit form Bhauṭṭa of the Indian geographical tradition. The history of a unified Tibet begins with the rule of Songtsän Gampo (604–650 CE), who united parts of the Yarlung River Valley and founded the Tibetan Empire. He also brought in many reforms, and Tibetan power spread rapidly, creating a large and powerful empire. It is traditionally considered that his first wife was the Princess of Nepal, Bhrikuti, and that she played a great role in the establishment of Buddhism in Tibet. In 640 he married Princess Wencheng, the niece of the powerful Chinese emperor Taizong of Tang China. The Mongol Yuan dynasty, through the Bureau of Buddhist and Tibetan Affairs, or Xuanzheng Yuan, ruled Tibet through a top-level administrative department. One of the department's purposes was to select a dpon-chen ('great administrator'), usually appointed by the lama and confirmed by the Mongol emperor in Beijing. The Sakya lama retained a degree of autonomy, acting as the political authority of the region, while the dpon-chen held administrative and military power. Mongol rule of Tibet remained separate from the main provinces of China, but the region existed under the administration of the Yuan dynasty. If the Sakya lama ever came into conflict with the dpon-chen, the dpon-chen had the authority to send Chinese troops into the region. Tibet (i/tᵻˈbɛt/; Wylie: Bod, pronounced [pʰø̀ʔ]; Chinese: 西藏; pinyin: Xīzàng) is a region on the Tibetan Plateau in Asia. It is the traditional homeland of the Tibetan people as well as some other ethnic groups such as Monpa, Qiang and Lhoba peoples and is now also inhabited by considerable numbers of Han Chinese and Hui people. Tibet is the highest region on Earth, with an average elevation of 4,900 metres (16,000 ft). The highest elevation in Tibet is Mount Everest, earth's highest mountain rising 8,848 m (29,029 ft) above sea level."
Financial_crisis_of_2007%E2%80%9308,"Rapid increases in a number of commodity prices followed the collapse in the housing bubble. The price of oil nearly tripled from $50 to $147 from early 2007 to 2008, before plunging as the financial crisis began to take hold in late 2008. Experts debate the causes, with some attributing it to speculative flow of money from housing and other investments into commodities, some to monetary policy, and some to the increasing feeling of raw materials scarcity in a fast-growing world, leading to long positions taken on those markets, such as Chinese increasing presence in Africa. An increase in oil prices tends to divert a larger share of consumer spending into gasoline, which creates downward pressure on economic growth in oil importing countries, as wealth flows to oil-producing states. A pattern of spiking instability in the price of oil over the decade leading up to the price high of 2008 has been recently identified. The destabilizing effects of this price variance has been proposed as a contributory factor in the financial crisis. Testimony given to the Financial Crisis Inquiry Commission by Richard M. Bowen III on events during his tenure as the Business Chief Underwriter for Correspondent Lending in the Consumer Lending Group for Citigroup (where he was responsible for over 220 professional underwriters) suggests that by the final years of the U.S. housing bubble (2006–2007), the collapse of mortgage underwriting standards was endemic. His testimony stated that by 2006, 60% of mortgages purchased by Citi from some 1,600 mortgage companies were ""defective"" (were not underwritten to policy, or did not contain all policy-required documents) – this, despite the fact that each of these 1,600 originators was contractually responsible (certified via representations and warrantees) that its mortgage originations met Citi's standards. Moreover, during 2007, ""defective mortgages (from mortgage originators contractually bound to perform underwriting to Citi's standards) increased... to over 80% of production"". The output of goods and services produced by labor and property located in the United States—decreased at an annual rate of approximately 6% in the fourth quarter of 2008 and first quarter of 2009, versus activity in the year-ago periods. The U.S. unemployment rate increased to 10.1% by October 2009, the highest rate since 1983 and roughly twice the pre-crisis rate. The average hours per work week declined to 33, the lowest level since the government began collecting the data in 1964. With the decline of gross domestic product came the decline in innovation. With fewer resources to risk in creative destruction, the number of patent applications flat-lined. Compared to the previous 5 years of exponential increases in patent application, this stagnation correlates to the similar drop in GDP during the same time period. Lower interest rates encouraged borrowing. From 2000 to 2003, the Federal Reserve lowered the federal funds rate target from 6.5% to 1.0%. This was done to soften the effects of the collapse of the dot-com bubble and the September 2001 terrorist attacks, as well as to combat a perceived risk of deflation. As early as 2002 it was apparent that credit was fueling housing instead of business investment as some economists went so far as to advocate that the Fed ""needs to create a housing bubble to replace the Nasdaq bubble"". Moreover, empirical studies using data from advanced countries show that excessive credit growth contributed greatly to the severity of the crisis. IndyMac reported that during April 2008, Moody's and Standard & Poor's downgraded the ratings on a significant number of Mortgage-backed security (MBS) bonds including $160 million of those issued by IndyMac and which the bank retained in its MBS portfolio. IndyMac concluded that these downgrades would have negatively impacted the Company's risk-based capital ratio as of June 30, 2008. Had these lowered ratings been in effect at March 31, 2008, IndyMac concluded that the bank's capital ratio would have been 9.27% total risk-based. IndyMac warned that if its regulators found its capital position to have fallen below ""well capitalized"" (minimum 10% risk-based capital ratio) to ""adequately capitalized"" (8–10% risk-based capital ratio) the bank might no longer be able to use brokered deposits as a source of funds. During April 2009, U.S. Federal Reserve vice-chair Janet Yellen discussed these paradoxes: ""Once this massive credit crunch hit, it didn’t take long before we were in a recession. The recession, in turn, deepened the credit crunch as demand and employment fell, and credit losses of financial institutions surged. Indeed, we have been in the grips of precisely this adverse feedback loop for more than a year. A process of balance sheet deleveraging has spread to nearly every corner of the economy. Consumers are pulling back on purchases, especially on durable goods, to build their savings. Businesses are cancelling planned investments and laying off workers to preserve cash. And, financial institutions are shrinking assets to bolster capital and improve their chances of weathering the current storm. Once again, Minsky understood this dynamic. He spoke of the paradox of deleveraging, in which precautions that may be smart for individuals and firms—and indeed essential to return the economy to a normal state—nevertheless magnify the distress of the economy as a whole."" In separate testimony to Financial Crisis Inquiry Commission, officers of Clayton Holdings—the largest residential loan due diligence and securitization surveillance company in the United States and Europe—testified that Clayton's review of over 900,000 mortgages issued from January 2006 to June 2007 revealed that scarcely 54% of the loans met their originators’ underwriting standards. The analysis (conducted on behalf of 23 investment and commercial banks, including 7 ""too big to fail"" banks) additionally showed that 28% of the sampled loans did not meet the minimal standards of any issuer. Clayton's analysis further showed that 39% of these loans (i.e. those not meeting any issuer's minimal underwriting standards) were subsequently securitized and sold to investors. The U.S. Federal Reserve and central banks around the world have taken steps to expand money supplies to avoid the risk of a deflationary spiral, in which lower wages and higher unemployment lead to a self-reinforcing decline in global consumption. In addition, governments have enacted large fiscal stimulus packages, by borrowing and spending to offset the reduction in private sector demand caused by the crisis. The U.S. Federal Reserve's new and expanded liquidity facilities were intended to enable the central bank to fulfill its traditional lender-of-last-resort role during the crisis while mitigating stigma, broadening the set of institutions with access to liquidity, and increasing the flexibility with which institutions could tap such liquidity. In the early and mid-2000s, the Bush administration called numerous times for investigation into the safety and soundness of the GSEs and their swelling portfolio of subprime mortgages. On September 10, 2003, the House Financial Services Committee held a hearing at the urging of the administration to assess safety and soundness issues and to review a recent report by the Office of Federal Housing Enterprise Oversight (OFHEO) that had uncovered accounting discrepancies within the two entities. The hearings never resulted in new legislation or formal investigation of Fannie Mae and Freddie Mac, as many of the committee members refused to accept the report and instead rebuked OFHEO for their attempt at regulation. Some believe this was an early warning to the systemic risk that the growing market in subprime mortgages posed to the U.S. financial system that went unheeded. Others have pointed out that there were not enough of these loans made to cause a crisis of this magnitude. In an article in Portfolio Magazine, Michael Lewis spoke with one trader who noted that ""There weren’t enough Americans with [bad] credit taking out [bad loans] to satisfy investors' appetite for the end product."" Essentially, investment banks and hedge funds used financial innovation to enable large wagers to be made, far beyond the actual value of the underlying mortgage loans, using derivatives called credit default swaps, collateralized debt obligations and synthetic CDOs. In a Peabody Award winning program, NPR correspondents argued that a ""Giant Pool of Money"" (represented by $70 trillion in worldwide fixed income investments) sought higher yields than those offered by U.S. Treasury bonds early in the decade. This pool of money had roughly doubled in size from 2000 to 2007, yet the supply of relatively safe, income generating investments had not grown as fast. Investment banks on Wall Street answered this demand with products such as the mortgage-backed security and the collateralized debt obligation that were assigned safe ratings by the credit rating agencies. Prior to the crisis, financial institutions became highly leveraged, increasing their appetite for risky investments and reducing their resilience in case of losses. Much of this leverage was achieved using complex financial instruments such as off-balance sheet securitization and derivatives, which made it difficult for creditors and regulators to monitor and try to reduce financial institution risk levels. These instruments also made it virtually impossible to reorganize financial institutions in bankruptcy, and contributed to the need for government bailouts. Initially the companies affected were those directly involved in home construction and mortgage lending such as Northern Rock and Countrywide Financial, as they could no longer obtain financing through the credit markets. Over 100 mortgage lenders went bankrupt during 2007 and 2008. Concerns that investment bank Bear Stearns would collapse in March 2008 resulted in its fire-sale to JP Morgan Chase. The financial institution crisis hit its peak in September and October 2008. Several major institutions either failed, were acquired under duress, or were subject to government takeover. These included Lehman Brothers, Merrill Lynch, Fannie Mae, Freddie Mac, Washington Mutual, Wachovia, Citigroup, and AIG. On Oct. 6, 2008, three weeks after Lehman Brothers filed the largest bankruptcy in U.S. history, Lehman's former CEO found himself before Representative Henry A. Waxman, the California Democrat who chaired the House Committee on Oversight and Government Reform. Fuld said he was a victim of the collapse, blaming a ""crisis of confidence"" in the markets for dooming his firm. The U.S. Financial Crisis Inquiry Commission reported its findings in January 2011. It concluded that ""the crisis was avoidable and was caused by: widespread failures in financial regulation, including the Federal Reserve’s failure to stem the tide of toxic mortgages; dramatic breakdowns in corporate governance including too many financial firms acting recklessly and taking on too much risk; an explosive mix of excessive borrowing and risk by households and Wall Street that put the financial system on a collision course with crisis; key policy makers ill prepared for the crisis, lacking a full understanding of the financial system they oversaw; and systemic breaches in accountability and ethics at all levels"". IndyMac often made loans without verification of the borrower’s income or assets, and to borrowers with poor credit histories. Appraisals obtained by IndyMac on underlying collateral were often questionable as well. As an Alt-A lender, IndyMac’s business model was to offer loan products to fit the borrower’s needs, using an extensive array of risky option-adjustable-rate-mortgages (option ARMs), subprime loans, 80/20 loans, and other nontraditional products. Ultimately, loans were made to many borrowers who simply could not afford to make their payments. The thrift remained profitable only as long as it was able to sell those loans in the secondary mortgage market. IndyMac resisted efforts to regulate its involvement in those loans or tighten their issuing criteria: see the comment by Ruthann Melbourne, Chief Risk Officer, to the regulating agencies. This meant that nearly one-third of the U.S. lending mechanism was frozen and continued to be frozen into June 2009. According to the Brookings Institution, the traditional banking system does not have the capital to close this gap as of June 2009: ""It would take a number of years of strong profits to generate sufficient capital to support that additional lending volume"". The authors also indicate that some forms of securitization are ""likely to vanish forever, having been an artifact of excessively loose credit conditions"". While traditional banks have raised their lending standards, it was the collapse of the shadow banking system that is the primary cause of the reduction in funds available for borrowing. Behavior that may be optimal for an individual (e.g., saving more during adverse economic conditions) can be detrimental if too many individuals pursue the same behavior, as ultimately one person's consumption is another person's income. Too many consumers attempting to save (or pay down debt) simultaneously is called the paradox of thrift and can cause or deepen a recession. Economist Hyman Minsky also described a ""paradox of deleveraging"" as financial institutions that have too much leverage (debt relative to equity) cannot all de-leverage simultaneously without significant declines in the value of their assets. The collateralized debt obligation in particular enabled financial institutions to obtain investor funds to finance subprime and other lending, extending or increasing the housing bubble and generating large fees. This essentially places cash payments from multiple mortgages or other debt obligations into a single pool from which specific securities draw in a specific sequence of priority. Those securities first in line received investment-grade ratings from rating agencies. Securities with lower priority had lower credit ratings but theoretically a higher rate of return on the amount invested. Krugman's contention (that the growth of a commercial real estate bubble indicates that U.S. housing policy was not the cause of the crisis) is challenged by additional analysis. After researching the default of commercial loans during the financial crisis, Xudong An and Anthony B. Sanders reported (in December 2010): ""We find limited evidence that substantial deterioration in CMBS [commercial mortgage-backed securities] loan underwriting occurred prior to the crisis."" Other analysts support the contention that the crisis in commercial real estate and related lending took place after the crisis in residential real estate. Business journalist Kimberly Amadeo reports: ""The first signs of decline in residential real estate occurred in 2006. Three years later, commercial real estate started feeling the effects. Denice A. Gierach, a real estate attorney and CPA, wrote: During a period of tough competition between mortgage lenders for revenue and market share, and when the supply of creditworthy borrowers was limited, mortgage lenders relaxed underwriting standards and originated riskier mortgages to less creditworthy borrowers. In the view of some analysts, the relatively conservative government-sponsored enterprises (GSEs) policed mortgage originators and maintained relatively high underwriting standards prior to 2003. However, as market power shifted from securitizers to originators and as intense competition from private securitizers undermined GSE power, mortgage standards declined and risky loans proliferated. The worst loans were originated in 2004–2007, the years of the most intense competition between securitizers and the lowest market share for the GSEs. One of the first victims was Northern Rock, a medium-sized British bank. The highly leveraged nature of its business led the bank to request security from the Bank of England. This in turn led to investor panic and a bank run in mid-September 2007. Calls by Liberal Democrat Treasury Spokesman Vince Cable to nationalise the institution were initially ignored; in February 2008, however, the British government (having failed to find a private sector buyer) relented, and the bank was taken into public hands. Northern Rock's problems proved to be an early indication of the troubles that would soon befall other banks and financial institutions. The pricing of risk refers to the incremental compensation required by investors for taking on additional risk, which may be measured by interest rates or fees. Several scholars have argued that a lack of transparency about banks' risk exposures prevented markets from correctly pricing risk before the crisis, enabled the mortgage market to grow larger than it otherwise would have, and made the financial crisis far more disruptive than it would have been if risk levels had been disclosed in a straightforward, readily understandable format. United States President Barack Obama and key advisers introduced a series of regulatory proposals in June 2009. The proposals address consumer protection, executive pay, bank financial cushions or capital requirements, expanded regulation of the shadow banking system and derivatives, and enhanced authority for the Federal Reserve to safely wind-down systemically important institutions, among others. In January 2010, Obama proposed additional regulations limiting the ability of banks to engage in proprietary trading. The proposals were dubbed ""The Volcker Rule"", in recognition of Paul Volcker, who has publicly argued for the proposed changes. Market strategist Phil Dow believes distinctions exist ""between the current market malaise"" and the Great Depression. He says the Dow Jones average's fall of more than 50% over a period of 17 months is similar to a 54.7% fall in the Great Depression, followed by a total drop of 89% over the following 16 months. ""It's very troubling if you have a mirror image,"" said Dow. Floyd Norris, the chief financial correspondent of The New York Times, wrote in a blog entry in March 2009 that the decline has not been a mirror image of the Great Depression, explaining that although the decline amounts were nearly the same at the time, the rates of decline had started much faster in 2007, and that the past year had only ranked eighth among the worst recorded years of percentage drops in the Dow. The past two years ranked third, however. European regulators introduced Basel III regulations for banks. It increased capital ratios, limits on leverage, narrow definition of capital (to exclude subordinated debt), limit counter-party risk, and new liquidity requirements. Critics argue that Basel III doesn’t address the problem of faulty risk-weightings. Major banks suffered losses from AAA-rated created by financial engineering (which creates apparently risk-free assets out of high risk collateral) that required less capital according to Basel II. Lending to AA-rated sovereigns has a risk-weight of zero, thus increasing lending to governments and leading to the next crisis. Johan Norberg argues that regulations (Basel III among others) have indeed led to excessive lending to risky governments (see European sovereign-debt crisis) and the ECB pursues even more lending as the solution. Countrywide, sued by California Attorney General Jerry Brown for ""unfair business practices"" and ""false advertising"" was making high cost mortgages ""to homeowners with weak credit, adjustable rate mortgages (ARMs) that allowed homeowners to make interest-only payments"". When housing prices decreased, homeowners in ARMs then had little incentive to pay their monthly payments, since their home equity had disappeared. This caused Countrywide's financial condition to deteriorate, ultimately resulting in a decision by the Office of Thrift Supervision to seize the lender. Moreover, a conflict of interest between professional investment managers and their institutional clients, combined with a global glut in investment capital, led to bad investments by asset managers in over-priced credit assets. Professional investment managers generally are compensated based on the volume of client assets under management. There is, therefore, an incentive for asset managers to expand their assets under management in order to maximize their compensation. As the glut in global investment capital caused the yields on credit assets to decline, asset managers were faced with the choice of either investing in assets where returns did not reflect true credit risk or returning funds to clients. Many asset managers chose to continue to invest client funds in over-priced (under-yielding) investments, to the detriment of their clients, in order to maintain their assets under management. This choice was supported by a ""plausible deniability"" of the risks associated with subprime-based credit assets because the loss experience with early ""vintages"" of subprime loans was so low. There is a direct relationship between declines in wealth and declines in consumption and business investment, which along with government spending, represent the economic engine. Between June 2007 and November 2008, Americans lost an estimated average of more than a quarter of their collective net worth.[citation needed] By early November 2008, a broad U.S. stock index the S&P 500, was down 45% from its 2007 high. Housing prices had dropped 20% from their 2006 peak, with futures markets signaling a 30–35% potential drop. Total home equity in the United States, which was valued at $13 trillion at its peak in 2006, had dropped to $8.8 trillion by mid-2008 and was still falling in late 2008. Total retirement assets, Americans' second-largest household asset, dropped by 22%, from $10.3 trillion in 2006 to $8 trillion in mid-2008. During the same period, savings and investment assets (apart from retirement savings) lost $1.2 trillion and pension assets lost $1.3 trillion. Taken together, these losses total a staggering $8.3 trillion. Since peaking in the second quarter of 2007, household wealth is down $14 trillion. The financial crisis was not widely predicted by mainstream economists except Raghuram Rajan, who instead spoke of the Great Moderation. A number of heterodox economists predicted the crisis, with varying arguments. Dirk Bezemer in his research credits (with supporting argument and estimates of timing) 12 economists with predicting the crisis: Dean Baker (US), Wynne Godley (UK), Fred Harrison (UK), Michael Hudson (US), Eric Janszen (US), Steve Keen (Australia), Jakob Brøchner Madsen & Jens Kjaer Sørensen (Denmark), Kurt Richebächer (US), Nouriel Roubini (US), Peter Schiff (US), and Robert Shiller (US). Examples of other experts who gave indications of a financial crisis have also been given. Not surprisingly, the Austrian economic school regarded the crisis as a vindication and classic example of a predictable credit-fueled bubble that could not forestall the disregarded but inevitable effect of an artificial, manufactured laxity in monetary supply, a perspective that even former Fed Chair Alan Greenspan in Congressional testimony confessed himself forced to return to. The Brookings Institution reported in June 2009 that U.S. consumption accounted for more than a third of the growth in global consumption between 2000 and 2007. ""The US economy has been spending too much and borrowing too much for years and the rest of the world depended on the U.S. consumer as a source of global demand."" With a recession in the U.S. and the increased savings rate of U.S. consumers, declines in growth elsewhere have been dramatic. For the first quarter of 2009, the annualized rate of decline in GDP was 14.4% in Germany, 15.2% in Japan, 7.4% in the UK, 18% in Latvia, 9.8% in the Euro area and 21.5% for Mexico. CDO issuance grew from an estimated $20 billion in Q1 2004 to its peak of over $180 billion by Q1 2007, then declined back under $20 billion by Q1 2008. Further, the credit quality of CDO's declined from 2000 to 2007, as the level of subprime and other non-prime mortgage debt increased from 5% to 36% of CDO assets. As described in the section on subprime lending, the CDS and portfolio of CDS called synthetic CDO enabled a theoretically infinite amount to be wagered on the finite value of housing loans outstanding, provided that buyers and sellers of the derivatives could be found. For example, buying a CDS to insure a CDO ended up giving the seller the same risk as if they owned the CDO, when those CDO's became worthless. The bursting of the U.S. (United States) housing bubble, which peaked in 2004, caused the values of securities tied to U.S. real estate pricing to plummet, damaging financial institutions globally. The financial crisis was triggered by a complex interplay of policies that encouraged home ownership, providing easier access to loans for subprime borrowers, overvaluation of bundled subprime mortgages based on the theory that housing prices would continue to escalate, questionable trading practices on behalf of both buyers and sellers, compensation structures that prioritize short-term deal flow over long-term value creation, and a lack of adequate capital holdings from banks and insurance companies to back the financial commitments they were making. Questions regarding bank solvency, declines in credit availability and damaged investor confidence had an impact on global stock markets, where securities suffered large losses during 2008 and early 2009. Economies worldwide slowed during this period, as credit tightened and international trade declined. Governments and central banks responded with unprecedented fiscal stimulus, monetary policy expansion and institutional bailouts. In the U.S., Congress passed the American Recovery and Reinvestment Act of 2009. The term financial innovation refers to the ongoing development of financial products designed to achieve particular client objectives, such as offsetting a particular risk exposure (such as the default of a borrower) or to assist with obtaining financing. Examples pertinent to this crisis included: the adjustable-rate mortgage; the bundling of subprime mortgages into mortgage-backed securities (MBS) or collateralized debt obligations (CDO) for sale to investors, a type of securitization; and a form of credit insurance called credit default swaps (CDS). The usage of these products expanded dramatically in the years leading up to the crisis. These products vary in complexity and the ease with which they can be valued on the books of financial institutions. On November 3, 2008, the European Commission at Brussels predicted for 2009 an extremely weak growth of GDP, by 0.1%, for the countries of the Eurozone (France, Germany, Italy, Belgium etc.) and even negative number for the UK (−1.0%), Ireland and Spain. On November 6, the IMF at Washington, D.C., launched numbers predicting a worldwide recession by −0.3% for 2009, averaged over the developed economies. On the same day, the Bank of England and the European Central Bank, respectively, reduced their interest rates from 4.5% down to 3%, and from 3.75% down to 3.25%. As a consequence, starting from November 2008, several countries launched large ""help packages"" for their economies. Economist Mark Zandi testified to the Financial Crisis Inquiry Commission in January 2010: ""The securitization markets also remain impaired, as investors anticipate more loan losses. Investors are also uncertain about coming legal and accounting rule changes and regulatory reforms. Private bond issuance of residential and commercial mortgage-backed securities, asset-backed securities, and CDOs peaked in 2006 at close to $2 trillion...In 2009, private issuance was less than $150 billion, and almost all of it was asset-backed issuance supported by the Federal Reserve's TALF program to aid credit card, auto and small-business lenders. Issuance of residential and commercial mortgage-backed securities and CDOs remains dormant."" For a variety of reasons, market participants did not accurately measure the risk inherent with financial innovation such as MBS and CDOs or understand its impact on the overall stability of the financial system. For example, the pricing model for CDOs clearly did not reflect the level of risk they introduced into the system. Banks estimated that $450bn of CDO were sold between ""late 2005 to the middle of 2007""; among the $102bn of those that had been liquidated, JPMorgan estimated that the average recovery rate for ""high quality"" CDOs was approximately 32 cents on the dollar, while the recovery rate for mezzanine CDO was approximately five cents for every dollar. In testimony before the Senate Committee on Commerce, Science, and Transportation on June 3, 2008, former director of the CFTC Division of Trading & Markets (responsible for enforcement) Michael Greenberger specifically named the Atlanta-based IntercontinentalExchange, founded by Goldman Sachs, Morgan Stanley and BP as playing a key role in speculative run-up of oil futures prices traded off the regulated futures exchanges in London and New York. However, the IntercontinentalExchange (ICE) had been regulated by both European and U.S. authorities since its purchase of the International Petroleum Exchange in 2001. Mr Greenberger was later corrected on this matter. In his dissent to the majority report of the Financial Crisis Inquiry Commission, American Enterprise Institute fellow Peter J. Wallison stated his belief that the roots of the financial crisis can be traced directly and primarily to affordable housing policies initiated by HUD in the 1990s and to massive risky loan purchases by government-sponsored entities Fannie Mae and Freddie Mac. Later, based upon information in the SEC's December 2011 securities fraud case against 6 ex-executives of Fannie and Freddie, Peter Wallison and Edward Pinto estimated that, in 2008, Fannie and Freddie held 13 million substandard loans totaling over $2 trillion. Some developing countries that had seen strong economic growth saw significant slowdowns. For example, growth forecasts in Cambodia show a fall from more than 10% in 2007 to close to zero in 2009, and Kenya may achieve only 3–4% growth in 2009, down from 7% in 2007. According to the research by the Overseas Development Institute, reductions in growth can be attributed to falls in trade, commodity prices, investment and remittances sent from migrant workers (which reached a record $251 billion in 2007, but have fallen in many countries since). This has stark implications and has led to a dramatic rise in the number of households living below the poverty line, be it 300,000 in Bangladesh or 230,000 in Ghana. Especially states with a fragile political system have to fear that investors from Western states withdraw their money because of the crisis. Bruno Wenn of the German DEG recommends to provide a sound economic policymaking and good governance to attract new investors This boom in innovative financial products went hand in hand with more complexity. It multiplied the number of actors connected to a single mortgage (including mortgage brokers, specialized originators, the securitizers and their due diligence firms, managing agents and trading desks, and finally investors, insurances and providers of repo funding). With increasing distance from the underlying asset these actors relied more and more on indirect information (including FICO scores on creditworthiness, appraisals and due diligence checks by third party organizations, and most importantly the computer models of rating agencies and risk management desks). Instead of spreading risk this provided the ground for fraudulent acts, misjudgments and finally market collapse. In 2005 a group of computer scientists built a computational model for the mechanism of biased ratings produced by rating agencies, which turned out to be adequate to what actually happened in 2006–2008.[citation needed] As part of the housing and credit booms, the number of financial agreements called mortgage-backed securities (MBS) and collateralized debt obligations (CDO), which derived their value from mortgage payments and housing prices, greatly increased. Such financial innovation enabled institutions and investors around the world to invest in the U.S. housing market. As housing prices declined, major global financial institutions that had borrowed and invested heavily in subprime MBS reported significant losses. Feminist economists Ailsa McKay and Margunn Bjørnholt argue that the financial crisis and the response to it revealed a crisis of ideas in mainstream economics and within the economics profession, and call for a reshaping of both the economy, economic theory and the economics profession. They argue that such a reshaping should include new advances within feminist economics and ecological economics that take as their starting point the socially responsible, sensible and accountable subject in creating an economy and economic theories that fully acknowledge care for each other as well as the planet. Many causes for the financial crisis have been suggested, with varying weight assigned by experts. The U.S. Senate's Levin–Coburn Report concluded that the crisis was the result of ""high risk, complex financial products; undisclosed conflicts of interest; the failure of regulators, the credit rating agencies, and the market itself to rein in the excesses of Wall Street."" The Financial Crisis Inquiry Commission concluded that the financial crisis was avoidable and was caused by ""widespread failures in financial regulation and supervision"", ""dramatic failures of corporate governance and risk management at many systemically important financial institutions"", ""a combination of excessive borrowing, risky investments, and lack of transparency"" by financial institutions, ill preparation and inconsistent action by government that ""added to the uncertainty and panic"", a ""systemic breakdown in accountability and ethics"", ""collapsing mortgage-lending standards and the mortgage securitization pipeline"", deregulation of over-the-counter derivatives, especially credit default swaps, and ""the failures of credit rating agencies"" to correctly price risk. The 1999 repeal of the Glass-Steagall Act effectively removed the separation between investment banks and depository banks in the United States. Critics argued that credit rating agencies and investors failed to accurately price the risk involved with mortgage-related financial products, and that governments did not adjust their regulatory practices to address 21st-century financial markets. Research into the causes of the financial crisis has also focused on the role of interest rate spreads. Typical American families did not fare as well, nor did those ""wealthy-but-not wealthiest"" families just beneath the pyramid's top. On the other hand, half of the poorest families did not have wealth declines at all during the crisis. The Federal Reserve surveyed 4,000 households between 2007 and 2009, and found that the total wealth of 63 percent of all Americans declined in that period. 77 percent of the richest families had a decrease in total wealth, while only 50 percent of those on the bottom of the pyramid suffered a decrease. Another example relates to AIG, which insured obligations of various financial institutions through the usage of credit default swaps. The basic CDS transaction involved AIG receiving a premium in exchange for a promise to pay money to party A in the event party B defaulted. However, AIG did not have the financial strength to support its many CDS commitments as the crisis progressed and was taken over by the government in September 2008. U.S. taxpayers provided over $180 billion in government support to AIG during 2008 and early 2009, through which the money flowed to various counterparties to CDS transactions, including many large global financial institutions. As financial assets became more and more complex, and harder and harder to value, investors were reassured by the fact that both the international bond rating agencies and bank regulators, who came to rely on them, accepted as valid some complex mathematical models which theoretically showed the risks were much smaller than they actually proved to be. George Soros commented that ""The super-boom got out of hand when the new products became so complicated that the authorities could no longer calculate the risks and started relying on the risk management methods of the banks themselves. Similarly, the rating agencies relied on the information provided by the originators of synthetic products. It was a shocking abdication of responsibility."" Despite the dominance of the above formula, there are documented attempts of the financial industry, occurring before the crisis, to address the formula limitations, specifically the lack of dependence dynamics and the poor representation of extreme events. The volume ""Credit Correlation: Life After Copulas"", published in 2007 by World Scientific, summarizes a 2006 conference held by Merrill Lynch in London where several practitioners attempted to propose models rectifying some of the copula limitations. See also the article by Donnelly and Embrechts and the book by Brigo, Pallavicini and Torresetti, that reports relevant warnings and research on CDOs appeared in 2006. In November 2008, economist Dean Baker observed: ""There is a really good reason for tighter credit. Tens of millions of homeowners who had substantial equity in their homes two years ago have little or nothing today. Businesses are facing the worst downturn since the Great Depression. This matters for credit decisions. A homeowner with equity in her home is very unlikely to default on a car loan or credit card debt. They will draw on this equity rather than lose their car and/or have a default placed on their credit record. On the other hand, a homeowner who has no equity is a serious default risk. In the case of businesses, their creditworthiness depends on their future profits. Profit prospects look much worse in November 2008 than they did in November 2007... While many banks are obviously at the brink, consumers and businesses would be facing a much harder time getting credit right now even if the financial system were rock solid. The problem with the economy is the loss of close to $6 trillion in housing wealth and an even larger amount of stock wealth. Countering Krugman, Peter J. Wallison wrote: ""It is not true that every bubble—even a large bubble—has the potential to cause a financial crisis when it deflates."" Wallison notes that other developed countries had ""large bubbles during the 1997–2007 period"" but ""the losses associated with mortgage delinquencies and defaults when these bubbles deflated were far lower than the losses suffered in the United States when the 1997–2007 [bubble] deflated."" According to Wallison, the reason the U.S. residential housing bubble (as opposed to other types of bubbles) led to financial crisis was that it was supported by a huge number of substandard loans – generally with low or no downpayments. In a June 2008 speech, President and CEO of the New York Federal Reserve Bank Timothy Geithner—who in 2009 became Secretary of the United States Treasury—placed significant blame for the freezing of credit markets on a ""run"" on the entities in the ""parallel"" banking system, also called the shadow banking system. These entities became critical to the credit markets underpinning the financial system, but were not subject to the same regulatory controls. Further, these entities were vulnerable because of maturity mismatch, meaning that they borrowed short-term in liquid markets to purchase long-term, illiquid and risky assets. This meant that disruptions in credit markets would make them subject to rapid deleveraging, selling their long-term assets at depressed prices. He described the significance of these entities: On July 11, 2008, citing liquidity concerns, the FDIC put IndyMac Bank into conservatorship. A bridge bank, IndyMac Federal Bank, FSB, was established to assume control of IndyMac Bank's assets, its secured liabilities, and its insured deposit accounts. The FDIC announced plans to open IndyMac Federal Bank, FSB on July 14, 2008. Until then, depositors would have access their insured deposits through ATMs, their existing checks, and their existing debit cards. Telephone and Internet account access was restored when the bank reopened. The FDIC guarantees the funds of all insured accounts up to US$100,000, and has declared a special advance dividend to the roughly 10,000 depositors with funds in excess of the insured amount, guaranteeing 50% of any amounts in excess of $100,000. Yet, even with the pending sale of Indymac to IMB Management Holdings, an estimated 10,000 uninsured depositors of Indymac are still at a loss of over $270 million. A cover story in BusinessWeek magazine claims that economists mostly failed to predict the worst international economic crisis since the Great Depression of the 1930s. The Wharton School of the University of Pennsylvania's online business journal examines why economists failed to predict a major global financial crisis. Popular articles published in the mass media have led the general public to believe that the majority of economists have failed in their obligation to predict the financial crisis. For example, an article in the New York Times informs that economist Nouriel Roubini warned of such crisis as early as September 2006, and the article goes on to state that the profession of economics is bad at predicting recessions. According to The Guardian, Roubini was ridiculed for predicting a collapse of the housing market and worldwide recession, while The New York Times labelled him ""Dr. Doom"". By September 2008, average U.S. housing prices had declined by over 20% from their mid-2006 peak. As prices declined, borrowers with adjustable-rate mortgages could not refinance to avoid the higher payments associated with rising interest rates and began to default. During 2007, lenders began foreclosure proceedings on nearly 1.3 million properties, a 79% increase over 2006. This increased to 2.3 million in 2008, an 81% increase vs. 2007. By August 2008, 9.2% of all U.S. mortgages outstanding were either delinquent or in foreclosure. By September 2009, this had risen to 14.4%. Stock trader and financial risk engineer Nassim Nicholas Taleb, author of the 2007 book The Black Swan, spent years warning against the breakdown of the banking system in particular and the economy in general owing to their use of bad risk models and reliance on forecasting, and their reliance on bad models, and framed the problem as part of ""robustness and fragility"". He also took action against the establishment view by making a big financial bet on banking stocks and making a fortune from the crisis (""They didn't listen, so I took their money""). According to David Brooks from the New York Times, ""Taleb not only has an explanation for what’s happening, he saw it coming."" To other analysts the delay between CRA rule changes (in 1995) and the explosion of subprime lending is not surprising, and does not exonerate the CRA. They contend that there were two, connected causes to the crisis: the relaxation of underwriting standards in 1995 and the ultra-low interest rates initiated by the Federal Reserve after the terrorist attack on September 11, 2001. Both causes had to be in place before the crisis could take place. Critics also point out that publicly announced CRA loan commitments were massive, totaling $4.5 trillion in the years between 1994 and 2007. They also argue that the Federal Reserve’s classification of CRA loans as “prime” is based on the faulty and self-serving assumption that high-interest-rate loans (3 percentage points over average) equal “subprime” loans. The World Bank reported in February 2009 that the Arab World was far less severely affected by the credit crunch. With generally good balance of payments positions coming into the crisis or with alternative sources of financing for their large current account deficits, such as remittances, Foreign Direct Investment (FDI) or foreign aid, Arab countries were able to avoid going to the market in the latter part of 2008. This group is in the best position to absorb the economic shocks. They entered the crisis in exceptionally strong positions. This gives them a significant cushion against the global downturn. The greatest impact of the global economic crisis will come in the form of lower oil prices, which remains the single most important determinant of economic performance. Steadily declining oil prices would force them to draw down reserves and cut down on investments. Significantly lower oil prices could cause a reversal of economic performance as has been the case in past oil shocks. Initial impact will be seen on public finances and employment for foreign workers. The first visible institution to run into trouble in the United States was the Southern California–based IndyMac, a spin-off of Countrywide Financial. Before its failure, IndyMac Bank was the largest savings and loan association in the Los Angeles market and the seventh largest mortgage originator in the United States. The failure of IndyMac Bank on July 11, 2008, was the fourth largest bank failure in United States history up until the crisis precipitated even larger failures, and the second largest failure of a regulated thrift. IndyMac Bank's parent corporation was IndyMac Bancorp until the FDIC seized IndyMac Bank. IndyMac Bancorp filed for Chapter 7 bankruptcy in July 2008. It threatened the collapse of large financial institutions, which was prevented by the bailout of banks by national governments, but stock markets still dropped worldwide. In many areas, the housing market also suffered, resulting in evictions, foreclosures and prolonged unemployment. The crisis played a significant role in the failure of key businesses, declines in consumer wealth estimated in trillions of U.S. dollars, and a downturn in economic activity leading to the 2008–2012 global recession and contributing to the European sovereign-debt crisis. The active phase of the crisis, which manifested as a liquidity crisis, can be dated from August 9, 2007, when BNP Paribas terminated withdrawals from three hedge funds citing ""a complete evaporation of liquidity"". A 2000 United States Department of the Treasury study of lending trends for 305 cities from 1993 to 1998 showed that $467 billion of mortgage lending was made by Community Reinvestment Act (CRA)-covered lenders into low and mid level income (LMI) borrowers and neighborhoods, representing 10% of all U.S. mortgage lending during the period. The majority of these were prime loans. Sub-prime loans made by CRA-covered institutions constituted a 3% market share of LMI loans in 1998, but in the run-up to the crisis, fully 25% of all sub-prime lending occurred at CRA-covered institutions and another 25% of sub-prime loans had some connection with CRA. In addition, an analysis by the Federal Reserve Bank of Dallas in 2009, however, concluded that the CRA was not responsible for the mortgage loan crisis, pointing out that CRA rules have been in place since 1995 whereas the poor lending emerged only a decade later. Furthermore, most sub-prime loans were not made to the LMI borrowers targeted by the CRA, especially in the years 2005–2006 leading up to the crisis. Nor did it find any evidence that lending under the CRA rules increased delinquency rates or that the CRA indirectly influenced independent mortgage lenders to ramp up sub-prime lending. The U.S. recession that began in December 2007 ended in June 2009, according to the U.S. National Bureau of Economic Research (NBER) and the financial crisis appears to have ended about the same time. In April 2009 TIME magazine declared ""More Quickly Than It Began, The Banking Crisis Is Over."" The United States Financial Crisis Inquiry Commission dates the crisis to 2008. President Barack Obama declared on January 27, 2010, ""the markets are now stabilized, and we've recovered most of the money we spent on the banks."" Current Governor of the Reserve Bank of India Raghuram Rajan had predicted the crisis in 2005 when he became chief economist at the International Monetary Fund.In 2005, at a celebration honouring Alan Greenspan, who was about to retire as chairman of the US Federal Reserve, Rajan delivered a controversial paper that was critical of the financial sector. In that paper, ""Has Financial Development Made the World Riskier?"", Rajan ""argued that disaster might loom."" Rajan argued that financial sector managers were encouraged to ""take risks that generate severe adverse consequences with small probability but, in return, offer generous compensation the rest of the time. These risks are known as tail risks. But perhaps the most important concern is whether banks will be able to provide liquidity to financial markets so that if the tail risk does materialise, financial positions can be unwound and losses allocated so that the consequences to the real economy are minimised."" The Fed then raised the Fed funds rate significantly between July 2004 and July 2006. This contributed to an increase in 1-year and 5-year adjustable-rate mortgage (ARM) rates, making ARM interest rate resets more expensive for homeowners. This may have also contributed to the deflating of the housing bubble, as asset prices generally move inversely to interest rates, and it became riskier to speculate in housing. U.S. housing and financial assets dramatically declined in value after the housing bubble burst. Advanced economies led global economic growth prior to the financial crisis with ""emerging"" and ""developing"" economies lagging behind. The crisis completely overturned this relationship. The International Monetary Fund found that ""advanced"" economies accounted for only 31% of global GDP while emerging and developing economies accounted for 69% of global GDP from 2007 to 2014. In the tables, the names of emergent economies are shown in boldface type, while the names of developed economies are in Roman (regular) type. Bernanke explained that between 1996 and 2004, the U.S. current account deficit increased by $650 billion, from 1.5% to 5.8% of GDP. Financing these deficits required the country to borrow large sums from abroad, much of it from countries running trade surpluses. These were mainly the emerging economies in Asia and oil-exporting nations. The balance of payments identity requires that a country (such as the U.S.) running a current account deficit also have a capital account (investment) surplus of the same amount. Hence large and growing amounts of foreign funds (capital) flowed into the U.S. to finance its imports. Predatory lending refers to the practice of unscrupulous lenders, enticing borrowers to enter into ""unsafe"" or ""unsound"" secured loans for inappropriate purposes. A classic bait-and-switch method was used by Countrywide Financial, advertising low interest rates for home refinancing. Such loans were written into extensively detailed contracts, and swapped for more expensive loan products on the day of closing. Whereas the advertisement might state that 1% or 1.5% interest would be charged, the consumer would be put into an adjustable rate mortgage (ARM) in which the interest charged would be greater than the amount of interest paid. This created negative amortization, which the credit consumer might not notice until long after the loan transaction had been consummated. From 2004 to 2007, the top five U.S. investment banks each significantly increased their financial leverage (see diagram), which increased their vulnerability to a financial shock. Changes in capital requirements, intended to keep U.S. banks competitive with their European counterparts, allowed lower risk weightings for AAA securities. The shift from first-loss tranches to AAA tranches was seen by regulators as a risk reduction that compensated the higher leverage. These five institutions reported over $4.1 trillion in debt for fiscal year 2007, about 30% of USA nominal GDP for 2007. Lehman Brothers went bankrupt and was liquidated, Bear Stearns and Merrill Lynch were sold at fire-sale prices, and Goldman Sachs and Morgan Stanley became commercial banks, subjecting themselves to more stringent regulation. With the exception of Lehman, these companies required or received government support. Lehman reported that it had been in talks with Bank of America and Barclays for the company's possible sale. However, both Barclays and Bank of America ultimately declined to purchase the entire company. Critics such as economist Paul Krugman and U.S. Treasury Secretary Timothy Geithner have argued that the regulatory framework did not keep pace with financial innovation, such as the increasing importance of the shadow banking system, derivatives and off-balance sheet financing. A recent OECD study suggest that bank regulation based on the Basel accords encourage unconventional business practices and contributed to or even reinforced the financial crisis. In other cases, laws were changed or enforcement weakened in parts of the financial system. Key examples include: Economist Paul Krugman and U.S. Treasury Secretary Timothy Geithner explain the credit crisis via the implosion of the shadow banking system, which had grown to nearly equal the importance of the traditional commercial banking sector as described above. Without the ability to obtain investor funds in exchange for most types of mortgage-backed securities or asset-backed commercial paper, investment banks and other entities in the shadow banking system could not provide funds to mortgage firms and other corporations. This credit freeze brought the global financial system to the brink of collapse. The response of the Federal Reserve, the European Central Bank, the Bank of England and other central banks was immediate and dramatic. During the last quarter of 2008, these central banks purchased US$2.5 trillion of government debt and troubled private assets from banks. This was the largest liquidity injection into the credit market, and the largest monetary policy action, in world history. Following a model initiated by the United Kingdom bank rescue package, the governments of European nations and the USA guaranteed the debt issued by their banks and raised the capital of their national banking systems, ultimately purchasing $1.5 trillion newly issued preferred stock in their major banks. In October 2010, Nobel laureate Joseph Stiglitz explained how the U.S. Federal Reserve was implementing another monetary policy —creating currency— as a method to combat the liquidity trap. By creating $600 billion and inserting[clarification needed] this directly into banks, the Federal Reserve intended to spur banks to finance more domestic loans and refinance mortgages. However, banks instead were spending the money in more profitable areas by investing internationally in emerging markets. Banks were also investing in foreign currencies, which Stiglitz and others point out may lead to currency wars while China redirects its currency holdings away from the United States. Senator Charles Schumer (D-NY) would later point out that brokered deposits made up more than 37 percent of IndyMac's total deposits and ask the Federal Deposit Insurance Corporation (FDIC) whether it had considered ordering IndyMac to reduce its reliance on these deposits. With $18.9 billion in total deposits reported on March 31, Senator Schumer would have been referring to a little over $7 billion in brokered deposits. While the breakout of maturities of these deposits is not known exactly, a simple averaging would have put the threat of brokered deposits loss to IndyMac at $500 million a month, had the regulator disallowed IndyMac from acquiring new brokered deposits on June 30. Falling prices also resulted in homes worth less than the mortgage loan, providing a financial incentive to enter foreclosure. The ongoing foreclosure epidemic that began in late 2006 in the U.S. continues to drain wealth from consumers and erodes the financial strength of banking institutions. Defaults and losses on other loan types also increased significantly as the crisis expanded from the housing market to other parts of the economy. Total losses are estimated in the trillions of U.S. dollars globally. The U.S. Senate passed a reform bill in May 2010, following the House which passed a bill in December 2009. These bills must now be reconciled. The New York Times provided a comparative summary of the features of the two bills, which address to varying extent the principles enumerated by the Obama administration. For instance, the Volcker Rule against proprietary trading is not part of the legislation, though in the Senate bill regulators have the discretion but not the obligation to prohibit these trades. These institutions, as well as certain regulated banks, had also assumed significant debt burdens while providing the loans described above and did not have a financial cushion sufficient to absorb large loan defaults or MBS losses. These losses impacted the ability of financial institutions to lend, slowing economic activity. Concerns regarding the stability of key financial institutions drove central banks to provide funds to encourage lending and restore faith in the commercial paper markets, which are integral to funding business operations. Governments also bailed out key financial institutions and implemented economic stimulus programs, assuming significant additional financial commitments. While the housing and credit bubbles were building, a series of factors caused the financial system to both expand and become increasingly fragile, a process called financialization. U.S. Government policy from the 1970s onward has emphasized deregulation to encourage business, which resulted in less oversight of activities and less disclosure of information about new activities undertaken by banks and other evolving financial institutions. Thus, policymakers did not immediately recognize the increasingly important role played by financial institutions such as investment banks and hedge funds, also known as the shadow banking system. Some experts believe these institutions had become as important as commercial (depository) banks in providing credit to the U.S. economy, but they were not subject to the same regulations. In September 2008, the crisis hit its most critical stage. There was the equivalent of a bank run on the money market funds, which frequently invest in commercial paper issued by corporations to fund their operations and payrolls. Withdrawal from money markets were $144.5 billion during one week, versus $7.1 billion the week prior. This interrupted the ability of corporations to rollover (replace) their short-term debt. The U.S. government responded by extending insurance for money market accounts analogous to bank deposit insurance via a temporary guarantee and with Federal Reserve programs to purchase commercial paper. The TED spread, an indicator of perceived credit risk in the general economy, spiked up in July 2007, remained volatile for a year, then spiked even higher in September 2008, reaching a record 4.65% on October 10, 2008. The majority report of the Financial Crisis Inquiry Commission, written by the six Democratic appointees, the minority report, written by 3 of the 4 Republican appointees, studies by Federal Reserve economists, and the work of several independent scholars generally contend that government affordable housing policy was not the primary cause of the financial crisis. Although they concede that governmental policies had some role in causing the crisis, they contend that GSE loans performed better than loans securitized by private investment banks, and performed better than some loans originated by institutions that held loans in their own portfolios. Paul Krugman has even claimed that the GSE never purchased subprime loans – a claim that is widely disputed. Several commentators have suggested that if the liquidity crisis continues, an extended recession or worse could occur. The continuing development of the crisis has prompted fears of a global economic collapse although there are now many cautiously optimistic forecasters in addition to some prominent sources who remain negative. The financial crisis is likely to yield the biggest banking shakeout since the savings-and-loan meltdown. Investment bank UBS stated on October 6 that 2008 would see a clear global recession, with recovery unlikely for at least two years. Three days later UBS economists announced that the ""beginning of the end"" of the crisis had begun, with the world starting to make the necessary actions to fix the crisis: capital injection by governments; injection made systemically; interest rate cuts to help borrowers. The United Kingdom had started systemic injection, and the world's central banks were now cutting interest rates. UBS emphasized the United States needed to implement systemic injection. UBS further emphasized that this fixes only the financial crisis, but that in economic terms ""the worst is still to come"". UBS quantified their expected recession durations on October 16: the Eurozone's would last two quarters, the United States' would last three quarters, and the United Kingdom's would last four quarters. The economic crisis in Iceland involved all three of the country's major banks. Relative to the size of its economy, Iceland’s banking collapse is the largest suffered by any country in economic history. When home prices declined in the latter half of 2007 and the secondary mortgage market collapsed, IndyMac was forced to hold $10.7 billion of loans it could not sell in the secondary market. Its reduced liquidity was further exacerbated in late June 2008 when account holders withdrew $1.55 billion or about 7.5% of IndyMac's deposits. This “run” on the thrift followed the public release of a letter from Senator Charles Schumer to the FDIC and OTS. The letter outlined the Senator’s concerns with IndyMac. While the run was a contributing factor in the timing of IndyMac’s demise, the underlying cause of the failure was the unsafe and unsound manner in which the thrift was operated. The securitization markets supported by the shadow banking system started to close down in the spring of 2007 and nearly shut-down in the fall of 2008. More than a third of the private credit markets thus became unavailable as a source of funds. According to the Brookings Institution, the traditional banking system does not have the capital to close this gap as of June 2009: ""It would take a number of years of strong profits to generate sufficient capital to support that additional lending volume."" The authors also indicate that some forms of securitization are ""likely to vanish forever, having been an artifact of excessively loose credit conditions."""
Catalan_language,"Catalan dialects are relatively uniform, and are mutually intelligible. They are divided into two blocks, Eastern and Western, differing mostly in pronunciation. The terms ""Catalan"" and ""Valencian"" (respectively used in Catalonia and the Valencian Community) are two different varieties of the same language. There are two institutions regulating the two standard varieties, the Institute of Catalan Studies in Catalonia and the Valencian Academy of the Language in Valencia. In the Balearic Islands, IEC's standard is used but adapted for the Balearic dialect by the University of the Balearic Islands's philological section. In this way, for instance, IEC says it is correct writing cantam as much as cantem ('we sing') but the University says that the priority form in the Balearic Islands must be ""cantam"" in all fields. Another feature of the Balearic standard is the non-ending in the 1st person singular present indicative: jo compr ('I buy'), jo tem ('I fear'), jo dorm ('I sleep'). In 2011, the Aragonese government passed a decree for the establishment of a new language regulator of Catalan in La Franja (the so-called Catalan-speaking areas of Aragon). The new entity, designated as Acadèmia Aragonesa del Català, shall allow a facultative education in Catalan and a standardization of the Catalan language in La Franja. The inflection of determinatives is complex, specially because of the high number of elisions, but is similar to the neighboring languages. Catalan has more contractions of preposition + article than Spanish, like dels (""of + the [plural]""), but not as many as Italian (which has sul, col, nel, etc.). In contrast with other Romance languages, Catalan has many monosyllabic words; and those ending in a wide variety of consonants and some consonant clusters. Also, Catalan has final obstruent devoicing, thus featuring many couplets like amic ""(male friend"") vs. amiga (""female friend""). With the union of the crowns of Castille and Aragon (1479), the use of Spanish gradually became more prestigious. Starting in the 16th century, Catalan literature experienced a decline, the language came under the influence of Spanish, and the urban and literary classes became bilingual. The most notable difference between both standards is some tonic ⟨e⟩ accentuation, for instance: francès, anglès (IEC) – francés, anglés (AVL). Nevertheless, AVL's standard keeps the grave accent ⟨è⟩, without pronouncing this ⟨e⟩ as /ɛ/, in some words like: què ('what'), or València. Other divergences include the use of ⟨tl⟩ (AVL) in some words instead of ⟨tll⟩ like in ametla/ametlla ('almond'), espatla/espatlla ('back'), the use of elided demonstratives (este 'this', eixe 'that') in the same level as reinforced ones (aquest, aqueix) or the use of many verbal forms common in Valencian, and some of these common in the rest of Western Catalan too, like subjunctive mood or inchoative conjugation in -ix- at the same level as -eix- or the priority use of -e morpheme in 1st person singular in present indicative (-ar verbs): jo compre instead of jo compro ('I buy'). In parallel, however, the 19th century saw a Catalan literary revival (Renaixença), which has continued up to the present day. This period starts with Aribau's Ode to the Homeland (1833); followed in the second half of the 19th century, and the early 20th by the work of Verdaguer (poetry), Oller (realist novel), and Guimerà (drama). Since the Spanish transition to democracy (1975–1982), Catalan has been recognized as an official language, language of education, and language of mass media, all of which have contributed to its increased prestige. There is no parallel in Europe of such a large, bilingual, non-state speech community. During the 11th and 12th centuries the Catalan rulers expanded up to north of the Ebro river, and in the 13th century they conquered the Land of Valencia and the Balearic Islands. The city of Alghero in Sardinia was repopulated with Catalan speakers in the 14th century. The language also reached Murcia, which became Spanish-speaking in the 15th century. As in the other Western Romance languages, the main plural expression is the suffix -s, which may create morphological alternations similar to the ones found in gender inflection, albeit more rarely. The most important one is the addition of -o- before certain consonant groups, a phonetic phenomenon that does not affect feminine forms: el pols/els polsos (""the pulse""/""the pulses"") vs. la pols/les pols (""the dust""/""the dusts""). In Central Catalan, unstressed vowels reduce to three: /a e ɛ/ > [ə]; /o ɔ u/ > [u]; /i/ remains distinct. The other dialects have different vowel reduction processes (see the section pronunciation of dialects in this article). The same happens with Arabic loanwords. Thus, Catalan alfàbia ""large earthenware jar"" and rajola ""tile"", of Arabic origin, contrast with Spanish tinaja and teja, of Latin origin; whereas Catalan oli ""oil"" and oliva ""olive"", of Latin origin, contrast with Spanish aceite and aceituna. However, the Arabic element in Spanish is generally much more prevalent. In Eastern Catalan (except Majorcan), unstressed vowels reduce to three: /a e ɛ/ > [ə]; /o ɔ u/ > [u]; /i/ remains distinct. There are a few instances of unreduced [e], [o] in some words. Alguerese has lowered [ə] to [a]. Literary Catalan allows the use of words from different dialects, except those of very restricted use. However, from the 19th century onwards, there is a tendency of favoring words of Northern dialects in detriment of others, even though nowadays there is a greater freedom of choice. The ascription of Catalan to the Occitano-Romance branch of Gallo-Romance languages is not shared by all linguists and philologists, particularly among Spanish ones, such as Ramón Menéndez Pidal. In the Alicante province Catalan is being replaced by Spanish, and in Alghero by Italian. There are also well ingrained diglossic attitudes against Catalan in the Valencian Community, Ibiza, and to a lesser extent, in the rest of the Balearic islands. Catalan shares many traits with the other neighboring Romance languages (Italian, Sardinian, Occitan, and Spanish). However, despite being mostly situated in the Iberian Peninsula, Catalan has marked differences with the Ibero-Romance group (Spanish and Portuguese) in terms of pronunciation, grammar, and especially vocabulary; showing instead its closest affinity with Occitan and to a lesser extent Gallo-Romance (French, Franco-Provençal, Gallo-Italian). There is a tendency to abandon traditionally gender-invariable adjectives in favour of marked ones, something prevalent in Occitan and French. Thus, one can find bullent/bullenta (""boiling"") in contrast with traditional bullent/bullent. This flexibility allows Catalan to use extraposition extensively, much more than French or Spanish. Thus, Catalan can have m'hi recomanaren (""they recommended me to him""), whereas in French one must say ils m'ont recommandé à lui, and Spanish me recomendaron a él. This allows the placement of almost any nominal term as a sentence topic, without having to use so often the passive voice (as in French or English), or identifying the direct object with a preposition (as in Spanish). The Germanic superstrate has had different outcomes in Spanish and Catalan. For example, Catalan fang ""mud"" and rostir ""to roast"", of Germanic origin, contrast with Spanish lodo and asar, of Latin origin; whereas Catalan filosa ""spinning wheel"" and pols ""temple"", of Latin origin, contrast with Spanish rueca and sien, of Germanic origin. The process of morphological derivation in Catalan follows the same principles as the other Romance languages, where agglutination is common. Many times, several affixes are appended to a preexisting lexeme, and some sound alternations can occur, for example elèctric [əˈlɛktrik] (""electrical"") vs. electricitat [ələktrisiˈtat]. Prefixes are usually appended to verbs, for as in preveure (""foresee""). Catalan pronouns exhibit T–V distinction, like all other Romance languages (and most European languages, but not Modern English). This feature implies the use of a different set of second person pronouns for formality. Catalan bears varying degrees of similarity to the linguistic varieties subsumed under the cover term Occitan language (see also differences between Occitan and Catalan and Gallo-Romance languages). Thus, as it should be expected from closely related languages, Catalan today shares many traits with other Romance languages. Following the French capture of Algeria (1833), that region saw several waves of Catalan-speaking settlers. People from the Spanish Alacant province settled around Oran, whereas Algiers received immigration from Northern Catalonia and Minorca. Their speech was known as patuet. By 1911, the number of Catalan speakers was around 100,000. After the declaration of independence of Algeria in 1962, almost all the Catalan speakers fled to Northern Catalonia (as Pieds-Noirs) or Alacant. Despite the position of the official organizations, an opinion poll carried out between 2001 and 2004 showed that the majority of the Valencian people consider Valencian different from Catalan. This position is promoted by people who do not use Valencian regularly. Furthermore, the data indicates that younger generations educated in Valencian are much less likely to hold these views. A minority of Valencian scholars active in fields other than linguistics defends the position of the Royal Academy of Valencian Culture (Acadèmia de Cultura Valenciana, RACV), which uses for Valencian a standard independent from Catalan. Since the Spanish transition to democracy (1975–1982), Catalan has been institutionalizated as an official language, language of education, and language of mass media; all of which have contributed to its increased prestige. In Catalonia, there is no parallel of a large, bilingual, European, non-state speech community. The teaching of Catalan is mandatory in all schools, but it is possible to use Spanish for studying in the public education system of Catalonia in two situations, if the teacher assigned to a class chooses to use Spanish, or during the learning process of one or some recently arrived students.  There is also some intergenerational shift towards Catalan. Catalan verbs are traditionally divided into three conjugations, with vowel themes -a-, -e-, -i-, the last two being split into two subtypes. However, this division is mostly theoretical. Only the first conjugation is nowadays productive (with about 3500 common verbs), whereas the third (the subtype of servir, with about 700 common verbs) is semiproductive. The verbs of the second conjugation are fewer than 100, and it is not possible to create new ones, except by compounding. In Alghero, the IEC has adapted its standard to the Alguerese dialect. In this standard one can find, among other features: the definite article lo instead of el, special possessive pronouns and determinants la mia ('mine'), lo sou/la sua ('his/her'), lo tou/la tua ('yours'), and so on, the use of -v- /v/ in the imperfect tense in all conjugations: cantava, creixiva, llegiva; the use of many archaic words, usual words in Alguerese: manco instead of menys ('less'), calqui u instead of algú ('someone'), qual/quala instead of quin/quina ('which'), and so on; and the adaptation of weak pronouns. Like all the Romance languages, Catalan verbal inflection is more complex than the nominal. Suffixation is omnipresent, whereas morphological alternations play a secondary role. Vowel alternances are active, as well as infixation and suppletion. However, these are not as productive as in Spanish, and are mostly restricted to irregular verbs. Linguists, including Valencian scholars, deal with Catalan and Valencian as the same language. The official regulating body of the language of the Valencian Community, the Valencian Academy of Language (Acadèmia Valenciana de la Llengua, AVL) declares the linguistic unity between Valencian and Catalan varieties. The morphology of Catalan personal pronouns is complex, specially in unstressed forms, which are numerous (13 distinct forms, compared to 11 in Spanish or 9 in Italian).  Features include the gender-neutral ho and the great degree of freedom when combining different unstressed pronouns (65 combinations). In nouns and adjectives, maintenance of /n/ of medieval plurals in proparoxytone words.
E.g. hòmens 'men', jóvens 'youth'. In nouns and adjectives, loss of /n/ of medieval plurals in proparoxytone words.
E.g. homes 'men', joves 'youth'. The Catalan verbal system is basically common to all Western Romance, except that most dialects have replaced the synthetic indicative perfect with a periphrastic form of anar (""to go"") + infinitive. Catalan shares many traits with its neighboring Romance languages. However, despite being mostly situated in the Iberian Peninsula, Catalan differs more from Iberian Romance (such as Spanish and Portuguese) in terms of vocabulary, pronunciation, and grammar than from Gallo-Romance (Occitan, French, Gallo-Italic languages, etc.). These similarities are most notable with Occitan. Shortly after the French Revolution (1789), the French First Republic prohibited official use of, and enacted discriminating policies against, the nonstandard languages of France (patois), such as Catalan, Alsatian, Breton, Occitan, Flemish, and Basque. Central, Western, and Balearic differ in the lexical incidence of stressed /e/ and /ɛ/. Usually, words with /ɛ/ in Central Catalan correspond to /ə/ in Balearic and /e/ in Western Catalan. Words with /e/ in Balearic almost always have /e/ in Central and Western Catalan as well.[vague] As a result, Central Catalan has a much higher incidence of /e/. With the Treaty of the Pyrenees (1659), Spain ceded the northern part of Catalonia to France, and soon thereafter the local Catalan varieties came under the influence of French, which in 1700 became the sole official language of the region. The endonym is pronounced /kə.təˈɫa/ in the Eastern Catalan dialects, and /ka.taˈɫa/ in the Western dialects. In the Valencian Community, the term valencià (/va.len.siˈa/) is frequently used instead. The names ""Catalan"" and ""Valencian"" are two names for the same language. See also status of Valencian below. In Majorcan, unstressed vowels reduce to four: /a e ɛ/ follow the Eastern Catalan reduction pattern; however /o ɔ/ reduce to [o], with /u/ remaining distinct, as in Western Catalan. In the Low Middle Ages, Catalan went through a golden age, reaching a peak of maturity and cultural richness. Examples include the work of Majorcan polymath Ramon Llull (1232–1315), the Four Great Chronicles (13th–14th centuries), and the Valencian school of poetry culminating in Ausiàs March (1397–1459). By the 15th century, the city of Valencia had become the sociocultural center of the Crown of Aragon, and Catalan was present all over the Mediterranean world. During this period, the Royal Chancery propagated a highly standardized language. Catalan was widely used as an official language in Sicily until the 15th century, and in Sardinia until the 17th. During this period, the language was what Costa Carreras terms ""one of the 'great languages' of medieval Europe"". Despite its relative lexical unity, the two dialectal blocks of Catalan (Eastern and Western) show some differences in word choices. Any lexical divergence within any of the two groups can be explained as an archaism. Also, usually Central Catalan acts as an innovative element. Catalan is split in two major dialectal blocks: Eastern Catalan, and Western Catalan. The main difference lies in the treatment of unstressed a and e; which have merged to /ə/ in Eastern dialects, but which remain distinct as /a/ and /e/ in Western dialects. There are a few other differences in pronunciation, verbal morphology, and vocabulary. Catalan evolved from Vulgar Latin around the eastern Pyrenees in the 9th century. During the Low Middle Ages it saw a golden age as the literary and dominant language of the Crown of Aragon, and was widely used all over the Mediterranean. The union of Aragon with the other territories of Spain in 1479 marked the start of the decline of the language. In 1659 Spain ceded Northern Catalonia to France, and Catalan was banned in both states in the early 18th century. 19th-century Spain saw a Catalan literary revival, which culminated in the 1913 orthographic standardization, and the officialization of the language during the Second Spanish Republic (1931–39). However, the Francoist dictatorship (1939–75) banned the language again. Nowadays, France only recognizes French as an official language. Nevertheless, on 10 December 2007, the General Council of the Pyrénées-Orientales officially recognized Catalan as one of the languages of the department and seeks to further promote it in public life and education. Central Catalan is considered the standard pronunciation of the language and has the highest number of speakers. It is spoken in the densely populated regions of the Barcelona province, the eastern half of the province of Tarragona, and most of the province of Girona. Martorell's outstanding novel of chivalry Tirant lo Blanc (1490) shows a transition from Medieval to Renaissance values, something that can also be seen in Metge's work. The first book produced with movable type in the Iberian Peninsula was printed in Catalan. Catalan sociolinguistics studies the situation of Catalan in the world and the different varieties that this language presents. It is a subdiscipline of Catalan philology and other affine studies and has as an objective to analyse the relation between the Catalan language, the speakers and the close reality (including the one of other languages in contact). Standard Catalan, virtually accepted by all speakers, is mostly based on Eastern Catalan, which is the most widely used dialect. Nevertheless, the standards of Valencia and the Balearics admit alternative forms, mostly traditional ones, which are not current in eastern Catalonia. The decline of Catalan continued in the 16th and 17th centuries. The Catalan defeat in the War of Spanish Succession (1714) initiated a series of measures imposing the use of Spanish in legal documentation. These territories are sometimes referred to as the Països Catalans (Catalan Countries), a denomination based on cultural affinity and common heritage, that has also had a subsequent political interpretation but no official status. Various interpretations of the term may include some or all of these regions. Central Catalan is considered the standard pronunciation of the language. The descriptions below are mostly for this variety. For the differences in pronunciation of the different dialects, see the section pronunciation of dialects in this article. On the other hand, there are several language shift processes currently taking place. In Northern Catalonia, Catalan has followed the same trend as the other minority languages of France, with most of its native speakers being 60 or older (as of 2004). Catalan is studied as a foreign language by 30% of the primary education students, and by 15% of the secondary. The cultural association La Bressola promotes a network of community-run schools engaged in Catalan language immersion programs. According to Ethnologue, the lexical similarity between Catalan and other Romance languages is: 87% with Italian; 85% with Portuguese; 80% with Spanish; 76% with Ladin; 75% with Sardinian; and 73% with Romanian. In Western Catalan, unstressed vowels reduce to five: /e ɛ/ > [e]; /o ɔ/ > [o]; /a u i/ remain distinct. This reduction pattern, inherited from Proto-Romance, is also found in Italian and Portuguese. Some Western dialects present further reduction or vowel harmony in some cases. Situated between two large linguistic blocks (Ibero-Romance and Gallo-Romance), Catalan has many unique lexical choices, such as enyorar ""to miss somebody"", apaivagar ""to calm down somebody"", or rebutjar ""reject"". Like other languages, Catalan has a large list of learned words from Greek and Latin. This process started very early, and one can find such examples in Ramon Llull's work. On the fourteenth and fifteenth centuries Catalan had a number of Greco-Latin learned words much superior to other Romance languages, as it can be attested for example in Roís de Corella's writings. The word Catalan derives from the territory of Catalonia, itself of disputed etymology. The main theory suggests that Catalunya (Latin Gathia Launia) derives from the name Gothia or Gauthia (""Land of the Goths""), since the origins of the Catalan counts, lords and people were found in the March of Gothia, whence Gothland > Gothlandia > Gothalania > Catalonia theoretically derived. There is evidence that, at least from the a.d. 2nd century, the vocabulary and phonology of Roman Tarraconensis was different from the rest of Roman Hispania. Differentiation has arisen generally because Spanish, Asturian, and Galician-Portuguese share certain peripheral archaisms (Spanish hervir, Asturian/Portuguese ferver vs. Catalan bullir, Occitan bolir ""to boil"") and innovatory regionalisms (Sp novillo, Ast nuviellu vs. Cat torell, Oc taurèl ""bullock""), while Catalan has a shared history with the Western Romance innovative core, especially Occitan. Central Catalan has abandoned almost completely unstressed possessives (mon, etc.) in favour of constructions of article + stressed forms (el meu, etc.), a feature shared with Italian. In the 11th century, documents written in macaronic Latin begin to show Catalan elements, with texts written almost completely in Romance appearing by 1080. Old Catalan shared many features with Gallo-Romance, diverging from Old Occitan between the 11th and 14th centuries. In Spain, every person officially has two surnames, one of which is the father's first surname and the other is the mother's first surname. The law contemplates the possibility of joining both surnames with the Catalan conjunction i (""and""). The AVL, created by the Valencian parliament, is in charge of dictating the official rules governing the use of Valencian, and its standard is based on the Norms of Castelló (Normes de Castelló). Currently, everyone who writes in Valencian uses this standard, except the Royal Academy of Valencian Culture (Acadèmia de Cultura Valenciana, RACV), which uses for Valencian an independent standard. This clash of opinions has sparked much controversy. For example, during the drafting of the European Constitution in 2004, the Spanish government supplied the EU with translations of the text into Basque, Galician, Catalan, and Valencian, but the latter two were identical. Catalan has an inflectional grammar, with two genders (masculine, feminine), and two numbers (singular, plural). Pronouns are also inflected for case, animacy[citation needed] and politeness, and can be combined in very complex ways. Verbs are split in several paradigms and are inflected for person, number, tense, aspect, mood, and gender. In terms of pronunciation, Catalan has many words ending in a wide variety of consonants and some consonant clusters, in contrast with many other Romance languages. By the 9th century, Catalan had evolved from Vulgar Latin on both sides of the eastern end of the Pyrenees, as well as the territories of the Roman province of Hispania Tarraconensis to the south. From the 8th century onwards the Catalan counts extended their territory southwards and westwards at the expense of the Muslims, bringing their language with them. This process was given definitive impetus with the separation of the County of Barcelona from the Carolingian Empire in 988. In verbs, 1st person present indicative desinence is -e (∅ in verbs of the 2nd and 3rd conjugation), or -o.
E.g. parle, tem, sent (Valencian); parlo, temo, sento (Northwestern). In verbs, 1st person present indicative desinence is -o, -i or ∅ in all conjugations.
E.g. parlo (Central), parl (Balearic), parli (Northern), ('I speak'). Valencian is classified as a Western dialect, along with the northwestern varieties spoken in Western Catalonia (provinces of Lleida and the western half of Tarragona). The various forms of Catalan and Valencian are mutually intelligible (ranging from 90% to 95%) In gender inflection, the most notable feature is (compared to Portuguese, Spanish or Italian), the loss of the typical masculine suffix -o. Thus, the alternance of -o/-a, has been replaced by ø/-a. There are only a few exceptions, like minso/minsa (""scarce""). Many not completely predictable morphological alternations may occur, such as: During much of its history, and especially during the Francoist dictatorship (1939–1975), the Catalan language has often been degraded as a mere dialect of Spanish. This view, based on political and ideological considerations, has no linguistic validity. Spanish and Catalan have important differences in their sound systems, lexicon, and grammatical features, placing the language in a number of respects closer to Occitan (and French).  In English, the term referring to a person first appears in the mid 14th century as Catelaner, followed in the 15th century as Catellain (from French). It is attested a language name since at least 1652. Catalan can be pronounced as /ˈkætəlæn/, /kætəˈlæn/ or /ˈkætələn/. In Andorra, Catalan has always been the sole official language. Since the promulgation of the 1993 constitution, several Andorranization policies have been enforced, like Catalan medium education. Catalan has inherited the typical vowel system of Vulgar Latin, with seven stressed phonemes: /a ɛ e i ɔ o u/, a common feature in Western Romance, except Spanish. Balearic has also instances of stressed /ə/. Dialects differ in the different degrees of vowel reduction, and the incidence of the pair /ɛ e/. According to the Statistical Institute of Catalonia in 2008 the Catalan language is the second most commonly used in Catalonia, after Spanish, as a native or self-defining language. The Generalitat of Catalunya spends part of its annual budget on the promotion of the use of Catalan in Catalonia and in other territories. Catalan has few suppletive couplets, like Italian and Spanish, and unlike French. Thus, Catalan has noi/noia (""boy""/""girl"") and gall/gallina (""cock""/""hen""), whereas French has garçon/fille and coq/poule. The dialects of the Catalan language feature a relative uniformity, especially when compared to other Romance languages; both in terms of vocabulary, semantics, syntax, morphology, and phonology. Mutual intelligibility between dialects is very high, estimates ranging from 90% to 95%. The only exception is the isolated idiosyncratic Alguerese dialect. Western Catalan comprises the two dialects of Northwestern Catalan and Valencian; the Eastern block comprises four dialects: Central Catalan, Balearic, Rossellonese, and Alguerese. Each dialect can be further subdivided in several subdialects. Catalan (/ˈkætəlæn/; autonym: català [kətəˈla] or [kataˈla]) is a Romance language named for its origins in Catalonia, in what is northeastern Spain and adjoining parts of France. It is the national and only official language of Andorra, and a co-official language of the Spanish autonomous communities of Catalonia, the Balearic Islands, and Valencia (where the language is known as Valencian, and there exist regional standards). It also has semi-official status in the city of Alghero on the Italian island of Sardinia. It is also spoken with no official recognition in parts of the Spanish autonomous communities of Aragon (La Franja) and Murcia (Carche), and in the historic French region of Roussillon/Northern Catalonia, roughly equivalent to the department of Pyrénées-Orientales."
Association_football,"The length of the pitch for international adult matches is in the range of 100–110 m (110–120 yd) and the width is in the range of 64–75 m (70–80 yd). Fields for non-international matches may be 90–120 m (100–130 yd) length and 45–90 m (50–100 yd) in width, provided that the pitch does not become square. In 2008, the IFAB initially approved a fixed size of 105 m (344 ft) long and 68 m (223 ft) wide as a standard pitch dimension for international matches; however, this decision was later put on hold and was never actually implemented. Phaininda and episkyros were Greek ball games. An image of an episkyros player depicted in low relief on a vase at the National Archaeological Museum of Athens appears on the UEFA European Championship Cup. Athenaeus, writing in 228 AD, referenced the Roman ball game harpastum. Phaininda, episkyros and harpastum were played involving hands and violence. They all appear to have resembled rugby football, wrestling and volleyball more than what is recognizable as modern football. As with pre-codified ""mob football"", the antecedent of all modern football codes, these three games involved more handling the ball than kicking. Non-competitive games included kemari in Japan, chuk-guk in Korea and woggabaliri in Australia. A number of players may be replaced by substitutes during the course of the game. The maximum number of substitutions permitted in most competitive international and domestic league games is three, though the permitted number may vary in other competitions or in friendly matches. Common reasons for a substitution include injury, tiredness, ineffectiveness, a tactical switch, or timewasting at the end of a finely poised game. In standard adult matches, a player who has been substituted may not take further part in a match. IFAB recommends ""that a match should not continue if there are fewer than seven players in either team."" Any decision regarding points awarded for abandoned games is left to the individual football associations. In the late 1990s and early 2000s, the IFAB experimented with ways of creating a winner without requiring a penalty shootout, which was often seen as an undesirable way to end a match. These involved rules ending a game in extra time early, either when the first goal in extra time was scored (golden goal), or if one team held a lead at the end of the first period of extra time (silver goal). Golden goal was used at the World Cup in 1998 and 2002. The first World Cup game decided by a golden goal was France's victory over Paraguay in 1998. Germany was the first nation to score a golden goal in a major competition, beating Czech Republic in the final of Euro 1996. Silver goal was used in Euro 2004. Both these experiments have been discontinued by IFAB. The growth in women's football has seen major competitions being launched at both national and international level mirroring the male competitions. Women's football has faced many struggles. It had a ""golden age"" in the United Kingdom in the early 1920s when crowds reached 50,000 at some matches; this was stopped on 5 December 1921 when England's Football Association voted to ban the game from grounds used by its member clubs. The FA's ban was rescinded in December 1969 with UEFA voting to officially recognise women's football in 1971. The FIFA Women's World Cup was inaugurated in 1991 and has been held every four years since, while women's football has been an Olympic event since 1996. A standard adult football match consists of two periods of 45 minutes each, known as halves. Each half runs continuously, meaning that the clock is not stopped when the ball is out of play. There is usually a 15-minute half-time break between halves. The end of the match is known as full-time. The referee is the official timekeeper for the match, and may make an allowance for time lost through substitutions, injured players requiring attention, or other stoppages. This added time is called additional time in FIFA documents, but is most commonly referred to as stoppage time or injury time, while loss time can also be used as a synonym. The duration of stoppage time is at the sole discretion of the referee. The referee alone signals the end of the match. In matches where a fourth official is appointed, toward the end of the half the referee signals how many minutes of stoppage time he intends to add. The fourth official then informs the players and spectators by holding up a board showing this number. The signalled stoppage time may be further extended by the referee. Added time was introduced because of an incident which happened in 1891 during a match between Stoke and Aston Villa. Trailing 1–0 and with just two minutes remaining, Stoke were awarded a penalty. Villa's goalkeeper kicked the ball out of the ground, and by the time the ball had been recovered, the 90 minutes had elapsed and the game was over. The same law also states that the duration of either half is extended until the penalty kick to be taken or retaken is completed, thus no game shall end with a penalty to be taken. At a professional level, most matches produce only a few goals. For example, the 2005–06 season of the English Premier League produced an average of 2.48 goals per match. The Laws of the Game do not specify any player positions other than goalkeeper, but a number of specialised roles have evolved. Broadly, these include three main categories: strikers, or forwards, whose main task is to score goals; defenders, who specialise in preventing their opponents from scoring; and midfielders, who dispossess the opposition and keep possession of the ball to pass it to the forwards on their team. Players in these positions are referred to as outfield players, to distinguish them from the goalkeeper. These positions are further subdivided according to the area of the field in which the player spends most time. For example, there are central defenders, and left and right midfielders. The ten outfield players may be arranged in any combination. The number of players in each position determines the style of the team's play; more forwards and fewer defenders creates a more aggressive and offensive-minded game, while the reverse creates a slower, more defensive style of play. While players typically spend most of the game in a specific position, there are few restrictions on player movement, and players can switch positions at any time. The layout of a team's players is known as a formation. Defining the team's formation and tactics is usually the prerogative of the team's manager. The governing bodies in each country operate league systems in a domestic season, normally comprising several divisions, in which the teams gain points throughout the season depending on results. Teams are placed into tables, placing them in order according to points accrued. Most commonly, each team plays every other team in its league at home and away in each season, in a round-robin tournament. At the end of a season, the top team is declared the champion. The top few teams may be promoted to a higher division, and one or more of the teams finishing at the bottom are relegated to a lower division. The goalkeepers are the only players allowed to touch the ball with their hands or arms while it is in play and only in their penalty area. Outfield players mostly use their feet to strike or pass the ball, but may also use their head or torso to do so instead. The team that scores the most goals by the end of the match wins. If the score is level at the end of the game, either a draw is declared or the game goes into extra time and/or a penalty shootout depending on the format of the competition. The Laws of the Game were originally codified in England by The Football Association in 1863. Association football is governed internationally by the International Federation of Association Football (FIFA; French: Fédération Internationale de Football Association), which organises World Cups for both men and women every four years. After the World Cup, the most important international football competitions are the continental championships, which are organised by each continental confederation and contested between national teams. These are the European Championship (UEFA), the Copa América (CONMEBOL), African Cup of Nations (CAF), the Asian Cup (AFC), the CONCACAF Gold Cup (CONCACAF) and the OFC Nations Cup (OFC). The FIFA Confederations Cup is contested by the winners of all six continental championships, the current FIFA World Cup champions and the country which is hosting the Confederations Cup. This is generally regarded as a warm-up tournament for the upcoming FIFA World Cup and does not carry the same prestige as the World Cup itself. The most prestigious competitions in club football are the respective continental championships, which are generally contested between national champions, for example the UEFA Champions League in Europe and the Copa Libertadores in South America. The winners of each continental competition contest the FIFA Club World Cup. Along with the general administration of the sport, football associations and competition organisers also enforce good conduct in wider aspects of the game, dealing with issues such as comments to the press, clubs' financial management, doping, age fraud and match fixing. Most competitions enforce mandatory suspensions for players who are sent off in a game. Some on-field incidents, if considered very serious (such as allegations of racial abuse), may result in competitions deciding to impose heavier sanctions than those normally associated with a red card. Some associations allow for appeals against player suspensions incurred on-field if clubs feel a referee was incorrect or unduly harsh. The referee may punish a player's or substitute's misconduct by a caution (yellow card) or dismissal (red card). A second yellow card at the same game leads to a red card, and therefore to a dismissal. A player given a yellow card is said to have been ""booked"", the referee writing the player's name in his official notebook. If a player has been dismissed, no substitute can be brought on in their place. Misconduct may occur at any time, and while the offences that constitute misconduct are listed, the definitions are broad. In particular, the offence of ""unsporting behaviour"" may be used to deal with most events that violate the spirit of the game, even if they are not listed as specific offences. A referee can show a yellow or red card to a player, substitute or substituted player. Non-players such as managers and support staff cannot be shown the yellow or red card, but may be expelled from the technical area if they fail to conduct themselves in a responsible manner. The primary law is that players other than goalkeepers may not deliberately handle the ball with their hands or arms during play, though they do use their hands during a throw-in restart. Although players usually use their feet to move the ball around, they may use any part of their body (notably, ""heading"" with the forehead) other than their hands or arms. Within normal play, all players are free to play the ball in any direction and move throughout the pitch, though the ball cannot be received in an offside position. The Cambridge Rules, first drawn up at Cambridge University in 1848, were particularly influential in the development of subsequent codes, including association football. The Cambridge Rules were written at Trinity College, Cambridge, at a meeting attended by representatives from Eton, Harrow, Rugby, Winchester and Shrewsbury schools. They were not universally adopted. During the 1850s, many clubs unconnected to schools or universities were formed throughout the English-speaking world, to play various forms of football. Some came up with their own distinct codes of rules, most notably the Sheffield Football Club, formed by former public school pupils in 1857, which led to formation of a Sheffield FA in 1867. In 1862, John Charles Thring of Uppingham School also devised an influential set of rules. There are 17 laws in the official Laws of the Game, each containing a collection of stipulation and guidelines. The same laws are designed to apply to all levels of football, although certain modifications for groups such as juniors, seniors, women and people with physical disabilities are permitted. The laws are often framed in broad terms, which allow flexibility in their application depending on the nature of the game. The Laws of the Game are published by FIFA, but are maintained by the International Football Association Board (IFAB). In addition to the seventeen laws, numerous IFAB decisions and other directives contribute to the regulation of football. Association football in itself does not have a classical history. Notwithstanding any similarities to other ball games played around the world FIFA have recognised that no historical connection exists with any game played in antiquity outside Europe. The modern rules of association football are based on the mid-19th century efforts to standardise the widely varying forms of football played in the public schools of England. The history of football in England dates back to at least the eighth century AD. In many parts of the world football evokes great passions and plays an important role in the life of individual fans, local communities, and even nations. R. Kapuscinski says that Europeans who are polite, modest, or humble fall easily into rage when playing or watching football games. The Côte d'Ivoire national football team helped secure a truce to the nation's civil war in 2006 and it helped further reduce tensions between government and rebel forces in 2007 by playing a match in the rebel capital of Bouaké, an occasion that brought both armies together peacefully for the first time. By contrast, football is widely considered to have been the final proximate cause for the Football War in June 1969 between El Salvador and Honduras. The sport also exacerbated tensions at the beginning of the Yugoslav Wars of the 1990s, when a match between Dinamo Zagreb and Red Star Belgrade degenerated into rioting in May 1990. The world's oldest football competition is the FA Cup, which was founded by C. W. Alcock and has been contested by English teams since 1872. The first official international football match also took place in 1872, between Scotland and England in Glasgow, again at the instigation of C. W. Alcock. England is also home to the world's first football league, which was founded in Birmingham in 1888 by Aston Villa director William McGregor. The original format contained 12 clubs from the Midlands and Northern England. In league competitions, games may end in a draw. In knockout competitions where a winner is required various methods may be employed to break such a deadlock, some competitions may invoke replays. A game tied at the end of regulation time may go into extra time, which consists of two further 15-minute periods. If the score is still tied after extra time, some competitions allow the use of penalty shootouts (known officially in the Laws of the Game as ""kicks from the penalty mark"") to determine which team will progress to the next stage of the tournament. Goals scored during extra time periods count toward the final score of the game, but kicks from the penalty mark are only used to decide the team that progresses to the next part of the tournament (with goals scored in a penalty shootout not making up part of the final score). Association football is played in accordance with a set of rules known as the Laws of the Game. The game is played using a spherical ball of 68.5–69.5 cm (27.0–27.4 in) circumference, known as the football (or soccer ball). Two teams of eleven players each compete to get the ball into the other team's goal (between the posts and under the bar), thereby scoring a goal. The team that has scored more goals at the end of the game is the winner; if both teams have scored an equal number of goals then the game is a draw. Each team is led by a captain who has only one official responsibility as mandated by the Laws of the Game: to be involved in the coin toss prior to kick-off or penalty kicks. The laws of the game are determined by the International Football Association Board (IFAB). The Board was formed in 1886 after a meeting in Manchester of The Football Association, the Scottish Football Association, the Football Association of Wales, and the Irish Football Association. FIFA, the international football body, was formed in Paris in 1904 and declared that they would adhere to Laws of the Game of the Football Association. The growing popularity of the international game led to the admittance of FIFA representatives to the International Football Association Board in 1913. The board consists of four representatives from FIFA and one representative from each of the four British associations. These ongoing efforts contributed to the formation of The Football Association (The FA) in 1863, which first met on the morning of 26 October 1863 at the Freemasons' Tavern in Great Queen Street, London. The only school to be represented on this occasion was Charterhouse. The Freemason's Tavern was the setting for five more meetings between October and December, which eventually produced the first comprehensive set of rules. At the final meeting, the first FA treasurer, the representative from Blackheath, withdrew his club from the FA over the removal of two draft rules at the previous meeting: the first allowed for running with the ball in hand; the second for obstructing such a run by hacking (kicking an opponent in the shins), tripping and holding. Other English rugby clubs followed this lead and did not join the FA and instead in 1871 formed the Rugby Football Union. The eleven remaining clubs, under the charge of Ebenezer Cobb Morley, went on to ratify the original thirteen laws of the game. These rules included handling of the ball by ""marks"" and the lack of a crossbar, rules which made it remarkably similar to Victorian rules football being developed at that time in Australia. The Sheffield FA played by its own rules until the 1870s with the FA absorbing some of its rules until there was little difference between the games. As the Laws were formulated in England, and were initially administered solely by the four British football associations within IFAB, the standard dimensions of a football pitch were originally expressed in imperial units. The Laws now express dimensions with approximate metric equivalents (followed by traditional units in brackets), though use of imperial units remains popular in English-speaking countries with a relatively recent history of metrication (or only partial metrication), such as Britain. The basic equipment or kit players are required to wear includes a shirt, shorts, socks, footwear and adequate shin guards. An athletic supporter and protective cup is highly recommended for male players by medical experts and professionals. Headgear is not a required piece of basic equipment, but players today may choose to wear it to protect themselves from head injury. Players are forbidden to wear or use anything that is dangerous to themselves or another player, such as jewellery or watches. The goalkeeper must wear clothing that is easily distinguishable from that worn by the other players and the match officials. In front of the goal is the penalty area. This area is marked by the goal line, two lines starting on the goal line 16.5 m (18 yd) from the goalposts and extending 16.5 m (18 yd) into the pitch perpendicular to the goal line, and a line joining them. This area has a number of functions, the most prominent being to mark where the goalkeeper may handle the ball and where a penalty foul by a member of the defending team becomes punishable by a penalty kick. Other markings define the position of the ball or players at kick-offs, goal kicks, penalty kicks and corner kicks. In game play, players attempt to create goal-scoring opportunities through individual control of the ball, such as by dribbling, passing the ball to a team-mate, and by taking shots at the goal, which is guarded by the opposing goalkeeper. Opposing players may try to regain control of the ball by intercepting a pass or through tackling the opponent in possession of the ball; however, physical contact between opponents is restricted. Football is generally a free-flowing game, with play stopping only when the ball has left the field of play or when play is stopped by the referee for an infringement of the rules. After a stoppage, play recommences with a specified restart. There has been a football tournament at every Summer Olympic Games since 1900, except at the 1932 games in Los Angeles. Before the inception of the World Cup, the Olympics (especially during the 1920s) had the same status as the World Cup. Originally, the event was for amateurs only; however, since the 1984 Summer Olympics, professional players have been permitted, albeit with certain restrictions which prevent countries from fielding their strongest sides. The Olympic men's tournament is played at Under-23 level. In the past the Olympics have allowed a restricted number of over-age players per team. A women's tournament was added in 1996; in contrast to the men's event, full international sides without age restrictions play the women's Olympic tournament. Each team consists of a maximum of eleven players (excluding substitutes), one of whom must be the goalkeeper. Competition rules may state a minimum number of players required to constitute a team, which is usually seven. Goalkeepers are the only players allowed to play the ball with their hands or arms, provided they do so within the penalty area in front of their own goal. Though there are a variety of positions in which the outfield (non-goalkeeper) players are strategically placed by a coach, these positions are not defined or required by the Laws."
Sumer,"Sumerian religion seems to have been founded upon two separate cosmogenic myths. The first saw creation as the result of a series of hieros gami or sacred marriages, involving the reconciliation of opposites, postulated as a coming together of male and female divine beings; the gods. This continued to influence the whole Mesopotamian mythos. Thus in the Enuma Elish the creation was seen as the union of fresh and salt water; as male Abzu, and female Tiamat. The product of that union, Lahm and Lahmu, ""the muddy ones"", were titles given to the gate keepers of the E-Abzu temple of Enki, in Eridu, the first Sumerian city. Describing the way that muddy islands emerge from the confluence of fresh and salty water at the mouth of the Euphrates, where the river deposited its load of silt, a second hieros gamos supposedly created Anshar and Kishar, the ""sky-pivot"" or axle, and the ""earth pivot"", parents in turn of Anu (the sky) and Ki (the earth). Another important Sumerian hieros gamos was that between Ki, here known as Ninhursag or ""Lady Sacred Mountain"", and Enki of Eridu, the god of fresh water which brought forth greenery and pasture. The most important archaeological discoveries in Sumer are a large number of tablets written in cuneiform. Sumerian writing, while proven to be not the oldest example of writing on earth, is considered to be a great milestone in the development of man's ability to not only create historical records but also in creating pieces of literature both in the form of poetic epics and stories as well as prayers and laws. Although pictures — that is, hieroglyphs — were first used, cuneiform and then Ideograms (where symbols were made to represent ideas) soon followed. Triangular or wedge-shaped reeds were used to write on moist clay. A large body of hundreds of thousands of texts in the Sumerian language have survived, such as personal or business letters, receipts, lexical lists, laws, hymns, prayers, stories, daily records, and even libraries full of clay tablets. Monumental inscriptions and texts on different objects like statues or bricks are also very common. Many texts survive in multiple copies because they were repeatedly transcribed by scribes-in-training. Sumerian continued to be the language of religion and law in Mesopotamia long after Semitic speakers had become dominant. The term ""Sumerian"" is the common name given to the ancient non-Semitic inhabitants of Mesopotamia, Sumer, by the Semitic Akkadians. The Sumerians referred to themselves as ùĝ saĝ gíg-ga (cuneiform: 𒌦 𒊕 𒈪 𒂵), phonetically /uŋ saŋ giga/, literally meaning ""the black-headed people"", and to their land as ki-en-gi(-r) ('place' + 'lords' + 'noble'), meaning ""place of the noble lords"". The Akkadian word Shumer may represent the geographical name in dialect, but the phonological development leading to the Akkadian term šumerû is uncertain. Hebrew Shinar, Egyptian Sngr, and Hittite Šanhar(a), all referring to southern Mesopotamia, could be western variants of Shumer. Periodically ""clean slate"" decrees were signed by rulers which cancelled all the rural (but not commercial) debt and allowed bondservants to return to their homes. Customarily rulers did it at the beginning of the first full year of their reign, but they could also be proclaimed at times of military conflict or crop failure. The first known ones were made by Enmetena and Urukagina of Lagash in 2400-2350 BC. According to Hudson, the purpose of these decrees was to prevent debts mounting to a degree that they threatened fighting force which could happen if peasants lost the subsistence land or became bondservants due to the inability to repay the debt. Although short-lived, one of the first empires known to history was that of Eannatum of Lagash, who annexed practically all of Sumer, including Kish, Uruk, Ur, and Larsa, and reduced to tribute the city-state of Umma, arch-rival of Lagash. In addition, his realm extended to parts of Elam and along the Persian Gulf. He seems to have used terror as a matter of policy. Eannatum's Stele of the Vultures depicts vultures pecking at the severed heads and other body parts of his enemies. His empire collapsed shortly after his death. They invented and developed arithmetic by using several different number systems including a mixed radix system with an alternating base 10 and base 6. This sexagesimal system became the standard number system in Sumer and Babylonia. They may have invented military formations and introduced the basic divisions between infantry, cavalry, and archers. They developed the first known codified legal and administrative systems, complete with courts, jails, and government records. The first true city-states arose in Sumer, roughly contemporaneously with similar entities in what are now Syria and Lebanon. Several centuries after the invention of cuneiform, the use of writing expanded beyond debt/payment certificates and inventory lists to be applied for the first time, about 2600 BC, to messages and mail delivery, history, legend, mathematics, astronomical records, and other pursuits. Conjointly with the spread of writing, the first formal schools were established, usually under the auspices of a city-state's primary temple. According to Archibald Sayce, the primitive pictograms of the early Sumerian (i.e. Uruk) era suggest that ""Stone was scarce, but was already cut into blocks and seals. Brick was the ordinary building material, and with it cities, forts, temples and houses were constructed. The city was provided with towers and stood on an artificial platform; the house also had a tower-like appearance. It was provided with a door which turned on a hinge, and could be opened with a sort of key; the city gate was on a larger scale, and seems to have been double. The foundation stones — or rather bricks — of a house were consecrated by certain objects that were deposited under them."" The most impressive and famous of Sumerian buildings are the ziggurats, large layered platforms which supported temples. Sumerian cylinder seals also depict houses built from reeds not unlike those built by the Marsh Arabs of Southern Iraq until as recently as 400 CE. The Sumerians also developed the arch, which enabled them to develop a strong type of dome. They built this by constructing and linking several arches. Sumerian temples and palaces made use of more advanced materials and techniques,[citation needed] such as buttresses, recesses, half columns, and clay nails. The Sumerians were one of the first known beer drinking societies. Cereals were plentiful and were the key ingredient in their early brew. They brewed multiple kinds of beer consisting of wheat, barley, and mixed grain beers. Beer brewing was very important to the Sumerians. It was referenced in the Epic of Gilgamesh when Enkidu was introduced to the food and beer of Gilgamesh's people: ""Drink the beer, as is the custom of the land... He drank the beer-seven jugs! and became expansive and sang with joy!"" It was believed that when people died, they would be confined to a gloomy world of Ereshkigal, whose realm was guarded by gateways with various monsters designed to prevent people entering or leaving. The dead were buried outside the city walls in graveyards where a small mound covered the corpse, along with offerings to monsters and a small amount of food. Those who could afford it sought burial at Dilmun. Human sacrifice was found in the death pits at the Ur royal cemetery where Queen Puabi was accompanied in death by her servants. It is also said that the Sumerians invented the first oboe-like instrument, and used them at royal funerals. In the early Sumerian Uruk period, the primitive pictograms suggest that sheep, goats, cattle, and pigs were domesticated. They used oxen as their primary beasts of burden and donkeys or equids as their primary transport animal and ""woollen clothing as well as rugs were made from the wool or hair of the animals. ... By the side of the house was an enclosed garden planted with trees and other plants; wheat and probably other cereals were sown in the fields, and the shaduf was already employed for the purpose of irrigation. Plants were also grown in pots or vases."" The Semitic Akkadian language is first attested in proper names of the kings of Kish c. 2800 BC, preserved in later king lists. There are texts written entirely in Old Akkadian dating from c. 2500 BC. Use of Old Akkadian was at its peak during the rule of Sargon the Great (c. 2270–2215 BC), but even then most administrative tablets continued to be written in Sumerian, the language used by the scribes. Gelb and Westenholz differentiate three stages of Old Akkadian: that of the pre-Sargonic era, that of the Akkadian empire, and that of the ""Neo-Sumerian Renaissance"" that followed it. Akkadian and Sumerian coexisted as vernacular languages for about one thousand years, but by around 1800 BC, Sumerian was becoming more of a literary language familiar mainly only to scholars and scribes. Thorkild Jacobsen has argued that there is little break in historical continuity between the pre- and post-Sargon periods, and that too much emphasis has been placed on the perception of a ""Semitic vs. Sumerian"" conflict. However, it is certain that Akkadian was also briefly imposed on neighboring parts of Elam that were previously conquered by Sargon. These deities formed a core pantheon; there were additionally hundreds of minor ones. Sumerian gods could thus have associations with different cities, and their religious importance often waxed and waned with those cities' political power. The gods were said to have created human beings from clay for the purpose of serving them. The temples organized the mass labour projects needed for irrigation agriculture. Citizens had a labor duty to the temple, though they could avoid it by a payment of silver. The Sumerians developed a complex system of metrology c. 4000 BC. This advanced metrology resulted in the creation of arithmetic, geometry, and algebra. From c. 2600 BC onwards, the Sumerians wrote multiplication tables on clay tablets and dealt with geometrical exercises and division problems. The earliest traces of the Babylonian numerals also date back to this period. The period c. 2700 – 2300 BC saw the first appearance of the abacus, and a table of successive columns which delimited the successive orders of magnitude of their sexagesimal number system. The Sumerians were the first to use a place value numeral system. There is also anecdotal evidence the Sumerians may have used a type of slide rule in astronomical calculations. They were the first to find the area of a triangle and the volume of a cube. It is speculated by some archaeologists that Sumerian speakers were farmers who moved down from the north, after perfecting irrigation agriculture there. The Ubaid pottery of southern Mesopotamia has been connected via Choga Mami transitional ware to the pottery of the Samarra period culture (c. 5700 – 4900 BC C-14) in the north, who were the first to practice a primitive form of irrigation agriculture along the middle Tigris River and its tributaries. The connection is most clearly seen at Tell Awayli (Oueilli, Oueili) near Larsa, excavated by the French in the 1980s, where eight levels yielded pre-Ubaid pottery resembling Samarran ware. According to this theory, farming peoples spread down into southern Mesopotamia because they had developed a temple-centered social organization for mobilizing labor and technology for water control, enabling them to survive and prosper in a difficult environment.[citation needed] Evidence of wheeled vehicles appeared in the mid 4th millennium BC, near-simultaneously in Mesopotamia, the Northern Caucasus (Maykop culture) and Central Europe. The wheel initially took the form of the potter's wheel. The new concept quickly led to wheeled vehicles and mill wheels. The Sumerians' cuneiform writing system is the oldest (or second oldest after the Egyptian hieroglyphs) which has been deciphered (the status of even older inscriptions such as the Jiahu symbols and Tartaria tablets is controversial). The Sumerians were among the first astronomers, mapping the stars into sets of constellations, many of which survived in the zodiac and were also recognized by the ancient Greeks. They were also aware of the five planets that are easily visible to the naked eye. The earliest dynastic king on the Sumerian king list whose name is known from any other legendary source is Etana, 13th king of the first dynasty of Kish. The earliest king authenticated through archaeological evidence is Enmebaragesi of Kish (c. 26th century BC), whose name is also mentioned in the Gilgamesh epic—leading to the suggestion that Gilgamesh himself might have been a historical king of Uruk. As the Epic of Gilgamesh shows, this period was associated with increased war. Cities became walled, and increased in size as undefended villages in southern Mesopotamia disappeared. (Gilgamesh is credited with having built the walls of Uruk). Ziggurats (Sumerian temples) each had an individual name and consisted of a forecourt, with a central pond for purification. The temple itself had a central nave with aisles along either side. Flanking the aisles would be rooms for the priests. At one end would stand the podium and a mudbrick table for animal and vegetable sacrifices. Granaries and storehouses were usually located near the temples. After a time the Sumerians began to place the temples on top of multi-layered square constructions built as a series of rising terraces, giving rise to the Ziggurat style. The almost constant wars among the Sumerian city-states for 2000 years helped to develop the military technology and techniques of Sumer to a high level. The first war recorded in any detail was between Lagash and Umma in c. 2525 BC on a stele called the Stele of the Vultures. It shows the king of Lagash leading a Sumerian army consisting mostly of infantry. The infantrymen carried spears, wore copper helmets, and carried rectangular shields. The spearmen are shown arranged in what resembles the phalanx formation, which requires training and discipline; this implies that the Sumerians may have made use of professional soldiers. Sumerian cities during the Uruk period were probably theocratic and were most likely headed by a priest-king (ensi), assisted by a council of elders, including both men and women. It is quite possible that the later Sumerian pantheon was modeled upon this political structure. There was little evidence of organized warfare or professional soldiers during the Uruk period, and towns were generally unwalled. During this period Uruk became the most urbanized city in the world, surpassing for the first time 50,000 inhabitants. This period is generally taken to coincide with a major shift in population from southern Mesopotamia toward the north. Ecologically, the agricultural productivity of the Sumerian lands was being compromised as a result of rising salinity. Soil salinity in this region had been long recognized as a major problem. Poorly drained irrigated soils, in an arid climate with high levels of evaporation, led to the buildup of dissolved salts in the soil, eventually reducing agricultural yields severely. During the Akkadian and Ur III phases, there was a shift from the cultivation of wheat to the more salt-tolerant barley, but this was insufficient, and during the period from 2100 BC to 1700 BC, it is estimated that the population in this area declined by nearly three fifths. This greatly upset the balance of power within the region, weakening the areas where Sumerian was spoken, and comparatively strengthening those where Akkadian was the major language. Henceforth Sumerian would remain only a literary and liturgical language, similar to the position occupied by Latin in medieval Europe. As is known from the ""Sumerian Farmer's Almanac"", after the flood season and after the Spring Equinox and the Akitu or New Year Festival, using the canals, farmers would flood their fields and then drain the water. Next they made oxen stomp the ground and kill weeds. They then dragged the fields with pickaxes. After drying, they plowed, harrowed, and raked the ground three times, and pulverized it with a mattock, before planting seed. Unfortunately the high evaporation rate resulted in a gradual increase in the salinity of the fields. By the Ur III period, farmers had switched from wheat to the more salt-tolerant barley as their principal crop. Later, the 3rd dynasty of Ur under Ur-Nammu and Shulgi, whose power extended as far as southern Assyria, was the last great ""Sumerian renaissance"", but already the region was becoming more Semitic than Sumerian, with the rise in power of the Akkadian speaking Semites in Assyria and elsewhere, and the influx of waves of Semitic Martu (Amorites) who were to found several competing local powers including Isin, Larsa, Eshnunna and eventually Babylon. The last of these eventually came to dominate the south of Mesopotamia as the Babylonian Empire, just as the Old Assyrian Empire had already done so in the north from the late 21st century BC. The Sumerian language continued as a sacerdotal language taught in schools in Babylonia and Assyria, much as Latin was used in the Medieval period, for as long as cuneiform was utilized. Commercial credit and agricultural consumer loans were the main types of loans. The trade credit was usually extended by temples in order to finance trade expeditions and was nominated in silver. The interest rate was set at 1/60 a month (one shekel per mina) some time before 2000 BC and it remained at that level for about two thousand years. Rural loans commonly arose as a result of unpaid obligations due to an institution (such as a temple), in this case the arrears were considered to be lent to the debtor. They were denominated in barley or other crops and the interest rate was typically much higher than for commercial loans and could amount to 1/3 to 1/2 of the loan principal. The Sumerian city-states rose to power during the prehistoric Ubaid and Uruk periods. Sumerian written history reaches back to the 27th century BC and before, but the historical record remains obscure until the Early Dynastic III period, c. the 23rd century BC, when a now deciphered syllabary writing system was developed, which has allowed archaeologists to read contemporary records and inscriptions. Classical Sumer ends with the rise of the Akkadian Empire in the 23rd century BC. Following the Gutian period, there is a brief Sumerian Renaissance in the 21st century BC, cut short in the 20th century BC by Semitic Amorite invasions. The Amorite ""dynasty of Isin"" persisted until c. 1700 BC, when Mesopotamia was united under Babylonian rule. The Sumerians were eventually absorbed into the Akkadian (Assyro-Babylonian) population. By the time of the Uruk period (c. 4100–2900 BC calibrated), the volume of trade goods transported along the canals and rivers of southern Mesopotamia facilitated the rise of many large, stratified, temple-centered cities (with populations of over 10,000 people) where centralized administrations employed specialized workers. It is fairly certain that it was during the Uruk period that Sumerian cities began to make use of slave labor captured from the hill country, and there is ample evidence for captured slaves as workers in the earliest texts. Artifacts, and even colonies of this Uruk civilization have been found over a wide area—from the Taurus Mountains in Turkey, to the Mediterranean Sea in the west, and as far east as central Iran. The Sumerians were a non-Semitic caucasoid people, and spoke a language isolate; a number of linguists believed they could detect a substrate language beneath Sumerian, because names of some of Sumer's major cities are not Sumerian, revealing influences of earlier inhabitants. However, the archaeological record shows clear uninterrupted cultural continuity from the time of the early Ubaid period (5300 – 4700 BC C-14) settlements in southern Mesopotamia. The Sumerian people who settled here farmed the lands in this region that were made fertile by silt deposited by the Tigris and the Euphrates rivers. Though women were protected by late Sumerian law and were able to achieve a higher status in Sumer than in other contemporary civilizations, the culture was male-dominated. The Code of Ur-Nammu, the oldest such codification yet discovered, dating to the Ur-III ""Sumerian Renaissance"", reveals a glimpse at societal structure in late Sumerian law. Beneath the lu-gal (""great man"" or king), all members of society belonged to one of two basic strata: The ""lu"" or free person, and the slave (male, arad; female geme). The son of a lu was called a dumu-nita until he married. A woman (munus) went from being a daughter (dumu-mi), to a wife (dam), then if she outlived her husband, a widow (numasu) and she could then remarry. The Ubaid period is marked by a distinctive style of fine quality painted pottery which spread throughout Mesopotamia and the Persian Gulf. During this time, the first settlement in southern Mesopotamia was established at Eridu (Cuneiform: NUN.KI), c. 5300 BC, by farmers who brought with them the Hadji Muhammed culture, which first pioneered irrigation agriculture. It appears that this culture was derived from the Samarran culture from northern Mesopotamia. It is not known whether or not these were the actual Sumerians who are identified with the later Uruk culture. Eridu remained an important religious center when it was gradually surpassed in size by the nearby city of Uruk. The story of the passing of the me (gifts of civilization) to Inanna, goddess of Uruk and of love and war, by Enki, god of wisdom and chief god of Eridu, may reflect this shift in hegemony. The Sumerian language is generally regarded as a language isolate in linguistics because it belongs to no known language family; Akkadian, by contrast, belongs to the Semitic branch of the Afroasiatic languages. There have been many failed attempts to connect Sumerian to other language groups. It is an agglutinative language; in other words, morphemes (""units of meaning"") are added together to create words, unlike analytic languages where morphemes are purely added together to create sentences. Some authors have proposed that there may be evidence of a sub-stratum or add-stratum language for geographic features and various crafts and agricultural activities, called variously Proto-Euphratean or Proto Tigrean, but this is disputed by others. Native Sumerian rule re-emerged for about a century in the Neo-Sumerian Empire or Third Dynasty of Ur (Sumerian Renaissance) approximately 2100-2000 BC, but the Akkadian language also remained in use. The Sumerian city of Eridu, on the coast of the Persian Gulf, is considered to have been the world's first city, where three separate cultures may have fused — that of peasant Ubaidian farmers, living in mud-brick huts and practicing irrigation; that of mobile nomadic Semitic pastoralists living in black tents and following herds of sheep and goats; and that of fisher folk, living in reed huts in the marshlands, who may have been the ancestors of the Sumerians. However, some scholars contest the idea of a Proto-Euphratean language or one substrate language. It has been suggested by them and others, that the Sumerian language was originally that of the hunter and fisher peoples, who lived in the marshland and the Eastern Arabia littoral region, and were part of the Arabian bifacial culture. Reliable historical records begin much later; there are none in Sumer of any kind that have been dated before Enmebaragesi (c. 26th century BC). Professor Juris Zarins believes the Sumerians were settled along the coast of Eastern Arabia, today's Persian Gulf region, before it flooded at the end of the Ice Age."
Germans,"In 1870, after France attacked Prussia, Prussia and its new allies in Southern Germany (among them Bavaria) were victorious in the Franco-Prussian War. It created the German Empire in 1871 as a German nation-state, effectively excluding the multi-ethnic Austrian Habsburg monarchy and Liechtenstein. Integrating the Austrians nevertheless remained a strong desire for many people of Germany and Austria, especially among the liberals, the social democrats and also the Catholics who were a minority within the Protestant Germany. More recently, films such as Das Boot (1981), The Never Ending Story (1984) Run Lola Run (1998), Das Experiment (2001), Good Bye Lenin! (2003), Gegen die Wand (Head-on) (2004) and Der Untergang (Downfall) (2004) have enjoyed international success. In 2002 the Academy Award for Best Foreign Language Film went to Caroline Link's Nowhere in Africa, in 2007 to Florian Henckel von Donnersmarck's The Lives of Others. The Berlin International Film Festival, held yearly since 1951, is one of the world's foremost film and cinema festivals. In the midst of the European sovereign-debt crisis, Radek Sikorski, Poland's Foreign Minister, stated in November 2011, ""I will probably be the first Polish foreign minister in history to say so, but here it is: I fear German power less than I am beginning to fear German inactivity. You have become Europe's indispensable nation."" According to Jacob Heilbrunn, a senior editor at The National Interest, such a statement is unprecedented when taking into consideration Germany's history. ""This was an extraordinary statement from a top official of a nation that was ravaged by Germany during World War II. And it reflects a profound shift taking place throughout Germany and Europe about Berlin's position at the center of the Continent."" Heilbrunn believes that the adage, ""what was good for Germany was bad for the European Union"" has been supplanted by a new mentality—what is in the interest of Germany is also in the interest of its neighbors. The evolution in Germany's national identity stems from focusing less on its Nazi past and more on its Prussian history, which many Germans believe was betrayed—and not represented—by Nazism. The evolution is further precipitated by Germany's conspicuous position as Europe's strongest economy. Indeed, this German sphere of influence has been welcomed by the countries that border it, as demonstrated by Polish foreign minister Radek Sikorski's effusive praise for his country's western neighbor. This shift in thinking is boosted by a newer generation of Germans who see World War II as a distant memory. Following the defeat in World War I, influence of German-speaking elites over Central and Eastern Europe was greatly limited. At the treaty of Versailles Germany was substantially reduced in size. Austria-Hungary was split up. Rump-Austria, which to a certain extent corresponded to the German-speaking areas of Austria-Hungary (a complete split into language groups was impossible due to multi-lingual areas and language-exclaves) adopted the name ""German-Austria"" (German: Deutschösterreich). The name German-Austria was forbidden by the victorious powers of World War I. Volga Germans living in the Soviet Union were interned in gulags or forcibly relocated during the Second World War. People of German origin are found in various places around the globe. United States is home to approximately 50 million German Americans or one third of the German diaspora, making it the largest centre of German-descended people outside Germany. Brazil is the second largest with 5 million people claiming German ancestry. Other significant centres are Canada, Argentina, South Africa and France each accounting for at least 1 million. While the exact number of German-descended people is difficult to calculate, the available data makes it safe to claim the number is exceeding 100 million people. Since the 2006 FIFA World Cup, the internal and external evaluation of Germany's national image has changed. In the annual Nation Brands Index global survey, Germany became significantly and repeatedly more highly ranked after the tournament. People in 20 different states assessed the country's reputation in terms of culture, politics, exports, its people and its attractiveness to tourists, immigrants and investments. Germany has been named the world's second most valued nation among 50 countries in 2010. Another global opinion poll, for the BBC, revealed that Germany is recognised for the most positive influence in the world in 2010. A majority of 59% have a positive view of the country, while 14% have a negative view. After World War II, eastern European countries such as the Soviet Union, Poland, Czechoslovakia, Hungary, Romania and Yugoslavia expelled the Germans from their territories. Many of those had inhabited these lands for centuries, developing a unique culture. Germans were also forced to leave the former eastern territories of Germany, which were annexed by Poland (Silesia, Pomerania, parts of Brandenburg and southern part of East Prussia) and the Soviet Union (northern part of East Prussia). Between 12 and 16,5 million ethnic Germans and German citizens were expelled westwards to allied-occupied Germany. The Napoleonic Wars were the cause of the final dissolution of the Holy Roman Empire, and ultimately the cause for the quest for a German nation state in 19th-century German nationalism. After the Congress of Vienna, Austria and Prussia emerged as two competitors. Austria, trying to remain the dominant power in Central Europe, led the way in the terms of the Congress of Vienna. The Congress of Vienna was essentially conservative, assuring that little would change in Europe and preventing Germany from uniting. These terms came to a sudden halt following the Revolutions of 1848 and the Crimean War in 1856, paving the way for German unification in the 1860s. By the 1820s, large numbers of Jewish German women had intermarried with Christian German men and had converted to Christianity. Jewish German Eduard Lasker was a prominent German nationalist figure who promoted the unification of Germany in the mid-19th century. Persons who speak German as their first language, look German and whose families have lived in Germany for generations are considered ""most German"", followed by categories of diminishing Germanness such as Aussiedler (people of German ancestry whose families have lived in Eastern Europe but who have returned to Germany), Restdeutsche (people living in lands that have historically belonged to Germany but which is currently outside of Germany), Auswanderer (people whose families have emigrated from Germany and who still speak German), German speakers in German-speaking nations such as Austrians, and finally people of German emigrant background who no longer speak German. Of approximately 100 million native speakers of German in the world, roughly 80 million consider themselves Germans.[citation needed] There are an additional 80 million people of German ancestry mainly in the United States, Brazil (mainly in the South Region of the country), Argentina, Canada, South Africa, the post-Soviet states (mainly in Russia and Kazakhstan), and France, each accounting for at least 1 million.[note 2] Thus, the total number of Germans lies somewhere between 100 and more than 150 million, depending on the criteria applied (native speakers, single-ancestry ethnic Germans, partial German ancestry, etc.). After Christianization, the Roman Catholic Church and local rulers led German expansion and settlement in areas inhabited by Slavs and Balts, known as Ostsiedlung. During the wars waged in the Baltic by the Catholic German Teutonic Knights; the lands inhabited by the ethnic group of the Old Prussians (the current reference to the people known then simply as the ""Prussians""), were conquered by the Germans. The Old Prussians were an ethnic group related to the Latvian and Lithuanian Baltic peoples. The former German state of Prussia took its name from the Baltic Prussians, although it was led by Germans who had assimilated the Old Prussians; the old Prussian language was extinct by the 17th or early 18th century. The Slavic people of the Teutonic-controlled Baltic were assimilated into German culture and eventually there were many intermarriages of Slavic and German families, including amongst the Prussia's aristocracy known as the Junkers. Prussian military strategist Karl von Clausewitz is a famous German whose surname is of Slavic origin. Massive German settlement led to the assimilation of Baltic (Old Prussians) and Slavic (Wends) populations, who were exhausted by previous warfare. Pan-Germanism's origins began in the early 19th century following the Napoleonic Wars. The wars launched a new movement that was born in France itself during the French Revolution. Nationalism during the 19th century threatened the old aristocratic regimes. Many ethnic groups of Central and Eastern Europe had been divided for centuries, ruled over by the old Monarchies of the Romanovs and the Habsburgs. Germans, for the most part, had been a loose and disunited people since the Reformation when the Holy Roman Empire was shattered into a patchwork of states. The new German nationalists, mostly young reformers such as Johann Tillmann of East Prussia, sought to unite all the German-speaking and ethnic-German (Volksdeutsche) people. For decades after the Second World War, any national symbol or expression was a taboo. However, the Germans are becoming increasingly patriotic. During a study in 2009, in which some 2,000 German citizens age 14 and upwards filled out a questionnaire, nearly 60% of those surveyed agreed with the sentiment ""I'm proud to be German."" And 78%, if free to choose their nation, would opt for German nationality with ""near or absolute certainty"". Another study in 2009, carried out by the Identity Foundation in Düsseldorf, showed that 73% of the Germans were proud of their country, twice more than 8 years earlier. According to Eugen Buss, a sociology professor at the University of Hohenheim, there's an ongoing normalisation and more and more Germans are becoming openly proud of their country. By the Middle Ages, large numbers of Jews lived in the Holy Roman Empire and had assimilated into German culture, including many Jews who had previously assimilated into French culture and had spoken a mixed Judeo-French language. Upon assimilating into German culture, the Jewish German peoples incorporated major parts of the German language and elements of other European languages into a mixed language known as Yiddish. However tolerance and assimilation of Jews in German society suddenly ended during the Crusades with many Jews being forcefully expelled from Germany and Western Yiddish disappeared as a language in Germany over the centuries, with German Jewish people fully adopting the German language. As of 2008[update], Germany is the fourth largest music market in the world and has exerted a strong influence on Dance and Rock music, and pioneered trance music. Artists such as Herbert Grönemeyer, Scorpions, Rammstein, Nena, Dieter Bohlen, Tokio Hotel and Modern Talking have enjoyed international fame. German musicians and, particularly, the pioneering bands Tangerine Dream and Kraftwerk have also contributed to the development of electronic music. Germany hosts many large rock music festivals annually. The Rock am Ring festival is the largest music festival in Germany, and among the largest in the world. German artists also make up a large percentage of Industrial music acts, which is called Neue Deutsche Härte. Germany hosts some of the largest Goth scenes and festivals in the entire world, with events like Wave-Gothic-Treffen and M'era Luna Festival easily attracting up to 30,000 people. Amongst Germany's famous artists there are various Dutch entertainers, such as Johannes Heesters. German philosophers have helped shape western philosophy from as early as the Middle Ages (Albertus Magnus). Later, Leibniz (17th century) and most importantly Kant played central roles in the history of philosophy. Kantianism inspired the work of Schopenhauer and Nietzsche as well as German idealism defended by Fichte and Hegel. Engels helped develop communist theory in the second half of the 19th century while Heidegger and Gadamer pursued the tradition of German philosophy in the 20th century. A number of German intellectuals were also influential in sociology, most notably Adorno, Habermas, Horkheimer, Luhmann, Simmel, Tönnies, and Weber. The University of Berlin founded in 1810 by linguist and philosopher Wilhelm von Humboldt served as an influential model for a number of modern western universities. The event of the Protestant Reformation and the politics that ensued has been cited as the origins of German identity that arose in response to the spread of a common German language and literature. Early German national culture was developed through literary and religious figures including Martin Luther, Johann Wolfgang von Goethe and Friedrich Schiller. The concept of a German nation was developed by German philosopher Johann Gottfried Herder. The popularity of German identity arose in the aftermath of the French Revolution. Sport forms an integral part of German life, as demonstrated by the fact that 27 million Germans are members of a sports club and an additional twelve million pursue such an activity individually. Football is by far the most popular sport, and the German Football Federation (Deutscher Fußballbund) with more than 6.3 million members is the largest athletic organisation in the country. It also attracts the greatest audience, with hundreds of thousands of spectators attending Bundesliga matches and millions more watching on television. The Germanic peoples during the Migrations Period came into contact with other peoples; in the case of the populations settling in the territory of modern Germany, they encountered Celts to the south, and Balts and Slavs towards the east. The Limes Germanicus was breached in AD 260. Migrating Germanic tribes commingled with the local Gallo-Roman populations in what is now Swabia and Bavaria. The arrival of the Huns in Europe resulted in Hun conquest of large parts of Eastern Europe, the Huns initially were allies of the Roman Empire who fought against Germanic tribes, but later the Huns cooperated with the Germanic tribe of the Ostrogoths, and large numbers of Germans lived within the lands of the Hunnic Empire of Attila. Attila had both Hunnic and Germanic families and prominent Germanic chiefs amongst his close entourage in Europe. The Huns living in Germanic territories in Eastern Europe adopted an East Germanic language as their lingua franca. A major part of Attila's army were Germans, during the Huns' campaign against the Roman Empire. After Attila's unexpected death the Hunnic Empire collapsed with the Huns disappearing as a people in Europe – who either escaped into Asia, or otherwise blended in amongst Europeans. Roman Catholicism was the sole established religion in the Holy Roman Empire until the Reformation changed this drastically. In 1517, Martin Luther challenged the Catholic Church as he saw it as a corruption of Christian faith. Through this, he altered the course of European and world history and established Protestantism. The Thirty Years' War (1618–1648) was one of the most destructive conflicts in European history. The war was fought primarily in what is now Germany, and at various points involved most of the countries of Europe. The war was fought largely as a religious conflict between Protestants and Catholics in the Holy Roman Empire. The work of David Hilbert and Max Planck was crucial to the foundation of modern physics, which Werner Heisenberg and Erwin Schrödinger developed further. They were preceded by such key physicists as Hermann von Helmholtz, Joseph von Fraunhofer, and Gabriel Daniel Fahrenheit, among others. Wilhelm Conrad Röntgen discovered X-rays, an accomplishment that made him the first winner of the Nobel Prize in Physics in 1901. The Walhalla temple for ""laudable and distinguished Germans"", features a number of scientists, and is located east of Regensburg, in Bavaria. The native language of Germans is German, a West Germanic language, related to and classified alongside English and Dutch, and sharing many similarities with the North Germanic and Scandinavian languages. Spoken by approximately 100 million native speakers, German is one of the world's major languages and the most widely spoken first language in the European Union. German has been replaced by English as the dominant language of science-related Nobel Prize laureates during the second half of the 20th century. It was a lingua franca in the Holy Roman Empire. At the same time, naval innovations led to a German domination of trade in the Baltic Sea and parts of Eastern Europe through the Hanseatic League. Along the trade routes, Hanseatic trade stations became centers of the German culture. German town law (Stadtrecht) was promoted by the presence of large, relatively wealthy German populations, their influence and political power. Thus people who would be considered ""Germans"", with a common culture, language, and worldview different from that of the surrounding rural peoples, colonized trading towns as far north of present-day Germany as Bergen (in Norway), Stockholm (in Sweden), and Vyborg (now in Russia). The Hanseatic League was not exclusively German in any ethnic sense: many towns who joined the league were outside the Holy Roman Empire and a number of them may only loosely be characterized as German. The Empire itself was not entirely German either. It had a multi-ethnic and multi-lingual structure, some of the smaller ethnicities and languages used at different times were Dutch, Italian, French, Czech and Polish. In 1866, the feud between Austria and Prussia finally came to a head. There were several reasons behind this war. As German nationalism grew strongly inside the German Confederation and neither could decide on how Germany was going to be unified into a nation-state. The Austrians favoured the Greater Germany unification but were not willing to give up any of the non-German-speaking land inside of the Austrian Empire and take second place to Prussia. The Prussians however wanted to unify Germany as Little Germany primarily by the Kingdom of Prussia, whilst excluding Austria. In the final battle of the German war (Battle of Königgrätz) the Prussians successfully defeated the Austrians and succeeded in creating the North German Confederation. Conflict between the Germanic tribes and the forces of Rome under Julius Caesar forced major Germanic tribes to retreat to the east bank of the Rhine. Roman emperor Augustus in 12 BC ordered the conquest of the Germans, but the catastrophic Roman defeat at the Battle of the Teutoburg Forest resulted in the Roman Empire abandoning its plans to completely conquer Germany. Germanic peoples in Roman territory were culturally Romanized, and although much of Germany remained free of direct Roman rule, Rome deeply influenced the development of German society, especially the adoption of Christianity by the Germans who obtained it from the Romans. In Roman-held territories with Germanic populations, the Germanic and Roman peoples intermarried, and Roman, Germanic, and Christian traditions intermingled. The adoption of Christianity would later become a major influence in the development of a common German identity. German cinema dates back to the very early years of the medium with the work of Max Skladanowsky. It was particularly influential during the years of the Weimar Republic with German expressionists such as Robert Wiene and Friedrich Wilhelm Murnau. The Nazi era produced mostly propaganda films although the work of Leni Riefenstahl still introduced new aesthetics in film. From the 1960s, New German Cinema directors such as Volker Schlöndorff, Werner Herzog, Wim Wenders, Rainer Werner Fassbinder placed West-German cinema back onto the international stage with their often provocative films, while the Deutsche Film-Aktiengesellschaft controlled film production in the GDR. According to the latest nationwide census, Roman Catholics constituted 30.8% of the total population of Germany, followed by the Evangelical Protestants at 30.3%. Other religions, atheists or not specified constituted 38.8% of the population at the time. Among ""others"" are Protestants not included in Evangelical Church of Germany, and other Christians such as the Restorationist New Apostolic Church. Protestantism was more common among the citizens of Germany. The North and East Germany is predominantly Protestant, the South and West rather Catholic. Nowadays there is a non-religious majority in Hamburg and the East German states. The migration-period peoples who later coalesced into a ""German"" ethnicity were the Germanic tribes of the Saxons, Franci, Thuringii, Alamanni and Bavarii. These five tribes, sometimes with inclusion of the Frisians, are considered as the major groups to take part in the formation of the Germans. The varieties of the German language are still divided up into these groups. Linguists distinguish low Saxon, Franconian, Bavarian, Thuringian and Alemannic varieties in modern German. By the 9th century, the large tribes which lived on the territory of modern Germany had been united under the rule of the Frankish king Charlemagne, known in German as Karl der Große. Much of what is now Eastern Germany became Slavonic-speaking (Sorbs and Veleti), after these areas were vacated by Germanic tribes (Vandals, Lombards, Burgundians and Suebi amongst others) which had migrated into the former areas of the Roman Empire. By the 1860s the Kingdom of Prussia and the Austrian Empire were the two most powerful nations dominated by German-speaking elites. Both sought to expand their influence and territory. The Austrian Empire – like the Holy Roman Empire – was a multi-ethnic state, but German-speaking people there did not have an absolute numerical majority; the creation of the Austro-Hungarian Empire was one result of the growing nationalism of other ethnicities especially the Hungarians. Prussia under Otto von Bismarck would ride on the coat-tails of nationalism to unite all of modern-day Germany. The German Empire (""Second Reich"") was created in 1871 following the proclamation of Wilhelm I as head of a union of German-speaking states, while disregarding millions of its non-German subjects who desired self-determination from German rule. In the field of music, Germany claims some of the most renowned classical composers of the world including Bach, Mozart and Beethoven, who marked the transition between the Classical and Romantic eras in Western classical music. Other composers of the Austro-German tradition who achieved international fame include Brahms, Wagner, Haydn, Schubert, Händel, Schumann, Liszt, Mendelssohn Bartholdy, Johann Strauss II, Bruckner, Mahler, Telemann, Richard Strauss, Schoenberg, Orff, and most recently, Henze, Lachenmann, and Stockhausen. A German ethnicity emerged in the course of the Middle Ages, ultimately as a result of the formation of the kingdom of Germany within East Francia and later the Holy Roman Empire, beginning in the 9th century. The process was gradual and lacked any clear definition, and the use of exonyms designating ""the Germans"" develops only during the High Middle Ages. The title of rex teutonicum ""King of the Germans"" is first used in the late 11th century, by the chancery of Pope Gregory VII, to describe the future Holy Roman Emperor of the German Nation Henry IV. Natively, the term ein diutscher (""a German"") is used for the people of Germany from the 12th century. The Nazis, led by Adolf Hitler, attempted to unite all the people they claimed were ""Germans"" (Volksdeutsche) into one realm, including ethnic Germans in eastern Europe, many of whom had emigrated more than one hundred fifty years before and developed separate cultures in their new lands. This idea was initially welcomed by many ethnic Germans in Sudetenland, Austria, Poland, Danzig and western Lithuania, particularly the Germans from Klaipeda (Memel). The Swiss resisted the idea. They had viewed themselves as a distinctly separate nation since the Peace of Westphalia of 1648."
Xbox_360,"The Xbox Live Marketplace is a virtual market designed for the console that allows Xbox Live users to download purchased or promotional content. The service offers movie and game trailers, game demos, Xbox Live Arcade games and Xbox 360 Dashboard themes as well as add-on game content (items, costumes, levels etc.). These features are available to both Free and Gold members on Xbox Live. A hard drive or memory unit is required to store products purchased from Xbox Live Marketplace. In order to download priced content, users are required to purchase Microsoft Points for use as scrip; though some products (such as trailers and demos) are free to download. Microsoft Points can be obtained through prepaid cards in 1,600 and 4,000-point denominations. Microsoft Points can also be purchased through Xbox Live with a credit card in 500, 1,000, 2,000 and 5,000-point denominations. Users are able to view items available to download on the service through a PC via the Xbox Live Marketplace website. An estimated seventy percent of Xbox Live users have downloaded items from the Marketplace. Launched worldwide across 2005–2006, the Xbox 360 was initially in short supply in many regions, including North America and Europe. The earliest versions of the console suffered from a high failure rate, indicated by the so-called ""Red Ring of Death"", necessitating an extension of the device's warranty period. Microsoft released two redesigned models of the console: the Xbox 360 S in 2010, and the Xbox 360 E in 2013. As of June 2014, 84 million Xbox 360 consoles have been sold worldwide, making it the sixth-highest-selling video game console in history, and the highest-selling console made by an American company. Although not the best-selling console of its generation, the Xbox 360 was deemed by TechRadar to be the most influential through its emphasis on digital media distribution and multiplayer gaming on Xbox Live. The Xbox 360's successor, the Xbox One, was released on November 22, 2013. Microsoft has stated they plan to support the Xbox 360 until 2016. The Xbox One is also backwards compatible with the Xbox 360. On May 26, 2009, Microsoft announced the future release of the Zune HD (in the fall of 2009), the next addition to the Zune product range. This is of an impact on the Xbox Live Video Store as it was also announced that the Zune Video Marketplace and the Xbox Live Video Store will be merged to form the Zune Marketplace, which will be arriving on Xbox Live in 7 countries initially, the United Kingdom, the United States, France, Italy, Germany, Ireland and Spain. Further details were released at the Microsoft press conference at E3 2009. Kinect is a ""controller-free gaming and entertainment experience"" for the Xbox 360. It was first announced on June 1, 2009 at the Electronic Entertainment Expo, under the codename, Project Natal. The add-on peripheral enables users to control and interact with the Xbox 360 without a game controller by using gestures, spoken commands and presented objects and images. The Kinect accessory is compatible with all Xbox 360 models, connecting to new models via a custom connector, and to older ones via a USB and mains power adapter. During their CES 2010 keynote speech, Robbie Bach and Microsoft CEO Steve Ballmer went on to say that Kinect will be released during the holiday period (November–January) and it will work with every 360 console. Its name and release date of 2010-11-04 were officially announced on 2010-06-13, prior to Microsoft's press conference at E3 2010. The Xbox 360 features an online service, Xbox Live, which was expanded from its previous iteration on the original Xbox and received regular updates during the console's lifetime. Available in free and subscription-based varieties, Xbox Live allows users to: play games online; download games (through Xbox Live Arcade) and game demos; purchase and stream music, television programs, and films through the Xbox Music and Xbox Video portals; and access third-party content services through media streaming applications. In addition to online multimedia features, the Xbox 360 allows users to stream media from local PCs. Several peripherals have been released, including wireless controllers, expanded hard drive storage, and the Kinect motion sensing camera. The release of these additional services and peripherals helped the Xbox brand grow from gaming-only to encompassing all multimedia, turning it into a hub for living-room computing entertainment. Since these problems surfaced, Microsoft has attempted to modify the console to improve its reliability. Modifications include a reduction in the number, size, and placement of components, the addition of dabs of epoxy on the corners and edges of the CPU and GPU as glue to prevent movement relative to the board during heat expansion, and a second GPU heatsink to dissipate more heat. With the release of the redesigned Xbox 360 S, the warranty for the newer models does not include the three-year extended coverage for ""General Hardware Failures"". The newer Xbox 360 S model indicates system overheating when the console's power button begins to flash red, unlike previous models where the first and third quadrant of the ring would light up red around the power button if overheating occurred. The system will then warn the user of imminent system shutdown until the system has cooled, whereas a flashing power button that alternates between green and red is an indication of a ""General Hardware Failure"" unlike older models where three of the quadrants would light up red. Xbox Live Arcade is an online service operated by Microsoft that is used to distribute downloadable video games to Xbox and Xbox 360 owners. In addition to classic arcade games such as Ms. Pac-Man, the service offers some new original games like Assault Heroes. The Xbox Live Arcade also features games from other consoles, such as the PlayStation game Castlevania: Symphony of the Night and PC games such as Zuma. The service was first launched on November 3, 2004, using a DVD to load, and offered games for about US$5 to $15. Items are purchased using Microsoft Points, a proprietary currency used to reduce credit card transaction charges. On November 22, 2005, Xbox Live Arcade was re-launched with the release of the Xbox 360, in which it was now integrated with the Xbox 360's dashboard. The games are generally aimed toward more casual gamers; examples of the more popular titles are Geometry Wars, Street Fighter II' Hyper Fighting, and Uno. On March 24, 2010, Microsoft introduced the Game Room to Xbox Live. Game Room is a gaming service for Xbox 360 and Microsoft Windows that lets players compete in classic arcade and console games in a virtual arcade. The Xbox 360's original graphical user interface was the Xbox 360 Dashboard; a tabbed interface that featured five ""Blades"" (formerly four blades), and was designed by AKQA and Audiobrain. It could be launched automatically when the console booted without a disc in it, or when the disc tray was ejected, but the user had the option to select what the console does if a game is in the tray on start up, or if inserted when already on. A simplified version of it was also accessible at any time via the Xbox Guide button on the gamepad. This simplified version showed the user's gamercard, Xbox Live messages and friends list. It also allowed for personal and music settings, in addition to voice or video chats, or returning to the Xbox Dashboard from the game. The Xbox 360 launched with 14 games in North America and 13 in Europe. The console's best-selling game for 2005, Call of Duty 2, sold over a million copies. Five other games sold over a million copies in the console's first year on the market: Ghost Recon Advanced Warfighter, The Elder Scrolls IV: Oblivion, Dead or Alive 4, Saints Row, and Gears of War. Gears of War would become the best-selling game on the console with 3 million copies in 2006, before being surpassed in 2007 by Halo 3 with over 8 million copies. When the Xbox 360 was released, Microsoft's online gaming service Xbox Live was shut down for 24 hours and underwent a major upgrade, adding a basic non-subscription service called Xbox Live Silver (later renamed Xbox Live Free) to its already established premium subscription-based service (which was renamed Gold). Xbox Live Free is included with all SKUs of the console. It allows users to create a user profile, join on message boards, and access Microsoft's Xbox Live Arcade and Marketplace and talk to other members. A Live Free account does not generally support multiplayer gaming; however, some games that have rather limited online functions already, (such as Viva Piñata) or games that feature their own subscription service (e.g. EA Sports games) can be played with a Free account. Xbox Live also supports voice the latter a feature possible with the Xbox Live Vision. While the original Xbox sold poorly in Japan, selling just 2 million units while it was on the market (between 2002 and 2005),[citation needed] the Xbox 360 sold even more poorly, selling only 1.5 million units from 2005 to 2011. Edge magazine reported in August 2011 that initially lackluster and subsequently falling sales in Japan, where Microsoft had been unable to make serious inroads into the dominance of domestic rivals Sony and Nintendo, had led to retailers scaling down and in some cases discontinuing sales of the Xbox 360 completely. To aid customers with defective consoles, Microsoft extended the Xbox 360's manufacturer's warranty to three years for hardware failure problems that generate a ""General Hardware Failure"" error report. A ""General Hardware Failure"" is recognized on all models released before the Xbox 360 S by three quadrants of the ring around the power button flashing red. This error is often known as the ""Red Ring of Death"". In April 2009 the warranty was extended to also cover failures related to the E74 error code. The warranty extension is not granted for any other types of failures that do not generate these specific error codes. Music, photos and videos can be played from standard USB mass storage devices, Xbox 360 proprietary storage devices (such as memory cards or Xbox 360 hard drives), and servers or computers with Windows Media Center or Windows XP with Service pack 2 or higher within the local-area network in streaming mode. As the Xbox 360 uses a modified version of the UPnP AV protocol, some alternative UPnP servers such as uShare (part of the GeeXboX project) and MythTV can also stream media to the Xbox 360, allowing for similar functionality from non-Windows servers. This is possible with video files up to HD-resolution and with several codecs (MPEG-2, MPEG-4, WMV) and container formats (WMV, MOV, TS). At launch, the Xbox 360 was available in two configurations: the ""Xbox 360"" package (unofficially known as the 20 GB Pro or Premium), priced at US$399 or GB£279.99, and the ""Xbox 360 Core"", priced at US$299 and GB£209.99. The original shipment of the Xbox 360 version included a cut-down version of the Media Remote as a promotion. The Elite package was launched later at US$479. The ""Xbox 360 Core"" was replaced by the ""Xbox 360 Arcade"" in October 2007 and a 60 GB version of the Xbox 360 Pro was released on August 1, 2008. The Pro package was discontinued and marked down to US$249 on August 28, 2009 to be sold until stock ran out, while the Elite was also marked down in price to US$299. In May 2008 Microsoft announced that 10 million Xbox 360s had been sold and that it was the ""first current generation gaming console"" to surpass the 10 million figure in the US. In the US, the Xbox 360 was the leader in current-generation home console sales until June 2008, when it was surpassed by the Wii. The Xbox 360 has sold a total of 870,000 units in Canada as of August 1, 2008. Between January 2011 and October 2013, the Xbox 360 was the best-selling console in the United States for these 32 consecutive months. The Xbox 360's advantage over its competitors was due to the release of high profile titles from both first party and third party developers. The 2007 Game Critics Awards honored the platform with 38 nominations and 12 wins – more than any other platform. By March 2008, the Xbox 360 had reached a software attach rate of 7.5 games per console in the US; the rate was 7.0 in Europe, while its competitors were 3.8 (PS3) and 3.5 (Wii), according to Microsoft. At the 2008 Game Developers Conference, Microsoft announced that it expected over 1,000 games available for Xbox 360 by the end of the year. As well as enjoying exclusives such as additions to the Halo franchise and Gears of War, the Xbox 360 has managed to gain a simultaneous release of titles that were initially planned to be PS3 exclusives, including Devil May Cry, Ace Combat, Virtua Fighter, Grand Theft Auto IV, Final Fantasy XIII, Tekken 6, Metal Gear Solid : Rising, and L.A. Noire. In addition, Xbox 360 versions of cross-platform games were generally considered superior to their PS3 counterparts in 2006 and 2007, due in part to the difficulties of programming for the PS3. In 2009, IGN named the Xbox 360 the sixth-greatest video game console of all time, out of a field of 25. Although not the best-selling console of the seventh-generation, the Xbox 360 was deemed by TechRadar to be the most influential, by emphasizing digital media distribution and online gaming through Xbox Live, and by popularizing game achievement awards. PC Magazine considered the Xbox 360 the prototype for online gaming as it ""proved that online gaming communities could thrive in the console space"". Five years after the Xbox 360's original debut, the well-received Kinect motion capture camera was released, which set the record of being the fastest selling consumer electronic device in history, and extended the life of the console. Edge ranked Xbox 360 the second-best console of the 1993–2013 period, stating ""It had its own social network, cross-game chat, new indie games every week, and the best version of just about every multiformat game...Killzone is no Halo and nowadays Gran Turismo is no Forza, but it's not about the exclusives—there's nothing to trump Naughty Dog's PS3 output, after all. Rather, it's about the choices Microsoft made back in the original Xbox's lifetime. The PC-like architecture meant those early EA Sports titles ran at 60fps compared to only 30 on PS3, Xbox Live meant every dedicated player had an existing friends list, and Halo meant Microsoft had the killer next-generation exclusive. And when developers demo games on PC now they do it with a 360 pad—another industry benchmark, and a critical one."" Known during development as Xbox Next, Xenon, Xbox 2, Xbox FS or NextBox, the Xbox 360 was conceived in early 2003. In February 2003, planning for the Xenon software platform began, and was headed by Microsoft's Vice President J Allard. That month, Microsoft held an event for 400 developers in Bellevue, Washington to recruit support for the system. Also that month, Peter Moore, former president of Sega of America, joined Microsoft. On August 12, 2003, ATI signed on to produce the graphic processing unit for the new console, a deal which was publicly announced two days later. Before the launch of the Xbox 360, several Alpha development kits were spotted using Apple's Power Mac G5 hardware. This was because the system's PowerPC 970 processor running the same PowerPC architecture that the Xbox 360 would eventually run under IBM's Xenon processor. The cores of the Xenon processor were developed using a slightly modified version of the PlayStation 3's Cell Processor PPE architecture. According to David Shippy and Mickie Phipps, the IBM employees were ""hiding"" their work from Sony and Toshiba, IBM's partners in developing the Cell Processor. Jeff Minter created the music visualization program Neon which is included with the Xbox 360. On November 6, 2006, Microsoft announced the Xbox Video Marketplace, an exclusive video store accessible through the console. Launched in the United States on November 22, 2006, the first anniversary of the Xbox 360's launch, the service allows users in the United States to download high-definition and standard-definition television shows and movies onto an Xbox 360 console for viewing. With the exception of short clips, content is not currently available for streaming, and must be downloaded. Movies are also available for rental. They expire in 14 days after download or at the end of the first 24 hours after the movie has begun playing, whichever comes first. Television episodes can be purchased to own, and are transferable to an unlimited number of consoles. Downloaded files use 5.1 surround audio and are encoded using VC-1 for video at 720p, with a bitrate of 6.8 Mbit/s. Television content is offered from MTV, VH1, Comedy Central, Turner Broadcasting, and CBS; and movie content is Warner Bros., Paramount, and Disney, along with other publishers. Two major hardware revisions of the Xbox 360 have succeeded the original models; the Xbox 360 S (also referred to as the ""Slim"") replaced the original ""Elite"" and ""Arcade"" models in 2010. The S model carries a smaller, streamlined appearance with an angular case, and utilizes a redesigned motherboard designed to alleviate the hardware and overheating issues experienced by prior models. It also includes a proprietary port for use with the Kinect sensor. The Xbox 360 E, a further streamlined variation of the 360 S with a two-tone rectangular case inspired by Xbox One, was released in 2013. In addition to its revised aesthetics, Xbox 360 E also has one fewer USB port and no longer supports S/PDIF. Six games were initially available in Japan, while eagerly anticipated titles such as Dead or Alive 4 and Enchanted Arms were released in the weeks following the console's launch. Games targeted specifically for the region, such as Chromehounds, Ninety-Nine Nights, and Phantasy Star Universe, were also released in the console's first year. Microsoft also had the support of Japanese developer Mistwalker, founded by Final Fantasy creator Hironobu Sakaguchi. Mistwalker's first game, Blue Dragon, was released in 2006 and had a limited-edition bundle which sold out quickly with over 10,000 pre-orders. Blue Dragon is one of three Xbox 360 games to surpass 200,000 units in Japan, along with Tales of Vesperia and Star Ocean: The Last Hope. Mistwalker's second game, Lost Odyssey also sold over 100,000 copies. The Xbox 360 sold much better than its predecessor, and although not the best-selling console of the seventh-generation, it is regarded as a success since it strengthened Microsoft as a major force in the console market at the expense of well-established rivals. The inexpensive Nintendo Wii did sell the most console units but eventually saw a collapse of third-party software support in its later years, and it has been viewed by some as a fad since the succeeding Wii U had a poor debut in 2012. The PlayStation 3 struggled for a time due to being too expensive and initially lacking quality titles, making it far less dominant than its predecessor, the PlayStation 2, and it took until late in the PlayStation 3's lifespan for its sales and game titles to reach parity with the Xbox 360. TechRadar proclaimed that ""Xbox 360 passes the baton as the king of the hill – a position that puts all the more pressure on its successor, Xbox One"". Xbox Live Gold includes the same features as Free and includes integrated online game playing capabilities outside of third-party subscriptions. Microsoft has allowed previous Xbox Live subscribers to maintain their profile information, friends list, and games history when they make the transition to Xbox Live Gold. To transfer an Xbox Live account to the new system, users need to link a Windows Live ID to their gamertag on Xbox.com. When users add an Xbox Live enabled profile to their console, they are required to provide the console with their passport account information and the last four digits of their credit card number, which is used for verification purposes and billing. An Xbox Live Gold account has an annual cost of US$59.99, C$59.99, NZ$90.00, GB£39.99, or €59.99. As of January 5, 2011, Xbox Live has over 30 million subscribers. TechRadar deemed the Xbox 360 as the most influential game system through its emphasis of digital media distribution, Xbox Live online gaming service, and game achievement feature. During the console's lifetime, the Xbox brand has grown from gaming-only to encompassing all multimedia, turning it into a hub for ""living-room computing environment"". Five years after the Xbox 360's original debut, the well-received Kinect motion capture camera was released, which became the fastest selling consumer electronic device in history, and extended the life of the console. At the 2007, 2008, and 2009 Consumer Electronics Shows, Microsoft had announced that IPTV services would soon be made available to use through the Xbox 360. In 2007, Microsoft chairman Bill Gates stated that IPTV on Xbox 360 was expected to be available to consumers by the holiday season, using the Microsoft TV IPTV Edition platform. In 2008, Gates and president of Entertainment & Devices Robbie Bach announced a partnership with BT in the United Kingdom, in which the BT Vision advanced TV service, using the newer Microsoft Mediaroom IPTV platform, would be accessible via Xbox 360, planned for the middle of the year. BT Vision's DVR-based features would not be available on Xbox 360 due to limited hard drive capacity. In 2010, while announcing version 2.0 of Microsoft Mediaroom, Microsoft CEO Steve Ballmer mentioned that AT&T's U-verse IPTV service would enable Xbox 360s to be used as set-top boxes later in the year. As of January 2010, IPTV on Xbox 360 has yet to be deployed beyond limited trials. The Xbox 360 supports videos in Windows Media Video (WMV) format (including high-definition and PlaysForSure videos), as well as H.264 and MPEG-4 media. The December 2007 dashboard update added support for the playback of MPEG-4 ASP format videos. The console can also display pictures and perform slideshows of photo collections with various transition effects, and supports audio playback, with music player controls accessible through the Xbox 360 Guide button. Users may play back their own music while playing games or using the dashboard, and can play music with an interactive visual synthesizer."
Chicago_Cubs,"The shift in the Cubs' fortunes was characterized June 23 on the ""NBC Saturday Game of the Week"" contest against the St. Louis Cardinals. it has since been dubbed simply ""The Sandberg Game."" With the nation watching and Wrigley Field packed, Sandberg emerged as a superstar with not one, but two game-tying home runs against Cardinals closer Bruce Sutter. With his shots in the 9th and 10th innings Wrigley Field erupted and Sandberg set the stage for a comeback win that cemented the Cubs as the team to beat in the East. No one would catch them, except the Padres in the playoffs. Jack Brickhouse manned the Cubs radio and especially the TV booth for parts of five decades, the 34-season span from 1948 to 1981. He covered the games with a level of enthusiasm that often seemed unjustified by the team's poor performance on the field for many of those years. His trademark call ""Hey Hey!"" always followed a home run. That expression is spelled out in large letters vertically on both foul pole screens at Wrigley Field. ""Whoo-boy!"" and ""Wheeee!"" and ""Oh, brother!"" were among his other pet expressions. When he approached retirement age, he personally recommended his successor. After finishing last in the NL Central with 66 wins in 2006, the Cubs re-tooled and went from ""worst to first"" in 2007. In the offseason they signed Alfonso Soriano to a contract at 8 years for $136 million, and replaced manager Dusty Baker with fiery veteran manager Lou Piniella. After a rough start, which included a brawl between Michael Barrett and Carlos Zambrano, the Cubs overcame the Milwaukee Brewers, who had led the division for most of the season, with winning streaks in June and July, coupled with a pair of dramatic, late-inning wins against the Reds, and ultimately clinched the NL Central with a record of 85–77. The Cubs traded Barrett to the Padres, and later acquired Jason Kendall from Oakland. Kendall was highly successful with his management of the pitching rotation and helped at the plate as well. By September, Geovany Soto became the full-time starter behind the plate, replacing the veteran Kendall. They met Arizona in the NLDS, but controversy followed as Piniella, in a move that has since come under scrutiny, pulled Carlos Zambrano after the sixth inning of a pitcher's duel with D-Backs ace Brandon Webb, to ""....save Zambrano for (a potential) Game 4."" The Cubs, however, were unable to come through, losing the first game and eventually stranding over 30 baserunners in a 3-game Arizona sweep. Rookie Starlin Castro debuted in early May (2010) as the starting shortstop. However, the club played poorly in the early season, finding themselves 10 games under .500 at the end of June. In addition, long-time ace Carlos Zambrano was pulled from a game against the White Sox on June 25 after a tirade and shoving match with Derrek Lee, and was suspended indefinitely by Jim Hendry, who called the conduct ""unacceptable."" On August 22, Lou Piniella, who had already announced his retirement at the end of the season, announced that he would leave the Cubs prematurely to take care of his sick mother. Mike Quade took over as the interim manager for the final 37 games of the year. Despite being well out of playoff contention the Cubs went 24–13 under Quade, the best record in baseball during that 37 game stretch, earning Quade to have the interim tag removed on October 19. The curious location on Catalina Island stemmed from Cubs owner William Wrigley Jr.'s then-majority interest in the island in 1919. Wrigley constructed a ballpark on the island to house the Cubs in spring training: it was built to the same dimensions as Wrigley Field. (The ballpark is long gone, but a clubhouse built by Wrigley to house the Cubs exists as the Catalina County Club.) However, by 1951 the team chose to leave Catalina Island and spring training was shifted to Mesa, Arizona. The Cubs' 30-year association with Catalina is chronicled in the book, The Cubs on Catalina, by Jim Vitti . . . which was named International 'Book of the Year' by The Sporting News. In the NLCS, the Cubs easily won the first two games at Wrigley Field against the San Diego Padres. The Padres were the winners of the Western Division with Steve Garvey, Tony Gwynn, Eric Show, Goose Gossage and Alan Wiggins. With wins of 13–0 and 4–2, the Cubs needed to win only one game of the next three in San Diego to make it to the World Series. After being beaten in Game 3 7–1, the Cubs lost Game 4 when Smith, with the game tied 5–5, allowed a game-winning home run to Garvey in the bottom of the ninth inning. In Game 5 the Cubs took a 3–0 lead into the 6th inning, and a 3–2 lead into the seventh with Sutcliffe (who won the Cy Young Award that year) still on the mound. Then, Leon Durham had a sharp grounder go under his glove. This critical error helped the Padres win the game 6–3, with a 4-run 7th inning and keep Chicago out of the 1984 World Series against the Detroit Tigers. The loss ended a spectacular season for the Cubs, one that brought alive a slumbering franchise and made the Cubs relevant for a whole new generation of Cubs fans. The Cubs enjoyed one more pennant at the close of World War II, finishing 98–56. Due to the wartime travel restrictions, the first three games of the 1945 World Series were played in Detroit, where the Cubs won two games, including a one-hitter by Claude Passeau, and the final four were played at Wrigley. In Game 4 of the Series, the Curse of the Billy Goat was allegedly laid upon the Cubs when P.K. Wrigley ejected Billy Sianis, who had come to Game 4 with two box seat tickets, one for him and one for his goat. They paraded around for a few innings, but Wrigley demanded the goat leave the park due to its unpleasant odor. Upon his ejection, Mr. Sianis uttered, ""The Cubs, they ain't gonna win no more."" The Cubs lost Game 4, lost the Series, and have not been back since. It has also been said by many that Sianis put a ""curse"" on the Cubs, apparently preventing the team from playing in the World Series. After losing the 1945 World Series to the Detroit Tigers, the Cubs finished with winning seasons the next two years, but those teams did not enter post-season play. The team's commitment to contend was complete when Green made a midseason deal on June 15 to shore up the starting rotation due to injuries to Rick Reuschel (5–5) and Sanderson. The deal brought 1979 NL Rookie of the Year pitcher Rick Sutcliffe from the Cleveland Indians. Joe Carter (who was with the Triple-A Iowa Cubs at the time) and center fielder Mel Hall were sent to Cleveland for Sutcliffe and back-up catcher Ron Hassey (.333 with Cubs in 1984). Sutcliffe (5–5 with the Indians) immediately joined Sanderson (8–5 3.14), Eckersley (10–8 3.03), Steve Trout (13–7 3.41) and Dick Ruthven (6–10 5.04) in the starting rotation. Sutcliffe proceeded to go 16–1 for Cubs and capture the Cy Young Award. In 2013, Tom Ricketts and team president Crane Kenney unveiled plans for a five-year, $575 million privately funded renovation of Wrigley Field. Called the 1060 Project, the proposed plans included vast improvements to the stadium's facade, infrastructure, restrooms, concourses, suites, press box, bullpens, and clubhouses, as well as a 6,000-square foot jumbotron to be added in the left field bleachers, batting tunnels, a 3,000-square-foot video board in right field, and, eventually, an adjacent hotel, plaza, and office-retail complex. In previously years mostly all efforts to conduct any large-scale renovations to the field had been opposed by the city, former mayor Richard M. Daley (a staunch White Sox fan), and especially the rooftop owners. Harry Caray's stamp on the team is perhaps even deeper than that of Brickhouse, although his 17-year tenure, from 1982 to 1997, was half as long. First, Caray had already become a well-known Chicago figure by broadcasting White Sox games for a decade, after having been a St Louis Cardinals icon for 25 years. Caray also had the benefit of being in the booth during the NL East title run in 1984, which was widely seen due to WGN's status as a cable-TV superstation. His trademark call of ""Holy Cow!"" and his enthusiastic singing of ""Take me out to the ballgame"" during the 7th inning stretch (as he had done with the White Sox) made Caray a fan favorite both locally and nationally. In 1989, the first full season with night baseball at Wrigley Field, Don Zimmer's Cubs were led by a core group of veterans in Ryne Sandberg, Rick Sutcliffe and Andre Dawson, who were boosted by a crop of youngsters such as Mark Grace, Shawon Dunston, Greg Maddux, Rookie of the Year Jerome Walton, and Rookie of the Year Runner-Up Dwight Smith. The Cubs won the NL East once again that season winning 93 games. This time the Cubs met the San Francisco Giants in the NLCS. After splitting the first two games at home, the Cubs headed to the Bay Area, where despite holding a lead at some point in each of the next three games, bullpen meltdowns and managerial blunders ultimately led to three straight losses. The Cubs couldn't overcome the efforts of Will Clark, whose home run off Maddux, just after a managerial visit to the mound, led Maddux to think Clark knew what pitch was coming. Afterward, Maddux would speak into his glove during any mound conversation, beginning what is a norm today. Mark Grace was 11–17 in the series with 8 RBI. Eventually, the Giants lost to the ""Bash Brothers"" and the Oakland A's in the famous ""Earthquake Series."" The official Cubs team mascot is a young bear cub, named Clark, described by the team's press release as a young and friendly Cub. Clark made his debut at Advocate Health Care on January 13, 2014, the same day as the press release announcing his installation as the club's first ever official physical mascot. The bear cub itself was used in the clubs since the early 1900s and was the inspiration of the Chicago Staleys changing their team's name to the Chicago Bears, due to the Cubs allowing the football team to play at Wrigley Field in the 1930s. An album entitled Take Me Out to a Cubs Game was released in 2008. It is a collection of 17 songs and other recordings related to the team, including Harry Caray's final performance of ""Take Me Out to the Ball Game"" on September 21, 1997, the Steve Goodman song mentioned above, and a newly recorded rendition of ""Talkin' Baseball"" (subtitled ""Baseball and the Cubs"") by Terry Cashman. The album was produced in celebration of the 100th anniversary of the Cubs' 1908 World Series victory and contains sounds and songs of the Cubs and Wrigley Field. In 1984, each league had two divisions, East and West. The divisional winners met in a best-of-5 series to advance to the World Series, in a ""2–3"" format, first two games were played at the home of the team who did not have home field advantage. Then the last three games were played at the home of the team, with home field advantage. Thus the first two games were played at Wrigley Field and the next three at the home of their opponents, San Diego. A common and unfounded myth is that since Wrigley Field did not have lights at that time the National League decided to give the home field advantage to the winner of the NL West. In fact, home field advantage had rotated between the winners of the East and West since 1969 when the league expanded. In even numbered years, the NL West had home field advantage. In odd numbered years, the NL East had home field advantage. Since the NL East winners had had home field advantage in 1983, the NL West winners were entitled to it. In 1902, Spalding, who by this time had revamped the roster to boast what would soon be one of the best teams of the early century, sold the club to Jim Hart. The franchise was nicknamed the Cubs by the Chicago Daily News in 1902, although not officially becoming the Chicago Cubs until the 1907 season. During this period, which has become known as baseball's dead-ball era, Cub infielders Joe Tinker, Johnny Evers, and Frank Chance were made famous as a double-play combination by Franklin P. Adams' poem Baseball's Sad Lexicon. The poem first appeared in the July 18, 1910 edition of the New York Evening Mail. Mordecai ""Three-Finger"" Brown, Jack Taylor, Ed Reulbach, Jack Pfiester, and Orval Overall were several key pitchers for the Cubs during this time period. With Chance acting as player-manager from 1905 to 1912, the Cubs won four pennants and two World Series titles over a five-year span. Although they fell to the ""Hitless Wonders"" White Sox in the 1906 World Series, the Cubs recorded a record 116 victories and the best winning percentage (.763) in Major League history. With mostly the same roster, Chicago won back-to-back World Series championships in 1907 and 1908, becoming the first Major League club to play three times in the Fall Classic and the first to win it twice. However, the Cubs have not won a World Series since; this remains the longest championship drought in North American professional sports. After losing an extra-inning game in Game 1, the Cubs rallied and took a 3 games to 1 lead over the Wild Card Florida Marlins in the NLCS. Florida shut the Cubs out in Game 5, but young pitcher Mark Prior led the Cubs in Game 6 as they took a 3–0 lead into the 8th inning and it was at this point when a now-infamous incident took place. Several spectators attempted to catch a foul ball off the bat of Luis Castillo. A Chicago Cubs fan by the name of Steve Bartman, of Northbrook, Illinois, reached for the ball and deflected it away from the glove of Moisés Alou for the second out of the 8th inning. Alou reacted angrily toward the stands, and after the game stated that he would have caught the ball. Alou at one point recanted, saying he would not have been able to make the play, but later said this was just an attempt to make Bartman feel better and believing the whole incident should be forgotten. Interference was not called on the play, as the ball was ruled to be on the spectator side of the wall. Castillo was eventually walked by Prior. Two batters later, and to the chagrin of the packed stadium, Cubs shortstop Alex Gonzalez misplayed an inning ending double play, loading the bases and leading to eight Florida runs and a Marlin victory. Despite sending Kerry Wood to the mound and holding a lead twice, the Cubs ultimately dropped Game 7, and failed to reach the World Series. In the following two decades after Sianis' ill will, the Cubs played mostly forgettable baseball, finishing among the worst teams in the National League on an almost annual basis. Longtime infielder/manager Phil Cavarretta, who had been a key player during the '45 season, was fired during spring training in 1954 after admitting the team was unlikely to finish above fifth place. Although shortstop Ernie Banks would become one of the star players in the league during the next decade, finding help for him proved a difficult task, as quality players such as Hank Sauer were few and far between. This, combined with poor ownership decisions such as the College of Coaches, and the ill-fated trade of future Hall of Famer Lou Brock to the Cardinals for pitcher Ernie Broglio (who won only 7 games over the next three seasons), hampered on-field performance. Despite losing fan favorite Grace to free agency, and the lack of production from newcomer Todd Hundley, skipper Don Baylor's Cubs put together a good season in 2001. The season started with Mack Newton being brought in to preach ""positive thinking."" One of the biggest stories of the season transpired as the club made a midseason deal for Fred McGriff, which was drawn out for nearly a month as McGriff debated waiving his no-trade clause, as the Cubs led the wild card race by 2.5 games in early September. That run died when Preston Wilson hit a three run walk off homer off of closer Tom ""Flash"" Gordon, which halted the team's momentum. The team was unable to make another serious charge, and finished at 88–74, five games behind both Houston and St. Louis, who tied for first. Sosa had perhaps his finest season and Jon Lieber led the staff with a 20 win season. The Ricketts family acquired a majority interest in the Cubs in 2009, ending the Tribune years. Apparently handcuffed by the Tribune's bankruptcy and the sale of the club to the Ricketts family, the Cubs' quest for a NL Central 3-peat started with notice that there would be less invested into contracts than in previous years. Chicago engaged St. Louis in a see-saw battle for first place into August 2009, but the Cardinals played to a torrid 20–6 pace that month, designating their rivals to battle in the Wild Card race, from which they were eliminated in the season's final week. The Cubs were plagued by injuries in 2009, and were only able to field their Opening Day starting lineup three times the entire season. Third baseman Aramis Ramírez injured his throwing shoulder in an early May game against the Milwaukee Brewers, sidelining him until early July and forcing journeyman players like Mike Fontenot and Aaron Miles into more prominent roles. Additionally, key players like Derrek Lee (who still managed to hit .306 with 35 HR and 111 RBI that season), Alfonso Soriano and Geovany Soto also nursed nagging injuries. The Cubs posted a winning record (83–78) for the third consecutive season, the first time the club had done so since 1972, and a new era of ownership under the Ricketts' family was approved by MLB owners in early October. On November 2, 2014, the Cubs announced that Joe Maddon had signed a five-year contract to be the 54th manager in team history. On December 10, 2014, Maddon announced that the team had signed free agent Jon Lester to a 6-year, $155 million contract. Many other trades and acquisitions occurred during the off season. The opening day lineup for the Cubs contained five new players including rookie right fielder Jorge Soler. Rookies Kris Bryant and Addison Russell were in the starting lineup by mid-April, and rookie Kyle Schwarber was added in mid-June. The Cubs finished the 2015 season with a record of 97–65, third best in the majors. On October 7, in the 2015 National League Wild Card Game, Jake Arrieta pitched a complete game shutout and the Cubs defeated the Pittsburgh Pirates 4–0. On April 25, 1976, at Dodger Stadium, father-and-son protestors ran into the outfield and tried to set fire to a U.S. flag. When Cubs outfielder Rick Monday noticed the flag on the ground and the man and boy fumbling with matches and lighter fluid, he dashed over and snatched the flag to thunderous applause. When he came up to bat in the next half-inning, he got a standing ovation from the crowd and the stadium titantron flashed the message, ""RICK MONDAY... YOU MADE A GREAT PLAY..."" Monday later said, ""If you're going to burn the flag, don't do it around me. I've been to too many veterans' hospitals and seen too many broken bodies of guys who tried to protect it."" The Cubs had no official physical mascot prior to Clark, though a man in a 'polar bear' looking outfit, called ""The Bear-man"" (or Beeman), which was mildly popular with the fans, paraded the stands briefly in the early 1990s. There is no record of whether or not he was just a fan in a costume or employed by the club. Through the 2013 season, there were ""Cubbie-bear"" mascots outside of Wrigley on game day, but none are employed by the team. They pose for pictures with fans for tips. The most notable of these was ""Billy Cub"" who worked outside of the stadium until for over 6 years until July 2013, when the club asked him to stop. Billy Cub, who is played by fan John Paul Weier, had unsuccessfully petitioned the team to become the official mascot. Another unofficial but much more well-known mascot is Ronnie ""Woo Woo"" Wickers who is a longtime fan and local celebrity in the Chicago area. He is known to Wrigley Field visitors for his idiosyncratic cheers at baseball games, generally punctuated with an exclamatory ""Woo!"" (e.g., ""Cubs, woo! Cubs, woo! Big-Z, woo! Zambrano, woo! Cubs, woo!"") Longtime Cubs announcer Harry Caray dubbed Wickers ""Leather Lungs"" for his ability to shout for hours at a time. He is not employed by the team, although the club has on two separate occasions allowed him into the broadcast booth and allow him some degree of freedom once he purchases or is given a ticket by fans to get into the games. He is largely allowed to roam the park and interact with fans by Wrigley Field security. The 2013 season resulted in much as the same the year before. Shortly before the trade deadline, the Cubs traded Matt Garza to the Texas Rangers for Mike Olt, C. J. Edwards, Neil Ramirez, and Justin Grimm. Three days later, the Cubs sent Alfonso Soriano to the New York Yankees for minor leaguer Corey Black. The mid season fire sale led to another last place finish in the NL Central, finishing with a record of 66-96. Although there was a five-game improvement in the record from the year before, Anthony Rizzo and Starlin Castro seemed to take steps backward in their development. On September 30, 2013, Theo Epstein made the decision to fire manager Dale Sveum after just two seasons at the helm of the Cubs. The regression of several young players was thought to be the main focus point, as the front office said Dale would not be judged based on wins and losses. In two seasons as skipper, Sveum finished with a record of 127-197. On September 23, 1908, the Cubs and New York Giants were involved in a tight pennant race. The two clubs were tied in the bottom of the ninth inning at the Polo Grounds, and N.Y. had runners on first and third and two outs when Al Bridwell singled, scoring Moose McCormick from third with the Giants' apparent winning run, but the runner on first base, rookie Fred Merkle, left the field without touching second base. As fans swarmed the field, Cub infielder Johnny Evers retrieved the ball and touched second. Since there were two outs, a forceout was called at second base, ending the inning and the game. Because of the tie the Giants and Cubs ended up tied for first place. The Giants lost the ensuing one-game playoff and the Cubs went on to the World Series. Despite the fact that the Cubs had won 89 games, this fallout was decidedly unlovable, as the Cubs traded superstar Sammy Sosa after he had left the season's final game early and then lied about it publicly. Already a controversial figure in the clubhouse after his corked-bat incident, Sammy's actions alienated much of his once strong fan base as well as the few teammates still on good terms with him, (many teammates grew tired of Sosa playing loud salsa music in the locker room) and possibly tarnished his place in Cubs' lore for years to come. The disappointing season also saw fans start to become frustrated with the constant injuries to ace pitchers Mark Prior and Kerry Wood. Additionally, the '04 season led to the departure of popular commentator Steve Stone, who had become increasingly critical of management during broadcasts and was verbally attacked by reliever Kent Mercker. Things were no better in 2005, despite a career year from first baseman Derrek Lee and the emergence of closer Ryan Dempster. The club struggled and suffered more key injuries, only managing to win 79 games after being picked by many to be a serious contender for the N.L. pennant. In 2006, bottom fell out as the Cubs finished 66–96, last in the NL Central. In 1969 the Cubs, managed by Leo Durocher, built a substantial lead in the newly created National League Eastern Division by mid-August. Ken Holtzman pitched a no-hitter on August 19, and the division lead grew to 8 1⁄2 games over the St. Louis Cardinals and by 9 1⁄2 games over the New York Mets. After the game of September 2, the Cubs record was 84-52 with the Mets in second place at 77-55. But then a losing streak began just as a Mets winning streak was beginning. The Cubs lost the final game of a series at Cincinnati, then came home to play the resurgent Pittsburgh Pirates (who would finish in third place). After losing the first two games by scores of 9-2 and 13-4, the Cubs led going into the ninth inning. A win would be a positive springboard since the Cubs were to play a crucial series with the Mets the very next day. But Willie Stargell drilled a 2-out, 2-strike pitch from the Cubs' ace reliever, Phil Regan, onto Sheffield Avenue to tie the score in the top of the ninth. The Cubs would lose 7-5 in extra innings. Burdened by a four-game losing streak, the Cubs traveled to Shea Stadium for a short two-game set. The Mets won both games, and the Cubs left New York with a record of 84-58 just 1⁄2 game in front. Disaster followed in Philadelphia, as a 99 loss Phillies team nonetheless defeated the Cubs twice, to extend Chicago's losing streak to eight games. In a key play in the second game, on September 11, Cubs starter Dick Selma threw a surprise pickoff attempt to third baseman Ron Santo, who was nowhere near the bag or the ball. Selma's throwing error opened the gates to a Phillies rally. After that second Philly loss, the Cubs were 84-60 and the Mets had pulled ahead at 85-57. The Mets would not look back. The Cubs' eight-game losing streak finally ended the next day in St. Louis, but the Mets were in the midst of a ten-game winning streak, and the Cubs, wilting from team fatigue, generally deteriorated in all phases of the game. The Mets (who had lost a record 120 games 7 years earlier), would go on to win the World Series. The Cubs, despite a respectable 92-70 record, would be remembered for having lost a remarkable 17½ games in the standings to the Mets in the last quarter of the season. After back-to-back pennants in 1880 and 1881, Hulbert died, and Spalding, who had retired to start Spalding sporting goods, assumed ownership of the club. The White Stockings, with Anson acting as player/manager, captured their third consecutive pennant in 1882, and Anson established himself as the game's first true superstar. In 1885 and '86, after winning N.L. pennants, the White Stockings met the short-lived American Association champion in that era's version of a World Series. Both seasons resulted in match ups with the St. Louis Brown Stockings, with the clubs tying in 1885 and with St. Louis winning in 1886. This was the genesis of what would eventually become one of the greatest rivalries in sports. In all, the Anson-led Chicago Base Ball Club won six National League pennants between 1876 and 1886. As a result, Chicago's club nickname transitioned, and by 1890 they had become known as the Chicago Colts, or sometimes ""Anson's Colts"", referring to Cap's influence within the club. Anson was the first player in history credited with collecting 3,000 career hits. After a disappointing record of 59-73 and a 9th-place finish in 1897, Anson was released by the Cubs as both a player and manager. Due to Anson's absence from the club after 22 years, local newspaper reporters started to refer to the Cubs as the ""Orphans"". In 1906, the franchise recorded a Major League record 116 wins (tied by the 2001 Seattle Mariners) and posted a modern-era record winning percentage of .763, which still stands today. They appeared in their first World Series the same year, falling to their crosstown rivals, the Chicago White Sox, four games to two. The Cubs won back-to-back World Series championships in 1907 and 1908, becoming the first Major League team to play in three consecutive Fall Classics, and the first to win it twice. The team has appeared in seven World Series following their 1908 title, most recently in 1945. The Cubs have not won the World Series in 107 years, the longest championship drought of any major North American professional sports team, and are often referred to as the ""Lovable Losers"" because of this distinction. They are also known as ""The North Siders"" because Wrigley Field, their home park since 1916, is located in Chicago's North Side Lake View community at 1060 West Addison Street. The Cubs have a major rivalry with the St. Louis Cardinals. The Chicago Cubs have not won a World Series championship since 1908, and have not appeared in the Fall Classic since 1945, although between their postseason appearance in 1984 and their most recent in 2015, they have made the postseason seven times. 107 seasons is the longest championship drought in all four of the major North American professional sports leagues, which also includes the National Football League (NFL), the National Basketball Association (NBA), and the National Hockey League (NHL). In fact, the Cubs' last World Series title occurred before those other three leagues even existed, and even the Cubs' last World Series appearance predates the founding of the NBA. The much publicized drought was concurrent to championship droughts by the Boston Red Sox and the Chicago White Sox, who both had over 80 years between championships. It is this unfortunate distinction that has led to the club often being known as ""The Lovable Losers."" The team was one win away from breaking what is often called the ""Curse of the Billy Goat"" in 1984 and 2003 (Steve Bartman incident), but was unable get the victory that would send it to the World Series. On May 11, 2000, Glenallen Hill, facing Brewers starter Steve Woodard, became the first, and thus far only player, to hit a pitched ball onto the roof of a five-story residential building across Waveland Ave, beyond Wrigley Field's left field wall. The shot was estimated at well over 500 feet (150 m), but the Cubs fell to Milwaukee 12–8. No batted ball has ever hit the center field scoreboard, although the original ""Slammin' Sammy"", golfer Sam Snead, hit it with a golf ball in an exhibition in the 1950s. In 1948, Bill Nicholson barely missed the scoreboard when he launched a home run ball onto Sheffield Avenue and in 1959, Roberto Clemente came even closer with a home run ball hit onto Waveland Avenue. In 2001, a Sammy Sosa shot landed across Waveland and bounced a block down Kenmore Avenue. Dave Kingman hit a shot in 1979 that hit the third porch roof on the east side of Kenmore, estimated at 555 feet (169 m), and is regarded as the longest home run in Wrigley Field history. On May 26, 2015, the Cubs rookie third baseman, Kris Bryant, hit a homerun that traveled an estimated 477 feet (145 m) off the park's new videoboard in left field. Later the same year, he hit a homer that traveled 495 feet (151 m) that also ricocheted off of the videoboard On October 13, 2015, Kyle Schwarber's 438-foot home run landed on the equally new right field videoboard. The '98 season would begin on a somber note with the death of legendary broadcaster Harry Caray. After the retirement of Sandberg and the trade of Dunston, the Cubs had holes to fill and the signing of Henry Rodríguez, known affectionately as ""H-Rod"" to bat cleanup provided protection for Sammy Sosa in the lineup, as Rodriguez slugged 31 round-trippers in his first season in Chicago. Kevin Tapani led the club with a career high 19 wins, Rod Beck anchored a strong bullpen and Mark Grace turned in one of his best seasons. The Cubs were swamped by media attention in 1998, and the team's two biggest headliners were Sosa and rookie flamethrower Kerry Wood. Wood's signature performance was one-hitting the Houston Astros, a game in which he tied the major league record of 20 strikeouts in nine innings. His torrid strikeout numbers earned Wood the nickname ""Kid K,"" and ultimately earned him the 1998 NL Rookie of the Year award. Sosa caught fire in June, hitting a major league record 20 home runs in the month, and his home run race with Cardinals slugger Mark McGwire transformed the pair into international superstars in a matter of weeks. McGwire finished the season with a new major league record of 70 home runs, but Sosa's .308 average and 66 homers earned him the National League MVP Award. After a down-to-the-wire Wild Card chase with the San Francisco Giants, Chicago and San Francisco ended the regular season tied, and thus squared off in a one-game playoff at Wrigley Field in which third baseman Gary Gaetti hit the eventual game winning homer. The win propelled the Cubs into the postseason once again with a 90–73 regular season tally. Unfortunately, the bats went cold in October, as manager Jim Riggleman's club batted .183 and scored only four runs en route to being swept by Atlanta. On a positive note, the home run chase between Sosa, McGwire and Ken Griffey, Jr. helped professional baseball to bring in a new crop of fans as well as bringing back some fans who had been disillusioned by the 1994 strike. The Cubs retained many players who experienced career years in '98, and after a fast start in 1999, they collapsed again (starting with being swept at the hands of the cross-town White Sox in mid-June) and finished in the bottom of the division for the next two seasons. In 1981, after 6 decades under the Wrigley family, the Cubs were purchased by Tribune Company for $20,500,000. Tribune, owners of the Chicago Tribune, Los Angeles Times, WGN Television, WGN Radio and many other media outlets, controlled the club until December 2007, when Sam Zell completed his purchase of the entire Tribune organization and announced his intention to sell the baseball team. After a nearly two-year process which involved potential buyers such as Mark Cuban and a group led by Hank Aaron, a family trust of TD Ameritrade founder Joe Ricketts won the bidding process as the 2009 season came to a close. Ultimately, the sale was unanimously approved by MLB owners and the Ricketts family took control on October 27, 2009. The Cubs began play as the Chicago White Stockings, joining the National League (NL) as a charter member. Owner William Hulbert signed multiple star players, such as pitcher Albert Spalding and infielders Ross Barnes, Deacon White, and Adrian ""Cap"" Anson, to join the team prior to the N.L.'s first season. The White Stockings played their home games at West Side Grounds,against the bloods and quickly established themselves as one of the new league's top teams. Spalding won forty-seven games and Barnes led the league in hitting at .429 as Chicago won the first ever National League pennant, which at the time was the game's top prize. The Chicago Cubs are an American professional baseball team located on the North Side of Chicago, Illinois. The Cubs compete in Major League Baseball (MLB) as a members of the National League (NL) Central division; the team plays its home baseball games at Wrigley Field. The Cubs are also one of two active major league teams based in Chicago; the other is the Chicago White Sox, who are a member of the American League (AL) Central division. The team is currently owned by Thomas S. Ricketts, son of TD Ameritrade founder Joe Ricketts. The Chicago White Stockings, (today's Chicago Cubs), began spring training in Hot Springs, Arkansas in 1886. President Albert Spalding (founder of Spalding Sporting Goods) and player/manager Cap Anson brought their players to Hot Springs and played at the Hot Springs Baseball Grounds. The concept was for the players to have training and fitness before the start of the regular season. After the White Stockings had a successful season in 1886, winning the National League Pennant, other teams began bringing their players to ""spring training"".  The Chicago Cubs, St. Louis Browns, New York Yankees, St. Louis Cardinals, Cleveland Spiders, Detroit Tigers, Pittsburgh Pirates, Cincinnati Reds, New York Highlanders, Brooklyn Dodgers and Boston Red Sox were among the early squads to arrive. Whittington Park (1894) and later Majestic Park (1909) and Fogel Field (1912) were all built in Hot Springs specifically to host Major League teams.  The Cubs' current spring training facility is located in Sloan Park in |Mesa, Arizona, where they play in the Cactus League. The park seats 15,000, making it Major League baseball's largest spring training facility by capacity. The Cubs annually sell out most of their games both at home and on the road. Before Sloan Park opened in 2014, the team played games at HoHoKam Park - Dwight Patterson Field from 1979. ""HoHoKam"" is literally translated from Native American as ""those who vanished."" The North Siders have called Mesa their spring home for most seasons since 1952. Located in Chicago's Lake View neighborhood, Wrigley Field sits on an irregular block bounded by Clark and Addison Streets and Waveland and Sheffield Avenues. The area surrounding the ballpark is typically referred to as Wrigleyville. There is a dense collection of sports bars and restaurants in the area, most with baseball inspired themes, including Sluggers, Murphy's Bleachers and The Cubby Bear. Many of the apartment buildings surrounding Wrigley Field on Waveland and Sheffield Avenues have built bleachers on their rooftops for fans to view games and other sell space for advertisement. One building on Sheffield Avenue has a sign atop its roof which says ""Eamus Catuli!"" which is Latin for ""Let's Go Cubs!"" and another chronicles the time since the last Division title, pennant, and World Series championship. The 02 denotes two years since the 2008 NL Central title, 65 years since the 1945 pennant and 102 years since the 1908 World Series championship. On game days, many residents rent out their yards and driveways to people looking for parking spots. The uniqueness of the neighborhood itself has ingrained itself into the culture of the Chicago Cubs as well as the Wrigleyville neighborhood, and has led to being used for concerts and other sporting events, such as the 2010 NHL Winter Classic between the Chicago Blackhawks and Detroit Red Wings, as well as a 2010 NCAA men's football game between the Northwestern Wildcats and Illinois Fighting Illini. During the summer of 1969, a Chicago studio group produced a single record called ""Hey Hey! Holy Mackerel! (The Cubs Song)"" whose title and lyrics incorporated the catch-phrases of the respective TV and radio announcers for the Cubs, Jack Brickhouse and Vince Lloyd. Several members of the Cubs recorded an album called Cub Power which contained a cover of the song. The song received a good deal of local airplay that summer, associating it very strongly with that bittersweet season. It was played much less frequently thereafter, although it remained an unofficial Cubs theme song for some years after. The team played its first games in 1876 as a founding member of the National League (NL), eventually becoming known officially as the Chicago Cubs for the 1903 season. Officially, the Cubs are tied for the distinction of being the oldest currently active U.S. professional sports club, along with the Atlanta Braves, which also began play in the NL in 1876 as the Boston Red Stockings (Major League Baseball does not officially recognize the National Association of Professional Base Ball Players as a major league.) The Cubs had high expectations in 2002, but the squad played poorly. On July 5, 2002 the Cubs promoted assistant general manager and player personnel director Jim Hendry to the General Manager position. The club responded by hiring Dusty Baker and by making some major moves in '03. Most notably, they traded with the Pittsburgh Pirates for outfielder Kenny Lofton and third baseman Aramis Ramírez, and rode dominant pitching, led by Kerry Wood and Mark Prior, as the Cubs led the division down the stretch. Caray had lively discussions with commentator Steve Stone, who was hand-picked by Harry himself, and producer Arne Harris. Caray often playfully quarreled with Stone over Stone's cigar and why Stone was single, while Stone would counter with poking fun at Harry being ""under the influence."" Stone disclosed in his book ""Where's Harry"" that most of this ""arguing"" was staged, and usually a ploy developed by Harry himself to add flavor to the broadcast. The Cubs still have a ""guest conductor"", usually a celebrity, lead the crowd in singing ""Take me out to the ballgame"" during the 7th inning stretch to honor Caray's memory. In 2004, the Cubs were a consensus pick by most media outlets to win the World Series. The offseason acquisition of Derek Lee (who was acquired in a trade with Florida for Hee-seop Choi) and the return of Greg Maddux only bolstered these expectation. Despite a mid-season deal for Nomar Garciaparra, misfortune struck the Cubs again. They led the Wild Card by 1.5 games over San Francisco and Houston on September 25, and both of those teams lost that day, giving the Cubs a chance at increasing the lead to a commanding 2.5 games with only eight games remaining in the season, but reliever LaTroy Hawkins blew a save to the Mets, and the Cubs lost the game in extra innings, a defeat that seemingly deflated the team, as they proceeded to drop 6 of their last 8 games as the Astros won the Wild Card. On April 23, 2008, against the Colorado Rockies, the Cubs recorded the 10,000th regular-season win in their franchise's history dating back to the beginning of the National League in 1876. The Cubs reached the milestone with an overall National League record of 10,000-9,465. Chicago was only the second club in Major League Baseball history to attain this milestone, the first having been the San Francisco Giants in mid-season 2005. The Cubs, however, hold the mark for victories for a team in a single city. The Chicago club's 77–77 record in the National Association (1871, 1874–1875) is not included in MLB record keeping. Post-season series are also not included in the totals. To honor the milestone, the Cubs flew an extra white flag displaying ""10,000"" in blue, along with the customary ""W"" flag. The former location in Mesa is actually the second HoHoKam Park; the first was built in 1976 as the spring-training home of the Oakland Athletics who left the park in 1979. Apart from HoHoKam Park and Sloan Park the Cubs also have another Mesa training facility called Fitch Park, this complex provides 25,000 square feet (2,300 m2) of team facilities, including major league clubhouse, four practice fields, one practice infield, enclosed batting tunnels, batting cages, a maintenance facility, and administrative offices for the Cubs. Green shored up the 1984 roster with a series of transactions. In December, 1983 Scott Sanderson was acquired from Montreal in a three-team deal with San Diego for Carmelo Martínez. Pinch hitter Richie Hebner (.333 BA in 1984) was signed as a free-agent. In spring training, moves continued: LF Gary Matthews and CF Bobby Dernier came from Philadelphia on March 26, for Bill Campbell and a minor leaguer. Reliever Tim Stoddard (10–6 3.82, 7 saves) was acquired the same day for a minor leaguer; veteran pitcher Ferguson Jenkins was released. The ""Bleacher Bums"" is a name given to fans, many of whom spend much of the day heckling, who sit in the bleacher section at Wrigley Field. Initially, the group was called ""bums"" because it referred to a group of fans who were at most games, and since those games were all day games, it was assumed they did not work. Many of those fans were, and are still, students at Chicago area colleges, such as DePaul University, Loyola, Northwestern University, and Illinois-Chicago. A Broadway play, starring Joe Mantegna, Dennis Farina, Dennis Franz, and James Belushi ran for years and was based on a group of Cub fans who frequented the club's games. The group was started in 1967 by dedicated fans Ron Grousl, Tom Nall and ""mad bugler"" Mike Murphy, who was a sports radio host during mid days on Chicago-based WSCR AM 670 ""The Score"". Murphy alleges that Grousl started the Wrigley tradition of throwing back opposing teams' home run balls. The current group is headed by Derek Schaul (Derek the Five Dollar Kid). Prior to the 2006 season, they were updated, with new shops and private bar (The Batter's Eye) being added, and Bud Light bought naming rights to the bleacher section, dubbing them the Bud Light Bleachers. Bleachers at Wrigley are general admission, except during the playoffs. The bleachers have been referred to as the ""World's Largest Beer Garden."" A popular T-shirt (sold inside the park and licensed by the club) which says ""Wrigley Bleachers"" on the front and the phrase ""Shut Up and Drink Your Beer"" on the reverse fuels this stereotype. In only his third career start, Kerry Wood struck out 20 batters against Houston on May 6, 1998. This is the franchise record and tied for the Major League record for the most strikeouts in one game by one pitcher (the only other pitcher to strike out 20 batters in a nine-inning game was Roger Clemens, who achieved it twice). The game is often considered the most dominant pitching performance of all time. Interestingly, Wood's first pitch struck home plate umpire Jerry Meals in the facemask. Wood then struck out the first five batters he faced. Wood hit one batter, Craig Biggio, and allowed one hit, a scratch single by Ricky Gutiérrez off third baseman Kevin Orie's glove. The play was nearly scored an error, which would have given Wood a no-hitter. The 1989 film Back to the Future Part II depicts the Chicago Cubs defeating a baseball team from Miami in the 2015 World Series, ending the longest championship drought in all four of the major North American professional sports leagues. In 2015, the Miami Marlins failed to make the playoffs and were able to make it to the 2015 National League Wild Card round and move on to the 2015 National League Championship Series by October 21, 2015, the date where protagonist Marty McFly traveled to the future in the film. However, it was on October 21 that the Cubs were swept by the New York Mets in the NLCS. ""Baseball's Sad Lexicon,"" also known as ""Tinker to Evers to Chance"" after its refrain, is a 1910 baseball poem by Franklin Pierce Adams. The poem is presented as a single, rueful stanza from the point of view of a New York Giants fan seeing the talented Chicago Cubs infield of shortstop Joe Tinker, second baseman Johnny Evers, and first baseman Frank Chance complete a double play. The trio began playing together with the Cubs in 1902, and formed a double play combination that lasted through April 1912. The Cubs won the pennant four times between 1906 and 1910, often defeating the Giants en route to the World Series. On October 1, 1932, in game three of the World Series between the Cubs and the New York Yankees, Babe Ruth allegedly stepped to the plate, pointed his finger to Wrigley Field's center field bleachers and hit a long home run to center. There is speculation as to whether the ""facts"" surrounding the story are true or not, but nevertheless Ruth did help the Yankees secure a World Series win that year and the home run accounted for his 15th and last home run in the post season before he retired in 1935. In June, 1998 Sammy Sosa exploded into the pursuit of Roger Maris' home run record. Sosa had 13 home runs entering the month, representing less than half of Mark McGwire's total. Sosa had his first of four multi-home run games that month on June 1, and went on to break Rudy York's record with 20 home runs in the month, a record that still stands. By the end of his historic month, the outfielder's 33 home runs tied him with Ken Griffey, Jr. and left him only four behind McGwire's 37. Sosa finished with 66 and won the NL MVP Award. The Cubs successfully defended their National League Central title in 2008, going to the postseason in consecutive years for the first time since 1906–08. The offseason was dominated by three months of unsuccessful trade talks with the Orioles involving 2B Brian Roberts, as well as the signing of Chunichi Dragons star Kosuke Fukudome. The team recorded their 10,000th win in April, while establishing an early division lead. Reed Johnson and Jim Edmonds were added early on and Rich Harden was acquired from the Oakland Athletics in early July. The Cubs headed into the All-Star break with the N.L.'s best record, and tied the league record with eight representatives to the All-Star game, including catcher Geovany Soto, who was named Rookie of the Year. The Cubs took control of the division by sweeping a four-game series in Milwaukee. On September 14, in a game moved to Miller Park due to Hurricane Ike, Zambrano pitched a no-hitter against the Astros, and six days later the team clinched by beating St. Louis at Wrigley. The club ended the season with a 97–64 record and met Los Angeles in the NLDS. The heavily favored Cubs took an early lead in Game 1, but James Loney's grand slam off Ryan Dempster changed the series' momentum. Chicago committed numerous critical errors and were outscored 20–6 in a Dodger sweep, which provided yet another sudden ending. Despite trading for pitcher Matt Garza and signing free-agent slugger Carlos Peña, the Cubs finished the 2011 season 20 games under .500 with a record of 71-91. Weeks after the season came to an end, the club was rejuvenated in the form of a new philosophy, as new owner Tom Ricketts signed Theo Epstein away from the Boston Red Sox, naming him club President and giving him a five-year contract worth over $18 million, and subsequently discharged manager Mike Quade. Epstein, a proponent of sabremetrics and one of the architects of two world series titles in Boston brought along Jed Hoyer to fill the role of GM and hired Dale Sveum as manager. Although the team had a dismal 2012 season, losing 101 games (the worst record since 1966) it was largely expected. The youth movement ushered in by Epstein and Hoyer began as longtime fan favorite Kerry Wood retired in May, followed by Ryan Dempster and Geovany Soto being traded to Texas at the All-Star break for a group of minor league prospects headlined by Christian Villanueva. The development of Castro, Anthony Rizzo, Darwin Barney, Brett Jackson and pitcher Jeff Samardzija as well as the replenishing of the minor-league system with prospects such as Javier Baez, Albert Almora, and Jorge Soler became the primary focus of the season, a philosophy which the new management said would carry over at least through the 2013 season. Near the end of the first decade of the double-Bills' guidance, the Cubs won the NL pennant in 1929 and then achieved the unusual feat of winning a pennant every three years, following up the 1929 flag with league titles in 1932, 1935, and 1938. Unfortunately, their success did not extend to the Fall Classic, as they fell to their AL rivals each time. The '32 series against the Yankees featured Babe Ruth's ""called shot"" at Wrigley Field in Game 3. There were some historic moments for the Cubs as well; In 1930, Hack Wilson, one of the top home run hitters in the game, had one of the most impressive seasons in MLB history, hitting 56 home runs and establishing the current runs-batted-in record of 191. That 1930 club, which boasted six eventual Hall of Famers (Wilson, Gabby Hartnett, Rogers Hornsby, George ""High Pockets"" Kelly, Kiki Cuyler and manager Joe McCarthy) established the current team batting average record of .309. In 1935 the Cubs claimed the pennant in thrilling fashion, winning a record 21 games in a row in September. The '38 club saw Dizzy Dean lead the team's pitching staff and provided a historic moment when they won a crucial late-season game at Wrigley Field over the Pittsburgh Pirates with a walk-off home run by Gabby Hartnett, which became known in baseball lore as ""The Homer in the Gloamin'"". In 1914, advertising executive Albert Lasker obtained a large block of the club's shares and before the 1916 season assumed majority ownership of the franchise. Lasker brought in a wealthy partner, Charles Weeghman, the proprietor of a popular chain of lunch counters who had previously owned the Chicago Whales of the short-lived Federal League. As principal owners, the pair moved the club from the West Side Grounds to the much newer Weeghman Park, which had been constructed for the Whales only two years earlier, where they remain to this day. The Cubs responded by winning a pennant in the war-shortened season of 1918, where they played a part in another team's curse: the Boston Red Sox defeated Grover Cleveland Alexander's Cubs four games to two in the 1918 World Series, Boston's last Series championship until 2004. Following the '69 season, the club posted winning records for the next few seasons, but no playoff action. After the core players of those teams started to move on, the 70s got worse for the team, and they became known as ""The Loveable Losers."" In 1977, the team found some life, but ultimately experienced one of its biggest collapses. The Cubs hit a high-water mark on June 28 at 47–22, boasting an 8 1⁄2 game NL East lead, as they were led by Bobby Murcer (27 Hr/89 RBI), and Rick Reuschel (20–10). However, the Philadelphia Phillies cut the lead to two by the All-star break, as the Cubs sat 19 games over .500, but they swooned late in the season, going 20–40 after July 31. The Cubs finished in 4th place at 81–81, while Philadelphia surged, finishing with 101 wins. The following two seasons also saw the Cubs get off to a fast start, as the team rallied to over 10 games above .500 well into both seasons, only to again wear down and play poorly later on, and ultimately settling back to mediocrity. This trait became known as the ""June Swoon."" Again, the Cubs' unusually high number of day games is often pointed to as one reason for the team's inconsistent late season play. Before signing a developmental agreement with the Kane County Cougars in 2012, the Cubs had a Class A minor league affiliation on two occasions with the Peoria Chiefs (1985–1995 and 2004–2012). Ryne Sandberg managed the Chiefs from 2006 to 2010. In the period between those associations with the Chiefs the club had affiliations with the Dayton Dragons and Lansing Lugnuts. The Lugnuts were often affectionately referred to by Chip Caray as ""Steve Stone's favorite team."" The 2007 developmental contract with the Tennessee Smokies was preceded by Double A affiliations with the Orlando Cubs and West Tenn Diamond Jaxx. On September 16, 2014 the Cubs announced a move of their top Class A affiliate from Daytona in the Florida State League to Myrtle Beach in the Carolina League for the 2015 season. Two days later, on the 18th, the Cubs signed a 4-year player development contract with the South Bend Silver Hawks of the Midwest League, ending their brief relationship with the Kane County Cougars and shortly thereafter renaming the Silver Hawks the South Bend Cubs. In 1975, a group of Chicago Cubs fans based in Washington, D.C. formed the Emil Verban Society. The society is a select club of high profile Cub fans, currently headed by Illinois Senator Dick Durbin which is named for Emil Verban, who in three seasons with the Cubs in the 1940s batted .280 with 39 runs batted in and one home run. Verban was picked as the epitome of a Cub player, explains columnist George Will, because ""He exemplified mediocrity under pressure, he was competent but obscure and typifying of the work ethics."" Verban initially believed he was being ridiculed, but his ill feeling disappeared several years later when he was flown to Washington to meet President Ronald Reagan, also a society member, at the White House. Hillary Clinton, Jim Belushi, Joe Mantegna, Rahm Emanuel, Dick Cheney and many others have been included among its membership. In addition to Mesa, the club has held spring training in Hot Springs, Arkansas (1886, 1896–1900), (1909–1910) New Orleans (1870, 1907, 1911–1912); Champaign, Illinois (1901–02, 1906); Los Angeles (1903–04, 1948–1949), Santa Monica, California (1905); French Lick, Indiana (1908, 1943–1945); Tampa, Florida (1913–1916); Pasadena, California (1917–1921); Santa Catalina Island, California (1922–1942, 1946–1947, 1950–1951); Rendezvous Park in Mesa (1952–1965); Blair Field in Long Beach, California (1966); and Scottsdale, Arizona (1967–1978). After over a dozen more subpar seasons, in 1981 the Cubs hired GM Dallas Green from Philadelphia to turn around the franchise. Green had managed the 1980 Phillies to the World Series title. One of his early GM moves brought in a young Phillies minor-league 3rd baseman named Ryne Sandberg, along with Larry Bowa for Iván DeJesús. The 1983 Cubs had finished 71–91 under Lee Elia, who was fired before the season ended by Green. Green continued the culture of change and overhauled the Cubs roster, front-office and coaching staff prior to 1984. Jim Frey was hired to manage the 1984 Cubs, with Don Zimmer coaching 3rd base and Billy Connors serving as pitching coach. The confusion may stem from the fact that Major League Baseball did decide that, should the Cubs make it to the World Series, the American League winner would have home field advantage unless the Cubs hosted home games at an alternate site since the Cubs home field of Wrigley Field did not yet have lights. Rumor was the Cubs could hold home games across town at Comiskey Park, home of the American League's Chicago White Sox. Rather than hold any games in the cross town rival Sox Park, the Cubs made arrangements with the August A. Busch, owner of the St. Louis Cardinals, to use Busch Stadium in St. Louis as the Cubs ""home field"" for the World Series. This was approved by Major League Baseball and would have enabled the Cubs to host games 1 and 2, along with games 6 and 7 if necessary. At the time home field advantage was rotated between each league. Odd numbered years the AL had home field advantage. Even numbered years the NL had home field advantage. In the 1982 World Series the St. Louis Cardinals of the NL had home field advantage. In the 1983 World Series the Baltimore Orioles of the AL had home field advantage. Hack Wilson set a record of 56 home-runs and 190 runs-batted-in in 1930, breaking Lou Gehrig's MLB record of 176 RBI. (In 1999, a long-lost extra RBI mistakenly credited to Charlie Grimm had been found by Cooperstown researcher Cliff Kachline and verified by historian Jerome Holtzman, increasing the record number to 191.) As of 2014 the record still stands, with no serious threats coming since Gehrig (184) and Hank Greenberg (183) in the same era. The closest anyone has come to the mark in the last 75 years was Manny Ramirez's 165 RBI in 1999. In addition to the RBI record, Wilson 56 home-runs stood as the National League record until 1998, when Sammy Sosa and Mark McGwire hit 66 and 70, respectively. Wilson was named ""Most Useful"" player that year by the Baseball Writers' Association of America, as the official N.L. Most Valuable Player Award was not awarded until the next season."
Somerset,"The county has several museums; those at Bath include the American Museum in Britain, the Museum of Bath Architecture, the Herschel Museum of Astronomy, the Jane Austen Centre, and the Roman Baths. Other visitor attractions which reflect the cultural heritage of the county include: Claverton Pumping Station, Dunster Working Watermill, the Fleet Air Arm Museum at Yeovilton, Nunney Castle, The Helicopter Museum in Weston-super-Mare, King John's Hunting Lodge in Axbridge, Blake Museum Bridgwater, Radstock Museum, Museum of Somerset in Taunton, the Somerset Rural Life Museum in Glastonbury, and Westonzoyland Pumping Station Museum. Towns such as Castle Cary and Frome grew around the medieval weaving industry. Street developed as a centre for the production of woollen slippers and, later, boots and shoes, with C. & J. Clark establishing its headquarters in the town. C&J Clark's shoes are no longer manufactured there as the work was transferred to lower-wage areas, such as China and Asia. Instead, in 1993, redundant factory buildings were converted to form Clarks Village, the first purpose-built factory outlet in the UK. C&J Clark also had shoe factories, at one time at Bridgwater, Minehead, Westfield and Weston super Mare to provide employment outside the main summer tourist season, but those satellite sites were closed in the late 1980s, before the main site at Street. Dr. Martens shoes were also made in Somerset, by the Northampton-based R. Griggs Group, using redundant skilled shoemakers from C&J Clark; that work has also been transferred to Asia. In Arthurian legend, Avalon became associated with Glastonbury Tor when monks at Glastonbury Abbey claimed to have discovered the bones of King Arthur and his queen. What is more certain is that Glastonbury was an important religious centre by 700 and claims to be ""the oldest above-ground Christian church in the World"" situated ""in the mystical land of Avalon."" The claim is based on dating the founding of the community of monks at AD 63, the year of the legendary visit of Joseph of Arimathea, who was supposed to have brought the Holy Grail. During the Middle Ages there were also important religious sites at Woodspring Priory and Muchelney Abbey. The present Diocese of Bath and Wells covers Somerset – with the exception of the Parish of Abbots Leigh with Leigh Woods in North Somerset – and a small area of Dorset. The Episcopal seat of the Bishop of Bath and Wells is now in the Cathedral Church of Saint Andrew in the city of Wells, having previously been at Bath Abbey. Before the English Reformation, it was a Roman Catholic diocese; the county now falls within the Roman Catholic Diocese of Clifton. The Benedictine monastery Saint Gregory's Abbey, commonly known as Downside Abbey, is at Stratton-on-the-Fosse, and the ruins of the former Cistercian Cleeve Abbey are near the village of Washford. The usefulness of the canals was short-lived, though some have now been restored for recreation. The 19th century also saw the construction of railways to and through Somerset. The county was served by five pre-1923 Grouping railway companies: the Great Western Railway (GWR); a branch of the Midland Railway (MR) to Bath Green Park (and another one to Bristol); the Somerset and Dorset Joint Railway, and the London and South Western Railway (L&SWR). The former main lines of the GWR are still in use today, although many of its branch lines were scrapped under the notorious Beeching Axe. The former lines of the Somerset and Dorset Joint Railway closed completely, as has the branch of the Midland Railway to Bath Green Park (and to Bristol St Philips); however, the L&SWR survived as a part of the present West of England Main Line. None of these lines, in Somerset, are electrified. Two branch lines, the West and East Somerset Railways, were rescued and transferred back to private ownership as ""heritage"" lines. The fifth railway was a short-lived light railway, the Weston, Clevedon and Portishead Light Railway. The West Somerset Mineral Railway carried the iron ore from the Brendon Hills to Watchet. Somerset has a high indigenous British population, with 98.8% registering as white British and 92.4% of these as born in the United Kingdom. Chinese is the largest ethnic group, while the black minority ethnic proportion of the total population is 2.9%. Over 25% of Somerset's population is concentrated in Taunton, Bridgwater and Yeovil. The rest of the county is rural and sparsely populated. Over 9 million tourist nights are spent in Somerset each year, which significantly increases the population at peak times. Until the 1960s the piers at Weston-super-Mare, Clevedon, Portishead and Minehead were served by the paddle steamers of P and A Campbell who ran regular services to Barry and Cardiff as well as Ilfracombe and Lundy Island. The pier at Burnham-on-Sea was used for commercial goods, one of the reasons for the Somerset and Dorset Joint Railway was to provide a link between the Bristol Channel and the English Channel. The pier at Burnham-on-Sea is the shortest pier in the UK. In the 1970s the Royal Portbury Dock was constructed to provide extra capacity for the Port of Bristol. Along with the rest of South West England, Somerset has a temperate climate which is generally wetter and milder than the rest of the country. The annual mean temperature is approximately 10 °C (50.0 °F). Seasonal temperature variation is less extreme than most of the United Kingdom because of the adjacent sea temperatures. The summer months of July and August are the warmest with mean daily maxima of approximately 21 °C (69.8 °F). In winter mean minimum temperatures of 1 °C (33.8 °F) or 2 °C (35.6 °F) are common. In the summer the Azores high pressure affects the south-west of England, but convective cloud sometimes forms inland, reducing the number of hours of sunshine. Annual sunshine rates are slightly less than the regional average of 1,600 hours. In December 1998 there were 20 days without sun recorded at Yeovilton. Most the rainfall in the south-west is caused by Atlantic depressions or by convection. Most of the rainfall in autumn and winter is caused by the Atlantic depressions, which is when they are most active. In summer, a large proportion of the rainfall is caused by sun heating the ground leading to convection and to showers and thunderstorms. Average rainfall is around 700 mm (28 in). About 8–15 days of snowfall is typical. November to March have the highest mean wind speeds, and June to August the lightest winds. The predominant wind direction is from the south-west. Many Somerset soldiers died during the First World War, with the Somerset Light Infantry suffering nearly 5,000 casualties. War memorials were put up in most of the county's towns and villages; only nine, described as the Thankful Villages, had none of their residents killed. During the Second World War the county was a base for troops preparing for the D-Day landings. Some of the hospitals which were built for the casualties of the war remain in use. The Taunton Stop Line was set up to repel a potential German invasion. The remains of its pill boxes can still be seen along the coast, and south through Ilminster and Chard. Tourism is a major industry, estimated in 2001 to support around 23,000 people. Attractions include the coastal towns, part of the Exmoor National Park, the West Somerset Railway (a heritage railway), and the museum of the Fleet Air Arm at RNAS Yeovilton. The town of Glastonbury has mythical associations, including legends of a visit by the young Jesus of Nazareth and Joseph of Arimathea, with links to the Holy Grail, King Arthur, and Camelot, identified by some as Cadbury Castle, an Iron Age hill fort. Glastonbury also gives its name to an annual open-air rock festival held in nearby Pilton. There are show caves open to visitors in the Cheddar Gorge, as well as its locally produced cheese, although there is now only one remaining cheese maker in the village of Cheddar. The people of Somerset are mentioned in the Anglo-Saxon Chronicle's entry for AD 845, in the inflected form ""Sumursætum"", and the county is recorded in the entry for 1015 using the same name. The archaic name Somersetshire was mentioned in the Chronicle's entry for 878. Although ""Somersetshire"" was in common use as an alternative name for the county, it went out of fashion in the late 19th century, and is no longer used possibly due to the adoption of ""Somerset"" as the county's official name after the establishment of the county council in 1889. As with other counties not ending in ""shire,"" the suffix was superfluous, as there was no need to differentiate between the county and a town within it. The county has a long tradition of supplying freestone and building stone. Quarries at Doulting supplied freestone used in the construction of Wells Cathedral. Bath stone is also widely used. Ralph Allen promoted its use in the early 18th century, as did Hans Price in the 19th century, but it was used long before then. It was mined underground at Combe Down and Bathampton Down Mines, and as a result of cutting the Box Tunnel, at locations in Wiltshire such as Box. Bath stone is still used on a reduced scale today, but more often as a cladding rather than a structural material. Further south, Hamstone is the colloquial name given to stone from Ham Hill, which is also widely used in the construction industry. Blue Lias has been used locally as a building stone and as a raw material for lime mortar and Portland cement. Until the 1960s, Puriton had Blue Lias stone quarries, as did several other Polden villages. Its quarries also supplied a cement factory at Dunball, adjacent to the King's Sedgemoor Drain. Its derelict, early 20th century remains, was removed when the M5 motorway was constructed in the mid-1970s. Since the 1920s, the county has supplied aggregates. Foster Yeoman is Europe's large supplier of limestone aggregates, with quarries at Merehead Quarry. It has a dedicated railway operation, Mendip Rail, which is used to transport aggregates by rail from a group of Mendip quarries. State schools in Somerset are provided by three local education authorities: Bath and North East Somerset, North Somerset, and the larger Somerset County Council. All state schools are comprehensive. In some areas primary, infant and junior schools cater for ages four to eleven, after which the pupils move on to secondary schools. There is a three-tier system of first, middle and upper schools in the Cheddar Valley, and in West Somerset, while most other schools in the county use the two-tier system. Somerset has 30 state and 17 independent secondary schools; Bath and North East Somerset has 13 state and 5 independent secondary schools; and North Somerset has 10 state and 2 independent secondary schools, excluding sixth form colleges. Somerset is a rural county of rolling hills such as the Blackdown Hills, Mendip Hills, Quantock Hills and Exmoor National Park, and large flat expanses of land including the Somerset Levels. There is evidence of human occupation from Paleolithic times, and of subsequent settlement in the Roman and Anglo-Saxon periods. The county played a significant part in the consolidation of power and rise of King Alfred the Great, and later in the English Civil War and the Monmouth Rebellion. The city of Bath is famous for its substantial Georgian architecture and is a UNESCO World Heritage Site. The boundaries of Somerset are largely unaltered from medieval times. The River Avon formed much of the border with Gloucestershire, except that the hundred of Bath Forum, which straddles the Avon, formed part of Somerset. Bristol began as a town on the Gloucestershire side of the Avon, however as it grew it extended across the river into Somerset. In 1373 Edward III proclaimed ""that the town of Bristol with its suburbs and precincts shall henceforth be separate from the counties of Gloucester and Somerset... and that it should be a county by itself"". The main coastal towns are, from the west to the north-east, Minehead, Watchet, Burnham-on-Sea, Weston-super-Mare, Clevedon and Portishead. The coastal area between Minehead and the eastern extreme of the administrative county's coastline at Brean Down is known as Bridgwater Bay, and is a National Nature Reserve. North of that, the coast forms Weston Bay and Sand Bay whose northern tip, Sand Point, marks the lower limit of the Severn Estuary. In the mid and north of the county the coastline is low as the level wetlands of the levels meet the sea. In the west, the coastline is high and dramatic where the plateau of Exmoor meets the sea, with high cliffs and waterfalls. Hinkley Point C nuclear power station is a project to construct a 3,200 MW two reactor nuclear power station. On 18 October 2010, the British government announced that Hinkley Point – already the site of the disused Hinkley Point A and the still operational Hinkley Point B power stations – was one of the eight sites it considered suitable for future nuclear power stations. NNB Generation Company, a subsidiary of EDF, submitted an application for development consent to the Infrastructure Planning Commission on 31 October 2011. A protest group, Stop Hinkley, was formed to campaign for the closure of Hinkley Point B and oppose any expansion at the Hinkley Point site. In December 2013, the European Commission opened an investigation to assess whether the project breaks state-aid rules. On 8 October 2014 it was announced that the European Commission has approved the project, with an overwhelming majority and only four commissioners voting against the decision. The ceremonial county of Somerset consists of a two-tier non-metropolitan county, which is administered by Somerset County Council and five district councils, and two unitary authority areas (whose councils combine the functions of a county and a district). The five districts of Somerset are West Somerset, South Somerset, Taunton Deane, Mendip, and Sedgemoor. The two unitary authorities — which were established on 1 April 1996 following the break-up of the short-lived county of Avon — are North Somerset, and Bath & North East Somerset. All of the ceremonial county of Somerset is covered by the Avon and Somerset Constabulary, a police force which also covers Bristol and South Gloucestershire. The Devon and Somerset Fire and Rescue Service was formed in 2007 upon the merger of the Somerset Fire and Rescue Service with its neighbouring Devon service; it covers the area of Somerset County Council as well as the entire ceremonial county of Devon. The unitary districts of North Somerset and Bath & North East Somerset are instead covered by the Avon Fire and Rescue Service, a service which also covers Bristol and South Gloucestershire. The South Western Ambulance Service covers the entire South West of England, including all of Somerset; prior to February 2013 the unitary districts of Somerset came under the Great Western Ambulance Service, which merged into South Western. The Dorset and Somerset Air Ambulance is a charitable organisation based in the county. A number of decoy towns were constructed in Somerset in World War II to protect Bristol and other towns, at night. They were designed to mimic the geometry of ""blacked out"" streets, railway lines, and Bristol Temple Meads railway station, to encourage bombers away from these targets. One, on the radio beam flight path to Bristol, was constructed on Beacon Batch. It was laid out by Shepperton Studios, based on aerial photographs of the city's railway marshalling yards. The decoys were fitted with dim red lights, simulating activities like the stoking of steam locomotives. Burning bales of straw soaked in creosote were used to simulate the effects of incendiary bombs dropped by the first wave of Pathfinder night bombers; meanwhile, incendiary bombs dropped on the correct location were quickly smothered, wherever possible. Drums of oil were also ignited to simulate the effect of a blazing city or town, with the aim of fooling subsequent waves of bombers into dropping their bombs on the wrong location. The Chew Magna decoy town was hit by half-a-dozen bombs on 2 December 1940, and over a thousand incendiaries on 3 January 1941. The following night the Uphill decoy town, protecting Weston-super-Mare's airfield, was bombed; a herd of dairy cows was hit, killing some and severely injuring others. The University of Bath and Bath Spa University are higher education establishments in the north-east of the county. The University of Bath gained its Royal Charter in 1966, although its origins go back to the Bristol Trade School (founded 1856) and Bath School of Pharmacy (founded 1907). It has a purpose-built campus at Claverton on the outskirts of Bath, and has 15,000 students. Bath Spa University, which is based at Newton St Loe, achieved university status in 2005, and has origins including the Bath Academy of Art (founded 1898), Bath Teacher Training College, and the Bath College of Higher Education. It has several campuses and 5,500 students. The Somerset Coal Canal was built in the early 19th century to reduce the cost of transportation of coal and other heavy produce. The first 16 kilometres (10 mi), running from a junction with the Kennet and Avon Canal, along the Cam valley, to a terminal basin at Paulton, were in use by 1805, together with several tramways. A planned 11.7 km (7.3 mi) branch to Midford was never built, but in 1815 a tramway was laid along its towing path. In 1871 the tramway was purchased by the Somerset and Dorset Joint Railway (S&DJR), and operated until the 1950s. Somerset has 11,500 listed buildings, 523 scheduled monuments, 192 conservation areas, 41 parks and gardens including those at Barrington Court, Holnicote Estate, Prior Park Landscape Garden and Tintinhull Garden, 36 English Heritage sites and 19 National Trust sites, including Clevedon Court, Fyne Court, Montacute House and Tyntesfield as well as Stembridge Tower Mill, the last remaining thatched windmill in England. Other historic houses in the county which have remained in private ownership or used for other purposes include Halswell House and Marston Bigot. A key contribution of Somerset architecture is its medieval church towers. Jenkins writes, ""These structures, with their buttresses, bell-opening tracery and crowns, rank with Nottinghamshire alabaster as England's finest contribution to medieval art."" There is an extensive network of caves, including Wookey Hole, underground rivers, and gorges, including the Cheddar Gorge and Ebbor Gorge. The county has many rivers, including the Axe, Brue, Cary, Parrett, Sheppey, Tone and Yeo. These both feed and drain the flat levels and moors of mid and west Somerset. In the north of the county the River Chew flows into the Bristol Avon. The Parrett is tidal almost to Langport, where there is evidence of two Roman wharfs. At the same site during the reign of King Charles I, river tolls were levied on boats to pay for the maintenance of the bridge. Bath Rugby play at the Recreation Ground in Bath, and the Somerset County Cricket Club are based at the County Ground in Taunton. The county gained its first Football League club in 2003, when Yeovil Town won promotion to Division Three as Football Conference champions. They had achieved numerous FA Cup victories over football League sides in the past 50 years, and since joining the elite they have won promotion again—as League Two champions in 2005. They came close to yet another promotion in 2007, when they reached the League One playoff final, but lost to Blackpool at the newly reopened Wembley Stadium. Yeovil achieved promotion to the Championship in 2013 after beating Brentford in the playoff final. Horse racing courses are at Taunton and Wincanton. Agriculture and food and drink production continue to be major industries in the county, employing over 15,000 people. Apple orchards were once plentiful, and Somerset is still a major producer of cider. The towns of Taunton and Shepton Mallet are involved with the production of cider, especially Blackthorn Cider, which is sold nationwide, and there are specialist producers such as Burrow Hill Cider Farm and Thatchers Cider. Gerber Products Company in Bridgwater is the largest producer of fruit juices in Europe, producing brands such as ""Sunny Delight"" and ""Ocean Spray."" Development of the milk-based industries, such as Ilchester Cheese Company and Yeo Valley Organic, have resulted in the production of ranges of desserts, yoghurts and cheeses, including Cheddar cheese—some of which has the West Country Farmhouse Cheddar Protected Designation of Origin (PDO). After the Romans left, Britain was invaded by Anglo-Saxon peoples. By AD 600 they had established control over much of what is now England, but Somerset was still in native British hands. The British held back Saxon advance into the south-west for some time longer, but by the early eighth century King Ine of Wessex had pushed the boundaries of the West Saxon kingdom far enough west to include Somerset. The Saxon royal palace in Cheddar was used several times in the 10th century to host the Witenagemot. After the Norman Conquest, the county was divided into 700 fiefs, and large areas were owned by the crown, with fortifications such as Dunster Castle used for control and defence. Somerset contains HM Prison Shepton Mallet, which was England's oldest prison still in use prior to its closure in 2013, having opened in 1610. In the English Civil War Somerset was largely Parliamentarian, with key engagements being the Sieges of Taunton and the Battle of Langport. In 1685 the Monmouth Rebellion was played out in Somerset and neighbouring Dorset. The rebels landed at Lyme Regis and travelled north, hoping to capture Bristol and Bath, but they were defeated in the Battle of Sedgemoor at Westonzoyland, the last pitched battle fought in England. Arthur Wellesley took his title, Duke of Wellington from the town of Wellington; he is commemorated on a nearby hill by a large, spotlit obelisk, known as the Wellington Monument. Somerton took over from Ilchester as the county town in the late thirteenth century, but it declined in importance and the status of county town transferred to Taunton about 1366. The county has two cities, Bath and Wells, and 30 towns (including the county town of Taunton, which has no town council but instead is the chief settlement of the county's only borough). The largest urban areas in terms of population are Bath, Weston-super-Mare, Taunton, Yeovil and Bridgwater. Many settlements developed because of their strategic importance in relation to geographical features, such as river crossings or valleys in ranges of hills. Examples include Axbridge on the River Axe, Castle Cary on the River Cary, North Petherton on the River Parrett, and Ilminster, where there was a crossing point on the River Isle. Midsomer Norton lies on the River Somer; while the Wellow Brook and the Fosse Way Roman road run through Radstock. Chard is the most southerly town in Somerset, and at an altitude of 121 m (397 ft) it is also the highest. The Industrial Revolution in the Midlands and Northern England spelled the end for most of Somerset's cottage industries. Farming continued to flourish, however, and the Bath and West of England Society for the Encouragement of Agriculture, Arts, Manufactures and Commerce was founded in 1777 to improve farming methods. Despite this, 20 years later John Billingsley conducted a survey of the county's agriculture in 1795 and found that agricultural methods could still be improved. Coal mining was an important industry in north Somerset during the 18th and 19th centuries, and by 1800 it was prominent in Radstock. The Somerset Coalfield reached its peak production by the 1920s, but all the pits have now been closed, the last in 1973. Most of the surface buildings have been removed, and apart from a winding wheel outside Radstock Museum, little evidence of their former existence remains. Further west, the Brendon Hills were mined for iron ore in the late 19th century; this was taken by the West Somerset Mineral Railway to Watchet Harbour for shipment to the furnaces at Ebbw Vale. Some of the county's secondary schools have specialist school status. Some schools have sixth forms and others transfer their sixth formers to colleges. Several schools can trace their origins back many years, such as The Blue School in Wells and Richard Huish College in Taunton. Others have changed their names over the years such as Beechen Cliff School which was started in 1905 as the City of Bath Boys' School and changed to its present name in 1972 when the grammar school was amalgamated with a local secondary modern school, to form a comprehensive school. Many others were established and built since the Second World War. In 2006, 5,900 pupils in Somerset sat GCSE examinations, with 44.5% achieving 5 grades A-C including English and Maths (compared to 45.8% for England). Bridgwater was developed during the Industrial Revolution as the area's leading port. The River Parrett was navigable by large ships as far as Bridgwater. Cargoes were then loaded onto smaller boats at Langport Quay, next to the Bridgwater Bridge, to be carried further up river to Langport; or they could turn off at Burrowbridge and then travel via the River Tone to Taunton. The Parrett is now only navigable as far as Dunball Wharf. Bridgwater, in the 19th and 20th centuries, was a centre for the manufacture of bricks and clay roof tiles, and later cellophane, but those industries have now stopped. With its good links to the motorway system, Bridgwater has developed as a distribution hub for companies such as Argos, Toolstation, Morrisons and Gerber Juice. AgustaWestland manufactures helicopters in Yeovil, and Normalair Garratt, builder of aircraft oxygen systems, is also based in the town. Many towns have encouraged small-scale light industries, such as Crewkerne's Ariel Motor Company, one of the UK's smallest car manufacturers. Somerset is an important supplier of defence equipment and technology. A Royal Ordnance Factory, ROF Bridgwater was built at the start of the Second World War, between the villages of Puriton and Woolavington, to manufacture explosives. The site was decommissioned and closed in July 2008. Templecombe has Thales Underwater Systems, and Taunton presently has the United Kingdom Hydrographic Office and Avimo, which became part of Thales Optics. It has been announced twice, in 2006 and 2007, that manufacturing is to end at Thales Optics' Taunton site, but the trade unions and Taunton Deane District Council are working to reverse or mitigate these decisions. Other high-technology companies include the optics company Gooch and Housego, at Ilminster. There are Ministry of Defence offices in Bath, and Norton Fitzwarren is the home of 40 Commando Royal Marines. The Royal Naval Air Station in Yeovilton, is one of Britain's two active Fleet Air Arm bases and is home to the Royal Navy's Lynx helicopters and the Royal Marines Commando Westland Sea Kings. Around 1,675 service and 2,000 civilian personnel are stationed at Yeovilton and key activities include training of aircrew and engineers and the Royal Navy's Fighter Controllers and surface-based aircraft controllers. Population growth is higher than the national average, with a 6.4% increase, in the Somerset County Council area, since 1991, and a 17% increase since 1981. The population density is 1.4 persons per hectare, which can be compared to 2.07 persons per hectare for the South West region. Within the county, population density ranges 0.5 in West Somerset to 2.2 persons per hectare in Taunton Deane. The percentage of the population who are economically active is higher than the regional and national average, and the unemployment rate is lower than the regional and national average. There is also a range of independent or public schools. Many of these are for pupils between 11 and 18 years, such as King's College, Taunton and Taunton School. King's School, Bruton, was founded in 1519 and received royal foundation status around 30 years later in the reign of Edward VI. Millfield is the largest co-educational boarding school. There are also preparatory schools for younger children, such as All Hallows, and Hazlegrove Preparatory School. Chilton Cantelo School offers places both to day pupils and boarders aged 7 to 16. Other schools provide education for children from the age of 3 or 4 years through to 18, such as King Edward's School, Bath, Queen's College, Taunton and Wells Cathedral School which is one of the five established musical schools for school-age children in Britain. Some of these schools have religious affiliations, such as Monkton Combe School, Prior Park College, Sidcot School which is associated with the Religious Society of Friends, Downside School which is a Roman Catholic public school in Stratton-on-the-Fosse, situated next to the Benedictine Downside Abbey, and Kingswood School, which was founded by John Wesley in 1748 in Kingswood near Bristol, originally for the education of the sons of the itinerant ministers (clergy) of the Methodist Church. To the north-east of the Somerset Levels, the Mendip Hills are moderately high limestone hills. The central and western Mendip Hills was designated an Area of Outstanding Natural Beauty in 1972 and covers 198 km2 (76 sq mi). The main habitat on these hills is calcareous grassland, with some arable agriculture. To the south-west of the Somerset Levels are the Quantock Hills which was England's first Area of Outstanding Natural Beauty designated in 1956 which is covered in heathland, oak woodlands, ancient parklands with plantations of conifer and covers 99 square kilometres. The Somerset Coalfield is part of a larger coalfield which stretches into Gloucestershire. To the north of the Mendip hills is the Chew Valley and to the south, on the clay substrate, are broad valleys which support dairy farming and drain into the Somerset Levels. Traditional willow growing and weaving (such as basket weaving) is not as extensive as it used to be but is still carried out on the Somerset Levels and is commemorated at the Willows and Wetlands Visitor Centre. Fragments of willow basket were found near the Glastonbury Lake Village, and it was also used in the construction of several Iron Age causeways. The willow was harvested using a traditional method of pollarding, where a tree would be cut back to the main stem. During the 1930s more than 3,600 hectares (8,900 acres) of willow were being grown commercially on the Levels. Largely due to the displacement of baskets with plastic bags and cardboard boxes, the industry has severely declined since the 1950s. By the end of the 20th century only about 140 hectares (350 acres) were grown commercially, near the villages of Burrowbridge, Westonzoyland and North Curry. The Somerset Levels is now the only area in the UK where basket willow is grown commercially. The Somerset Levels (or Somerset Levels and Moors as they are less commonly but more correctly known) are a sparsely populated wetland area of central Somerset, between the Quantock and Mendip hills. They consist of marine clay levels along the coast, and the inland (often peat based) moors. The Levels are divided into two by the Polden Hills; land to the south is drained by the River Parrett while land to the north is drained by the River Axe and the River Brue. The total area of the Levels amounts to about 647.5 square kilometres (160,000 acres) and broadly corresponds to the administrative district of Sedgemoor but also includes the south west of Mendip district. Approximately 70% of the area is grassland and 30% is arable. Stretching about 32 kilometres (20 mi) inland, this expanse of flat land barely rises above sea level. Before it was drained, much of the land was under a shallow brackish sea in winter and was marsh land in summer. Drainage began with the Romans, and was restarted at various times: by the Anglo-Saxons; in the Middle Ages by the Glastonbury Abbey, from 1400–1770; and during the Second World War, with the construction of the Huntspill River. Pumping and management of water levels still continues. The Glastonbury Festival of Contemporary Performing Arts takes place most years in Pilton, near Shepton Mallet, attracting over 170,000 music and culture lovers from around the world to see world-famous entertainers. The Big Green Gathering which grew out of the Green fields at the Glastonbury Festival is held in the Mendip Hills between Charterhouse and Compton Martin each summer. The annual Bath Literature Festival is one of several local festivals in the county; others include the Frome Festival and the Trowbridge Village Pump Festival, which, despite its name, is held at Farleigh Hungerford in Somerset. The annual circuit of West Country Carnivals is held in a variety of Somerset towns during the autumn, forming a major regional festival, and the largest Festival of Lights in Europe."
Seven_Years%27_War,"In April 1758, the British concluded the Anglo-Prussian Convention with Frederick in which they committed to pay him an annual subsidy of £670,000. Britain also dispatched 9,000 troops to reinforce Ferdinand's Hanoverian army, the first British troop commitment on the continent and a reversal in the policy of Pitt. Ferdinand had succeeded in driving the French from Hanover and Westphalia and re-captured the port of Emden in March 1758 before crossing the Rhine with his own forces, which caused alarm in France. Despite Ferdinand's victory over the French at the Battle of Krefeld and the brief occupation of Düsseldorf, he was compelled by the successful manoeuvering of larger French forces to withdraw across the Rhine. The war had also brought to an end the ""Old System"" of alliances in Europe, In the years after the war, under the direction of Lord Sandwich, the British did try to re-establish this system. But after her surprising grand success against a coalition of great powers, European states such as Austria, The Dutch Republic, Sweden, Denmark-Norway, Ottoman Empire, and Russia now saw Britain as a greater threat than France and did not join them, while the Prussians were angered by what they considered a British betrayal in 1762. Consequently, when the American War of Independence turned into a global war between 1778–83, Britain found itself opposed by a strong coalition of European powers, and lacking any substantial ally. Many middle and small powers in Europe, unlike in the previous wars, tried to steer clear away from the escalating conflict, even though they had interests in the conflict or with the belligerents, like Denmark-Norway. The Dutch Republic, long-time British ally, kept its neutrality intact, fearing the odds against Britain and Prussia fighting the great powers of Europe, even tried to prevent Britain's domination in India. Naples, Sicily, and Savoy, although sided with Franco-Spanish party, declined to join the coalition under the fear of British power. The taxation needed for war caused the Russian people considerable hardship, being added to the taxation of salt and alcohol begun by Empress Elizabeth in 1759 to complete her addition to the Winter Palace. Like Sweden, Russia concluded a separate peace with Prussia. Years later, Kaunitz kept trying to establish France's alliance with Austria. He tried as hard as he could for Austria to not get entangled in Hanover's political affairs, and was even willing to trade Austrian Netherlands for France's aid in recapturing Silesia. Frustrated by this decision and by the Dutch Republic's insistence on neutrality, Britain soon turned to Russia. On September 30, 1755, Britain pledged financial aid to Russia in order to station 50,000 troops on the Livonian-Lithunian border, so they could defend Britain's interests in Hanover immediately. Besthuzev, assuming the preparation was directed against Prussia, was more than happy to obey the request of the British. Unbeknownst to the other powers, King George II also made overtures to the Prussian king; Frederick, who began fearing the Austro-Russian intentions, and was excited to welcome a rapprochement with Britain. On January 16, 1756, the Convention of Westminster was signed wherein Britain and Prussia promised to aid one another in order to achieve lasting peace and stability in Europe. The most important French fort planned was intended to occupy a position at ""the Forks"" where the Allegheny and Monongahela Rivers meet to form the Ohio River (present day Pittsburgh, Pennsylvania). Peaceful British attempts to halt this fort construction were unsuccessful, and the French proceeded to build the fort they named Fort Duquesne. British colonial militia from Virginia were then sent to drive them out. Led by George Washington, they ambushed a small French force at Jumonville Glen on 28 May 1754 killing ten, including commander Jumonville. The French retaliated by attacking Washington's army at Fort Necessity on 3 July 1754 and forced Washington to surrender. The Hanoverian king George II of Great Britain was passionately devoted to his family’s continental holdings, but his commitments in Germany were counterbalanced by the demands of the British colonies overseas. If war against France for colonial expansion was to be resumed, then Hanover had to be secured against Franco-Prussian attack. France was very much interested in colonial expansion and was willing to exploit the vulnerability of Hanover in war against Great Britain, but it had no desire to divert forces to central Europe for Prussia's interest. During the war, the Seven Nations of Canada were allied with the French. These were Native Americans of the Laurentian valley—the Algonquin, the Abenaki, the Huron, and others. Although the Algonquin tribes and the Seven Nations were not directly concerned with the fate of the Ohio River Valley, they had been victims of the Iroquois Confederation. The Iroquois had encroached on Algonquin territory and pushed the Algonquins west beyond Lake Michigan. Therefore, the Algonquin and the Seven Nations were interested in fighting against the Iroquois. Throughout New England, New York, and the North-west Native American tribes formed differing alliances with the major belligerents. The Iroquois, dominant in what is now Upstate New York, sided with the British but did not play a large role in the war. Accordingly, leaving Field Marshal Count Kurt von Schwerin in Silesia with 25,000 soldiers to guard against incursions from Moravia or Hungary, and leaving Field Marshal Hans von Lehwaldt in East Prussia to guard against Russian invasion from the east, Frederick set off with his army for Saxony. The Prussian army marched in three columns. On the right was a column of about 15,000 men under the command of Prince Ferdinand of Brunswick. On the left was a column of 18,000 men under the command of the Duke of Brunswick-Bevern. In the centre was Frederick II, himself with Field Marshal James Keith commanding a corps of 30,000 troops. Ferdinand of Brunswick was to close in on the town of Chemnitz. The Duke of Brunswick-Bevern was to traverse Lusatia to close in on Bautzen. Meanwhile, Frederick and Field Marshal Keith would make for Dresden. The war was successful for Great Britain, which gained the bulk of New France in North America, Spanish Florida, some individual Caribbean islands in the West Indies, the colony of Senegal on the West African coast, and superiority over the French trading outposts on the Indian subcontinent. The Native American tribes were excluded from the settlement; a subsequent conflict, known as Pontiac's War, was also unsuccessful in returning them to their pre-war status. In Europe, the war began disastrously for Prussia, but a combination of good luck and successful strategy saw King Frederick the Great manage to retrieve the Prussian position and retain the status quo ante bellum. Prussia emerged as a new European great power. The involvement of Portugal, Spain and Sweden did not return them to their former status as great powers. France was deprived of many of its colonies and had saddled itself with heavy war debts that its inefficient financial system could barely handle. Spain lost Florida but gained French Louisiana and regained control of its colonies, e.g., Cuba and the Philippines, which had been captured by the British during the war. France and other European powers will soon avenge their defeat in 1778 when American Revolutionary War broke out, with hopes of destroying Britain's dominance once and for all. In 1762, towards the end of the war, French forces attacked St. John's, Newfoundland. If successful, the expedition would have strengthened France's hand at the negotiating table. Although they took St. John's and raided nearby settlements, the French forces were eventually defeated by British troops at the Battle of Signal Hill. This was the final battle of the war in North America, and it forced the French to surrender to Lieutenant Colonel William Amherst. The victorious British now controlled all of eastern North America. The war has been described as the first ""world war"", although this label was also given to various earlier conflicts like the Eighty Years' War, the Thirty Years' War, the War of the Spanish Succession and the War of the Austrian Succession, and to later conflicts like the Napoleonic Wars. The term ""Second Hundred Years' War"" has been used in order to describe the almost continuous level of world-wide conflict during the entire 18th century, reminiscent of the more famous and compact struggle of the 14th century. The Treaty of Hubertusburg, between Austria, Prussia, and Saxony, was signed on February 15, 1763, at a hunting lodge between Dresden and Leipzig. Negotiations had started there on December 31, 1762. Frederick, who had considered ceding East Prussia to Russia if Peter III helped him secure Saxony, finally insisted on excluding Russia (in fact, no longer a belligerent) from the negotiations. At the same time, he refused to evacuate Saxony until its elector had renounced any claim to reparation. The Austrians wanted at least to retain Glatz, which they had in fact reconquered, but Frederick would not allow it. The treaty simply restored the status quo of 1748, with Silesia and Glatz reverting to Frederick and Saxony to its own elector. The only concession that Prussia made to Austria was to consent to the election of Archduke Joseph as Holy Roman emperor. News of this arrived in Europe, where Britain and France unsuccessfully attempted to negotiate a solution. The two nations eventually dispatched regular troops to North America to enforce their claims. The first British action was the assault on Acadia on 16 June 1755 in the Battle of Fort Beauséjour, which was immediately followed by their expulsion of the Acadians. In July British Major General Edward Braddock led about 2,000 army troops and provincial militia on an expedition to retake Fort Duquesne, but the expedition ended in disastrous defeat. In further action, Admiral Edward Boscawen fired on the French ship Alcide on 8 June 1755, capturing it and two troop ships. In September 1755, French and British troops met in the inconclusive Battle of Lake George. Britain now threatened to withdraw its subsidies if Prussia didn't consider offering concessions to secure peace. As the Prussian armies had dwindled to just 60,000 men and with Berlin itself under siege, Frederick's survival was severely threatened. Then on 5 January 1762 the Russian Empress Elizabeth died. Her Prussophile successor, Peter III, at once recalled Russian armies from Berlin (see: the Treaty of Saint Petersburg (1762)) and mediated Frederick's truce with Sweden. He also placed a corps of his own troops under Frederick's command This turn of events has become known as the Miracle of the House of Brandenburg. Frederick was then able to muster a larger army of 120,000 men and concentrate it against Austria. He drove them from much of Saxony, while his brother Henry won a victory in Silesia in the Battle of Freiberg (29 October 1762). At the same time, his Brunswick allies captured the key town of Göttingen and compounded this by taking Cassel. Despite this, the Austrians, under the command of General Laudon, captured Glatz (now Kłodzko, Poland) in Silesia. In the Battle of Liegnitz Frederick scored a strong victory despite being outnumbered three to one. The Russians under General Saltykov and Austrians under General Lacy briefly occupied his capital, Berlin, in October, but could not hold it for long. The end of that year saw Frederick once more victorious, defeating the able Daun in the Battle of Torgau; but he suffered very heavy casualties, and the Austrians retreated in good order. The troops were reembarked and moved to the Bay of St. Lunaire in Brittany where, on 3 September, they were landed to operate against St. Malo; however, this action proved impractical. Worsening weather forced the two armies to separate: the ships sailed for the safer anchorage of St. Cast, while the army proceeded overland. The tardiness of Bligh in moving his forces allowed a French force of 10,000 from Brest to catch up with him and open fire on the reembarkation troops. A rear-guard of 1,400 under General Dury held off the French while the rest of the army embarked. They could not be saved; 750, including Dury, were killed and the rest captured. The British—by inclination as well as for practical reasons—had tended to avoid large-scale commitments of troops on the Continent. They sought to offset the disadvantage of this in Europe by allying themselves with one or more Continental powers whose interests were antithetical to those of their enemies, particularly France.:15–16 By subsidising the armies of continental allies, Britain could turn London's enormous financial power to military advantage. In the Seven Years' War, the British chose as their principal partner the greatest general of the day, Frederick the Great of Prussia, then the rising power in central Europe, and paid Frederick substantial subsidies for his campaigns.:106 This was accomplished in the Diplomatic Revolution of 1756, in which Britain ended its long-standing alliance with Austria in favor of Prussia, leaving Austria to side with France. In marked contrast to France, Britain strove to prosecute the war actively in the colonies, taking full advantage of its naval power. :64–66 The British pursued a dual strategy – naval blockade and bombardment of enemy ports, and rapid movement of troops by sea. They harassed enemy shipping and attacked enemy colonies, frequently using colonists from nearby British colonies in the effort. British Prime Minister William Pitt's focus on the colonies for the 1758 campaign paid off with the taking of Louisbourg after French reinforcements were blocked by British naval victory in the Battle of Cartagena and in the successful capture of Fort Duquesne and Fort Frontenac. The British also continued the process of deporting the Acadian population with a wave of major operations against Île Saint-Jean (present-day Prince Edward Island), the St. John River valley, and the Petitcodiac River valley. The celebration of these successes was dampened by their embarrassing defeat in the Battle of Carillon (Ticonderoga), in which 4,000 French troops repulsed 16,000 British. Austria was not able to retake Silesia or make any significant territorial gain. However, it did prevent Prussia from invading parts of Saxony. More significantly, its military performance proved far better than during the War of the Austrian Succession and seemed to vindicate Maria Theresa's administrative and military reforms. Hence, Austria's prestige was restored in great part and the empire secured its position as a major player in the European system. Also, by promising to vote for Joseph II in the Imperial elections, Frederick II accepted the Habsburg preeminence in the Holy Roman Empire. The survival of Prussia as a first-rate power and the enhanced prestige of its king and its army, however, was potentially damaging in the long run to Austria's influence in Germany. 1762 brought two new countries into the war. Britain declared war against Spain on 4 January 1762; Spain reacted by issuing their own declaration of war against Britain on 18 January. Portugal followed by joining the war on Britain's side. Spain, aided by the French, launched an invasion of Portugal and succeeded in capturing Almeida. The arrival of British reinforcements stalled a further Spanish advance, and the Battle of Valencia de Alcántara saw British-Portuguese forces overrun a major Spanish supply base. The invaders were stopped on the heights in front of Abrantes (called the pass to Lisbon) where the Anglo-Portuguese were entrenched. Eventually the Anglo-Portuguese army, aided by guerrillas and practicing a scorched earth strategy, chased the greatly reduced Franco-Spanish army back to Spain, recovering almost all the lost towns, among them the Spanish headquarters in Castelo Branco full of wounded and sick that had been left behind. The War of the Austrian Succession had seen the belligerents aligned on a time-honoured basis. France’s traditional enemies, Great Britain and Austria, had coalesced just as they had done against Louis XIV. Prussia, the leading anti-Austrian state in Germany, had been supported by France. Neither group, however, found much reason to be satisfied with its partnership: British subsidies to Austria had produced nothing of much help to the British, while the British military effort had not saved Silesia for Austria. Prussia, having secured Silesia, had come to terms with Austria in disregard of French interests. Even so, France had concluded a defensive alliance with Prussia in 1747, and the maintenance of the Anglo-Austrian alignment after 1748 was deemed essential by the Duke of Newcastle, British secretary of state in the ministry of his brother Henry Pelham. The collapse of that system and the aligning of France with Austria and of Great Britain with Prussia constituted what is known as the “diplomatic revolution” or the “reversal of alliances.” Despite the debatable strategic success and the operational failure of the descent on Rochefort, William Pitt—who saw purpose in this type of asymmetric enterprise—prepared to continue such operations. An army was assembled under the command of Charles Spencer, 3rd Duke of Marlborough; he was aided by Lord George Sackville. The naval squadron and transports for the expedition were commanded by Richard Howe. The army landed on 5 June 1758 at Cancalle Bay, proceeded to St. Malo, and, finding that it would take prolonged siege to capture it, instead attacked the nearby port of St. Servan. It burned shipping in the harbor, roughly 80 French privateers and merchantmen, as well as four warships which were under construction. The force then re-embarked under threat of the arrival of French relief forces. An attack on Havre de Grace was called off, and the fleet sailed on to Cherbourg; the weather being bad and provisions low, that too was abandoned, and the expedition returned having damaged French privateering and provided further strategic demonstration against the French coast. By this point Frederick was increasingly concerned by the Russian advance from the east and marched to counter it. Just east of the Oder in Brandenburg-Neumark, at the Battle of Zorndorf (now Sarbinowo, Poland), a Prussian army of 35,000 men under Frederick on Aug. 25, 1758, fought a Russian army of 43,000 commanded by Count William Fermor. Both sides suffered heavy casualties – the Prussians 12,800, the Russians 18,000 – but the Russians withdrew, and Frederick claimed victory. In the undecided Battle of Tornow on 25 September, a Swedish army repulsed six assaults by a Prussian army but did not push on Berlin following the Battle of Fehrbellin. The year 1759 saw several Prussian defeats. At the Battle of Kay, or Paltzig, the Russian Count Saltykov with 47,000 Russians defeated 26,000 Prussians commanded by General Carl Heinrich von Wedel. Though the Hanoverians defeated an army of 60,000 French at Minden, Austrian general Daun forced the surrender of an entire Prussian corps of 13,000 in the Battle of Maxen. Frederick himself lost half his army in the Battle of Kunersdorf (now Kunowice Poland), the worst defeat in his military career and one that drove him to the brink of abdication and thoughts of suicide. The disaster resulted partly from his misjudgment of the Russians, who had already demonstrated their strength at Zorndorf and at Gross-Jägersdorf (now Motornoye, Russia), and partly from good cooperation between the Russian and Austrian forces. The Saxon and Austrian armies were unprepared, and their forces were scattered. Frederick occupied Dresden with little or no opposition from the Saxons. At the Battle of Lobositz on 1 October 1756, Frederick prevented the isolated Saxon army from being reinforced by an Austrian army under General Browne. The Prussians then occupied Saxony; after the Siege of Pirna, the Saxon army surrendered in October 1756, and was forcibly incorporated into the Prussian army. The attack on neutral Saxony caused outrage across Europe and led to the strengthening of the anti-Prussian coalition. The only significant Austrian success was the partial occupation of Silesia. Far from being easy, Frederick's early successes proved indecisive and very costly for Prussia's smaller army. This led him to remark that he did not fight the same Austrians as he had during the previous war. The history of the Seven Years' War in North America, particularly the expulsion of the Acadians, the siege of Quebec, the death of Wolfe, and the Battle of Fort William Henry generated a vast number of ballads, broadsides, images, and novels (see Longfellow's Evangeline, Benjamin West's The Death of General Wolfe, James Fenimore Cooper's The Last of the Mohicans), maps and other printed materials, which testify to how this event held the imagination of the British and North American public long after Wolfe's death in 1759. In 1756 Austria was making military preparations for war with Prussia and pursuing an alliance with Russia for this purpose. On June 2, 1746, Austria and Russia concluded a defensive alliance that covered their own territory and Poland against attack by Prussia or the Ottoman Empire. They also agreed to a secret clause that promised the restoration of Silesia and the countship of Glatz (now Kłodzko, Poland) to Austria in the event of hostilities with Prussia. Their real desire, however, was to destroy Frederick’s power altogether, reducing his sway to his electorate of Brandenburg and giving East Prussia to Poland, an exchange that would be accompanied by the cession of the Polish Duchy of Courland to Russia. Aleksey Petrovich, Graf (count) Bestuzhev-Ryumin, grand chancellor of Russia under Empress Elizabeth, was hostile to both France and Prussia, but he could not persuade Austrian statesman Wenzel Anton von Kaunitz to commit to offensive designs against Prussia so long as Prussia was able to rely on French support. All of Britain's campaigns against New France succeeded in 1759, part of what became known as an Annus Mirabilis. Fort Niagara and Fort Carillon on 8 July 1758 fell to sizable British forces, cutting off French frontier forts further west. On 13 September 1759, following a three-month siege of Quebec, General James Wolfe defeated the French on the Plains of Abraham outside the city. The French staged a counteroffensive in the spring of 1760, with initial success at the Battle of Sainte-Foy, but they were unable to retake Quebec, due to British naval superiority following the battle of Neuville. The French forces retreated to Montreal, where on 8 September they surrendered to overwhelming British numerical superiority. The carefully coded word in the agreement proved no less catalytic for the other European powers. The results were absolute chaos. Empress Elizabeth of Russia was outraged at the duplicity of Britain's position. Not only that France was so enraged, and terrified, by the sudden betrayal of its only ally. Austria, particularly Kaunitz, used this situation to their utmost advantage. The now-isolated France was forced to accede to the Austro-Russian alliance or face ruin. Thereafter, on May 1, 1756, the First Treaty of Versailles was signed, in which both nations pledged 24.000 troops to defend each other in the case of an attack. This diplomatic revolution proved to be an important cause of the war; although both treaties were self-defensive in nature, the actions of both coalitions made the war virtually inevitable. The British government was close to bankruptcy, and Britain now faced the delicate task of pacifying its new French-Canadian subjects as well as the many American Indian tribes who had supported France. George III's Proclamation of 1763, which forbade white settlement beyond the crest of the Appalachians, was intended to appease the latter but led to considerable outrage in the Thirteen Colonies, whose inhabitants were eager to acquire native lands. The Quebec Act of 1774, similarly intended to win over the loyalty of French Canadians, also spurred resentment among American colonists. The act protected Catholic religion and French language, which enraged the Americans, but the Québécois remained loyal and did not rebel. The war was continuing indecisively when on 14 October Marshal Daun's Austrians surprised the main Prussian army at the Battle of Hochkirch in Saxony. Frederick lost much of his artillery but retreated in good order, helped by dense woods. The Austrians had ultimately made little progress in the campaign in Saxony despite Hochkirch and had failed to achieve a decisive breakthrough. After a thwarted attempt to take Dresden, Daun's troops were forced to withdraw to Austrian territory for the winter, so that Saxony remained under Prussian occupation. At the same time, the Russians failed in an attempt to take Kolberg in Pomerania (now Kołobrzeg, Poland) from the Prussians. Not only that, Austria now found herself estranged with the new developments within the empire itself. Beside the rise of Prussia, Augustus III, although ineffective, could mustered up an army not only from Saxony, but also Poland, considering the elector was also the King of Poland. Bavaria's growing power and independence was also apparent as she had more voices on the path that its army should have taken, and managed to slip out of the war at its own will. Most importantly, with the now somehow-belligerent Hanover united personally under George III of Great Britain, It can amassed a considerable power, even brought Britain in, on the future conflicts. This power dynamic is important to the future and the latter conflicts of the empire. The war also proved that Maria Theresa's reforms were still not enough to compete with Prussia: unlike its enemy, the Austrians went almost bankrupt at the end of war. Hence, she dedicated the next two decades to the consolidation of her administration. Frederick saw Saxony and Polish west Prussia as potential fields for expansion but could not expect French support if he started an aggressive war for them. If he joined the French against the British in the hope of annexing Hanover, he might fall victim to an Austro-Russian attack. The hereditary elector of Saxony, Augustus III, was also elective King of Poland as Augustus III, but the two territories were physically separated by Brandenburg and Silesia. Neither state could pose as a great power. Saxony was merely a buffer between Prussia and Austrian Bohemia, whereas Poland, despite its union with the ancient lands of Lithuania, was prey to pro-French and pro-Russian factions. A Prussian scheme for compensating Frederick Augustus with Bohemia in exchange for Saxony obviously presupposed further spoliation of Austria. The French planned to invade the British Isles during 1759 by accumulating troops near the mouth of the Loire and concentrating their Brest and Toulon fleets. However, two sea defeats prevented this. In August, the Mediterranean fleet under Jean-François de La Clue-Sabran was scattered by a larger British fleet under Edward Boscawen at the Battle of Lagos. In the Battle of Quiberon Bay on 20 November, the British admiral Edward Hawke with 23 ships of the line caught the French Brest fleet with 21 ships of the line under Marshal de Conflans and sank, captured, or forced many of them aground, putting an end to the French plans. In the attempt to satisfy Austria at the time, Britain gave their electoral vote in Hanover for the candidacy of Maria Theresa's son, Joseph, as the Holy Roman Emperor, much to the dismay of Frederick and Prussia. Not only that, Britain would soon join the Austro-Russian alliance, but complications arose. Britain's basic framework for the alliance itself was to protect Hanover's interests against France. While at the same time, Kaunitz kept approaching the French in the hope of establishing such alliance with Austria. Not only that, France had no intention to ally with Russia, who meddled with their affairs in Austria's succession war, years earlier, and saw the complete dismemberment of Prussia as unacceptable to the stability of Central Europe. Great Britain lost Minorca in the Mediterranean to the French in 1756 but captured the French colonies in Senegal in 1758. The British Royal Navy took the French sugar colonies of Guadeloupe in 1759 and Martinique in 1762 as well as the Spanish cities of Havana in Cuba, and Manila in the Philippines, both prominent Spanish colonial cities. However, expansion into the hinterlands of both cities met with stiff resistance. In the Philippines, the British were confined to Manila until their agreed upon withdrawal at the war's end. Britain had been surprised by the sudden Prussian offensive but now began shipping supplies and ₤670,000 (equivalent to ₤89.9 million in 2015) to its new ally. A combined force of allied German states was organised by the British to protect Hanover from French invasion, under the command of the Duke of Cumberland. The British attempted to persuade the Dutch Republic to join the alliance, but the request was rejected, as the Dutch wished to remain fully neutral. Despite the huge disparity in numbers, the year had been successful for the Prussian-led forces on the continent, in contrast to disappointing British campaigns in North America. In early 1758, Frederick launched an invasion of Moravia, and laid siege to Olmütz (now Olomouc, Czech Republic). Following an Austrian victory at the Battle of Domstadtl that wiped out a supply convoy destined for Olmütz, Frederick broke off the siege and withdrew from Moravia. It marked the end of his final attempt to launch a major invasion of Austrian territory. East Prussia had been occupied by Russian forces over the winter and would remain under their control until 1762, although Frederick did not see the Russians as an immediate threat and instead entertained hopes of first fighting a decisive battle against Austria that would knock them out of the war. Pitt now prepared to send troops into Germany; and both Marlborough and Sackville, disgusted by what they perceived as the futility of the ""descents"", obtained commissions in that army. The elderly General Bligh was appointed to command a new ""descent"", escorted by Howe. The campaign began propitiously with the Raid on Cherbourg. Covered by naval bombardment, the army drove off the French force detailed to oppose their landing, captured Cherbourg, and destroyed its fortifications, docks, and shipping. Between 10 and 17 October 1757, a Hungarian general, Count András Hadik, serving in the Austrian army, executed what may be the most famous hussar action in history. When the Prussian King Frederick was marching south with his powerful armies, the Hungarian general unexpectedly swung his force of 5,000, mostly hussars, around the Prussians and occupied part of their capital, Berlin, for one night. The city was spared for a negotiated ransom of 200,000 thalers. When Frederick heard about this humiliating occupation, he immediately sent a larger force to free the city. Hadik, however, left the city with his Hussars and safely reached the Austrian lines. Subsequently, Hadik was promoted to the rank of Marshal in the Austrian army. In India, the British retained the Northern Circars, but returned all the French trading ports. The treaty, however, required that the fortifications of these settlements be destroyed and never rebuilt, while only minimal garrisons could be maintained there, thus rendering them worthless as military bases. Combined with the loss of France's ally in Bengal and the defection of Hyderabad to the British as a result of the war, this effectively brought French power in India to an end, making way for British hegemony and eventual control of the subcontinent. Realizing that war was imminent, Prussia preemptively struck Saxony and quickly overran it. The result caused uproar across Europe. Because of Prussia's alliance with Britain, Austria formed an alliance with France, seeing an opportunity to recapture Silesia, which had been lost in a previous war. Reluctantly, by following the imperial diet, most of the states of the empire joined Austria's cause. The Anglo-Prussian alliance was joined by smaller German states (especially Hanover). Sweden, fearing Prussia's expansionist tendencies, went to war in 1757 to protect its Baltic dominions, seeing its chance when virtually all of Europe opposed Prussia. Spain, bound by the Pacte de Famille, intervened on behalf of France and together they launched an utterly unsuccessful invasion of Portugal in 1762. The Russian Empire was originally aligned with Austria, fearing Prussia's ambition on the Polish-Lithuanian Commonwealth, but switched sides upon the succession of Tsar Peter III in 1762. In 1756 and 1757 the French captured forts Oswego and William Henry from the British. The latter victory was marred when France's native allies broke the terms of capitulation and attacked the retreating British column, which was under French guard, slaughtering and scalping soldiers and taking captive many men, women and children while the French refused to protect their captives. French naval deployments in 1757 also successfully defended the key Fortress of Louisbourg on Cape Breton Island, securing the seaward approaches to Quebec. The Anglo-French hostilities were ended in 1763 by the Treaty of Paris, which involved a complex series of land exchanges, the most important being France's cession to Spain of Louisiana, and to Great Britain the rest of New France except for the islands of St. Pierre and Miquelon. Faced with the choice of retrieving either New France or its Caribbean island colonies of Guadeloupe and Martinique, France chose the latter to retain these lucrative sources of sugar, writing off New France as an unproductive, costly territory. France also returned Minorca to the British. Spain lost control of Florida to Great Britain, but it received from the French the Île d'Orléans and all of the former French holdings west of the Mississippi River. The exchanges suited the British as well, as their own Caribbean islands already supplied ample sugar, and, with the acquisition of New France and Florida, they now controlled all of North America east of the Mississippi. In early 1757, Frederick II again took the initiative by marching into the Kingdom of Bohemia, hoping to inflict a decisive defeat on Austrian forces. After winning the bloody Battle of Prague on 6 May 1757, in which both forces suffered major casualties, the Prussians forced the Austrians back into the fortifications of Prague. The Prussian army then laid siege to the city. Following the battle at Prague, Frederick took 5,000 troops from the siege at Prague and sent them to reinforce the 19,000-man army under the Duke of Brunswick-Bevern at Kolin in Bohemia. Austrian Marshal Daun arrived too late to participate in the battle of Prague, but picked up 16,000 men who had escaped from the battle. With this army he slowly moved to relieve Prague. The Prussian army was too weak to simultaneously besiege Prague and keep Daun away, and Frederick was forced to attack prepared positions. The resulting Battle of Kolin was a sharp defeat for Frederick, his first military defeat. His losses further forced him to lift the siege and withdraw from Bohemia altogether. Realizing that war was imminent, Prussia preemptively struck Saxony and quickly overran it. The result caused uproar across Europe. Because of Prussia's alliance with Britain, Austria formed an alliance with France, seeing an opportunity to recapture Silesia, which had been lost in a previous war. Reluctantly, by following the imperial diet, most of the states of the empire joined Austria's cause. The Anglo-Prussian alliance was joined by smaller German states (especially Hanover). Sweden, fearing Prussia's expansionist tendencies, went to war in 1757 to protect its Baltic dominions, seeing its chance when virtually all of Europe opposed Prussia. Spain, bound by the Pacte de Famille, intervened on behalf of France and together they launched a disastrous invasion of Portugal in 1762. The Russian Empire was originally aligned with Austria, fearing Prussia's ambition on the Polish-Lithuanian Commonwealth, but switched sides upon the succession of Tsar Peter III in 1762. Things were looking grim for Prussia now, with the Austrians mobilising to attack Prussian-controlled soil and a French army under Soubise approaching from the west. However, in November and December of 1757, the whole situation in Germany was reversed. First, Frederick devastated Prince Soubise's French force at the Battle of Rossbach on 5 November 1757 and then routed a vastly superior Austrian force at the Battle of Leuthen on 5 December 1757 With these victories, Frederick once again established himself as Europe's premier general and his men as Europe's most accomplished soldiers. In spite of this, the Prussians were now facing the prospect of four major powers attacking on four fronts (France from the West, Austria from the South, Russia from the East and Sweden from the North). Meanwhile, a combined force from a number of smaller German states such as Bavaria had been established under Austrian leadership, thus threatening Prussian control of Saxony. For much of the eighteenth century, France approached its wars in the same way. It would let colonies defend themselves or would offer only minimal help (sending them limited numbers of troops or inexperienced soldiers), anticipating that fights for the colonies would most likely be lost anyway. This strategy was to a degree forced upon France: geography, coupled with the superiority of the British navy, made it difficult for the French navy to provide significant supplies and support to French colonies. Similarly, several long land borders made an effective domestic army imperative for any French ruler. Given these military necessities, the French government, unsurprisingly, based its strategy overwhelmingly on the army in Europe: it would keep most of its army on the continent, hoping for victories closer to home. The plan was to fight to the end of hostilities and then, in treaty negotiations, to trade territorial acquisitions in Europe to regain lost overseas possessions. This approach did not serve France well in the war, as the colonies were indeed lost, but although much of the European war went well, by its end France had few counterbalancing European successes. By 1763, the war in Central Europe was essentially a stalemate. Frederick had retaken most of Silesia and Saxony but not the latter's capital, Dresden. His financial situation was not dire, but his kingdom was devastated and his army severely weakened. His manpower had dramatically decreased, and he had lost so many effective officers and generals that a new offensive was perhaps impossible. British subsidies had been stopped by the new Prime Minister Lord Bute, and the Russian Emperor had been overthrown by his wife, Catherine, who ended Russia's alliance with Prussia and withdrew from the war. Austria, however, like most participants, was facing a severe financial crisis and had to decrease the size of its army, something which greatly affected its offensive power. Indeed, after having effectively sustained a long war, its administration was in disarray. By that time, it still held Dresden, the southeastern parts of Saxony, the county of Glatz, and southern Silesia, but the prospect of victory was dim without Russian support. In 1763 a peace settlement was reached at the Treaty of Hubertusburg, ending the war in central Europe. The British Prime Minister, the Duke of Newcastle, was optimistic that the new series of alliances could prevent war from breaking out in Europe. However, a large French force was assembled at Toulon, and the French opened the campaign against the British by an attack on Minorca in the Mediterranean. A British attempt at relief was foiled at the Battle of Minorca, and the island was captured on 28 June (for which Admiral Byng was court-martialed and executed). War between Britain and France had been formally declared on 18 May nearly two years after fighting had broken out in the Ohio Country. Prussia emerged from the war as a great power whose importance could no longer be challenged. Frederick the Great’s personal reputation was enormously enhanced, as his debt to fortune (Russia’s volte-face after Elizabeth’s death) and to the British subsidy were soon forgotten while the memory of his energy and his military genius was strenuously kept alive. Russia, on the other hand, made one great invisible gain from the war: the elimination of French influence in Poland. The First Partition of Poland (1772) was to be a Russo-Prussian transaction, with Austria only reluctantly involved and with France simply ignored. Later that summer, the Russians invaded Memel with 75,000 troops. Memel had one of the strongest fortresses in Prussia. However, after five days of artillery bombardment the Russian army was able to storm it. The Russians then used Memel as a base to invade East Prussia and defeated a smaller Prussian force in the fiercely contested Battle of Gross-Jägersdorf on 30 August 1757. However, it was not yet able to take Königsberg and retreated soon afterward. Still, it was a new threat to Prussia. Not only was Frederick forced to break off his invasion of Bohemia, he was now forced to withdraw further into Prussian-controlled territory. His defeats on the battlefield brought still more opportunist nations into the war. Sweden declared war on Prussia and invaded Pomerania with 17,000 men. Sweden felt this small army was all that was needed to occupy Pomerania and felt the Swedish army would not need to engage with the Prussians because the Prussians were occupied on so many other fronts. William Pitt, who entered the cabinet in 1756, had a grand vision for the war that made it entirely different from previous wars with France. As prime minister Pitt committed Britain to a grand strategy of seizing the entire French Empire, especially its possessions in North America and India. Britain's main weapon was the Royal Navy, which could control the seas and bring as many invasion troops as were needed. He also planned to use colonial forces from the Thirteen American colonies, working under the command of British regulars, to invade new France. In order to tie the French army down he subsidized his European allies. Pitt Head of the government from 1756 to 1761, and even after that the British continued his strategy. It proved completely successful. Pitt had a clear appreciation of the enormous value of imperial possessions, and realized how vulnerable was the French Empire. The war was successful for Great Britain, which gained the bulk of New France in North America, Spanish Florida, some individual Caribbean islands in the West Indies, the colony of Senegal on the West African coast, and superiority over the French trading outposts on the Indian subcontinent. The Native American tribes were excluded from the settlement; a subsequent conflict, known as Pontiac's War, was also unsuccessful in returning them to their pre-war status. In Europe, the war began disastrously for Prussia, but a combination of good luck and successful strategy saw King Frederick the Great manage to retrieve the Prussian position and retain the status quo ante bellum. Prussia emerged as a new European great power. Although Austria failed to retrieve the territory of Silesia from Prussia (its original goal) its military prowess was also noted by the other powers. The involvement of Portugal, Spain and Sweden did not return them to their former status as great powers. France was deprived of many of its colonies and had saddled itself with heavy war debts that its inefficient financial system could barely handle. Spain lost Florida but gained French Louisiana and regained control of its colonies, e.g., Cuba and the Philippines, which had been captured by the British during the war. France and other European powers avenged their defeat in 1778 when the American Revolutionary War broke out, with hopes of destroying Britain's dominance once and for all. This problem was compounded when the main Hanoverian army under Cumberland was defeated at the Battle of Hastenbeck and forced to surrender entirely at the Convention of Klosterzeven following a French Invasion of Hanover. The Convention removed Hanover and Brunswick from the war, leaving the Western approach to Prussian territory extremely vulnerable. Frederick sent urgent requests to Britain for more substantial assistance, as he was now without any outside military support for his forces in Germany. On the eastern front, progress was very slow. The Russian army was heavily dependent upon its main magazines in Poland, and the Prussian army launched several successful raids against them. One of them, led by general Platen in September resulted in the loss of 2,000 Russians, mostly captured, and the destruction of 5,000 wagons. Deprived of men, the Prussians had to resort to this new sort of warfare, raiding, to delay the advance of their enemies. Nonetheless, at the end of the year, they suffered two critical setbacks. The Russians under Zakhar Chernyshev and Pyotr Rumyantsev stormed Kolberg in Pomerania, while the Austrians captured Schweidnitz. The loss of Kolberg cost Prussia its last port on the Baltic Sea. In Britain, it was speculated that a total Prussian collapse was now imminent. Calculating that no further Russian advance was likely until 1758, Frederick moved the bulk of his eastern forces to Pomerania under the command of Marshal Lehwaldt where they were to repel the Swedish invasion. In short order, the Prussian army drove the Swedes back, occupied most of Swedish Pomerania, and blockaded its capital Stralsund. George II of Great Britain, on the advice of his British ministers, revoked the Convention of Klosterzeven, and Hanover reentered the war. Over the winter the new commander of the Hanoverian forces, Duke Ferdinand of Brunswick, regrouped his army and launched a series of offensives that drove the French back across the River Rhine. The British had suffered further defeats in North America, particularly at Fort William Henry. At home, however, stability had been established. Since 1756, successive governments led by Newcastle and Pitt had fallen. In August 1757, the two men agreed to a political partnership and formed a coalition government that gave new, firmer direction to the war effort. The new strategy emphasised both Newcastle's commitment to British involvement on the Continent, particularly in defence of Germany, and William Pitt's determination to use naval power to seize French colonies around the globe. This ""dual strategy"" would dominate British policy for the next five years. Frederick II of Prussia had received reports of the clashes in North America and had formed an alliance with Great Britain. On 29 August 1756, he led Prussian troops across the border of Saxony, one of the small German states in league with Austria. He intended this as a bold pre-emption of an anticipated Austro-French invasion of Silesia. He had three goals in his new war on Austria. First, he would seize Saxony and eliminate it as a threat to Prussia, then using the Saxon army and treasury to aid the Prussian war effort. His second goal was to advance into Bohemia where he might set up winter quarters at Austria's expense. Thirdly, he wanted to invade Moravia from Silesia, seize the fortress at Olmütz, and advance on Vienna to force an end to the war. French policy was, moreover, complicated by the existence of the le Secret du roi—a system of private diplomacy conducted by King Louis XV. Unbeknownst to his foreign minister, Louis had established a network of agents throughout Europe with the goal of pursuing personal political objectives that were often at odds with France’s publicly stated policies. Louis’s goals for le Secret du roi included an attempt to win the Polish crown for his kinsman Louis François de Bourbon, prince de Conti, and the maintenance of Poland, Sweden, and Turkey as French client states in opposition to Russian and Austrian interests. The Seven Years' War was fought between 1755 and 1764, the main conflict occurring in the seven-year period from 1756 to 1763. It involved every great power of the time except the Ottoman Empire, and affected Europe, the Americas, West Africa, India, and the Philippines. Considered a prelude to the two world wars and the greatest European war since the Thirty Years War of the 17th century, it once again split Europe into two coalitions, led by Great Britain on one side and France on the other. For the first time, aiming to curtail Britain and Prussia's ever-growing might, France formed a grand coalition of its own, which ended with failure as Britain rose as the world's predominant power, altering the European balance of power."
Communication,"In a slightly more complex form a sender and a receiver are linked reciprocally. This second attitude of communication, referred to as the constitutive model or constructionist view, focuses on how an individual communicates as the determining factor of the way the message will be interpreted. Communication is viewed as a conduit; a passage in which information travels from one individual to another and this information becomes separate from the communication itself. A particular instance of communication is called a speech act. The sender's personal filters and the receiver's personal filters may vary depending upon different regional traditions, cultures, or gender; which may alter the intended meaning of message contents. In the presence of ""communication noise"" on the transmission channel (air, in this case), reception and decoding of content may be faulty, and thus the speech act may not achieve the desired effect. One problem with this encode-transmit-receive-decode model is that the processes of encoding and decoding imply that the sender and receiver each possess something that functions as a codebook, and that these two code books are, at the very least, similar if not identical. Although something like code books is implied by the model, they are nowhere represented in the model, which creates many conceptual difficulties. Nonverbal communication describes the process of conveying meaning in the form of non-word messages. Examples of nonverbal communication include haptic communication, chronemic communication, gestures, body language, facial expression, eye contact, and how one dresses. Nonverbal communication also relates to intent of a message. Examples of intent are voluntary, intentional movements like shaking a hand or winking, as well as involuntary, such as sweating. Speech also contains nonverbal elements known as paralanguage, e.g. rhythm, intonation, tempo, and stress. There may even be a pheromone component. Research has shown that up to 55% of human communication may occur through non-verbal facial expressions, and a further 38% through paralanguage. It affects communication most at the subconscious level and establishes trust. Likewise, written texts include nonverbal elements such as handwriting style, spatial arrangement of words and the use of emoticons to convey emotion. In a simple model, often referred to as the transmission model or standard view of communication, information or content (e.g. a message in natural language) is sent in some form (as spoken language) from an emisor/ sender/ encoder to a destination/ receiver/ decoder. This common conception of communication simply views communication as a means of sending and receiving information. The strengths of this model are simplicity, generality, and quantifiability. Claude Shannon and Warren Weaver structured this model based on the following elements: Communication is usually described along a few major dimensions: Message (what type of things are communicated), source / emisor / sender / encoder (by whom), form (in which form), channel (through which medium), destination / receiver / target / decoder (to whom), and Receiver. Wilbur Schram (1954) also indicated that we should also examine the impact that a message has (both desired and undesired) on the target of the message. Between parties, communication includes acts that confer knowledge and experiences, give advice and commands, and ask questions. These acts may take many forms, in one of the various manners of communication. The form depends on the abilities of the group communicating. Together, communication content and form make messages that are sent towards a destination. The target can be oneself, another person or being, another entity (such as a corporation or group of beings). Fungi communicate to coordinate and organize their growth and development such as the formation of Marcelia and fruiting bodies. Fungi communicate with their own and related species as well as with non fungal organisms in a great variety of symbiotic interactions, especially with bacteria, unicellular eukaryote, plants and insects through biochemicals of biotic origin. The biochemicals trigger the fungal organism to react in a specific manner, while if the same chemical molecules are not part of biotic messages, they do not trigger the fungal organism to react. This implies that fungal organisms can differentiate between molecules taking part in biotic messages and similar molecules being irrelevant in the situation. So far five different primary signalling molecules are known to coordinate different behavioral patterns such as filamentation, mating, growth, and pathogenicity. Behavioral coordination and production of signaling substances is achieved through interpretation processes that enables the organism to differ between self or non-self, a biotic indicator, biotic message from similar, related, or non-related species, and even filter out ""noise"", i.e. similar molecules without biotic content. Communication is observed within the plant organism, i.e. within plant cells and between plant cells, between plants of the same or related species, and between plants and non-plant organisms, especially in the root zone. Plant roots communicate with rhizome bacteria, fungi, and insects within the soil. These interactions are governed by syntactic, pragmatic, and semantic rules,[citation needed] and are possible because of the decentralized ""nervous system"" of plants. The original meaning of the word ""neuron"" in Greek is ""vegetable fiber"" and recent research has shown that most of the microorganism plant communication processes are neuron-like. Plants also communicate via volatiles when exposed to herbivory attack behavior, thus warning neighboring plants. In parallel they produce other volatiles to attract parasites which attack these herbivores. In stress situations plants can overwrite the genomes they inherited from their parents and revert to that of their grand- or great-grandparents.[citation needed] Family communication study looks at topics such as family rules, family roles or family dialectics and how those factors could affect the communication between family members. Researchers develop theories to understand communication behaviors. Family communication study also digs deep into certain time periods of family life such as marriage, parenthood or divorce and how communication stands in those situations. It is important for family members to understand communication as a trusted way which leads to a well constructed family. The first major model for communication was introduced by Claude Shannon and Warren Weaver for Bell Laboratories in 1949 The original model was designed to mirror the functioning of radio and telephone technologies. Their initial model consisted of three primary parts: sender, channel, and receiver. The sender was the part of a telephone a person spoke into, the channel was the telephone itself, and the receiver was the part of the phone where one could hear the other person. Shannon and Weaver also recognized that often there is static that interferes with one listening to a telephone conversation, which they deemed noise. Companies with limited resources may choose to engage in only a few of these activities, while larger organizations may employ a full spectrum of communications. Since it is difficult to develop such a broad range of skills, communications professionals often specialize in one or two of these areas but usually have at least a working knowledge of most of them. By far, the most important qualifications communications professionals can possess are excellent writing ability, good 'people' skills, and the capacity to think critically and strategically. Theories of coregulation describe communication as a creative and dynamic continuous process, rather than a discrete exchange of information. Canadian media scholar Harold Innis had the theory that people use different types of media to communicate and which one they choose to use will offer different possibilities for the shape and durability of society (Wark, McKenzie 1997). His famous example of this is using ancient Egypt and looking at the ways they built themselves out of media with very different properties stone and papyrus. Papyrus is what he called 'Space Binding'. it made possible the transmission of written orders across space, empires and enables the waging of distant military campaigns and colonial administration. The other is stone and 'Time Binding', through the construction of temples and the pyramids can sustain their authority generation to generation, through this media they can change and shape communication in their society (Wark, McKenzie 1997). The broad field of animal communication encompasses most of the issues in ethology. Animal communication can be defined as any behavior of one animal that affects the current or future behavior of another animal. The study of animal communication, called zoo semiotics (distinguishable from anthroposemiotics, the study of human communication) has played an important part in the development of ethology, sociobiology, and the study of animal cognition. Animal communication, and indeed the understanding of the animal world in general, is a rapidly growing field, and even in the 21st century so far, a great share of prior understanding related to diverse fields such as personal symbolic name use, animal emotions, animal culture and learning, and even sexual conduct, long thought to be well understood, has been revolutionized. A special field of animal communication has been investigated in more detail such as vibrational communication. Effective verbal or spoken communication is dependent on a number of factors and cannot be fully isolated from other important interpersonal skills such as non-verbal communication, listening skills and clarification. Human language can be defined as a system of symbols (sometimes known as lexemes) and the grammars (rules) by which the symbols are manipulated. The word ""language"" also refers to common properties of languages. Language learning normally occurs most intensively during human childhood. Most of the thousands of human languages use patterns of sound or gesture for symbols which enable communication with others around them. Languages tend to share certain properties, although there are exceptions. There is no defined line between a language and a dialect. Constructed languages such as Esperanto, programming languages, and various mathematical formalism is not necessarily restricted to the properties shared by human languages. Communication is two-way process not merely one-way."
Police,"Colquhoun's utilitarian approach to the problem – using a cost-benefit argument to obtain support from businesses standing to benefit – allowed him to achieve what Henry and John Fielding failed for their Bow Street detectives. Unlike the stipendiary system at Bow Street, the river police were full-time, salaried officers prohibited from taking private fees. His other contribution was the concept of preventive policing; his police were to act as a highly visible deterrent to crime by their permanent presence on the Thames. Colquhoun's innovations were a critical development leading up to Robert Peel's ""new"" police three decades later. In Canada, the Royal Newfoundland Constabulary was founded in 1729, making it the first police force in present-day Canada. It was followed in 1834 by the Toronto Police, and in 1838 by police forces in Montreal and Quebec City. A national force, the Dominion Police, was founded in 1868. Initially the Dominion Police provided security for parliament, but its responsibilities quickly grew. The famous Royal Northwest Mounted Police was founded in 1873. The merger of these two police forces in 1920 formed the world-famous Royal Canadian Mounted Police. In Miranda the court created safeguards against self-incriminating statements made after an arrest. The court held that ""The prosecution may not use statements, whether exculpatory or inculpatory, stemming from questioning initiated by law enforcement officers after a person has been taken into custody or otherwise deprived of his freedom of action in any significant way, unless it demonstrates the use of procedural safeguards effective to secure the Fifth Amendment's privilege against self-incrimination"" Meanwhile, the authorities in Glasgow, Scotland successfully petitioned the government to pass the Glasgow Police Act establishing the City of Glasgow Police in 1800. Other Scottish towns soon followed suit and set up their own police forces through acts of parliament. In Ireland, the Irish Constabulary Act of 1822 marked the beginning of the Royal Irish Constabulary. The Act established a force in each barony with chief constables and inspectors general under the control of the civil administration at Dublin Castle. By 1841 this force numbered over 8,600 men. In France during the Middle Ages, there were two Great Officers of the Crown of France with police responsibilities: The Marshal of France and the Constable of France. The military policing responsibilities of the Marshal of France were delegated to the Marshal's provost, whose force was known as the Marshalcy because its authority ultimately derived from the Marshal. The marshalcy dates back to the Hundred Years' 'War, and some historians trace it back to the early 12th century. Another organisation, the Constabulary (French: Connétablie), was under the command of the Constable of France. The constabulary was regularised as a military body in 1337. Under King Francis I (who reigned 1515–1547), the Maréchaussée was merged with the Constabulary. The resulting force was also known as the Maréchaussée, or, formally, the Constabulary and Marshalcy of France. In 1797, Patrick Colquhoun was able to persuade the West Indies merchants who operated at the Pool of London on the River Thames, to establish a police force at the docks to prevent rampant theft that was causing annual estimated losses of £500,000 worth of cargo. The idea of a police, as it then existed in France, was considered as a potentially undesirable foreign import. In building the case for the police in the face of England's firm anti-police sentiment, Colquhoun framed the political rationale on economic indicators to show that a police dedicated to crime prevention was ""perfectly congenial to the principle of the British constitution."" Moreover, he went so far as to praise the French system, which had reached ""the greatest degree of perfection"" in his estimation. In 1566, the first police investigator of Rio de Janeiro was recruited. By the 17th century, most captaincies already had local units with law enforcement functions. On July 9, 1775 a Cavalry Regiment was created in the state of Minas Gerais for maintaining law and order. In 1808, the Portuguese royal family relocated to Brazil, because of the French invasion of Portugal. King João VI established the ""Intendência Geral de Polícia"" (General Police Intendancy) for investigations. He also created a Royal Police Guard for Rio de Janeiro in 1809. In 1831, after independence, each province started organizing its local ""military police"", with order maintenance tasks. The Federal Railroad Police was created in 1852. Michel Foucault claims that the contemporary concept of police as a paid and funded functionary of the state was developed by German and French legal scholars and practitioners in Public administration and Statistics in the 17th and early 18th centuries, most notably with Nicolas Delamare's Traité de la Police (""Treatise on the Police""), first published in 1705. The German Polizeiwissenschaft (Science of Police) first theorized by Philipp von Hörnigk a 17th-century Austrian Political economist and civil servant and much more famously by Johann Heinrich Gottlob Justi who produced an important theoretical work known as Cameral science on the formulation of police. Foucault cites Magdalene Humpert author of Bibliographie der Kameralwissenschaften (1937) in which the author makes note of a substantial bibliography was produced of over 4000 pieces of the practice of Polizeiwissenschaft however, this maybe a mistranslation of Foucault's own work the actual source of Magdalene Humpert states over 14,000 items were produced from the 16th century dates ranging from 1520-1850. Law enforcement, however, constitutes only part of policing activity. Policing has included an array of activities in different situations, but the predominant ones are concerned with the preservation of order. In some societies, in the late 18th and early 19th centuries, these developed within the context of maintaining the class system and the protection of private property. Many police forces suffer from police corruption to a greater or lesser degree. The police force is usually a public sector service, meaning they are paid through taxes. Not a lot of empirical work on the practices of inter/transnational information and intelligence sharing has been undertaken. A notable exception is James Sheptycki's study of police cooperation in the English Channel region (2002), which provides a systematic content analysis of information exchange files and a description of how these transnational information and intelligence exchanges are transformed into police case-work. The study showed that transnational police information sharing was routinized in the cross-Channel region from 1968 on the basis of agreements directly between the police agencies and without any formal agreement between the countries concerned. By 1992, with the signing of the Schengen Treaty, which formalized aspects of police information exchange across the territory of the European Union, there were worries that much, if not all, of this intelligence sharing was opaque, raising questions about the efficacy of the accountability mechanisms governing police information sharing in Europe (Joubert and Bevers, 1996). Motorcycles are also commonly used, particularly in locations that a car may not be able to reach, to control potential public order situations involving meetings of motorcyclists and often in escort duties where motorcycle police officers can quickly clear a path for escorted vehicles. Bicycle patrols are used in some areas because they allow for more open interaction with the public. In addition, their quieter operation can facilitate approaching suspects unawares and can help in pursuing them attempting to escape on foot. Edwin Chadwick's 1829 article, ""Preventive police"" in the London Review, argued that prevention ought to be the primary concern of a police body, which was not the case in practice. The reason, argued Chadwick, was that ""A preventive police would act more immediately by placing difficulties in obtaining the objects of temptation."" In contrast to a deterrent of punishment, a preventive police force would deter criminality by making crime cost-ineffective - ""crime doesn't pay"". In the second draft of his 1829 Police Act, the ""object"" of the new Metropolitan Police, was changed by Robert Peel to the ""principal object,"" which was the ""prevention of crime."" Later historians would attribute the perception of England's ""appearance of orderliness and love of public order"" to the preventive principle entrenched in Peel's police system. The word ""police"" was borrowed from French into the English language in the 18th century, but for a long time it applied only to French and continental European police forces. The word, and the concept of police itself, were ""disliked as a symbol of foreign oppression"" (according to Britannica 1911). Before the 19th century, the first use of the word ""police"" recorded in government documents in the United Kingdom was the appointment of Commissioners of Police for Scotland in 1714 and the creation of the Marine Police in 1798. In the American Old West, policing was often of very poor quality.[citation needed] The Army often provided some policing alongside poorly resourced sheriffs and temporarily organized posses.[citation needed] Public organizations were supplemented by private contractors, notably the Pinkerton National Detective Agency, which was hired by individuals, businessmen, local governments and the federal government. At its height, the Pinkerton Agency's numbers exceeded those of the United States Army.[citation needed] As conceptualized by the Polizeiwissenschaft,according to Foucault the police had an administrative,economic and social duty (""procuring abundance""). It was in charge of demographic concerns and needed to be incorporated within the western political philosophy system of raison d'état and therefore giving the superficial appearance of empowering the population (and unwittingly supervising the population), which, according to mercantilist theory, was to be the main strength of the state. Thus, its functions largely overreached simple law enforcement activities and included public health concerns, urban planning (which was important because of the miasma theory of disease; thus, cemeteries were moved out of town, etc.), and surveillance of prices. In Terry v. Ohio (1968) the court divided seizure into two parts, the investigatory stop and arrest. The court further held that during an investigatory stop a police officer's search "" [is] confined to what [is] minimally necessary to determine whether [a suspect] is armed, and the intrusion, which [is] made for the sole purpose of protecting himself and others nearby, [is] confined to ascertaining the presence of weapons"" (U.S. Supreme Court). Before Terry, every police encounter constituted an arrest, giving the police officer the full range of search authority. Search authority during a Terry stop (investigatory stop) is limited to weapons only. All police officers in the United Kingdom, whatever their actual rank, are 'constables' in terms of their legal position. This means that a newly appointed constable has the same arrest powers as a Chief Constable or Commissioner. However, certain higher ranks have additional powers to authorize certain aspects of police operations, such as a power to authorize a search of a suspect's house (section 18 PACE in England and Wales) by an officer of the rank of Inspector, or the power to authorize a suspect's detention beyond 24 hours by a Superintendent. The 1829 Metropolitan Police Act created a modern police force by limiting the purview of the force and its powers, and envisioning it as merely an organ of the judicial system. Their job was apolitical; to maintain the peace and apprehend criminals for the courts to process according to the law. This was very different to the 'Continental model' of the police force that had been developed in France, where the police force worked within the parameters of the absolutist state as an extension of the authority of the monarch and functioned as part of the governing state. Studies of this kind outside of Europe are even rarer, so it is difficult to make generalizations, but one small-scale study that compared transnational police information and intelligence sharing practices at specific cross-border locations in North America and Europe confirmed that low visibility of police information and intelligence sharing was a common feature (Alain, 2001). Intelligence-led policing is now common practice in most advanced countries (Ratcliffe, 2007) and it is likely that police intelligence sharing and information exchange has a common morphology around the world (Ratcliffe, 2007). James Sheptycki has analyzed the effects of the new information technologies on the organization of policing-intelligence and suggests that a number of 'organizational pathologies' have arisen that make the functioning of security-intelligence processes in transnational policing deeply problematic. He argues that transnational police information circuits help to ""compose the panic scenes of the security-control society"". The paradoxical effect is that, the harder policing agencies work to produce security, the greater are feelings of insecurity. The terms international policing, transnational policing, and/or global policing began to be used from the early 1990s onwards to describe forms of policing that transcended the boundaries of the sovereign nation-state (Nadelmann, 1993), (Sheptycki, 1995). These terms refer in variable ways to practices and forms for policing that, in some sense, transcend national borders. This includes a variety of practices, but international police cooperation, criminal intelligence exchange between police agencies working in different nation-states, and police development-aid to weak, failed or failing states are the three types that have received the most scholarly attention. Modern police forces make extensive use of radio communications equipment, carried both on the person and installed in vehicles, to co-ordinate their work, share information, and get help quickly. In recent years, vehicle-installed computers have enhanced the ability of police communications, enabling easier dispatching of calls, criminal background checks on persons of interest to be completed in a matter of seconds, and updating officers' daily activity log and other, required reports on a real-time basis. Other common pieces of police equipment include flashlights/torches, whistles, police notebooks and ""ticket books"" or citations. This office was first held by Gabriel Nicolas de la Reynie, who had 44 commissaires de police (police commissioners) under his authority. In 1709, these commissioners were assisted by inspecteurs de police (police inspectors). The city of Paris was divided into 16 districts policed by the commissaires, each assigned to a particular district and assisted by a growing bureaucracy. The scheme of the Paris police force was extended to the rest of France by a royal edict of October 1699, resulting in the creation of lieutenants general of police in all large French cities and towns. Police development-aid to weak, failed or failing states is another form of transnational policing that has garnered attention. This form of transnational policing plays an increasingly important role in United Nations peacekeeping and this looks set to grow in the years ahead, especially as the international community seeks to develop the rule of law and reform security institutions in States recovering from conflict (Goldsmith and Sheptycki, 2007) With transnational police development-aid the imbalances of power between donors and recipients are stark and there are questions about the applicability and transportability of policing models between jurisdictions (Hills, 2009). Law enforcement in Ancient China was carried out by ""prefects"" for thousands of years since it developed in both the Chu and Jin kingdoms of the Spring and Autumn period. In Jin, dozens of prefects were spread across the state, each having limited authority and employment period. They were appointed by local magistrates, who reported to higher authorities such as governors, who in turn were appointed by the emperor, and they oversaw the civil administration of their ""prefecture"", or jurisdiction. Under each prefect were ""subprefects"" who helped collectively with law enforcement in the area. Some prefects were responsible for handling investigations, much like modern police detectives. Prefects could also be women. The concept of the ""prefecture system"" spread to other cultures such as Korea and Japan. With the initial investment of £4,200, the new trial force of the Thames River Police began with about 50 men charged with policing 33,000 workers in the river trades, of whom Colquhoun claimed 11,000 were known criminals and ""on the game."" The force was a success after its first year, and his men had ""established their worth by saving £122,000 worth of cargo and by the rescuing of several lives."" Word of this success spread quickly, and the government passed the Marine Police Bill on 28 July 1800, transforming it from a private to public police agency; now the oldest police force in the world. Colquhoun published a book on the experiment, The Commerce and Policing of the River Thames. It found receptive audiences far outside London, and inspired similar forces in other cities, notably, New York City, Dublin, and Sydney. In contrast, the police are entitled to protect private rights in some jurisdictions. To ensure that the police would not interfere in the regular competencies of the courts of law, some police acts require that the police may only interfere in such cases where protection from courts cannot be obtained in time, and where, without interference of the police, the realization of the private right would be impeded. This would, for example, allow police to establish a restaurant guest's identity and forward it to the innkeeper in a case where the guest cannot pay the bill at nighttime because his wallet had just been stolen from the restaurant table. They can also be armed with non-lethal (more accurately known as ""less than lethal"" or ""less-lethal"") weaponry, particularly for riot control. Non-lethal weapons include batons, tear gas, riot control agents, rubber bullets, riot shields, water cannons and electroshock weapons. Police officers often carry handcuffs to restrain suspects. The use of firearms or deadly force is typically a last resort only to be used when necessary to save human life, although some jurisdictions (such as Brazil) allow its use against fleeing felons and escaped convicts. A ""shoot-to-kill"" policy was recently introduced in South Africa, which allows police to use deadly force against any person who poses a significant threat to them or civilians. With the country having one of the highest rates of violent crime, president Jacob Zuma states that South Africa needs to handle crime differently from other countries. As one of their first acts after end of the War of the Castilian Succession in 1479, Ferdinand and Isabella established the centrally organized and efficient Holy Brotherhood (Santa Hermandad) as a national police force. They adapted an existing brotherhood to the purpose of a general police acting under officials appointed by themselves, and endowed with great powers of summary jurisdiction even in capital cases. The original brotherhoods continued to serve as modest local police-units until their final suppression in 1835. Despite popular conceptions promoted by movies and television, many US police departments prefer not to maintain officers in non-patrol bureaus and divisions beyond a certain period of time, such as in the detective bureau, and instead maintain policies that limit service in such divisions to a specified period of time, after which officers must transfer out or return to patrol duties.[citation needed] This is done in part based upon the perception that the most important and essential police work is accomplished on patrol in which officers become acquainted with their beats, prevent crime by their presence, respond to crimes in progress, manage crises, and practice their skills.[citation needed] Historical studies reveal that policing agents have undertaken a variety of cross-border police missions for many years (Deflem, 2002). For example, in the 19th century a number of European policing agencies undertook cross-border surveillance because of concerns about anarchist agitators and other political radicals. A notable example of this was the occasional surveillance by Prussian police of Karl Marx during the years he remained resident in London. The interests of public police agencies in cross-border co-operation in the control of political radicalism and ordinary law crime were primarily initiated in Europe, which eventually led to the establishment of Interpol before the Second World War. There are also many interesting examples of cross-border policing under private auspices and by municipal police forces that date back to the 19th century (Nadelmann, 1993). It has been established that modern policing has transgressed national boundaries from time to time almost from its inception. It is also generally agreed that in the post–Cold War era this type of practice became more significant and frequent (Sheptycki, 2000). A police force is a constituted body of persons empowered by the state to enforce the law, protect property, and limit civil disorder. Their powers include the legitimized use of force. The term is most commonly associated with police services of a sovereign state that are authorized to exercise the police power of that state within a defined legal or territorial area of responsibility. Police forces are often defined as being separate from military or other organizations involved in the defense of the state against foreign aggressors; however, gendarmerie are military units charged with civil policing. In the United States, August Vollmer introduced other reforms, including education requirements for police officers. O.W. Wilson, a student of Vollmer, helped reduce corruption and introduce professionalism in Wichita, Kansas, and later in the Chicago Police Department. Strategies employed by O.W. Wilson included rotating officers from community to community to reduce their vulnerability to corruption, establishing of a non-partisan police board to help govern the police force, a strict merit system for promotions within the department, and an aggressive recruiting drive with higher police salaries to attract professionally qualified officers. During the professionalism era of policing, law enforcement agencies concentrated on dealing with felonies and other serious crime, rather than broader focus on crime prevention. The first centrally organised police force was created by the government of King Louis XIV in 1667 to police the city of Paris, then the largest city in Europe. The royal edict, registered by the Parlement of Paris on March 15, 1667 created the office of lieutenant général de police (""lieutenant general of police""), who was to be the head of the new Paris police force, and defined the task of the police as ""ensuring the peace and quiet of the public and of private individuals, purging the city of what may cause disturbances, procuring abundance, and having each and everyone live according to their station and their duties"". Unmarked vehicles are used primarily for sting operations or apprehending criminals without alerting them to their presence. Some police forces use unmarked or minimally marked cars for traffic law enforcement, since drivers slow down at the sight of marked police vehicles and unmarked vehicles make it easier for officers to catch speeders and traffic violators. This practice is controversial, with for example, New York State banning this practice in 1996 on the grounds that it endangered motorists who might be pulled over by people impersonating police officers. Perhaps the greatest question regarding the future development of transnational policing is: in whose interest is it? At a more practical level, the question translates into one about how to make transnational policing institutions democratically accountable (Sheptycki, 2004). For example, according to the Global Accountability Report for 2007 (Lloyd, et al. 2007) Interpol had the lowest scores in its category (IGOs), coming in tenth with a score of 22% on overall accountability capabilities (p. 19). As this report points out, and the existing academic literature on transnational policing seems to confirm, this is a secretive area and one not open to civil society involvement. Peel, widely regarded as the father of modern policing, was heavily influenced by the social and legal philosophy of Jeremy Bentham, who called for a strong and centralized, but politically neutral, police force for the maintenance of social order, for the protection of people from crime and to act as a visible deterrent to urban crime and disorder. Peel decided to standardise the police force as an official paid profession, to organise it in a civilian fashion, and to make it answerable to the public."
Karl_Popper,"In an interview that Popper gave in 1969 with the condition that it shall be kept secret until after his death, he summarised his position on God as follows: ""I don't know whether God exists or not. ... Some forms of atheism are arrogant and ignorant and should be rejected, but agnosticism—to admit that we don't know and to search—is all right. ... When I look at what I call the gift of life, I feel a gratitude which is in tune with some religious ideas of God. However, the moment I even speak of it, I am embarrassed that I may do something wrong to God in talking about God."" He objected to organised religion, saying ""it tends to use the name of God in vain"", noting the danger of fanaticism because of religious conflicts: ""The whole thing goes back to myths which, though they may have a kernel of truth, are untrue. Why then should the Jewish myth be true and the Indian and Egyptian myths not be true?"" In a letter unrelated to the interview, he stressed his tolerant attitude: ""Although I am not for religion, I do think that we should show respect for anybody who believes honestly."" Popper left school at the age of 16 and attended lectures in mathematics, physics, philosophy, psychology and the history of music as a guest student at the University of Vienna. In 1919, Popper became attracted by Marxism and subsequently joined the Association of Socialist School Students. He also became a member of the Social Democratic Workers' Party of Austria, which was at that time a party that fully adopted the Marxist ideology. After the street battle in the Hörlgasse on 15 June 1919, when police shot eight of his unarmed party comrades, he became disillusioned by what he saw to be the ""pseudo-scientific"" historical materialism of Marx, abandoned the ideology, and remained a supporter of social liberalism throughout his life. In 2004, philosopher and psychologist Michel ter Hark (Groningen, The Netherlands) published a book, called Popper, Otto Selz and the rise of evolutionary epistemology, in which he claimed that Popper took some of his ideas from his tutor, the German psychologist Otto Selz. Selz never published his ideas, partly because of the rise of Nazism, which forced him to quit his work in 1933, and the prohibition of referring to Selz' work. Popper, the historian of ideas and his scholarship, is criticised in some academic quarters for his rejection of Plato, Hegel and Marx. Popper contrasts his views with the notion of the ""hopeful monster"" that has large phenotype mutations and calls it the ""hopeful behavioural monster"". After behaviour has changed radically, small but quick changes of the phenotype follow to make the organism fitter to its changed goals. This way it looks as if the phenotype were changing guided by some invisible hand, while it is merely natural selection working in combination with the new behaviour. For example, according to this hypothesis, the eating habits of the giraffe must have changed before its elongated neck evolved. Popper contrasted this view as ""evolution from within"" or ""active Darwinism"" (the organism actively trying to discover new ways of life and being on a quest for conquering new ecological niches), with the naturalistic ""evolution from without"" (which has the picture of a hostile environment only trying to kill the mostly passive organism, or perhaps segregate some of its groups). Popper won many awards and honours in his field, including the Lippincott Award of the American Political Science Association, the Sonning Prize, the Otto Hahn Peace Medal of the United Nations Association of Germany in Berlin and fellowships in the Royal Society, British Academy, London School of Economics, King's College London, Darwin College, Cambridge, and Charles University, Prague. Austria awarded him the Grand Decoration of Honour in Gold for Services to the Republic of Austria in 1986, and the Federal Republic of Germany its Grand Cross with Star and Sash of the Order of Merit, and the peace class of the Order Pour le Mérite. He received the Humanist Laureate Award from the International Academy of Humanism. He was knighted by Queen Elizabeth II in 1965, and was elected a Fellow of the Royal Society in 1976. He was invested with the Insignia of a Companion of Honour in 1982. Another objection is that it is not always possible to demonstrate falsehood definitively, especially if one is using statistical criteria to evaluate a null hypothesis. More generally it is not always clear, if evidence contradicts a hypothesis, that this is a sign of flaws in the hypothesis rather than of flaws in the evidence. However, this is a misunderstanding of what Popper's philosophy of science sets out to do. Rather than offering a set of instructions that merely need to be followed diligently to achieve science, Popper makes it clear in The Logic of Scientific Discovery that his belief is that the resolution of conflicts between hypotheses and observations can only be a matter of the collective judgment of scientists, in each individual case. Popper died of ""complications of cancer, pneumonia and kidney failure"" in Kenley at the age of 92 on 17 September 1994. He had been working continuously on his philosophy until two weeks before, when he suddenly fell terminally ill. After cremation, his ashes were taken to Vienna and buried at Lainzer cemetery adjacent to the ORF Centre, where his wife Josefine Anna Popper (called ‘Hennie’) had already been buried. Popper's estate is managed by his secretary and personal assistant Melitta Mew and her husband Raymond. Popper's manuscripts went to the Hoover Institution at Stanford University, partly during his lifetime and partly as supplementary material after his death. Klagenfurt University possesses Popper's library, including his precious bibliophilia, as well as hard copies of the original Hoover material and microfilms of the supplementary material. The remaining parts of the estate were mostly transferred to The Karl Popper Charitable Trust. In October 2008 Klagenfurt University acquired the copyrights from the estate. Popper puzzled over the stark contrast between the non-scientific character of Freud and Adler's theories in the field of psychology and the revolution set off by Einstein's theory of relativity in physics in the early 20th century. Popper thought that Einstein's theory, as a theory properly grounded in scientific thought and method, was highly ""risky"", in the sense that it was possible to deduce consequences from it which were, in the light of the then-dominant Newtonian physics, highly improbable (e.g., that light is deflected towards solid bodies—confirmed by Eddington's experiments in 1919), and which would, if they turned out to be false, falsify the whole theory. In contrast, nothing could, even in principle, falsify psychoanalytic theories. He thus came to the conclusion that psychoanalytic theories had more in common with primitive myths than with genuine science. About the creation-evolution controversy, Popper wrote that he considered it ""a somewhat sensational clash between a brilliant scientific hypothesis concerning the history of the various species of animals and plants on earth, and an older metaphysical theory which, incidentally, happened to be part of an established religious belief"" with a footnote to the effect that ""[he] agree[s] with Professor C.E. Raven when, in his Science, Religion, and the Future, 1943, he calls this conflict ""a storm in a Victorian tea-cup""; though the force of this remark is perhaps a little impaired by the attention he pays to the vapours still emerging from the cup—to the Great Systems of Evolutionist Philosophy, produced by Bergson, Whitehead, Smuts, and others."" Popper played a vital role in establishing the philosophy of science as a vigorous, autonomous discipline within philosophy, through his own prolific and influential works, and also through his influence on his own contemporaries and students. Popper founded in 1946 the Department of Philosophy, Logic and Scientific Method at the London School of Economics and there lectured and influenced both Imre Lakatos and Paul Feyerabend, two of the foremost philosophers of science in the next generation of philosophy of science. (Lakatos significantly modified Popper's position,:1 and Feyerabend repudiated it entirely, but the work of both is deeply influenced by Popper and engaged with many of the problems that Popper set.) In a book called Science Versus Crime, Houck writes that Popper's falsificationism can be questioned logically: it is not clear how Popper would deal with a statement like ""for every metal, there is a temperature at which it will melt."" The hypothesis cannot be falsified by any possible observation, for there will always be a higher temperature than tested at which the metal may in fact melt, yet it seems to be a valid scientific hypothesis. These examples were pointed out by Carl Gustav Hempel. Hempel came to acknowledge that Logical Positivism's verificationism was untenable, but argued that falsificationism was equally untenable on logical grounds alone. The simplest response to this is that, because Popper describes how theories attain, maintain and lose scientific status, individual consequences of currently accepted scientific theories are scientific in the sense of being part of tentative scientific knowledge, and both of Hempel's examples fall under this category. For instance, atomic theory implies that all metals melt at some temperature. In his early years Popper was impressed by Marxism, whether of Communists or socialists. An event that happened in 1919 had a profound effect on him: During a riot, caused by the Communists, the police shot several unarmed people, including some of Popper's friends, when they tried to free party comrades from prison. The riot had, in fact, been part of a plan by which leaders of the Communist party with connections to Béla Kun tried to take power by a coup; Popper did not know about this at that time. However, he knew that the riot instigators were swayed by the Marxist doctrine that class struggle would produce vastly more dead men than the inevitable revolution brought about as quickly as possible, and so had no scruples to put the life of the rioters at risk to achieve their selfish goal of becoming the future leaders of the working class. This was the start of his later criticism of historicism. Popper began to reject Marxist historicism, which he associated with questionable means, and later socialism, which he associated with placing equality before freedom (to the possible disadvantage of equality). Logically, no number of positive outcomes at the level of experimental testing can confirm a scientific theory, but a single counterexample is logically decisive: it shows the theory, from which the implication is derived, to be false. To say that a given statement (e.g., the statement of a law of some scientific theory) -- [call it ""T""] -- is ""falsifiable"" does not mean that ""T"" is false. Rather, it means that, if ""T"" is false, then (in principle), ""T"" could be shown to be false, by observation or by experiment. Popper's account of the logical asymmetry between verification and falsifiability lies at the heart of his philosophy of science. It also inspired him to take falsifiability as his criterion of demarcation between what is, and is not, genuinely scientific: a theory should be considered scientific if, and only if, it is falsifiable. This led him to attack the claims of both psychoanalysis and contemporary Marxism to scientific status, on the basis that their theories are not falsifiable. In response to a given problem situation (), a number of competing conjectures, or tentative theories (), are systematically subjected to the most rigorous attempts at falsification possible. This process, error elimination (), performs a similar function for science that natural selection performs for biological evolution. Theories that better survive the process of refutation are not more true, but rather, more ""fit""—in other words, more applicable to the problem situation at hand (). Consequently, just as a species' biological fitness does not ensure continued survival, neither does rigorous testing protect a scientific theory from refutation in the future. Yet, as it appears that the engine of biological evolution has, over many generations, produced adaptive traits equipped to deal with more and more complex problems of survival, likewise, the evolution of theories through the scientific method may, in Popper's view, reflect a certain type of progress: toward more and more interesting problems (). For Popper, it is in the interplay between the tentative theories (conjectures) and error elimination (refutation) that scientific knowledge advances toward greater and greater problems; in a process very much akin to the interplay between genetic variation and natural selection. Popper held that rationality is not restricted to the realm of empirical or scientific theories, but that it is merely a special case of the general method of criticism, the method of finding and eliminating contradictions in knowledge without ad-hoc-measures. According to this view, rational discussion about metaphysical ideas, about moral values and even about purposes is possible. Popper's student W.W. Bartley III tried to radicalise this idea and made the controversial claim that not only can criticism go beyond empirical knowledge, but that everything can be rationally criticised. Knowledge, for Popper, was objective, both in the sense that it is objectively true (or truthlike), and also in the sense that knowledge has an ontological status (i.e., knowledge as object) independent of the knowing subject (Objective Knowledge: An Evolutionary Approach, 1972). He proposed three worlds: World One, being the physical world, or physical states; World Two, being the world of mind, or mental states, ideas, and perceptions; and World Three, being the body of human knowledge expressed in its manifold forms, or the products of the second world made manifest in the materials of the first world (i.e., books, papers, paintings, symphonies, and all the products of the human mind). World Three, he argued, was the product of individual human beings in exactly the same sense that an animal path is the product of individual animals, and that, as such, has an existence and evolution independent of any individual knowing subjects. The influence of World Three, in his view, on the individual human mind (World Two) is at least as strong as the influence of World One. In other words, the knowledge held by a given individual mind owes at least as much to the total accumulated wealth of human knowledge, made manifest, as to the world of direct experience. As such, the growth of human knowledge could be said to be a function of the independent evolution of World Three. Many contemporary philosophers, such as Daniel Dennett, have not embraced Popper's Three World conjecture, due mostly, it seems, to its resemblance to mind-body dualism. Karl Popper was born in Vienna (then in Austria-Hungary) in 1902, to upper middle-class parents. All of Karl Popper's grandparents were Jewish, but the Popper family converted to Lutheranism before Karl was born, and so he received Lutheran baptism. They understood this as part of their cultural assimilation, not as an expression of devout belief. Karl's father Simon Siegmund Carl Popper was a lawyer from Bohemia and a doctor of law at the Vienna University, and mother Jenny Schiff was of Silesian and Hungarian descent. After establishing themselves in Vienna, the Poppers made a rapid social climb in Viennese society: Simon Siegmund Carl became a partner in the law firm of Vienna's liberal Burgomaster Herr Grübl and, after Grübl's death in 1898, Simon took over the business. (Malachi Hacohen records that Herr Grübl's first name was Raimund, after which Karl received his middle name. Popper himself, in his autobiography, erroneously recalls that Herr Grübl's first name was Carl.) His father was a bibliophile who had 12,000–14,000 volumes in his personal library. Popper inherited both the library and the disposition from him. In 1937, Popper finally managed to get a position that allowed him to emigrate to New Zealand, where he became lecturer in philosophy at Canterbury University College of the University of New Zealand in Christchurch. It was here that he wrote his influential work The Open Society and its Enemies. In Dunedin he met the Professor of Physiology John Carew Eccles and formed a lifelong friendship with him. In 1946, after the Second World War, he moved to the United Kingdom to become reader in logic and scientific method at the London School of Economics. Three years later, in 1949, he was appointed professor of logic and scientific method at the University of London. Popper was president of the Aristotelian Society from 1958 to 1959. He retired from academic life in 1969, though he remained intellectually active for the rest of his life. In 1985, he returned to Austria so that his wife could have her relatives around her during the last months of her life; she died in November that year. After the Ludwig Boltzmann Gesellschaft failed to establish him as the director of a newly founded branch researching the philosophy of science, he went back again to the United Kingdom in 1986, settling in Kenley, Surrey. To Popper, who was an anti-justificationist, traditional philosophy is misled by the false principle of sufficient reason. He thinks that no assumption can ever be or needs ever to be justified, so a lack of justification is not a justification for doubt. Instead, theories should be tested and scrutinised. It is not the goal to bless theories with claims of certainty or justification, but to eliminate errors in them. He writes, ""there are no such things as good positive reasons; nor do we need such things [...] But [philosophers] obviously cannot quite bring [themselves] to believe that this is my opinion, let alone that it is right"" (The Philosophy of Karl Popper, p. 1043) He worked in street construction for a short amount of time, but was unable to cope with the heavy labour. Continuing to attend university as a guest student, he started an apprenticeship as cabinetmaker, which he completed as a journeyman. He was dreaming at that time of starting a daycare facility for children, for which he assumed the ability to make furniture might be useful. After that he did voluntary service in one of psychoanalyst Alfred Adler's clinics for children. In 1922, he did his matura by way of a second chance education and finally joined the University as an ordinary student. He completed his examination as an elementary teacher in 1924 and started working at an after-school care club for socially endangered children. In 1925, he went to the newly founded Pädagogisches Institut and continued studying philosophy and psychology. Around that time he started courting Josefine Anna Henninger, who later became his wife. Upon this basis, along with that of the logical content of assertions (where logical content is inversely proportional to probability), Popper went on to develop his important notion of verisimilitude or ""truthlikeness"". The intuitive idea behind verisimilitude is that the assertions or hypotheses of scientific theories can be objectively measured with respect to the amount of truth and falsity that they imply. And, in this way, one theory can be evaluated as more or less true than another on a quantitative basis which, Popper emphasises forcefully, has nothing to do with ""subjective probabilities"" or other merely ""epistemic"" considerations. In All Life is Problem Solving, Popper sought to explain the apparent progress of scientific knowledge – that is, how it is that our understanding of the universe seems to improve over time. This problem arises from his position that the truth content of our theories, even the best of them, cannot be verified by scientific testing, but can only be falsified. Again, in this context the word ""falsified"" does not refer to something being ""fake""; rather, that something can be (i.e., is capable of being) shown to be false by observation or experiment. Some things simply do not lend themselves to being shown to be false, and therefore, are not falsifiable. If so, then how is it that the growth of science appears to result in a growth in knowledge? In Popper's view, the advance of scientific knowledge is an evolutionary process characterised by his formula: Popper had his own sophisticated views on evolution that go much beyond what the frequently-quoted passages say. In effect, Popper agreed with some of the points of both creationists and naturalists, but also disagreed with both views on crucial aspects. Popper understood the universe as a creative entity that invents new things, including life, but without the necessity of something like a god, especially not one who is pulling strings from behind the curtain. He said that evolution must, as the creationists say, work in a goal-directed way but disagreed with their view that it must necessarily be the hand of god that imposes these goals onto the stage of life. Gray does not, however, give any indication of what available evidence these theories were at odds with, and his appeal to ""crucial support"" illustrates the very inductivist approach to science that Popper sought to show was logically illegitimate. For, according to Popper, Einstein's theory was at least equally as well corroborated as Newton's upon its initial conception; they both equally well accounted for all the hitherto available evidence. Moreover, since Einstein also explained the empirical refutations of Newton's theory, general relativity was immediately deemed suitable for tentative acceptance on the Popperian account. Indeed, Popper wrote, several decades before Gray's criticism, in reply to a critical essay by Imre Lakatos: In The Open Society and Its Enemies and The Poverty of Historicism, Popper developed a critique of historicism and a defence of the ""Open Society"". Popper considered historicism to be the theory that history develops inexorably and necessarily according to knowable general laws towards a determinate end. He argued that this view is the principal theoretical presupposition underpinning most forms of authoritarianism and totalitarianism. He argued that historicism is founded upon mistaken assumptions regarding the nature of scientific law and prediction. Since the growth of human knowledge is a causal factor in the evolution of human history, and since ""no society can predict, scientifically, its own future states of knowledge"", it follows, he argued, that there can be no predictive science of human history. For Popper, metaphysical and historical indeterminism go hand in hand. The failure of democratic parties to prevent fascism from taking over Austrian politics in the 1920s and 1930s traumatised Popper. He suffered from the direct consequences of this failure, since events after the Anschluss, the annexation of Austria by the German Reich in 1938, forced him into permanent exile. His most important works in the field of social science—The Poverty of Historicism (1944) and The Open Society and Its Enemies (1945)—were inspired by his reflection on the events of his time and represented, in a sense, a reaction to the prevalent totalitarian ideologies that then dominated Central European politics. His books defended democratic liberalism as a social and political philosophy. They also represented extensive critiques of the philosophical presuppositions underpinning all forms of totalitarianism. In 1928, he earned a doctorate in psychology, under the supervision of Karl Bühler. His dissertation was entitled ""Die Methodenfrage der Denkpsychologie"" (The question of method in cognitive psychology). In 1929, he obtained the authorisation to teach mathematics and physics in secondary school, which he started doing. He married his colleague Josefine Anna Henninger (1906–1985) in 1930. Fearing the rise of Nazism and the threat of the Anschluss, he started to use the evenings and the nights to write his first book Die beiden Grundprobleme der Erkenntnistheorie (The Two Fundamental Problems of the Theory of Knowledge). He needed to publish one to get some academic position in a country that was safe for people of Jewish descent. However, he ended up not publishing the two-volume work, but a condensed version of it with some new material, Logik der Forschung (The Logic of Scientific Discovery), in 1934. Here, he criticised psychologism, naturalism, inductionism, and logical positivism, and put forth his theory of potential falsifiability as the criterion demarcating science from non-science. In 1935 and 1936, he took unpaid leave to go to the United Kingdom for a study visit. As early as 1934, Popper wrote of the search for truth as ""one of the strongest motives for scientific discovery."" Still, he describes in Objective Knowledge (1972) early concerns about the much-criticised notion of truth as correspondence. Then came the semantic theory of truth formulated by the logician Alfred Tarski and published in 1933. Popper writes of learning in 1935 of the consequences of Tarski's theory, to his intense joy. The theory met critical objections to truth as correspondence and thereby rehabilitated it. The theory also seemed, in Popper's eyes, to support metaphysical realism and the regulative idea of a search for truth. Other awards and recognition for Popper included the City of Vienna Prize for the Humanities (1965), Karl Renner Prize (1978), Austrian Decoration for Science and Art (1980), Dr. Leopold Lucas Prize (1981), Ring of Honour of the City of Vienna (1983) and the Premio Internazionale of the Italian Federico Nietzsche Society (1988). In 1992, he was awarded the Kyoto Prize in Arts and Philosophy for ""symbolising the open spirit of the 20th century"" and for his ""enormous influence on the formation of the modern intellectual climate"". He does not argue that any such conclusions are therefore true, or that this describes the actual methods of any particular scientist.[citation needed] Rather, it is recommended as an essential principle of methodology that, if enacted by a system or community, will lead to slow but steady progress of a sort (relative to how well the system or community enacts the method). It has been suggested that Popper's ideas are often mistaken for a hard logical account of truth because of the historical co-incidence of their appearing at the same time as logical positivism, the followers of which mistook his aims for their own. The Quine-Duhem thesis argues that it's impossible to test a single hypothesis on its own, since each one comes as part of an environment of theories. Thus we can only say that the whole package of relevant theories has been collectively falsified, but cannot conclusively say which element of the package must be replaced. An example of this is given by the discovery of the planet Neptune: when the motion of Uranus was found not to match the predictions of Newton's laws, the theory ""There are seven planets in the solar system"" was rejected, and not Newton's laws themselves. Popper discussed this critique of naïve falsificationism in Chapters 3 and 4 of The Logic of Scientific Discovery. For Popper, theories are accepted or rejected via a sort of selection process. Theories that say more about the way things appear are to be preferred over those that do not; the more generally applicable a theory is, the greater its value. Thus Newton's laws, with their wide general application, are to be preferred over the much more specific ""the solar system has seven planets"".[dubious – discuss] While there is some dispute as to the matter of influence, Popper had a long-standing and close friendship with economist Friedrich Hayek, who was also brought to the London School of Economics from Vienna. Each found support and similarities in the other's work, citing each other often, though not without qualification. In a letter to Hayek in 1944, Popper stated, ""I think I have learnt more from you than from any other living thinker, except perhaps Alfred Tarski."" Popper dedicated his Conjectures and Refutations to Hayek. For his part, Hayek dedicated a collection of papers, Studies in Philosophy, Politics, and Economics, to Popper, and in 1982 said, ""...ever since his Logik der Forschung first came out in 1934, I have been a complete adherent to his general theory of methodology."" Karl Popper's rejection of Marxism during his teenage years left a profound mark on his thought. He had at one point joined a socialist association, and for a few months in 1919 considered himself a communist. During this time he became familiar with the Marxist view of economics, class-war, and history. Although he quickly became disillusioned with the views expounded by Marxism, his flirtation with the ideology led him to distance himself from those who believed that spilling blood for the sake of a revolution was necessary. He came to realise that when it came to sacrificing human lives, one was to think and act with extreme prudence. Popper coined the term ""critical rationalism"" to describe his philosophy. Concerning the method of science, the term indicates his rejection of classical empiricism, and the classical observationalist-inductivist account of science that had grown out of it. Popper argued strongly against the latter, holding that scientific theories are abstract in nature, and can be tested only indirectly, by reference to their implications. He also held that scientific theory, and human knowledge generally, is irreducibly conjectural or hypothetical, and is generated by the creative imagination to solve problems that have arisen in specific historico-cultural settings. According to this theory, the conditions for the truth of a sentence as well as the sentences themselves are part of a metalanguage. So, for example, the sentence ""Snow is white"" is true if and only if snow is white. Although many philosophers have interpreted, and continue to interpret, Tarski's theory as a deflationary theory, Popper refers to it as a theory in which ""is true"" is replaced with ""corresponds to the facts"". He bases this interpretation on the fact that examples such as the one described above refer to two things: assertions and the facts to which they refer. He identifies Tarski's formulation of the truth conditions of sentences as the introduction of a ""metalinguistic predicate"" and distinguishes the following cases: Popper claimed to have recognised already in the 1934 version of his Logic of Discovery a fact later stressed by Kuhn, ""that scientists necessarily develop their ideas within a definite theoretical framework"", and to that extent to have anticipated Kuhn's central point about ""normal science"". (But Popper criticised what he saw as Kuhn's relativism.) Also, in his collection Conjectures and Refutations: The Growth of Scientific Knowledge (Harper & Row, 1963), Popper writes, ""Science must begin with myths, and with the criticism of myths; neither with the collection of observations, nor with the invention of experiments, but with the critical discussion of myths, and of magical techniques and practices. The scientific tradition is distinguished from the pre-scientific tradition in having two layers. Like the latter, it passes on its theories; but it also passes on a critical attitude towards them. The theories are passed on, not as dogmas, but rather with the challenge to discuss them and improve upon them."" Among his contributions to philosophy is his claim to have solved the philosophical problem of induction. He states that while there is no way to prove that the sun will rise, it is possible to formulate the theory that every day the sun will rise; if it does not rise on some particular day, the theory will be falsified and will have to be replaced by a different one. Until that day, there is no need to reject the assumption that the theory is true. Nor is it rational according to Popper to make instead the more complex assumption that the sun will rise until a given day, but will stop doing so the day after, or similar statements with additional conditions. The creation–evolution controversy in the United States raises the issue of whether creationistic ideas may be legitimately called science and whether evolution itself may be legitimately called science. In the debate, both sides and even courts in their decisions have frequently invoked Popper's criterion of falsifiability (see Daubert standard). In this context, passages written by Popper are frequently quoted in which he speaks about such issues himself. For example, he famously stated ""Darwinism is not a testable scientific theory, but a metaphysical research program—a possible framework for testable scientific theories."" He continued: According to John N. Gray, Popper held that ""a theory is scientific only in so far as it is falsifiable, and should be given up as soon as it is falsified."" By applying Popper's account of scientific method, Gray's Straw Dogs states that this would have ""killed the theories of Darwin and Einstein at birth."" When they were first advanced, Gray claims, each of them was ""at odds with some available evidence; only later did evidence become available that gave them crucial support."" Against this, Gray seeks to establish the irrationalist thesis that ""the progress of science comes from acting against reason."" Such a theory would be true with higher probability, because it cannot be attacked so easily: to falsify the first one, it is sufficient to find that the sun has stopped rising; to falsify the second one, one additionally needs the assumption that the given day has not yet been reached. Popper held that it is the least likely, or most easily falsifiable, or simplest theory (attributes which he identified as all the same thing) that explains known facts that one should rationally prefer. His opposition to positivism, which held that it is the theory most likely to be true that one should prefer, here becomes very apparent. It is impossible, Popper argues, to ensure a theory to be true; it is more important that its falsity can be detected as easily as possible. Instead, he formulated the spearhead model of evolution, a version of genetic pluralism. According to this model, living organisms themselves have goals, and act according to these goals, each guided by a central control. In its most sophisticated form, this is the brain of humans, but controls also exist in much less sophisticated ways for species of lower complexity, such as the amoeba. This control organ plays a special role in evolution—it is the ""spearhead of evolution"". The goals bring the purpose into the world. Mutations in the genes that determine the structure of the control may then cause drastic changes in behaviour, preferences and goals, without having an impact on the organism's phenotype. Popper postulates that such purely behavioural changes are less likely to be lethal for the organism compared to drastic changes of the phenotype. Popper is known for his rejection of the classical inductivist views on the scientific method, in favour of empirical falsification: A theory in the empirical sciences can never be proven, but it can be falsified, meaning that it can and should be scrutinized by decisive experiments. He used the black swan fallacy to discuss falsification. If the outcome of an experiment contradicts the theory, one should refrain from ad hoc manoeuvres that evade the contradiction merely by making it less falsifiable. Popper is also known for his opposition to the classical justificationist account of knowledge, which he replaced with critical rationalism, ""the first non-justificational philosophy of criticism in the history of philosophy."" This led Popper to conclude that what were regarded[by whom?] as the remarkable strengths of psychoanalytical theories were actually their weaknesses. Psychoanalytical theories were crafted in a way that made them able to refute any criticism and to give an explanation for every possible form of human behaviour. The nature of such theories made it impossible for any criticism or experiment - even in principle - to show them to be false. This realisation had an important consequence when Popper later tackled the problem of demarcation in the philosophy of science, as it led him to posit that the strength of a scientific theory lies in its both being susceptible to falsification, and not actually being falsified by criticism made of it. He considered that if a theory cannot, in principle, be falsified by criticism, it is not a scientific theory."
New_York_City,"New York is also a major center for non-commercial educational media. The oldest public-access television channel in the United States is the Manhattan Neighborhood Network, founded in 1971. WNET is the city's major public television station and a primary source of national Public Broadcasting Service (PBS) television programming. WNYC, a public radio station owned by the city until 1997, has the largest public radio audience in the United States. New York City also has an extensive web of expressways and parkways, which link the city's boroughs to each other as well as to northern New Jersey, Westchester County, Long Island, and southwestern Connecticut through various bridges and tunnels. Because these highways serve millions of outer borough and suburban residents who commute into Manhattan, it is quite common for motorists to be stranded for hours in traffic jams that are a daily occurrence, particularly during rush hour. On August 24, 1673, Dutch captain Anthonio Colve took over the colony of New York from England and rechristened it ""New Orange"" to honor the Prince of Orange, King William III. However, facing defeat from the British and French, who had teamed up to destroy Dutch trading routes, the Dutch returned the island to England in 1674. The character of New York's large residential districts is often defined by the elegant brownstone rowhouses and townhouses and shabby tenements that were built during a period of rapid expansion from 1870 to 1930. In contrast, New York City also has neighborhoods that are less densely populated and feature free-standing dwellings. In neighborhoods such as Riverdale (in the Bronx), Ditmas Park (in Brooklyn), and Douglaston (in Queens), large single-family homes are common in various architectural styles such as Tudor Revival and Victorian. The first documented visit by a European was in 1524 by Giovanni da Verrazzano, a Florentine explorer in the service of the French crown, who sailed his ship La Dauphine into New York Harbor. He claimed the area for France and named it ""Nouvelle Angoulême"" (New Angoulême). New York City's commuter rail network is the largest in North America. The rail network, connecting New York City to its suburbs, consists of the Long Island Rail Road, Metro-North Railroad, and New Jersey Transit. The combined systems converge at Grand Central Terminal and Pennsylvania Station and contain more than 250 stations and 20 rail lines. In Queens, the elevated AirTrain people mover system connects JFK International Airport to the New York City Subway and the Long Island Rail Road; a separate AirTrain system is planned alongside the Grand Central Parkway to connect LaGuardia Airport to these transit systems. For intercity rail, New York City is served by Amtrak, whose busiest station by a significant margin is Pennsylvania Station on the West Side of Manhattan, from which Amtrak provides connections to Boston, Philadelphia, and Washington, D.C. along the Northeast Corridor, as well as long-distance train service to other North American cities. The Great Irish Famine brought a large influx of Irish immigrants. Over 200,000 were living in New York by 1860, upwards of a quarter of the city's population. There was also extensive immigration from the German provinces, where revolutions had disrupted societies, and Germans comprised another 25% of New York's population by 1860. The city receives 49.9 inches (1,270 mm) of precipitation annually, which is fairly spread throughout the year. Average winter snowfall between 1981 and 2010 has been 25.8 inches (66 cm), but this varies considerably from year to year. Hurricanes and tropical storms are rare in the New York area, but are not unheard of and always have the potential to strike the area. Hurricane Sandy brought a destructive storm surge to New York City on the evening of October 29, 2012, flooding numerous streets, tunnels, and subway lines in Lower Manhattan and other areas of the city and cutting off electricity in many parts of the city and its suburbs. The storm and its profound impacts have prompted the discussion of constructing seawalls and other coastal barriers around the shorelines of the city and the metropolitan area to minimize the risk of destructive consequences from another such event in the future. Much of the scientific research in the city is done in medicine and the life sciences. New York City has the most post-graduate life sciences degrees awarded annually in the United States, with 127 Nobel laureates having roots in local institutions as of 2004; while in 2012, 43,523 licensed physicians were practicing in New York City. Major biomedical research institutions include Memorial Sloan–Kettering Cancer Center, Rockefeller University, SUNY Downstate Medical Center, Albert Einstein College of Medicine, Mount Sinai School of Medicine, and Weill Cornell Medical College, being joined by the Cornell University/Technion-Israel Institute of Technology venture on Roosevelt Island. Major tourist destinations include Times Square; Broadway theater productions; the Empire State Building; the Statue of Liberty; Ellis Island; the United Nations Headquarters; museums such as the Metropolitan Museum of Art; greenspaces such as Central Park and Washington Square Park; Rockefeller Center; the Manhattan Chinatown; luxury shopping along Fifth and Madison Avenues; and events such as the Halloween Parade in Greenwich Village; the Macy's Thanksgiving Day Parade; the lighting of the Rockefeller Center Christmas Tree; the St. Patrick's Day parade; seasonal activities such as ice skating in Central Park in the wintertime; the Tribeca Film Festival; and free performances in Central Park at Summerstage. Major attractions in the boroughs outside Manhattan include Flushing Meadows-Corona Park and the Unisphere in Queens; the Bronx Zoo; Coney Island, Brooklyn; and the New York Botanical Garden in the Bronx. The New York Wheel, a 630-foot ferris wheel, was under construction at the northern shore of Staten Island in 2015, overlooking the Statue of Liberty, New York Harbor, and the Lower Manhattan skyline. In the 1970s, job losses due to industrial restructuring caused New York City to suffer from economic problems and rising crime rates. While a resurgence in the financial industry greatly improved the city's economic health in the 1980s, New York's crime rate continued to increase through that decade and into the beginning of the 1990s. By the mid 1990s, crime rates started to drop dramatically due to revised police strategies, improving economic opportunities, gentrification, and new residents, both American transplants and new immigrants from Asia and Latin America. Important new sectors, such as Silicon Alley, emerged in the city's economy. New York's population reached all-time highs in the 2000 Census and then again in the 2010 Census. New York City has over 28,000 acres (110 km2) of municipal parkland and 14 miles (23 km) of public beaches. Parks in New York City include Central Park, Prospect Park, Flushing Meadows–Corona Park, Forest Park, and Washington Square Park. The largest municipal park in the city is Pelham Bay Park with 2,700 acres (1,093 ha). New York has been described as the ""Capital of Baseball"". There have been 35 Major League Baseball World Series and 73 pennants won by New York teams. It is one of only five metro areas (Los Angeles, Chicago, Baltimore–Washington, and the San Francisco Bay Area being the others) to have two baseball teams. Additionally, there have been 14 World Series in which two New York City teams played each other, known as a Subway Series and occurring most recently in 2000. No other metropolitan area has had this happen more than once (Chicago in 1906, St. Louis in 1944, and the San Francisco Bay Area in 1989). The city's two current Major League Baseball teams are the New York Mets, who play at Citi Field in Queens, and the New York Yankees, who play at Yankee Stadium in the Bronx. who compete in six games of interleague play every regular season that has also come to be called the Subway Series. The Yankees have won a record 27 championships, while the Mets have won the World Series twice. The city also was once home to the Brooklyn Dodgers (now the Los Angeles Dodgers), who won the World Series once, and the New York Giants (now the San Francisco Giants), who won the World Series five times. Both teams moved to California in 1958. There are also two Minor League Baseball teams in the city, the Brooklyn Cyclones and Staten Island Yankees. The New York Public Library, which has the largest collection of any public library system in the United States, serves Manhattan, the Bronx, and Staten Island. Queens is served by the Queens Borough Public Library, the nation's second largest public library system, while the Brooklyn Public Library serves Brooklyn. Organized crime has long been associated with New York City, beginning with the Forty Thieves and the Roach Guards in the Five Points in the 1820s. The 20th century saw a rise in the Mafia, dominated by the Five Families, as well as in gangs, including the Black Spades. The Mafia presence has declined in the city in the 21st century. New York City has a high degree of income disparity as indicated by its Gini Coefficient of 0.5 for the city overall and 0.6 for Manhattan. The disparity is driven by wage growth in high-income brackets, while wages have stagnated for middle and lower-income brackets. In the first quarter of 2014, the average weekly wage in New York County (Manhattan) was $2,749, representing the highest total among large counties in the United States. In 2013, New York City had the highest number of billionaires of any city in the world, higher than the next five U.S. cities combined, including former Mayor Michael R. Bloomberg. New York also had the highest density of millionaires per capita among major U.S. cities in 2014, at 4.6% of residents. Lower Manhattan has been experiencing a baby boom, with the area south of Canal Street witnessing 1,086 births in 2010, 12% greater than 2009 and over twice the number born in 2001. Forty of the city's theaters, with more than 500 seats each, are collectively known as Broadway, after the major thoroughfare that crosses the Times Square Theater District, sometimes referred to as ""The Great White Way"". According to The Broadway League, Broadway shows sold approximately US$1.27 billion worth of tickets in the 2013–2014 season, an 11.4% increase from US$1.139 billion in the 2012–2013 season. Attendance in 2013–2014 stood at 12.21 million, representing a 5.5% increase from the 2012–2013 season's 11.57 million. In the precolonial era, the area of present-day New York City was inhabited by various bands of Algonquian tribes of Native Americans, including the Lenape, whose homeland, known as Lenapehoking, included Staten Island; the western portion of Long Island, including the area that would become Brooklyn and Queens; Manhattan; the Bronx; and the Lower Hudson Valley. The Hudson River flows through the Hudson Valley into New York Bay. Between New York City and Troy, New York, the river is an estuary. The Hudson River separates the city from the U.S. state of New Jersey. The East River—a tidal strait—flows from Long Island Sound and separates the Bronx and Manhattan from Long Island. The Harlem River, another tidal strait between the East and Hudson Rivers, separates most of Manhattan from the Bronx. The Bronx River, which flows through the Bronx and Westchester County, is the only entirely fresh water river in the city. When one Republican presidential candidate for the 2016 election ridiculed the liberalism of ""New York values"" in January 2016, Donald Trump, leading in the polls, vigorously defended his city. The National Review, a conservative magazine published in the city since its founding by William F. Buckley, Jr. in 1955, commented, ""By hearkening back to New York's heart after 9/11, for a moment Trump transcended politics. How easily we forget, but for weeks after the terror attacks, New York was America."" Other important sectors include medical research and technology, non-profit institutions, and universities. Manufacturing accounts for a significant but declining share of employment, although the city's garment industry is showing a resurgence in Brooklyn. Food processing is a US$5 billion industry that employs more than 19,000 residents. The Staten Island Railway rapid transit system solely serves Staten Island, operating 24 hours a day. The Port Authority Trans-Hudson (PATH train) links Midtown and Lower Manhattan to northeastern New Jersey, primarily Hoboken, Jersey City, and Newark. Like the New York City Subway, the PATH operates 24 hours a day; meaning three of the six rapid transit systems in the world which operate on 24-hour schedules are wholly or partly in New York (the others are a portion of the Chicago 'L', the PATCO Speedline serving Philadelphia, and the Copenhagen Metro). Stone and brick became the city's building materials of choice after the construction of wood-frame houses was limited in the aftermath of the Great Fire of 1835. A distinctive feature of many of the city's buildings is the wooden roof-mounted water towers. In the 1800s, the city required their installation on buildings higher than six stories to prevent the need for excessively high water pressures at lower elevations, which could break municipal water pipes. Garden apartments became popular during the 1920s in outlying areas, such as Jackson Heights. The Queensboro Bridge is an important piece of cantilever architecture. The Manhattan Bridge, Throgs Neck Bridge, Triborough Bridge, and Verrazano-Narrows Bridge are all examples of Structural Expressionism. The traditional New York area accent is characterized as non-rhotic, so that the sound [ɹ] does not appear at the end of a syllable or immediately before a consonant; hence the pronunciation of the city name as ""New Yawk."" There is no [ɹ] in words like park [pɑək] or [pɒək] (with vowel backed and diphthongized due to the low-back chain shift), butter [bʌɾə], or here [hiə]. In another feature called the low back chain shift, the [ɔ] vowel sound of words like talk, law, cross, chocolate, and coffee and the often homophonous [ɔr] in core and more are tensed and usually raised more than in General American. In the most old-fashioned and extreme versions of the New York dialect, the vowel sounds of words like ""girl"" and of words like ""oil"" became a diphthong [ɜɪ]. This would often be misperceived by speakers of other accents as a reversal of the er and oy sounds, so that girl is pronounced ""goil"" and oil is pronounced ""erl""; this leads to the caricature of New Yorkers saying things like ""Joizey"" (Jersey), ""Toidy-Toid Street"" (33rd St.) and ""terlet"" (toilet). The character Archie Bunker from the 1970s sitcom All in the Family (played by Carroll O'Connor) was a notable example of having used this pattern of speech, which continues to fade in its overall presence. The city's population in 2010 was 44% white (33.3% non-Hispanic white), 25.5% black (23% non-Hispanic black), 0.7% Native American, and 12.7% Asian. Hispanics of any race represented 28.6% of the population, while Asians constituted the fastest-growing segment of the city's population between 2000 and 2010; the non-Hispanic white population declined 3 percent, the smallest recorded decline in decades; and for the first time since the Civil War, the number of blacks declined over a decade. Newtown Creek, a 3.5-mile (6-kilometer) a long estuary that forms part of the border between the boroughs of Brooklyn and Queens, has been designated a Superfund site for environmental clean-up and remediation of the waterway's recreational and economic resources for many communities. One of the most heavily used bodies of water in the Port of New York and New Jersey, it had been one of the most contaminated industrial sites in the country, containing years of discarded toxins, an estimated 30 million US gallons (110,000 m3) of spilled oil, including the Greenpoint oil spill, raw sewage from New York City's sewer system, and other accumulation. Manhattan's skyline, with its many skyscrapers, is universally recognized, and the city has been home to several of the tallest buildings in the world. As of 2011, New York City had 5,937 high-rise buildings, of which 550 completed structures were at least 330 feet (100 m) high, both second in the world after Hong Kong, with over 50 completed skyscrapers taller than 656 feet (200 m). These include the Woolworth Building (1913), an early gothic revival skyscraper built with massively scaled gothic detailing. Other features of the city's transportation infrastructure encompass more than 12,000 yellow taxicabs; various competing startup transportation network companies; and an aerial tramway that transports commuters between Roosevelt Island and Manhattan Island. New York City is situated in the Northeastern United States, in southeastern New York State, approximately halfway between Washington, D.C. and Boston. The location at the mouth of the Hudson River, which feeds into a naturally sheltered harbor and then into the Atlantic Ocean, has helped the city grow in significance as a trading port. Most of New York City is built on the three islands of Long Island, Manhattan, and Staten Island. The New York Islanders and the New York Rangers represent the city in the National Hockey League. Also within the metropolitan area are the New Jersey Devils, who play in nearby Newark, New Jersey. The New York City Fire Department (FDNY), provides fire protection, technical rescue, primary response to biological, chemical, and radioactive hazards, and emergency medical services for the five boroughs of New York City. The New York City Fire Department is the largest municipal fire department in the United States and the second largest in the world after the Tokyo Fire Department. The FDNY employs approximately 11,080 uniformed firefighters and over 3,300 uniformed EMTs and paramedics. The FDNY's motto is New York's Bravest. The FDNY headquarters is located at 9 MetroTech Center in Downtown Brooklyn, and the FDNY Fire Academy is located on Randalls Island. There are three Bureau of Fire Communications alarm offices which receive and dispatch alarms to appropriate units. One office, at 11 Metrotech Center in Brooklyn, houses Manhattan/Citywide, Brooklyn, and Staten Island Fire Communications. The Bronx and Queens offices are in separate buildings. The Mayor and council members are elected to four-year terms. The City Council is a unicameral body consisting of 51 council members whose districts are defined by geographic population boundaries. Each term for the mayor and council members lasts four years and has a three consecutive-term limit, but can resume after a four-year break. The New York City Administrative Code, the New York City Rules, and the City Record are the code of local laws, compilation of regulations, and official journal, respectively. New York's non-white population was 36,620 in 1890. New York City was a prime destination in the early twentieth century for African Americans during the Great Migration from the American South, and by 1916, New York City was home to the largest urban African diaspora in North America. The Harlem Renaissance of literary and cultural life flourished during the era of Prohibition. The larger economic boom generated construction of skyscrapers competing in height and creating an identifiable skyline. At the end of the Second Anglo-Dutch War, the English gained New Amsterdam (New York) in North America in exchange for Dutch control of Run, an Indonesian island. Several intertribal wars among the Native Americans and some epidemics brought on by contact with the Europeans caused sizable population losses for the Lenape between the years 1660 and 1670. By 1700, the Lenape population had diminished to 200. Real estate is a major force in the city's economy, as the total value of all New York City property was assessed at US$914.8 billion for the 2015 fiscal year. The Time Warner Center is the property with the highest-listed market value in the city, at US$1.1 billion in 2006. New York City is home to some of the nation's—and the world's—most valuable real estate. 450 Park Avenue was sold on July 2, 2007 for US$510 million, about $1,589 per square foot ($17,104/m²), breaking the barely month-old record for an American office building of $1,476 per square foot ($15,887/m²) set in the June 2007 sale of 660 Madison Avenue. According to Forbes, in 2014, Manhattan was home to six of the top ten zip codes in the United States by median housing price. In 1904, the steamship General Slocum caught fire in the East River, killing 1,021 people on board. In 1911, the Triangle Shirtwaist Factory fire, the city's worst industrial disaster, took the lives of 146 garment workers and spurred the growth of the International Ladies' Garment Workers' Union and major improvements in factory safety standards. Returning World War II veterans created a post-war economic boom and the development of large housing tracts in eastern Queens. New York emerged from the war unscathed as the leading city of the world, with Wall Street leading America's place as the world's dominant economic power. The United Nations Headquarters was completed in 1952, solidifying New York's global geopolitical influence, and the rise of abstract expressionism in the city precipitated New York's displacement of Paris as the center of the art world. A Spanish expedition led by captain Estêvão Gomes, a Portuguese sailing for Emperor Charles V, arrived in New York Harbor in January 1525 aboard the purpose-built caravel ""La Anunciada"" and charted the mouth of the Hudson River, which he named Rio de San Antonio. Heavy ice kept him from further exploration, and he returned to Spain in August. The first scientific map to show the North American East coast continuously, the 1527 world map known as the Padrón Real, was informed by Gomes' expedition, and labeled the Northeast as Tierra de Esteban Gómez in his honor. Throughout its history, the city has been a major port of entry for immigrants into the United States; more than 12 million European immigrants were received at Ellis Island between 1892 and 1924. The term ""melting pot"" was first coined to describe densely populated immigrant neighborhoods on the Lower East Side. By 1900, Germans constituted the largest immigrant group, followed by the Irish, Jews, and Italians. In 1940, whites represented 92% of the city's population. Christianity (59%), particularly Catholicism (33%), was the most prevalently practiced religion in New York as of 2014, followed by Judaism, with approximately 1.1 million Jews in New York City, over half living in Brooklyn. Islam ranks third in New York City, with official estimates ranging between 600,000 and 1,000,000 observers and including 10% of the city's public schoolchildren, followed by Hinduism, Buddhism, and a variety of other religions, as well as atheism. In 2014, 24% self-identified with no organized religious affiliation. The iconic New York City Subway system is the largest rapid transit system in the world when measured by stations in operation, with 469, and by length of routes. New York's subway is notable for nearly the entire system remaining open 24 hours a day, in contrast to the overnight shutdown common to systems in most cities, including Hong Kong, London, Paris, Seoul, and Tokyo. The New York City Subway is also the busiest metropolitan rail transit system in the Western Hemisphere, with 1.75 billion passengers rides in 2014, while Grand Central Terminal, also popularly referred to as ""Grand Central Station"", is the world's largest railway station by number of train platforms. The first non-Native American inhabitant of what would eventually become New York City was Dominican trader Juan Rodriguez (transliterated to Dutch as Jan Rodrigues). Born in Santo Domingo of Portuguese and African descent, he arrived in Manhattan during the winter of 1613–1614, trapping for pelts and trading with the local population as a representative of the Dutch. Broadway, from 159th Street to 218th Street, is named Juan Rodriguez Way in his honor. Under the Köppen climate classification, using the 0 °C (32 °F) coldest month (January) isotherm, New York City itself experiences a humid subtropical climate (Cfa) and is thus the northernmost major city on the North American continent with this categorization. The suburbs to the immediate north and west lie in the transition zone from a humid subtropical (Cfa) to a humid continental climate (Dfa). The area averages 234 days with at least some sunshine annually, and averages 57% of possible sunshine annually, accumulating 2,535 hours of sunshine per annum. The city falls under USDA 7b Plant Hardiness zone. New York is a global hub of international business and commerce. In 2012, New York City topped the first Global Economic Power Index, published by The Atlantic (to be differentiated from a namesake list published by the Martin Prosperity Institute), with cities ranked according to criteria reflecting their presence on similar lists as published by other entities. The city is a major center for banking and finance, retailing, world trade, transportation, tourism, real estate, new media as well as traditional media, advertising, legal services, accountancy, insurance, theater, fashion, and the arts in the United States; while Silicon Alley, metonymous for New York's broad-spectrum high technology sphere, continues to expand. The Port of New York and New Jersey is also a major economic engine, handling record cargo volume in the first half of 2014. New York grew in importance as a trading port while under British rule in the early 1700s. It also became a center of slavery, with 42% of households holding slaves by 1730, more than any other city other than Charleston, South Carolina. Most slaveholders held a few or several domestic slaves, but others hired them out to work at labor. Slavery became integrally tied to New York's economy through the labor of slaves throughout the port, and the banks and shipping tied to the South. Discovery of the African Burying Ground in the 1990s, during construction of a new federal courthouse near Foley Square, revealed that tens of thousands of Africans had been buried in the area in the colonial years. The biotechnology sector is also growing in New York City, based upon the city's strength in academic scientific research and public and commercial financial support. On December 19, 2011, then Mayor Michael R. Bloomberg announced his choice of Cornell University and Technion-Israel Institute of Technology to build a US$2 billion graduate school of applied sciences called Cornell Tech on Roosevelt Island with the goal of transforming New York City into the world's premier technology capital. By mid-2014, Accelerator, a biotech investment firm, had raised more than US$30 million from investors, including Eli Lilly and Company, Pfizer, and Johnson & Johnson, for initial funding to create biotechnology startups at the Alexandria Center for Life Science, which encompasses more than 700,000 square feet (65,000 m2) on East 29th Street and promotes collaboration among scientists and entrepreneurs at the center and with nearby academic, medical, and research institutions. The New York City Economic Development Corporation's Early Stage Life Sciences Funding Initiative and venture capital partners, including Celgene, General Electric Ventures, and Eli Lilly, committed a minimum of US$100 million to help launch 15 to 20 ventures in life sciences and biotechnology. Numerous major American cultural movements began in the city, such as the Harlem Renaissance, which established the African-American literary canon in the United States. The city was a center of jazz in the 1940s, abstract expressionism in the 1950s, and the birthplace of hip hop in the 1970s. The city's punk and hardcore scenes were influential in the 1970s and 1980s. New York has long had a flourishing scene for Jewish American literature. The city's total area is 468.9 square miles (1,214 km2). 164.1 sq mi (425 km2) of this is water and 304.8 sq mi (789 km2) is land. The highest point in the city is Todt Hill on Staten Island, which, at 409.8 feet (124.9 m) above sea level, is the highest point on the Eastern Seaboard south of Maine. The summit of the ridge is mostly covered in woodlands as part of the Staten Island Greenbelt. In 1898, the modern City of New York was formed with the consolidation of Brooklyn (until then a separate city), the County of New York (which then included parts of the Bronx), the County of Richmond, and the western portion of the County of Queens. The opening of the subway in 1904, first built as separate private systems, helped bind the new city together. Throughout the first half of the 20th century, the city became a world center for industry, commerce, and communication. In 2006, the Sister City Program of the City of New York, Inc. was restructured and renamed New York City Global Partners. New York City has expanded its international outreach via this program to a network of cities worldwide, promoting the exchange of ideas and innovation between their citizenry and policymakers, according to the city's website. New York's historic sister cities are denoted below by the year they joined New York City's partnership network. The television industry developed in New York and is a significant employer in the city's economy. The three major American broadcast networks are all headquartered in New York: ABC, CBS, and NBC. Many cable networks are based in the city as well, including MTV, Fox News, HBO, Showtime, Bravo, Food Network, AMC, and Comedy Central. The City of New York operates a public broadcast service, NYCTV, that has produced several original Emmy Award-winning shows covering music and culture in city neighborhoods and city government. Manhattan was on track to have an estimated 90,000 hotel rooms at the end of 2014, a 10% increase from 2013. In October 2014, the Anbang Insurance Group, based in China, purchased the Waldorf Astoria New York for US$1.95 billion, making it the world's most expensive hotel ever sold. New York City is home to Fort Hamilton, the U.S. military's only active duty installation within the city. Established in 1825 in Brooklyn on the site of a small battery utilized during the American Revolution, it is one of America's longest serving military forts. Today Fort Hamilton serves as the headquarters of the North Atlantic Division of the United States Army Corps of Engineers as well as for the New York City Recruiting Battalion. It also houses the 1179th Transportation Brigade, the 722nd Aeromedical Staging Squadron, and a military entrance processing station. Other formerly active military reservations still utilized for National Guard and military training or reserve operations in the city include Fort Wadsworth in Staten Island and Fort Totten in Queens. New York's airspace is the busiest in the United States and one of the world's busiest air transportation corridors. The three busiest airports in the New York metropolitan area include John F. Kennedy International Airport, Newark Liberty International Airport, and LaGuardia Airport; 109 million travelers used these three airports in 2012, and the city's airspace is the busiest in the nation. JFK and Newark Liberty were the busiest and fourth busiest U.S. gateways for international air passengers, respectively, in 2012; as of 2011, JFK was the busiest airport for international passengers in North America. Plans have advanced to expand passenger volume at a fourth airport, Stewart International Airport near Newburgh, New York, by the Port Authority of New York and New Jersey. Plans were announced in July 2015 to entirely rebuild LaGuardia Airport in a multibillion-dollar project to replace its aging facilities. The Staten Island Ferry is the world's busiest ferry route, carrying approximately 20 million passengers on the 5.2-mile (8.4 km) route between Staten Island and Lower Manhattan and running 24 hours a day. Other ferry systems shuttle commuters between Manhattan and other locales within the city and the metropolitan area. The Stonewall riots were a series of spontaneous, violent demonstrations by members of the gay community against a police raid that took place in the early morning hours of June 28, 1969, at the Stonewall Inn in the Greenwich Village neighborhood of Lower Manhattan. They are widely considered to constitute the single most important event leading to the gay liberation movement and the modern fight for LGBT rights in the United States. In 2014, the city had an estimated population density of 27,858 people per square mile (10,756/km²), rendering it the most densely populated of all municipalities housing over 100,000 residents in the United States; however, several small cities (of fewer than 100,000) in adjacent Hudson County, New Jersey are more dense overall, as per the 2000 Census. Geographically co-extensive with New York County, the borough of Manhattan's population density of 71,672 people per square mile (27,673/km²) makes it the highest of any county in the United States and higher than the density of any individual American city. New York is a prominent location for the American entertainment industry, with many films, television series, books, and other media being set there. As of 2012, New York City was the second largest center for filmmaking and television production in the United States, producing about 200 feature films annually, employing 130,000 individuals, and generating an estimated $7.1 billion in direct expenditures, and by volume, New York is the world leader in independent film production; one-third of all American independent films are produced in New York City. The Association of Independent Commercial Producers is also based in New York. In the first five months of 2014 alone, location filming for television pilots in New York City exceeded the record production levels for all of 2013, with New York surpassing Los Angeles as the top North American city for the same distinction during the 2013/2014 cycle. In its 2013 ParkScore ranking, The Trust for Public Land reported that the park system in New York City was the second best park system among the 50 most populous U.S. cities, behind the park system of Minneapolis. ParkScore ranks urban park systems by a formula that analyzes median park size, park acres as percent of city area, the percent of city residents within a half-mile of a park, spending of park services per resident, and the number of playgrounds per 10,000 residents. The annual United States Open Tennis Championships is one of the world's four Grand Slam tennis tournaments and is held at the National Tennis Center in Flushing Meadows-Corona Park, Queens. The New York Marathon is one of the world's largest, and the 2004–2006 events hold the top three places in the marathons with the largest number of finishers, including 37,866 finishers in 2006. The Millrose Games is an annual track and field meet whose featured event is the Wanamaker Mile. Boxing is also a prominent part of the city's sporting scene, with events like the Amateur Boxing Golden Gloves being held at Madison Square Garden each year. The city is also considered the host of the Belmont Stakes, the last, longest and oldest of horse racing's Triple Crown races, held just over the city's border at Belmont Park on the first or second Sunday of June. The city also hosted the 1932 U.S. Open golf tournament and the 1930 and 1939 PGA Championships, and has been host city for both events several times, most notably for nearby Winged Foot Golf Club. Uniquely among major American cities, New York is divided between, and is host to the main branches of, two different US district courts: the District Court for the Southern District of New York, whose main courthouse is on Foley Square near City Hall in Manhattan and whose jurisdiction includes Manhattan and the Bronx, and the District Court for the Eastern District of New York, whose main courthouse is in Brooklyn and whose jurisdiction includes Brooklyn, Queens, and Staten Island. The US Court of Appeals for the Second Circuit and US Court of International Trade are also based in New York, also on Foley Square in Manhattan. The city's land has been altered substantially by human intervention, with considerable land reclamation along the waterfronts since Dutch colonial times; reclamation is most prominent in Lower Manhattan, with developments such as Battery Park City in the 1970s and 1980s. Some of the natural relief in topography has been evened out, especially in Manhattan. Ecuador, Colombia, Guyana, Peru, and Brazil were the top source countries from South America for legal immigrants to the New York City region in 2013; the Dominican Republic, Jamaica, Haiti, and Trinidad and Tobago in the Caribbean; Egypt, Ghana, and Nigeria from Africa; and El Salvador, Honduras, and Guatemala in Central America. Amidst a resurgence of Puerto Rican migration to New York City, this population had increased to approximately 1.3 million in the metropolitan area as of 2013. New York City's most important economic sector lies in its role as the headquarters for the U.S.financial industry, metonymously known as Wall Street. The city's securities industry, enumerating 163,400 jobs in August 2013, continues to form the largest segment of the city's financial sector and an important economic engine, accounting in 2012 for 5 percent of the city's private sector jobs, 8.5 percent (US$3.8 billion) of its tax revenue, and 22 percent of the city's total wages, including an average salary of US$360,700. Many large financial companies are headquartered in New York City, and the city is also home to a burgeoning number of financial startup companies. New York City is additionally a center for the advertising, music, newspaper, digital media, and publishing industries and is also the largest media market in North America. Some of the city's media conglomerates and institutions include Time Warner, the Thomson Reuters Corporation, the Associated Press, Bloomberg L.P., the News Corporation, The New York Times Company, NBCUniversal, the Hearst Corporation, AOL, and Viacom. Seven of the world's top eight global advertising agency networks have their headquarters in New York. Two of the top three record labels' headquarters are in New York: Sony Music Entertainment and Warner Music Group. Universal Music Group also has offices in New York. New media enterprises are contributing an increasingly important component to the city's central role in the media sphere. More than 200 newspapers and 350 consumer magazines have an office in the city, and the publishing industry employs about 25,000 people. Two of the three national daily newspapers in the United States are New York papers: The Wall Street Journal and The New York Times, which has won the most Pulitzer Prizes for journalism. Major tabloid newspapers in the city include: The New York Daily News, which was founded in 1919 by Joseph Medill Patterson and The New York Post, founded in 1801 by Alexander Hamilton. The city also has a comprehensive ethnic press, with 270 newspapers and magazines published in more than 40 languages. El Diario La Prensa is New York's largest Spanish-language daily and the oldest in the nation. The New York Amsterdam News, published in Harlem, is a prominent African American newspaper. The Village Voice is the largest alternative newspaper. According to the United States Geological Survey, an updated analysis of seismic hazard in July 2014 revealed a ""slightly lower hazard for tall buildings"" in New York City than previously assessed. Scientists estimated this lessened risk based upon a lower likelihood than previously thought of slow shaking near the city, which would be more likely to cause damage to taller structures from an earthquake in the vicinity of the city. The city and surrounding area suffered the bulk of the economic damage and largest loss of human life in the aftermath of the September 11, 2001 attacks when 10 of the 19 terrorists associated with Al-Qaeda piloted American Airlines Flight 11 into the North Tower of the World Trade Center and United Airlines Flight 175 into the South Tower of the World Trade Center, and later destroyed them, killing 2,192 civilians, 343 firefighters, and 71 law enforcement officers who were in the towers and in the surrounding area. The rebuilding of the area, has created a new One World Trade Center, and a 9/11 memorial and museum along with other new buildings and infrastructure. The World Trade Center PATH station, which opened on July 19, 1909 as the Hudson Terminal, was also destroyed in the attack. A temporary station was built and opened on November 23, 2003. A permanent station, the World Trade Center Transportation Hub, is currently under construction. The new One World Trade Center is the tallest skyscraper in the Western Hemisphere and the fourth-tallest building in the world by pinnacle height, with its spire reaching a symbolic 1,776 feet (541.3 m) in reference to the year of American independence. In 1785, the assembly of the Congress of the Confederation made New York the national capital shortly after the war. New York was the last capital of the U.S. under the Articles of Confederation and the first capital under the Constitution of the United States. In 1789, the first President of the United States, George Washington, was inaugurated; the first United States Congress and the Supreme Court of the United States each assembled for the first time, and the United States Bill of Rights was drafted, all at Federal Hall on Wall Street. By 1790, New York had surpassed Philadelphia as the largest city in the United States. As of 2013, the global advertising agencies of Omnicom Group and Interpublic Group, both based in Manhattan, had combined annual revenues of approximately US$21 billion, reflecting New York City's role as the top global center for the advertising industry, which is metonymously referred to as ""Madison Avenue"". The city's fashion industry provides approximately 180,000 employees with $11 billion in annual wages. The New York City Police Department (NYPD) has been the largest police force in the United States by a significant margin, with over 35,000 sworn officers. Members of the NYPD are frequently referred to by politicians, the media, and their own police cars by the nickname, New York's Finest. Many of the world's largest media conglomerates are also based in the city. Manhattan contained over 500 million square feet (46.5 million m2) of office space in 2015, making it the largest office market in the United States, while Midtown Manhattan, with nearly 400 million square feet (37.2 million m2) in 2015, is the largest central business district in the world. New York City is located on one of the world's largest natural harbors, and the boroughs of Manhattan and Staten Island are (primarily) coterminous with islands of the same names, while Queens and Brooklyn are located at the west end of the larger Long Island, and The Bronx is located at the southern tip of New York State's mainland. This situation of boroughs separated by water led to the development of an extensive infrastructure of bridges and tunnels. Nearly all of the city's major bridges and tunnels are notable, and several have broken or set records. New York is the most important source of political fundraising in the United States, as four of the top five ZIP codes in the nation for political contributions are in Manhattan. The top ZIP code, 10021 on the Upper East Side, generated the most money for the 2004 presidential campaigns of George W. Bush and John Kerry. The city has a strong imbalance of payments with the national and state governments. It receives 83 cents in services for every $1 it sends to the federal government in taxes (or annually sends $11.4 billion more than it receives back). The city also sends an additional $11 billion more each year to the state of New York than it receives back. The 1916 Zoning Resolution required setbacks in new buildings, and restricted towers to a percentage of the lot size, to allow sunlight to reach the streets below. The Art Deco style of the Chrysler Building (1930) and Empire State Building (1931), with their tapered tops and steel spires, reflected the zoning requirements. The buildings have distinctive ornamentation, such as the eagles at the corners of the 61st floor on the Chrysler Building, and are considered some of the finest examples of the Art Deco style. A highly influential example of the international style in the United States is the Seagram Building (1957), distinctive for its façade using visible bronze-toned I-beams to evoke the building's structure. The Condé Nast Building (2000) is a prominent example of green design in American skyscrapers and has received an award from the American Institute of Architects as well as AIA New York State for its design. The most well-known hospital in the HHC system is Bellevue Hospital, the oldest public hospital in the United States. Bellevue is the designated hospital for treatment of the President of the United States and other world leaders if they become sick or injured while in New York City. The president of HHC is Ramanathan Raju, MD, a surgeon and former CEO of the Cook County health system in Illinois. Many sports are associated with New York's immigrant communities. Stickball, a street version of baseball, was popularized by youths in the 1930s, and a street in the Bronx was renamed Stickball Boulevard in the late 2000s to memorialize this. Each year HHC's facilities provide about 225,000 admissions, one million emergency room visits and five million clinic visits to New Yorkers. HHC facilities treat nearly one-fifth of all general hospital discharges and more than one third of emergency room and hospital-based clinic visits in New York City. New York—often called New York City or the City of New York to distinguish it from the State of New York, of which it is a part—is the most populous city in the United States and the center of the New York metropolitan area, the premier gateway for legal immigration to the United States and one of the most populous urban agglomerations in the world. A global power city, New York exerts a significant impact upon commerce, finance, media, art, fashion, research, technology, education, and entertainment, its fast pace defining the term New York minute. Home to the headquarters of the United Nations, New York is an important center for international diplomacy and has been described as the cultural and financial capital of the world. The New York City Health and Hospitals Corporation (HHC) operates the public hospitals and clinics in New York City. A public benefit corporation with $6.7 billion in annual revenues, HHC is the largest municipal healthcare system in the United States serving 1.4 million patients, including more than 475,000 uninsured city residents. HHC was created in 1969 by the New York State Legislature as a public benefit corporation (Chapter 1016 of the Laws 1969). It is similar to a municipal agency but has a Board of Directors. HHC operates 11 acute care hospitals, five nursing homes, six diagnostic and treatment centers, and more than 70 community-based primary care sites, serving primarily the poor and working class. HHC's MetroPlus Health Plan is one of the New York area's largest providers of government-sponsored health insurance and is the plan of choice for nearly half million New Yorkers. Situated on one of the world's largest natural harbors, New York City consists of five boroughs, each of which is a separate county of New York State. The five boroughs – Brooklyn, Queens, Manhattan, the Bronx, and Staten Island – were consolidated into a single city in 1898. With a census-estimated 2014 population of 8,491,079 distributed over a land area of just 305 square miles (790 km2), New York is the most densely populated major city in the United States. As many as 800 languages are spoken in New York, making it the most linguistically diverse city in the world. By 2014 census estimates, the New York City metropolitan region remains by a significant margin the most populous in the United States, as defined by both the Metropolitan Statistical Area (20.1 million residents) and the Combined Statistical Area (23.6 million residents). In 2013, the MSA produced a gross metropolitan product (GMP) of nearly US$1.39 trillion, while in 2012, the CSA generated a GMP of over US$1.55 trillion, both ranking first nationally by a wide margin and behind the GDP of only twelve and eleven countries, respectively. Public transport is essential in New York City. 54.6% of New Yorkers commuted to work in 2005 using mass transit. This is in contrast to the rest of the United States, where about 90% of commuters drive automobiles to their workplace. According to the US Census Bureau, New York City residents spend an average of 38.4 minutes a day getting to work, the longest commute time in the nation among large cities. New York is the only US city in which a majority (52%) of households do not have a car; only 22% of Manhattanites own a car. Due to their high usage of mass transit, New Yorkers spend less of their household income on transportation than the national average, saving $19 billion annually on transportation compared to other urban Americans. Despite New York's heavy reliance on its vast public transit system, streets are a defining feature of the city. Manhattan's street grid plan greatly influenced the city's physical development. Several of the city's streets and avenues, like Broadway, Wall Street, Madison Avenue, and Seventh Avenue are also used as metonyms for national industries there: the theater, finance, advertising, and fashion organizations, respectively. The city is represented in the National Football League by the New York Giants and the New York Jets, although both teams play their home games at MetLife Stadium in nearby East Rutherford, New Jersey, which hosted Super Bowl XLVIII in 2014. The trial in Manhattan of John Peter Zenger in 1735 helped to establish the freedom of the press in North America. In 1754, Columbia University was founded under charter by King George II as King's College in Lower Manhattan. The Stamp Act Congress met in New York in October 1765 as the Sons of Liberty organized in the city, skirmishing over the next ten years with British troops stationed there. Over 600,000 students are enrolled in New York City's over 120 higher education institutions, the highest number of any city in the United States, including over half million in the City University of New York (CUNY) system alone in 2014. In 2005, three out of five Manhattan residents were college graduates, and one out of four had a postgraduate degree, forming one of the highest concentrations of highly educated people in any American city. New York City is home to such notable private universities as Barnard College, Columbia University, Cooper Union, Fordham University, New York University, New York Institute of Technology, Pace University, and Yeshiva University. The public CUNY system is one of the largest universities in the nation, comprising 24 institutions across all five boroughs: senior colleges, community colleges, and other graduate/professional schools. The public State University of New York (SUNY) system also serves New York City, as well as the rest of the state. The city also has other smaller private colleges and universities, including many religious and special-purpose institutions, such as St. John's University, The Juilliard School, Manhattan College, The College of Mount Saint Vincent, The New School, Pratt Institute, The School of Visual Arts, The King's College, and Wagner College. New York became the most populous urbanized area in the world in the early 1920s, overtaking London. The metropolitan area surpassed the 10 million mark in the early 1930s, becoming the first megacity in human history. The difficult years of the Great Depression saw the election of reformer Fiorello La Guardia as mayor and the fall of Tammany Hall after eighty years of political dominance. Many Fortune 500 corporations are headquartered in New York City, as are a large number of foreign corporations. One out of ten private sector jobs in the city is with a foreign company. New York City has been ranked first among cities across the globe in attracting capital, business, and tourists. This ability to attract foreign investment helped New York City top the FDi Magazine American Cities of the Future ranking for 2013. Lincoln Center for the Performing Arts, anchoring Lincoln Square on the Upper West Side of Manhattan, is home to numerous influential arts organizations, including the Metropolitan Opera, New York City Opera, New York Philharmonic, and New York City Ballet, as well as the Vivian Beaumont Theater, the Juilliard School, Jazz at Lincoln Center, and Alice Tully Hall. The Lee Strasberg Theatre and Film Institute is in Union Square, and Tisch School of the Arts is based at New York University, while Central Park SummerStage presents performances of free plays and music in Central Park. The city is the birthplace of many cultural movements, including the Harlem Renaissance in literature and visual art; abstract expressionism (also known as the New York School) in painting; and hip hop, punk, salsa, disco, freestyle, Tin Pan Alley, and Jazz in music. New York City has been considered the dance capital of the world. The city is also widely celebrated in popular lore, frequently the setting for books, movies (see List of films set in New York City), and television programs. New York Fashion Week is one of the world's preeminent fashion events and is afforded extensive coverage by the media. New York has also frequently been ranked the top fashion capital of the world on the annual list compiled by the Global Language Monitor. The New York area is home to a distinctive regional speech pattern called the New York dialect, alternatively known as Brooklynese or New Yorkese. It has generally been considered one of the most recognizable accents within American English. The classic version of this dialect is centered on middle and working-class people of European descent. However, the influx of non-European immigrants in recent decades has led to changes in this distinctive dialect, and the traditional form of this speech pattern is no longer as prevalent among general New Yorkers as in the past. Multibillion US$ heavy-rail transit projects under construction in New York City include the Second Avenue Subway, the East Side Access project, and the 7 Subway Extension. New York City is home to hundreds of cultural institutions and historic sites, many of which are internationally known. Museum Mile is the name for a section of Fifth Avenue running from 82nd to 105th streets on the Upper East Side of Manhattan, in an area sometimes called Upper Carnegie Hill. The Mile, which contains one of the densest displays of culture in the world, is actually three blocks longer than one mile (1.6 km). Ten museums occupy the length of this section of Fifth Avenue. The tenth museum, the Museum for African Art, joined the ensemble in 2009, however its Museum at 110th Street, the first new museum constructed on the Mile since the Guggenheim in 1959, opened in late 2012. In addition to other programming, the museums collaborate for the annual Museum Mile Festival, held each year in June, to promote the museums and increase visitation. Many of the world's most lucrative art auctions are held in New York City. The Democratic Party holds the majority of public offices. As of November 2008, 67% of registered voters in the city are Democrats. New York City has not been carried by a Republican in a statewide or presidential election since President Calvin Coolidge won the five boroughs in 1924. In 2012, Democrat Barack Obama became the first presidential candidate of any party to receive more than 80% of the overall vote in New York City, sweeping all five boroughs. Party platforms center on affordable housing, education, and economic development, and labor politics are of importance in the city. The city's National Basketball Association teams are the Brooklyn Nets and the New York Knicks, while the New York Liberty is the city's Women's National Basketball Association. The first national college-level basketball championship, the National Invitation Tournament, was held in New York in 1938 and remains in the city. The city is well known for its links to basketball, which is played in nearly every park in the city by local youth, many of whom have gone on to play for major college programs and in the NBA. There are hundreds of distinct neighborhoods throughout the five boroughs of New York City, many with a definable history and character to call their own. If the boroughs were each independent cities, four of the boroughs (Brooklyn, Queens, Manhattan, and the Bronx) would be among the ten most populous cities in the United States. Winters are cold and damp, and prevailing wind patterns that blow offshore minimize the moderating effects of the Atlantic Ocean; yet the Atlantic and the partial shielding from colder air by the Appalachians keep the city warmer in the winter than inland North American cities at similar or lesser latitudes such as Pittsburgh, Cincinnati, and Indianapolis. The daily mean temperature in January, the area's coldest month, is 32.6 °F (0.3 °C); however, temperatures usually drop to 10 °F (−12 °C) several times per winter, and reach 50 °F (10 °C) several days each winter month. Spring and autumn are unpredictable and can range from chilly to warm, although they are usually mild with low humidity. Summers are typically warm to hot and humid, with a daily mean temperature of 76.5 °F (24.7 °C) in July and an average humidity level of 72%. Nighttime conditions are often exacerbated by the urban heat island phenomenon, while daytime temperatures exceed 90 °F (32 °C) on average of 17 days each summer and in some years exceed 100 °F (38 °C). In the warmer months, the dew point, a measure of atmospheric moisture, ranges from 57.3 °F (14.1 °C) in June to 62.0 °F (16.7 °C) in August. Extreme temperatures have ranged from −15 °F (−26 °C), recorded on February 9, 1934, up to 106 °F (41 °C) on July 9, 1936. Democratic Party candidates were consistently elected to local office, increasing the city's ties to the South and its dominant party. In 1861, Mayor Fernando Wood called on the aldermen to declare independence from Albany and the United States after the South seceded, but his proposal was not acted on. Anger at new military conscription laws during the American Civil War (1861–1865), which spared wealthier men who could afford to pay a $300 (equivalent to $5,766 in 2016) commutation fee to hire a substitute, led to the Draft Riots of 1863, whose most visible participants were ethnic Irish working class. The situation deteriorated into attacks on New York's elite, followed by attacks on black New Yorkers and their property after fierce competition for a decade between Irish immigrants and blacks for work. Rioters burned the Colored Orphan Asylum to the ground, but more than 200 children escaped harm due to efforts of the New York City Police Department, which was mainly made up of Irish immigrants. According to historian James M. McPherson (2001), at least 120 people were killed. In all, eleven black men were lynched over five days, and the riots forced hundreds of blacks to flee the city for Williamsburg, Brooklyn, as well as New Jersey; the black population in Manhattan fell below 10,000 by 1865, which it had last been in 1820. The white working class had established dominance. Violence by longshoremen against black men was especially fierce in the docks area. It was one of the worst incidents of civil unrest in American history. Sociologists and criminologists have not reached consensus on the explanation for the dramatic decrease in the city's crime rate. Some attribute the phenomenon to new tactics used by the NYPD, including its use of CompStat and the broken windows theory. Others cite the end of the crack epidemic and demographic changes, including from immigration. Another theory is that widespread exposure to lead pollution from automobile exhaust, which can lower intelligence and increase aggression levels, incited the initial crime wave in the mid-20th century, most acutely affecting heavily trafficked cities like New York. A strong correlation was found demonstrating that violent crime rates in New York and other big cities began to fall after lead was removed from American gasoline in the 1970s. Another theory cited to explain New York City's falling homicide rate is the inverse correlation between the number of murders and the increasingly wetter climate in the city. The George Washington Bridge is the world's busiest motor vehicle bridge, connecting Manhattan to Bergen County, New Jersey. The Verrazano-Narrows Bridge is the longest suspension bridge in the Americas and one of the world's longest. The Brooklyn Bridge is an icon of the city itself. The towers of the Brooklyn Bridge are built of limestone, granite, and Rosendale cement, and their architectural style is neo-Gothic, with characteristic pointed arches above the passageways through the stone towers. This bridge was also the longest suspension bridge in the world from its opening until 1903, and is the first steel-wire suspension bridge. New York City has been a metropolitan municipality with a mayor-council form of government since its consolidation in 1898. The government of New York is more centralized than that of most other U.S. cities. In New York City, the city government is responsible for public education, correctional institutions, public safety, recreational facilities, sanitation, water supply, and welfare services. The Occupy Wall Street protests in Zuccotti Park in the Financial District of Lower Manhattan began on September 17, 2011, receiving global attention and spawning the Occupy movement against social and economic inequality worldwide. New York City's public bus fleet is the largest in North America, and the Port Authority Bus Terminal, the main intercity bus terminal of the city, serves 7,000 buses and 200,000 commuters daily, making it the busiest bus station in the world. In 2012, New York City had the lowest overall crime rate and the second lowest murder rate among the largest U.S. cities, having become significantly safer after a spike in crime in the 1970s through 1990s. Violent crime in New York City decreased more than 75% from 1993 to 2005, and continued decreasing during periods when the nation as a whole saw increases. By 2002, New York City's crime rate was similar to that of Provo, Utah, and was ranked 197th in crime among the 216 U.S. cities with populations greater than 100,000. In 2005 the homicide rate was at its lowest level since 1966, and in 2007 the city recorded fewer than 500 homicides for the first time ever since crime statistics were first published in 1963. In the first six months of 2010, 95.1% of all murder victims and 95.9% of all shooting victims in New York City were black or Hispanic; additionally, 90.2 percent of those arrested for murder and 96.7 percent of those arrested for shooting someone were black or Hispanic. New York experienced a record low of 328 homicides in 2014 and has a far lower murder rate than other major American cities. New York's high rate of public transit use, over 200,000 daily cyclists as of 2014, and many pedestrian commuters make it the most energy-efficient major city in the United States. Walk and bicycle modes of travel account for 21% of all modes for trips in the city; nationally the rate for metro regions is about 8%. In both its 2011 and 2015 rankings, Walk Score named New York City the most walkable large city in the United States. Citibank sponsored the introduction of 10,000 public bicycles for the city's bike-share project in the summer of 2013. Research conducted by Quinnipiac University showed that a majority of New Yorkers support the initiative. New York City's numerical ""in-season cycling indicator"" of bicycling in the city hit an all-time high in 2013. The city government was a petitioner in the landmark Massachusetts v. Environmental Protection Agency Supreme Court case forcing the EPA to regulate greenhouse gases as pollutants. The city is also a leader in the construction of energy-efficient green office buildings, including the Hearst Tower among others. Mayor Bill de Blasio has committed to an 80% reduction in greenhouse gas emissions between 2014 and 2050 to reduce the city's contributions to climate change, beginning with a comprehensive ""Green Buildings"" plan. New York City is the most-populous city in the United States, with an estimated record high of 8,491,079 residents as of 2014, incorporating more immigration into the city than outmigration since the 2010 United States Census. More than twice as many people live in New York City as in the second-most populous U.S. city (Los Angeles), and within a smaller area. New York City gained more residents between April 2010 and July 2014 (316,000) than any other U.S. city. New York City's population amounts to about 40% of New York State's population and a similar percentage of the New York metropolitan area population. New York City's food culture includes a variety of international cuisines influenced by the city's immigrant history. Central European and Italian immigrants originally made the city famous for bagels, cheesecake, and New York-style pizza, while Chinese and other Asian restaurants, sandwich joints, trattorias, diners, and coffeehouses have become ubiquitous. Some 4,000 mobile food vendors licensed by the city, many immigrant-owned, have made Middle Eastern foods such as falafel and kebabs popular examples of modern New York street food. The city is also home to nearly one thousand of the finest and most diverse haute cuisine restaurants in the world, according to Michelin. The New York City Department of Health and Mental Hygiene assigns letter grades to the city's 24,000 restaurants based upon their inspection results. In 1609, English explorer Henry Hudson re-discovered the region when he sailed his ship the Halve Maen (""Half Moon"" in Dutch) into New York Harbor while searching for the Northwest Passage to the Orient for his employer, the Dutch East India Company. He proceeded to sail up what he named the North River, also called the Mauritis River, and now known as the Hudson River, to the site of the present-day New York State capital of Albany in the belief that it might represent an oceanic tributary. When the river narrowed and was no longer saline, he realized it was not a maritime passage and sailed back downriver. He made a ten-day exploration of the area and claimed the region for his employer. In 1614, the area between Cape Cod and Delaware Bay would be claimed by the Netherlands and called Nieuw-Nederland (New Netherland). The Statue of Liberty National Monument and Ellis Island Immigration Museum are managed by the National Park Service and are in both the states of New York and New Jersey. They are joined in the harbor by Governors Island National Monument, in New York. Historic sites under federal management on Manhattan Island include Castle Clinton National Monument; Federal Hall National Memorial; Theodore Roosevelt Birthplace National Historic Site; General Grant National Memorial (""Grant's Tomb""); African Burial Ground National Monument; and Hamilton Grange National Memorial. Hundreds of private properties are listed on the National Register of Historic Places or as a National Historic Landmark such as, for example, the Stonewall Inn in Greenwich Village as the catalyst of the modern gay rights movement. I Love New York (stylized I ❤ NY) is both a logo and a song that are the basis of an advertising campaign and have been used since 1977 to promote tourism in New York City, and later to promote New York State as well. The trademarked logo, owned by New York State Empire State Development, appears in souvenir shops and brochures throughout the city and state, some licensed, many not. The song is the state song of New York. New York City has more than 2,000 arts and cultural organizations and more than 500 art galleries of all sizes. The city government funds the arts with a larger annual budget than the National Endowment for the Arts. Wealthy business magnates in the 19th century built a network of major cultural institutions, such as the famed Carnegie Hall and The Metropolitan Museum of Art, that would become internationally established. The advent of electric lighting led to elaborate theater productions, and in the 1880s, New York City theaters on Broadway and along 42nd Street began featuring a new stage form that became known as the Broadway musical. Strongly influenced by the city's immigrants, productions such as those of Harrigan and Hart, George M. Cohan, and others used song in narratives that often reflected themes of hope and ambition. Mass transit in New York City, most of which runs 24 hours a day, accounts for one in every three users of mass transit in the United States, and two-thirds of the nation's rail riders live in the New York City Metropolitan Area. New York City is home to the headquarters of the National Football League, Major League Baseball, the National Basketball Association, the National Hockey League, and Major League Soccer. The New York metropolitan area hosts the most sports teams in these five professional leagues. Participation in professional sports in the city predates all professional leagues, and the city has been continuously hosting professional sports since the birth of the Brooklyn Dodgers in 1882. The city has played host to over forty major professional teams in the five sports and their respective competing leagues, both current and historic. Four of the ten most expensive stadiums ever built worldwide (MetLife Stadium, the new Yankee Stadium, Madison Square Garden, and Citi Field) are located in the New York metropolitan area. Madison Square Garden, its predecessor, as well as the original Yankee Stadium and Ebbets Field, are some of the most famous sporting venues in the world, the latter two having been commemorated on U.S. postage stamps. The New York City Charter School Center assists the setup of new charter schools. There are approximately 900 additional privately run secular and religious schools in the city. Asian Americans in New York City, according to the 2010 Census, number more than one million, greater than the combined totals of San Francisco and Los Angeles. New York contains the highest total Asian population of any U.S. city proper. The New York City borough of Queens is home to the state's largest Asian American population and the largest Andean (Colombian, Ecuadorian, Peruvian, and Bolivian) populations in the United States, and is also the most ethnically diverse urban area in the world. The Chinese population constitutes the fastest-growing nationality in New York State; multiple satellites of the original Manhattan Chinatown (紐約華埠), in Brooklyn (布鲁克林華埠), and around Flushing, Queens (法拉盛華埠), are thriving as traditionally urban enclaves, while also expanding rapidly eastward into suburban Nassau County (拿騷縣) on Long Island (長島), as the New York metropolitan region and New York State have become the top destinations for new Chinese immigrants, respectively, and large-scale Chinese immigration continues into New York City and surrounding areas. In 2012, 6.3% of New York City was of Chinese ethnicity, with nearly three-fourths living in either Queens or Brooklyn, geographically on Long Island. A community numbering 20,000 Korean-Chinese (Chaoxianzu (Chinese: 朝鲜族) or Joseonjok (Hangul: 조선족)) is centered in Flushing, Queens, while New York City is also home to the largest Tibetan population outside China, India, and Nepal, also centered in Queens. Koreans made up 1.2% of the city's population, and Japanese 0.3%. Filipinos were the largest Southeast Asian ethnic group at 0.8%, followed by Vietnamese, who made up 0.2% of New York City's population in 2010. Indians are the largest South Asian group, comprising 2.4% of the city's population, with Bangladeshis and Pakistanis at 0.7% and 0.5%, respectively. Queens is the preferred borough of settlement for Asian Indians, Koreans, and Filipinos, as well as Malaysians and other Southeast Asians; while Brooklyn is receiving large numbers of both West Indian as well as Asian Indian immigrants. The New York metropolitan area is home to a self-identifying gay and bisexual community estimated at 568,903 individuals, the largest in the United States and one of the world's largest. Same-sex marriages in New York were legalized on June 24, 2011 and were authorized to take place beginning 30 days thereafter. New York City has focused on reducing its environmental impact and carbon footprint. Mass transit use in New York City is the highest in the United States. Also, by 2010, the city had 3,715 hybrid taxis and other clean diesel vehicles, representing around 28% of New York's taxi fleet in service, the most of any city in North America. Each borough is coextensive with a judicial district of the state Unified Court System, of which the Criminal Court and the Civil Court are the local courts, while the New York Supreme Court conducts major trials and appeals. Manhattan hosts the First Department of the Supreme Court, Appellate Division while Brooklyn hosts the Second Department. There are also several extrajudicial administrative courts, which are executive agencies and not part of the state Unified Court System. During the Wisconsinan glaciation, the New York City region was situated at the edge of a large ice sheet over 1,000 feet in depth. The ice sheet scraped away large amounts of soil, leaving the bedrock that serves as the geologic foundation for much of New York City today. Later on, the ice sheet would help split apart what are now Long Island and Staten Island. New York City is supplied with drinking water by the protected Catskill Mountains watershed. As a result of the watershed's integrity and undisturbed natural water filtration system, New York is one of only four major cities in the United States the majority of whose drinking water is pure enough not to require purification by water treatment plants. The Croton Watershed north of the city is undergoing construction of a US$3.2 billion water purification plant to augment New York City's water supply by an estimated 290 million gallons daily, representing a greater than 20% addition to the city's current availability of water. The ongoing expansion of New York City Water Tunnel No. 3, an integral part of the New York City water supply system, is the largest capital construction project in the city's history. Many districts and landmarks in New York City have become well known, and the city received a record 56 million tourists in 2014, hosting three of the world's ten most visited tourist attractions in 2013. Several sources have ranked New York the most photographed city in the world. Times Square, iconic as the world's ""heart"" and its ""Crossroads"", is the brightly illuminated hub of the Broadway Theater District, one of the world's busiest pedestrian intersections, and a major center of the world's entertainment industry. The names of many of the city's bridges, skyscrapers, and parks are known around the world. Anchored by Wall Street in the Financial District of Lower Manhattan, New York City has been called both the most economically powerful city and the leading financial center of the world, and the city is home to the world's two largest stock exchanges by total market capitalization, the New York Stock Exchange and NASDAQ. Manhattan's real estate market is among the most expensive in the world. Manhattan's Chinatown incorporates the highest concentration of Chinese people in the Western Hemisphere, with multiple signature Chinatowns developing across the city. Providing continuous 24/7 service, the New York City Subway is one of the most extensive metro systems worldwide, with 469 stations in operation. New York City's higher education network comprises over 120 colleges and universities, including Columbia University, New York University, and Rockefeller University, which have been ranked among the top 35 in the world. A permanent European presence in New Netherland began in 1624 – making New York the 12th oldest continuously occupied European-established settlement in the continental United States  – with the founding of a Dutch fur trading settlement on Governors Island. In 1625, construction was started on a citadel and a Fort Amsterdam on Manhattan Island, later called New Amsterdam (Nieuw Amsterdam). The colony of New Amsterdam was centered at the site which would eventually become Lower Manhattan. The Dutch colonial Director-General Peter Minuit purchased the island of Manhattan from the Canarsie, a small band of the Lenape, in 1626 for a value of 60 guilders (about $1000 in 2006); a disproved legend says that Manhattan was purchased for $24 worth of glass beads. The New York City Fire Department faces highly multifaceted firefighting challenges in many ways unique to New York. In addition to responding to building types that range from wood-frame single family homes to high-rise structures, there are many secluded bridges and tunnels, as well as large parks and wooded areas that can give rise to brush fires. New York is also home to one of the largest subway systems in the world, consisting of hundreds of miles of tunnel with electrified track. Under New York State's gradual abolition act of 1799, children of slave mothers were born to be eventually liberated but were held in indentured servitude until their mid-to-late twenties. Together with slaves freed by their masters after the Revolutionary War and escaped slaves, a significant free-black population gradually developed in Manhattan. Under such influential United States founders as Alexander Hamilton and John Jay, the New York Manumission Society worked for abolition and established the African Free School to educate black children. It was not until 1827 that slavery was completely abolished in the state, and free blacks struggled afterward with discrimination. New York interracial abolitionist activism continued; among its leaders were graduates of the African Free School. The city's black population reached more than 16,000 in 1840. The Battle of Long Island, the largest battle of the American Revolutionary War, was fought in August 1776 entirely within the modern-day borough of Brooklyn. After the battle, in which the Americans were defeated, leaving subsequent smaller armed engagements following in its wake, the city became the British military and political base of operations in North America. The city was a haven for Loyalist refugees, as well as escaped slaves who joined the British lines for freedom newly promised by the Crown for all fighters. As many as 10,000 escaped slaves crowded into the city during the British occupation. When the British forces evacuated at the close of the war in 1783, they transported 3,000 freedmen for resettlement in Nova Scotia. They resettled other freedmen in England and the Caribbean. New York City has the largest European and non-Hispanic white population of any American city. At 2.7 million in 2012, New York's non-Hispanic white population is larger than the non-Hispanic white populations of Los Angeles (1.1 million), Chicago (865,000), and Houston (550,000) combined. The European diaspora residing in the city is very diverse. According to 2012 Census estimates, there were roughly 560,000 Italian Americans, 385,000 Irish Americans, 253,000 German Americans, 223,000 Russian Americans, 201,000 Polish Americans, and 137,000 English Americans. Additionally, Greek and French Americans numbered 65,000 each, with those of Hungarian descent estimated at 60,000 people. Ukrainian and Scottish Americans numbered 55,000 and 35,000, respectively. People identifying ancestry from Spain numbered 30,838 total in 2010. People of Norwegian and Swedish descent both stood at about 20,000 each, while people of Czech, Lithuanian, Portuguese, Scotch-Irish, and Welsh descent all numbered between 12,000–14,000 people. Arab Americans number over 160,000 in New York City, with the highest concentration in Brooklyn. Central Asians, primarily Uzbek Americans, are a rapidly growing segment of the city's non-Hispanic white population, enumerating over 30,000, and including over half of all Central Asian immigrants to the United States, most settling in Queens or Brooklyn. Albanian Americans are most highly concentrated in the Bronx. Approximately 37% of the city's population is foreign born. In New York, no single country or region of origin dominates. The ten largest sources of foreign-born individuals in the city as of 2011 were the Dominican Republic, China, Mexico, Guyana, Jamaica, Ecuador, Haiti, India, Russia, and Trinidad and Tobago, while the Bangladeshi immigrant population has since become one of the fastest growing in the city, counting over 74,000 by 2013. Silicon Alley, centered in Manhattan, has evolved into a metonym for the sphere encompassing the New York City metropolitan region's high technology industries involving the Internet, new media, telecommunications, digital media, software development, biotechnology, game design, financial technology (""fintech""), and other fields within information technology that are supported by its entrepreneurship ecosystem and venture capital investments. In the first half of 2015, Silicon Alley generated over US$3.7 billion in venture capital investment across a broad spectrum of high technology enterprises, most based in Manhattan, with others in Brooklyn, Queens, and elsewhere in the region. High technology startup companies and employment are growing in New York City and the region, bolstered by the city's position in North America as the leading Internet hub and telecommunications center, including its vicinity to several transatlantic fiber optic trunk lines, New York's intellectual capital, and its extensive outdoor wireless connectivity. Verizon Communications, headquartered at 140 West Street in Lower Manhattan, was at the final stages in 2014 of completing a US$3 billion fiberoptic telecommunications upgrade throughout New York City. As of 2014, New York City hosted 300,000 employees in the tech sector. In 1664, Peter Stuyvesant, the Director-General of the colony of New Netherland, surrendered New Amsterdam to the English without bloodshed. The English promptly renamed the fledgling city ""New York"" after the Duke of York (later King James II). The City of New York has a complex park system, with various lands operated by the National Park Service, the New York State Office of Parks, Recreation and Historic Preservation, and the New York City Department of Parks and Recreation. There are seven state parks within the confines of New York City, including Clay Pit Ponds State Park Preserve, a natural area which includes extensive riding trails, and Riverbank State Park, a 28-acre (110,000 m2) facility that rises 69 feet (21 m) over the Hudson River. The New York City Public Schools system, managed by the New York City Department of Education, is the largest public school system in the United States, serving about 1.1 million students in more than 1,700 separate primary and secondary schools. The city's public school system includes nine specialized high schools to serve academically and artistically gifted students. Tourism is a vital industry for New York City, which has witnessed a growing combined volume of international and domestic tourists – receiving approximately 51 million tourists in 2011, 54 million in 2013, and a record 56.4 million in 2014. Tourism generated an all-time high US$61.3 billion in overall economic impact for New York City in 2014. Manhattan Island is linked to New York City's outer boroughs and New Jersey by several tunnels as well. The Lincoln Tunnel, which carries 120,000 vehicles a day under the Hudson River between New Jersey and Midtown Manhattan, is the busiest vehicular tunnel in the world. The tunnel was built instead of a bridge to allow unfettered passage of large passenger and cargo ships that sailed through New York Harbor and up the Hudson River to Manhattan's piers. The Holland Tunnel, connecting Lower Manhattan to Jersey City, New Jersey, was the world's first mechanically ventilated vehicular tunnel when it opened in 1927. The Queens-Midtown Tunnel, built to relieve congestion on the bridges connecting Manhattan with Queens and Brooklyn, was the largest non-federal project in its time when it was completed in 1940. President Franklin D. Roosevelt was the first person to drive through it. The Hugh L. Carey Tunnel runs underneath Battery Park and connects the Financial District at the southern tip of Manhattan to Red Hook in Brooklyn. Chocolate is New York City's leading specialty-food export, with up to US$234 million worth of exports each year. Entrepreneurs were forming a ""Chocolate District"" in Brooklyn as of 2014, while Godiva, one of the world's largest chocolatiers, continues to be headquartered in Manhattan. In soccer, New York City is represented by New York City FC of Major League Soccer, who play their home games at Yankee Stadium. The New York Red Bulls play their home games at Red Bull Arena in nearby Harrison, New Jersey. Historically, the city is known for the New York Cosmos, the highly successful former professional soccer team which was the American home of Pelé, one of the world's most famous soccer players. A new version of the New York Cosmos was formed in 2010, and began play in the second division North American Soccer League in 2013. The Cosmos play their home games at James M. Shuart Stadium on the campus of Hofstra University, just outside the New York City limits in Hempstead, New York. New York City traces its roots to its 1624 founding as a trading post by colonists of the Dutch Republic and was named New Amsterdam in 1626. The city and its surroundings came under English control in 1664. New York served as the capital of the United States from 1785 until 1790. It has been the country's largest city since 1790. The Statue of Liberty greeted millions of immigrants as they came to the Americas by ship in the late 19th and early 20th centuries and is a globally recognized symbol of the United States and its democracy. Gateway National Recreation Area contains over 26,000 acres (10,521.83 ha) in total, most of it surrounded by New York City, including the Jamaica Bay Wildlife Refuge in Brooklyn and Queens, over 9,000 acres (36 km2) of salt marsh, islands, and water, including most of Jamaica Bay. Also in Queens, the park includes a significant portion of the western Rockaway Peninsula, most notably Jacob Riis Park and Fort Tilden. In Staten Island, the park includes Fort Wadsworth, with historic pre-Civil War era Battery Weed and Fort Tompkins, and Great Kills Park, with beaches, trails, and a marina. Several prominent American literary figures lived in New York during the 1830s and 1840s, including William Cullen Bryant, Washington Irving, Herman Melville, Rufus Wilmot Griswold, John Keese, Nathaniel Parker Willis, and Edgar Allan Poe. Public-minded members of the contemporaneous business elite lobbied for the establishment of Central Park, which in 1857 became the first landscaped park in an American city. New York City has been described as the cultural capital of the world by the diplomatic consulates of Iceland and Latvia and by New York's Baruch College. A book containing a series of essays titled New York, culture capital of the world, 1940–1965 has also been published as showcased by the National Library of Australia. In describing New York, author Tom Wolfe said, ""Culture just seems to be in the air, like part of the weather."" The wider New York City metropolitan area, with over 20 million people, about 50% greater than the second-place Los Angeles metropolitan area in the United States, is also ethnically diverse. The New York region continues to be by far the leading metropolitan gateway for legal immigrants admitted into the United States, substantially exceeding the combined totals of Los Angeles and Miami, the next most popular gateway regions. It is home to the largest Jewish as well as Israeli communities outside Israel, with the Jewish population in the region numbering over 1.5 million in 2012 and including many diverse Jewish sects from around the Middle East and Eastern Europe. The metropolitan area is also home to 20% of the nation's Indian Americans and at least 20 Little India enclaves, as well as 15% of all Korean Americans and four Koreatowns; the largest Asian Indian population in the Western Hemisphere; the largest Russian American, Italian American, and African American populations; the largest Dominican American, Puerto Rican American, and South American and second-largest overall Hispanic population in the United States, numbering 4.8 million; and includes at least 6 established Chinatowns within New York City alone, with the urban agglomeration comprising a population of 779,269 overseas Chinese as of 2013 Census estimates, the largest outside of Asia. The only attempt at a peaceful solution to the war took place at the Conference House on Staten Island between American delegates, including Benjamin Franklin, and British general Lord Howe on September 11, 1776. Shortly after the British occupation began, the Great Fire of New York occurred, a large conflagration on the West Side of Lower Manhattan, which destroyed about a quarter of the buildings in the city, including Trinity Church. Lower Manhattan is the third-largest central business district in the United States and is home to the New York Stock Exchange, on Wall Street, and the NASDAQ, at 165 Broadway, representing the world's largest and second largest stock exchanges, respectively, when measured both by overall average daily trading volume and by total market capitalization of their listed companies in 2013. Investment banking fees on Wall Street totaled approximately $40 billion in 2012, while in 2013, senior New York City bank officers who manage risk and compliance functions earned as much as $324,000 annually. In fiscal year 2013–14, Wall Street's securities industry generated 19% of New York State's tax revenue. New York City remains the largest global center for trading in public equity and debt capital markets, driven in part by the size and financial development of the U.S. economy.:31–32 In July 2013, NYSE Euronext, the operator of the New York Stock Exchange, took over the administration of the London interbank offered rate from the British Bankers Association. New York also leads in hedge fund management; private equity; and the monetary volume of mergers and acquisitions. Several investment banks and investment mangers headquartered in Manhattan are important participants in other global financial centers.:34–35 New York is also the principal commercial banking center of the United States. In the 19th century, the city was transformed by development relating to its status as a trading center, as well as by European immigration. The city adopted the Commissioners' Plan of 1811, which expanded the city street grid to encompass all of Manhattan. The 1825 completion of the Erie Canal through central New York connected the Atlantic port to the agricultural markets and commodities of the North American interior via the Hudson River and the Great Lakes. Local politics became dominated by Tammany Hall, a political machine supported by Irish and German immigrants. New York has architecturally noteworthy buildings in a wide range of styles and from distinct time periods, from the saltbox style Pieter Claesen Wyckoff House in Brooklyn, the oldest section of which dates to 1656, to the modern One World Trade Center, the skyscraper at Ground Zero in Lower Manhattan and currently the most expensive new office tower in the world."
Central_African_Republic,"Agriculture is dominated by the cultivation and sale of food crops such as cassava, peanuts, maize, sorghum, millet, sesame, and plantain. The annual real GDP growth rate is just above 3%. The importance of food crops over exported cash crops is indicated by the fact that the total production of cassava, the staple food of most Central Africans, ranges between 200,000 and 300,000 tonnes a year, while the production of cotton, the principal exported cash crop, ranges from 25,000 to 45,000 tonnes a year. Food crops are not exported in large quantities, but still constitute the principal cash crops of the country, because Central Africans derive far more income from the periodic sale of surplus food crops than from exported cash crops such as cotton or coffee.[citation needed] Much of the country is self-sufficient in food crops; however, livestock development is hindered by the presence of the tsetse fly.[citation needed] During the 16th and 17th centuries slave traders began to raid the region as part of the expansion of the Saharan and Nile River slave routes. Their captives were slaved and shipped to the Mediterranean coast, Europe, Arabia, the Western Hemisphere, or to the slave ports and factories along the West and North Africa or South the Ubanqui and Congo rivers. In the mid 19th century, the Bobangi people became major slave traders and sold their captives to the Americas using the Ubangi river to reach the coast. During the 18th century Bandia-Nzakara peoples established the Bangassou Kingdom along the Ubangi River. A new government was appointed on 31 March 2013, which consisted of members of Séléka and representatives of the opposition to Bozizé, one pro-Bozizé individual, and a number representatives of civil society. On 1 April, the former opposition parties declared that they would boycott the government. After African leaders in Chad refused to recognize Djotodia as President, proposing to form a transitional council and the holding of new elections, Djotodia signed a decree on 6 April for the formation of a council that would act as a transitional parliament. The council was tasked with electing a president to serve prior to elections in 18 months. In 1920 French Equatorial Africa was established and Ubangi-Shari was administered from Brazzaville. During the 1920s and 1930s the French introduced a policy of mandatory cotton cultivation, a network of roads was built, attempts were made to combat sleeping sickness and Protestant missions were established to spread Christianity. New forms of forced labor were also introduced and a large number of Ubangians were sent to work on the Congo-Ocean Railway. Many of these forced laborers died of exhaustion, illness, or the poor conditions which claimed between 20% and 25% of the 127,000 workers. In the aftermath of the failed coup, militias loyal to Patassé sought revenge against rebels in many neighborhoods of Bangui and incited unrest including the murder of many political opponents. Eventually, Patassé came to suspect that General François Bozizé was involved in another coup attempt against him, which led Bozizé to flee with loyal troops to Chad. In March 2003, Bozizé launched a surprise attack against Patassé, who was out of the country. Libyan troops and some 1,000 soldiers of Bemba's Congolese rebel organization failed to stop the rebels and Bozizé's forces succeeded in overthrowing Patassé.[citation needed] Michel Djotodia took over as president and in May 2013 Central African Republic's Prime Minister Nicolas Tiangaye requested a UN peacekeeping force from the UN Security Council and on 31 May former President Bozizé was indicted for crimes against humanity and incitement of genocide. The security situation did not improve during June–August 2013 and there were reports of over 200,000 internally displaced persons (IDPs) as well as human rights abuses and renewed fighting between Séléka and Bozizé supporters. The per capita income of the Republic is often listed as being approximately $400 a year, one of the lowest in the world, but this figure is based mostly on reported sales of exports and largely ignores the unregistered sale of foods, locally produced alcoholic beverages, diamonds, ivory, bushmeat, and traditional medicine. For most Central Africans, the informal economy of the CAR is more important than the formal economy.[citation needed] Export trade is hindered by poor economic development and the country's landlocked position.[citation needed] In September 1940, during the Second World War, pro-Gaullist French officers took control of Ubangi-Shari and General Leclerc established his headquarters for the Free French Forces in Bangui. In 1946 Barthélémy Boganda was elected with 9,000 votes to the French National Assembly, becoming the first representative for CAR in the French government. Boganda maintained a political stance against racism and the colonial regime but gradually became disheartened with the French political system and returned to CAR to establish the Movement for the Social Evolution of Black Africa (MESAN) in 1950. In April 1979, young students protested against Bokassa's decree that all school attendees would need to buy uniforms from a company owned by one of his wives. The government violently suppressed the protests, killing 100 children and teenagers. Bokassa himself may have been personally involved in some of the killings. In September 1979, France overthrew Bokassa and ""restored"" Dacko to power (subsequently restoring the name of the country to the Central African Republic). Dacko, in turn, was again overthrown in a coup by General André Kolingba on 1 September 1981. Patassé purged many of the Kolingba elements from the government and Kolingba supporters accused Patassé's government of conducting a ""witch hunt"" against the Yakoma. A new constitution was approved on 28 December 1994 but had little impact on the country's politics. In 1996–1997, reflecting steadily decreasing public confidence in the government's erratic behaviour, three mutinies against Patassé's administration were accompanied by widespread destruction of property and heightened ethnic tension. During this time (1996) the Peace Corps evacuated all its volunteers to neighboring Cameroon. To date, the Peace Corps has not returned to the Central African Republic. The Bangui Agreements, signed in January 1997, provided for the deployment of an inter-African military mission, to Central African Republic and re-entry of ex-mutineers into the government on 7 April 1997. The inter-African military mission was later replaced by a U.N. peacekeeping force (MINURCA). There are many missionary groups operating in the country, including Lutherans, Baptists, Catholics, Grace Brethren, and Jehovah's Witnesses. While these missionaries are predominantly from the United States, France, Italy, and Spain, many are also from Nigeria, the Democratic Republic of the Congo, and other African countries. Large numbers of missionaries left the country when fighting broke out between rebel and government forces in 2002–3, but many of them have now returned to continue their work. In 2006, due to ongoing violence, over 50,000 people in the country's northwest were at risk of starvation but this was averted due to assistance from the United Nations.[citation needed] On 8 January 2008, the UN Secretary-General Ban Ki-Moon declared that the Central African Republic was eligible to receive assistance from the Peacebuilding Fund. Three priority areas were identified: first, the reform of the security sector; second, the promotion of good governance and the rule of law; and third, the revitalization of communities affected by conflicts. On 12 June 2008, the Central African Republic requested assistance from the UN Peacebuilding Commission, which was set up in 2005 to help countries emerging from conflict avoid devolving back into war or chaos. What is today the Central African Republic has been inhabited for millennia; however, the country's current borders were established by France, which ruled the country as a colony starting in the late 19th century. After gaining independence from France in 1960, the Central African Republic was ruled by a series of autocratic leaders; by the 1990s, calls for democracy led to the first multi-party democratic elections in 1993. Ange-Félix Patassé became president, but was later removed by General François Bozizé in the 2003 coup. The Central African Republic Bush War began in 2004 and, despite a peace treaty in 2007 and another in 2011, fighting broke out between various factions in December 2012, leading to ethnic and religious cleansing of the Muslim minority and massive population displacement in 2013 and 2014. The Central African Republic (CAR; Sango: Ködörösêse tî Bêafrîka; French: République centrafricaine  pronounced: [ʁepyblik sɑ̃tʁafʁikɛn], or Centrafrique [sɑ̃tʀafʁik]) is a landlocked country in Central Africa. It is bordered by Chad to the north, Sudan to the northeast, South Sudan to the east, the Democratic Republic of the Congo and the Republic of the Congo to the south and Cameroon to the west. The CAR covers a land area of about 620,000 square kilometres (240,000 sq mi) and had an estimated population of around 4.7 million as of 2014[update]. Approximately 10,000 years ago, desertification forced hunter-gatherer societies south into the Sahel regions of northern Central Africa, where some groups settled and began farming as part of the Neolithic Revolution. Initial farming of white yam progressed into millet and sorghum, and before 3000 BC the domestication of African oil palm improved the groups' nutrition and allowed for expansion of the local populations. Bananas arrived in the region and added an important source of carbohydrates to the diet; they were also used in the production of alcoholic beverages.[when?] This Agricultural Revolution, combined with a ""Fish-stew Revolution"", in which fishing began to take place, and the use of boats, allowed for the transportation of goods. Products were often moved in ceramic pots, which are the first known examples of artistic expression from the region's inhabitants. The Syrte Agreement in February and the Birao Peace Agreement in April 2007 called for a cessation of hostilities, the billeting of FDPC fighters and their integration with FACA, the liberation of political prisoners, integration of FDPC into government, an amnesty for the UFDR, its recognition as a political party, and the integration of its fighters into the national army. Several groups continued to fight but other groups signed on to the agreement, or similar agreements with the government (e.g. UFR on 15 December 2008). The only major group not to sign an agreement at the time was the CPJP, which continued its activities and signed a peace agreement with the government on 25 August 2012. By 1990, inspired by the fall of the Berlin Wall, a pro-democracy movement arose. Pressure from the United States, France, and from a group of locally represented countries and agencies called GIBAFOR (France, the USA, Germany, Japan, the EU, the World Bank, and the UN) finally led Kolingba to agree, in principle, to hold free elections in October 1992 with help from the UN Office of Electoral Affairs. After using the excuse of alleged irregularities to suspend the results of the elections as a pretext for holding on to power, President Kolingba came under intense pressure from GIBAFOR to establish a ""Conseil National Politique Provisoire de la République"" (Provisional National Political Council, CNPPR) and to set up a ""Mixed Electoral Commission"", which included representatives from all political parties.[citation needed] In the Ubangi-Shari Territorial Assembly election in 1957, MESAN captured 347,000 out of the total 356,000 votes, and won every legislative seat, which led to Boganda being elected president of the Grand Council of French Equatorial Africa and vice-president of the Ubangi-Shari Government Council. Within a year, he declared the establishment of the Central African Republic and served as the country's first prime minister. MESAN continued to exist, but its role was limited. After Boganda's death in a plane crash on 29 March 1959, his cousin, David Dacko, took control of MESAN and became the country's first president after the CAR had formally received independence from France. Dacko threw out his political rivals, including former Prime Minister and Mouvement d'évolution démocratique de l'Afrique centrale (MEDAC), leader Abel Goumba, whom he forced into exile in France. With all opposition parties suppressed by November 1962, Dacko declared MESAN as the official party of the state. When a second round of elections were finally held in 1993, again with the help of the international community coordinated by GIBAFOR, Ange-Félix Patassé won in the second round of voting with 53% of the vote while Goumba won 45.6%. Patassé's party, the Mouvement pour la Libération du Peuple Centrafricain (MLPC) or Movement for the Liberation of the Central African People, gained a simple but not an absolute majority of seats in parliament, which meant Patassé's party required coalition partners.[citation needed] Presently, the Central African Republic has active television services, radio stations, internet service providers, and mobile phone carriers; Socatel is the leading provider for both internet and mobile phone access throughout the country. The primary governmental regulating bodies of telecommunications are the Ministère des Postes and Télécommunications et des Nouvelles Technologies. In addition, the Central African Republic receives international support on telecommunication related operations from ITU Telecommunication Development Sector (ITU-D) within the International Telecommunication Union to improve infrastructure. The 2009 Human Rights Report by the United States Department of State noted that human rights in CAR were poor and expressed concerns over numerous government abuses. The U.S. State Department alleged that major human rights abuses such as extrajudicial executions by security forces, torture, beatings and rape of suspects and prisoners occurred with impunity. It also alleged harsh and life-threatening conditions in prisons and detention centers, arbitrary arrest, prolonged pretrial detention and denial of a fair trial, restrictions on freedom of movement, official corruption, and restrictions on workers' rights. In 2004 the Central African Republic Bush War began as forces opposed to Bozizé took up arms against his government. In May 2005 Bozizé won a presidential election that excluded Patassé and in 2006 fighting continued between the government and the rebels. In November 2006, Bozizé's government requested French military support to help them repel rebels who had taken control of towns in the country's northern regions. Though the initially public details of the agreement pertained to logistics and intelligence, the French assistance eventually included strikes by Mirage jets against rebel positions. In the southwest, the Dzanga-Sangha National Park is located in a rain forest area. The country is noted for its population of forest elephants and western lowland gorillas. In the north, the Manovo-Gounda St Floris National Park is well-populated with wildlife, including leopards, lions, cheetahs and rhinos, and the Bamingui-Bangoran National Park is located in the northeast of CAR. The parks have been seriously affected by the activities of poachers, particularly those from Sudan, over the past two decades.[citation needed]"
Packet_switching,"Internet2 is a not-for-profit United States computer networking consortium led by members from the research and education communities, industry, and government. The Internet2 community, in partnership with Qwest, built the first Internet2 Network, called Abilene, in 1998 and was a prime investor in the National LambdaRail (NLR) project. In 2006, Internet2 announced a partnership with Level 3 Communications to launch a brand new nationwide network, boosting its capacity from 10 Gbit/s to 100 Gbit/s. In October, 2007, Internet2 officially retired Abilene and now refers to its new, higher capacity network as the Internet2 Network. Starting in 1965, Donald Davies at the National Physical Laboratory, UK, independently developed the same message routing methodology as developed by Baran. He called it packet switching, a more accessible name than Baran's, and proposed to build a nationwide network in the UK. He gave a talk on the proposal in 1966, after which a person from the Ministry of Defence (MoD) told him about Baran's work. A member of Davies' team (Roger Scantlebury) met Lawrence Roberts at the 1967 ACM Symposium on Operating System Principles and suggested it for use in the ARPANET. In connectionless mode each packet includes complete addressing information. The packets are routed individually, sometimes resulting in different paths and out-of-order delivery. Each packet is labeled with a destination address, source address, and port numbers. It may also be labeled with the sequence number of the packet. This precludes the need for a dedicated path to help the packet find its way to its destination, but means that much more information is needed in the packet header, which is therefore larger, and this information needs to be looked up in power-hungry content-addressable memory. Each packet is dispatched and may go via different routes; potentially, the system has to do as much work for every packet as the connection-oriented system has to do in connection set-up, but with less information as to the application's requirements. At the destination, the original message/data is reassembled in the correct order, based on the packet sequence number. Thus a virtual connection, also known as a virtual circuit or byte stream is provided to the end-user by a transport layer protocol, although intermediate network nodes only provides a connectionless network layer service. Connection-oriented transmission requires a setup phase in each involved node before any packet is transferred to establish the parameters of communication. The packets include a connection identifier rather than address information and are negotiated between endpoints so that they are delivered in order and with error checking. Address information is only transferred to each node during the connection set-up phase, when the route to the destination is discovered and an entry is added to the switching table in each network node through which the connection passes. The signaling protocols used allow the application to specify its requirements and discover link parameters. Acceptable values for service parameters may be negotiated. Routing a packet requires the node to look up the connection id in a table. The packet header can be small, as it only needs to contain this code and any information, such as length, timestamp, or sequence number, which is different for different packets. ARPANET and SITA HLN became operational in 1969. Before the introduction of X.25 in 1973, about twenty different network technologies had been developed. Two fundamental differences involved the division of functions and tasks between the hosts at the edge of the network and the network core. In the datagram system, the hosts have the responsibility to ensure orderly delivery of packets. The User Datagram Protocol (UDP) is an example of a datagram protocol. In the virtual call system, the network guarantees sequenced delivery of data to the host. This results in a simpler host interface with less functionality than in the datagram model. The X.25 protocol suite uses this network type. Datanet 1 was the public switched data network operated by the Dutch PTT Telecom (now known as KPN). Strictly speaking Datanet 1 only referred to the network and the connected users via leased lines (using the X.121 DNIC 2041), the name also referred to the public PAD service Telepad (using the DNIC 2049). And because the main Videotex service used the network and modified PAD devices as infrastructure the name Datanet 1 was used for these services as well. Although this use of the name was incorrect all these services were managed by the same people within one department of KPN contributed to the confusion. Baran developed the concept of distributed adaptive message block switching during his research at the RAND Corporation for the US Air Force into survivable communications networks, first presented to the Air Force in the summer of 1961 as briefing B-265, later published as RAND report P-2626 in 1962, and finally in report RM 3420 in 1964. Report P-2626 described a general architecture for a large-scale, distributed, survivable communications network. The work focuses on three key ideas: use of a decentralized network with multiple paths between any two points, dividing user messages into message blocks, later called packets, and delivery of these messages by store and forward switching. In 1965, at the instigation of Warner Sinback, a data network based on this voice-phone network was designed to connect GE's four computer sales and service centers (Schenectady, Phoenix, Chicago, and Phoenix) to facilitate a computer time-sharing service, apparently the world's first commercial online service. (In addition to selling GE computers, the centers were computer service bureaus, offering batch processing services. They lost money from the beginning, and Sinback, a high-level marketing manager, was given the job of turning the business around. He decided that a time-sharing system, based on Kemney's work at Dartmouth—which used a computer on loan from GE—could be profitable. Warner was right.) The National Science Foundation Network (NSFNET) was a program of coordinated, evolving projects sponsored by the National Science Foundation (NSF) beginning in 1985 to promote advanced research and education networking in the United States. NSFNET was also the name given to several nationwide backbone networks operating at speeds of 56 kbit/s, 1.5 Mbit/s (T1), and 45 Mbit/s (T3) that were constructed to support NSF's networking initiatives from 1985-1995. Initially created to link researchers to the nation's NSF-funded supercomputing centers, through further public funding and private industry partnerships it developed into a major part of the Internet backbone. The Very high-speed Backbone Network Service (vBNS) came on line in April 1995 as part of a National Science Foundation (NSF) sponsored project to provide high-speed interconnection between NSF-sponsored supercomputing centers and select access points in the United States. The network was engineered and operated by MCI Telecommunications under a cooperative agreement with the NSF. By 1998, the vBNS had grown to connect more than 100 universities and research and engineering institutions via 12 national points of presence with DS-3 (45 Mbit/s), OC-3c (155 Mbit/s), and OC-12c (622 Mbit/s) links on an all OC-12c backbone, a substantial engineering feat for that time. The vBNS installed one of the first ever production OC-48c (2.5 Gbit/s) IP links in February 1999 and went on to upgrade the entire backbone to OC-48c. Starting in the late 1950s, American computer scientist Paul Baran developed the concept Distributed Adaptive Message Block Switching with the goal to provide a fault-tolerant, efficient routing method for telecommunication messages as part of a research program at the RAND Corporation, funded by the US Department of Defense. This concept contrasted and contradicted the theretofore established principles of pre-allocation of network bandwidth, largely fortified by the development of telecommunications in the Bell System. The new concept found little resonance among network implementers until the independent work of Donald Davies at the National Physical Laboratory (United Kingdom) (NPL) in the late 1960s. Davies is credited with coining the modern name packet switching and inspiring numerous packet switching networks in Europe in the decade following, including the incorporation of the concept in the early ARPANET in the United States. Packet mode communication may be implemented with or without intermediate forwarding nodes (packet switches or routers). Packets are normally forwarded by intermediate network nodes asynchronously using first-in, first-out buffering, but may be forwarded according to some scheduling discipline for fair queuing, traffic shaping, or for differentiated or guaranteed quality of service, such as weighted fair queuing or leaky bucket. In case of a shared physical medium (such as radio or 10BASE5), the packets may be delivered according to a multiple access scheme. The CYCLADES packet switching network was a French research network designed and directed by Louis Pouzin. First demonstrated in 1973, it was developed to explore alternatives to the early ARPANET design and to support network research generally. It was the first network to make the hosts responsible for reliable delivery of data, rather than the network itself, using unreliable datagrams and associated end-to-end protocol mechanisms. Concepts of this network influenced later ARPANET architecture. AUSTPAC was an Australian public X.25 network operated by Telstra. Started by Telecom Australia in the early 1980s, AUSTPAC was Australia's first public packet-switched data network, supporting applications such as on-line betting, financial applications — the Australian Tax Office made use of AUSTPAC — and remote terminal access to academic institutions, who maintained their connections to AUSTPAC up until the mid-late 1990s in some cases. Access can be via a dial-up terminal to a PAD, or, by linking a permanent X.25 node to the network.[citation needed] DECnet is a suite of network protocols created by Digital Equipment Corporation, originally released in 1975 in order to connect two PDP-11 minicomputers. It evolved into one of the first peer-to-peer network architectures, thus transforming DEC into a networking powerhouse in the 1980s. Initially built with three layers, it later (1982) evolved into a seven-layer OSI-compliant networking protocol. The DECnet protocols were designed entirely by Digital Equipment Corporation. However, DECnet Phase II (and later) were open standards with published specifications, and several implementations were developed outside DEC, including one for Linux. Tymnet was an international data communications network headquartered in San Jose, CA that utilized virtual call packet switched technology and used X.25, SNA/SDLC, BSC and ASCII interfaces to connect host computers (servers)at thousands of large companies, educational institutions, and government agencies. Users typically connected via dial-up connections or dedicated async connections. The business consisted of a large public network that supported dial-up users and a private network business that allowed government agencies and large companies (mostly banks and airlines) to build their own dedicated networks. The private networks were often connected via gateways to the public network to reach locations not on the private network. Tymnet was also connected to dozens of other public networks in the U.S. and internationally via X.25/X.75 gateways. (Interesting note: Tymnet was not named after Mr. Tyme. Another employee suggested the name.)   AppleTalk was a proprietary suite of networking protocols developed by Apple Inc. in 1985 for Apple Macintosh computers. It was the primary protocol used by Apple devices through the 1980s and 90s. AppleTalk included features that allowed local area networks to be established ad hoc without the requirement for a centralized router or server. The AppleTalk system automatically assigned addresses, updated the distributed namespace, and configured any required inter-network routing. It was a plug-n-play system. Telenet was the first FCC-licensed public data network in the United States. It was founded by former ARPA IPTO director Larry Roberts as a means of making ARPANET technology public. He had tried to interest AT&T in buying the technology, but the monopoly's reaction was that this was incompatible with their future. Bolt, Beranack and Newman (BBN) provided the financing. It initially used ARPANET technology but changed the host interface to X.25 and the terminal interface to X.29. Telenet designed these protocols and helped standardize them in the CCITT. Telenet was incorporated in 1973 and started operations in 1975. It went public in 1979 and was then sold to GTE. There were two kinds of X.25 networks. Some such as DATAPAC and TRANSPAC were initially implemented with an X.25 external interface. Some older networks such as TELENET and TYMNET were modified to provide a X.25 host interface in addition to older host connection schemes. DATAPAC was developed by Bell Northern Research which was a joint venture of Bell Canada (a common carrier) and Northern Telecom (a telecommunications equipment supplier). Northern Telecom sold several DATAPAC clones to foreign PTTs including the Deutsche Bundespost. X.75 and X.121 allowed the interconnection of national X.25 networks. A user or host could call a host on a foreign network by including the DNIC of the remote network as part of the destination address.[citation needed] Both X.25 and Frame Relay provide connection-oriented operations. But X.25 does it at the network layer of the OSI Model. Frame Relay does it at level two, the data link layer. Another major difference between X.25 and Frame Relay is that X.25 requires a handshake between the communicating parties before any user packets are transmitted. Frame Relay does not define any such handshakes. X.25 does not define any operations inside the packet network. It only operates at the user-network-interface (UNI). Thus, the network provider is free to use any procedure it wishes inside the network. X.25 does specify some limited re-transmission procedures at the UNI, and its link layer protocol (LAPB) provides conventional HDLC-type link management procedures. Frame Relay is a modified version of ISDN's layer two protocol, LAPD and LAPB. As such, its integrity operations pertain only between nodes on a link, not end-to-end. Any retransmissions must be carried out by higher layer protocols. The X.25 UNI protocol is part of the X.25 protocol suite, which consists of the lower three layers of the OSI Model. It was widely used at the UNI for packet switching networks during the 1980s and early 1990s, to provide a standardized interface into and out of packet networks. Some implementations used X.25 within the network as well, but its connection-oriented features made this setup cumbersome and inefficient. Frame relay operates principally at layer two of the OSI Model. However, its address field (the Data Link Connection ID, or DLCI) can be used at the OSI network layer, with a minimum set of procedures. Thus, it rids itself of many X.25 layer 3 encumbrances, but still has the DLCI as an ID beyond a node-to-node layer two link protocol. The simplicity of Frame Relay makes it faster and more efficient than X.25. Because Frame relay is a data link layer protocol, like X.25 it does not define internal network routing operations. For X.25 its packet IDs---the virtual circuit and virtual channel numbers have to be correlated to network addresses. The same is true for Frame Relays DLCI. How this is done is up to the network provider. Frame Relay, by virtue of having no network layer procedures is connection-oriented at layer two, by using the HDLC/LAPD/LAPB Set Asynchronous Balanced Mode (SABM). X.25 connections are typically established for each communication session, but it does have a feature allowing a limited amount of traffic to be passed across the UNI without the connection-oriented handshake. For a while, Frame Relay was used to interconnect LANs across wide area networks. However, X.25 and well as Frame Relay have been supplanted by the Internet Protocol (IP) at the network layer, and the Asynchronous Transfer Mode (ATM) and or versions of Multi-Protocol Label Switching (MPLS) at layer two. A typical configuration is to run IP over ATM or a version of MPLS. <Uyless Black, X.25 and Related Protocols, IEEE Computer Society, 1991> <Uyless Black, Frame Relay Networks, McGraw-Hill, 1998> <Uyless Black, MPLS and Label Switching Networks, Prentice Hall, 2001> < Uyless Black, ATM, Volume I, Prentice Hall, 1995> Merit Network, Inc., an independent non-profit 501(c)(3) corporation governed by Michigan's public universities, was formed in 1966 as the Michigan Educational Research Information Triad to explore computer networking between three of Michigan's public universities as a means to help the state's educational and economic development. With initial support from the State of Michigan and the National Science Foundation (NSF), the packet-switched network was first demonstrated in December 1971 when an interactive host to host connection was made between the IBM mainframe computer systems at the University of Michigan in Ann Arbor and Wayne State University in Detroit. In October 1972 connections to the CDC mainframe at Michigan State University in East Lansing completed the triad. Over the next several years in addition to host to host interactive connections the network was enhanced to support terminal to host connections, host to host batch connections (remote job submission, remote printing, batch file transfer), interactive file transfer, gateways to the Tymnet and Telenet public data networks, X.25 host attachments, gateways to X.25 data networks, Ethernet attached hosts, and eventually TCP/IP and additional public universities in Michigan join the network. All of this set the stage for Merit's role in the NSFNET project starting in the mid-1980s. Packet switching contrasts with another principal networking paradigm, circuit switching, a method which pre-allocates dedicated network bandwidth specifically for each communication session, each having a constant bit rate and latency between nodes. In cases of billable services, such as cellular communication services, circuit switching is characterized by a fee per unit of connection time, even when no data is transferred, while packet switching may be characterized by a fee per unit of information transmitted, such as characters, packets, or messages. The Computer Science Network (CSNET) was a computer network funded by the U.S. National Science Foundation (NSF) that began operation in 1981. Its purpose was to extend networking benefits, for computer science departments at academic and research institutions that could not be directly connected to ARPANET, due to funding or authorization limitations. It played a significant role in spreading awareness of, and access to, national networking and was a major milestone on the path to development of the global Internet."
Hanover,"The Schnellweg (en: expressway) system, a number of Bundesstraße roads, forms a structure loosely resembling a large ring road together with A2 and A7. The roads are B 3, B 6 and B 65, called Westschnellweg (B6 on the northern part, B3 on the southern part), Messeschnellweg (B3, becomes A37 near Burgdorf, crosses A2, becomes B3 again, changes to B6 at Seelhorster Kreuz, then passes the Hanover fairground as B6 and becomes A37 again before merging into A7) and Südschnellweg (starts out as B65, becomes B3/B6/B65 upon crossing Westschnellweg, then becomes B65 again at Seelhorster Kreuz). After Napoleon imposed the Convention of Artlenburg (Convention of the Elbe) on July 5, 1803, about 30,000 French soldiers occupied Hanover. The Convention also required disbanding the army of Hanover. However, George III did not recognize the Convention of the Elbe. This resulted in a great number of soldiers from Hanover eventually emigrating to Great Britain, where the King's German Legion was formed. It was the only German army to fight against France throughout the entire Napoleonic wars. The Legion later played an important role in the Battle of Waterloo in 1815. The Congress of Vienna in 1815 elevated the electorate to the Kingdom of Hanover. The capital town Hanover expanded to the western bank of the Leine and since then has grown considerably. Another point of interest is the Old Town. In the centre are the large Marktkirche (Church St. Georgii et Jacobi, preaching venue of the bishop of the Lutheran Landeskirche Hannovers) and the Old Town Hall. Nearby are the Leibniz House, the Nolte House, and the Beguine Tower. A very nice quarter of the Old Town is the Kreuz-Church-Quarter around the Kreuz Church with many nice little lanes. Nearby is the old royal sports hall, now called the Ballhof theatre. On the edge of the Old Town are the Market Hall, the Leine Palace, and the ruin of the Aegidien Church which is now a monument to the victims of war and violence. Through the Marstall Gate you arrive at the bank of the river Leine, where the world-famous Nanas of Niki de Saint-Phalle are located. They are part of the Mile of Sculptures, which starts from Trammplatz, leads along the river bank, crosses Königsworther Square, and ends at the entrance of the Georgengarten. Near the Old Town is the district of Calenberger Neustadt where the Catholic Basilica Minor of St. Clemens, the Reformed Church and the Lutheran Neustädter Hof- und Stadtkirche St. Johannis are located. Hanover's leading cabaret-stage is the GOP Variety theatre which is located in the Georgs Palace. Some other famous cabaret-stages are the Variety Marlene, the Uhu-Theatre. the theatre Die Hinterbühne, the Rampenlich Variety and the revue-stage TAK. The most important Cabaret-Event is the Kleines Fest im Großen Garten (Little Festival in the Great Garden) which is the most successful Cabaret Festival in Germany. It features artists from around the world. Some other important events are the Calenberger Cabaret Weeks, the Hanover Cabaret Festival and the Wintervariety. The Hanover Zoo is one of the most spectacular and best zoos in Europe. The zoo received the Park Scout Award for the fourth year running in 2009/10, placing it among the best zoos in Germany. The zoo consists of several theme areas: Sambesi, Meyers Farm, Gorilla-Mountain, Jungle-Palace, and Mullewapp. Some smaller areas are Australia, the wooded area for wolves, and the so-called swimming area with many seabirds. There is also a tropical house, a jungle house, and a show arena. The new Canadian-themed area, Yukon Bay, opened in 2010. In 2010 the Hanover Zoo had over 1.6 million visitors. Hannover 96 (nickname Die Roten or 'The Reds') is the top local football team that plays in the Bundesliga top division. Home games are played at the HDI-Arena, which hosted matches in the 1974 and 2006 World Cups and the Euro 1988. Their reserve team Hannover 96 II plays in the fourth league. Their home games were played in the traditional Eilenriedestadium till they moved to the HDI Arena due to DFL directives. Arminia Hannover is another very traditional soccer team in Hanover that has played in the first league for years and plays now in the Niedersachsen-West Liga (Lower Saxony League West). Home matches are played in the Rudolf-Kalweit-Stadium. Around 40 theatres are located in Hanover. The Opera House, the Schauspielhaus (Play House), the Ballhofeins, the Ballhofzwei and the Cumberlandsche Galerie belong to the Lower Saxony State Theatre. The Theater am Aegi is Hanover's big theatre for musicals, shows and guest performances. The Neues Theater (New Theatre) is the Boulevard Theatre of Hanover. The Theater für Niedersachsen is another big theatre in Hanover, which also has an own Musical-Company. Some of the most important Musical-Productions are the rock musicals of the German rock musician Heinz Rudolph Kunze, which take place at the Garden-Theatre in the Great Garden. Some other popular sights are the Waterloo Column, the Laves House, the Wangenheim Palace, the Lower Saxony State Archives, the Hanover Playhouse, the Kröpcke Clock, the Anzeiger Tower Block, the Administration Building of the NORD/LB, the Cupola Hall of the Congress Centre, the Lower Saxony Stock, the Ministry of Finance, the Garten Church, the Luther Church, the Gehry Tower (designed by the American architect Frank O. Gehry), the specially designed Bus Stops, the Opera House, the Central Station, the Maschsee lake and the city forest Eilenriede, which is one of the largest of its kind in Europe. With around 40 parks, forests and gardens, a couple of lakes, two rivers and one canal, Hanover offers a large variety of leisure activities. A cabinet of coins is the Münzkabinett der TUI-AG. The Polizeigeschichtliche Sammlung Niedersachsen is the largest police museum in Germany. Textiles from all over the world can be visited in the Museum for textile art. The EXPOseeum is the museum of the world-exhibition ""EXPO 2000 Hannover"". Carpets and objects from the orient can be visited in the Oriental Carpet Museum. The Blind Man Museum is a rarity in Germany, another one is only in Berlin. The Museum of veterinary medicine is unique in Germany. The Museum for Energy History describes the 150 years old history of the application of energy. The Home Museum Ahlem shows the history of the district of Ahlem. The Mahn- und Gedenkstätte Ahlem describes the history of the Jewish people in Hanover and the Stiftung Ahlers Pro Arte / Kestner Pro Arte shows modern art. Modern art is also the main topic of the Kunsthalle Faust, the Nord/LB Art Gellery and of the Foro Artistico / Eisfabrik. After 1937 the Lord Mayor and the state commissioners of Hanover were members of the NSDAP (Nazi party). A large Jewish population then existed in Hanover. In October 1938, 484 Hanoverian Jews of Polish origin were expelled to Poland, including the Grynszpan family. However, Poland refused to accept them, leaving them stranded at the border with thousands of other Polish-Jewish deportees, fed only intermittently by the Polish Red Cross and Jewish welfare organisations. The Gryszpan's son Herschel Grynszpan was in Paris at the time. When he learned of what was happening, he drove to the German embassy in Paris and shot the German diplomat Eduard Ernst vom Rath, who died shortly afterwards. ""Hanover"" is the traditional English spelling. The German spelling (with a double n) is becoming more popular in English; recent editions of encyclopedias prefer the German spelling, and the local government uses the German spelling on English websites. The English pronunciation /ˈhænəvər/, with stress on the first syllable and a reduced second syllable, is applied to both the German and English spellings, which is different from German pronunciation [haˈnoːfɐ], with stress on the second syllable and a long second vowel. The traditional English spelling is still used in historical contexts, especially when referring to the British House of Hanover. In 1837, the personal union of the United Kingdom and Hanover ended because William IV's heir in the United Kingdom was female (Queen Victoria). Hanover could be inherited only by male heirs. Thus, Hanover passed to William IV's brother, Ernest Augustus, and remained a kingdom until 1866, when it was annexed by Prussia during the Austro-Prussian war. Despite being expected to defeat Prussia at the Battle of Langensalza, Prussia employed Moltke the Elder's Kesselschlacht order of battle to instead destroy the Hanoverian army. The city of Hanover became the capital of the Prussian Province of Hanover. After the annexation, the people of Hanover generally opposed the Prussian government. The Berggarten is an important European botanical garden.[citation needed] Some points of interest are the Tropical House, the Cactus House, the Canary House and the Orchid House, which hosts one of the world's biggest collection of orchids, and free-flying birds and butterflies. Near the entrance to the Berggarten is the historic Library Pavillon. The Mausoleum of the Guelphs is also located in the Berggarten. Like the Great Garden, the Berggarten also consists of several parts, for example the Paradies and the Prairie Garden. There is also the Sea Life Centre Hanover, which is the first tropical aquarium in Germany.[citation needed] In September 1941, through the ""Action Lauterbacher"" plan, a ghettoisation of the remaining Hanoverian Jewish families began. Even before the Wannsee Conference, on 15 December 1941, the first Jews from Hanover were deported to Riga. A total of 2,400 people were deported, and very few survived. During the war seven concentration camps were constructed in Hanover, in which many Jews were confined. Of the approximately 4,800 Jews who had lived in Hannover in 1938, fewer than 100 were still in the city when troops of the United States Army arrived on 10 April 1945 to occupy Hanover at the end of the war.[citation needed] Today, a memorial at the Opera Square is a reminder of the persecution of the Jews in Hanover. After the war a large group of Orthodox Jewish survivors of the nearby Bergen-Belsen concentration camp settled in Hanover. But Hanover is not only one of the most important Exhibition Cities in the world, it is also one of the German capitals for marksmen. The Schützenfest Hannover is the largest Marksmen's Fun Fair in the world and takes place once a year (late June to early July) (2014 - July 4th to the 13th). It consists of more than 260 rides and inns, five large beer tents and a big entertainment programme. The highlight of this fun fair is the 12 kilometres (7 mi) long Parade of the Marksmen with more than 12.000 participants from all over the world, among them around 5.000 marksmen, 128 bands and more than 70 wagons, carriages and big festival vehicles. It is the longest procession in Europe. Around 2 million people visit this fun fair every year. The landmark of this Fun Fair is the biggest transportable Ferris Wheel in the world (60 m or 197 ft high). The origins of this fun fair is located in the year 1529. In 1636 George, Duke of Brunswick-Lüneburg, ruler of the Brunswick-Lüneburg principality of Calenberg, moved his residence to Hanover. The Dukes of Brunswick-Lüneburg were elevated by the Holy Roman Emperor to the rank of Prince-Elector in 1692, and this elevation was confirmed by the Imperial Diet in 1708. Thus the principality was upgraded to the Electorate of Brunswick-Lüneburg, colloquially known as the Electorate of Hanover after Calenberg's capital (see also: House of Hanover). Its electors would later become monarchs of Great Britain (and from 1801, of the United Kingdom of Great Britain and Ireland). The first of these was George I Louis, who acceded to the British throne in 1714. The last British monarch who ruled in Hanover was William IV. Semi-Salic law, which required succession by the male line if possible, forbade the accession of Queen Victoria in Hanover. As a male-line descendant of George I, Queen Victoria was herself a member of the House of Hanover. Her descendants, however, bore her husband's titular name of Saxe-Coburg-Gotha. Three kings of Great Britain, or the United Kingdom, were concurrently also Electoral Princes of Hanover. With a population of 518,000, Hanover is a major centre of Northern Germany and the country's thirteenth largest city. Hanover also hosts annual commercial trade fairs such as the Hanover Fair and the CeBIT. Every year Hanover hosts the Schützenfest Hannover, the world's largest marksmen's festival, and the Oktoberfest Hannover, the second largest Oktoberfest in the world (beside Oktoberfest of Blumenau). In 2000, Hanover hosted the world fair Expo 2000. The Hanover fairground, due to numerous extensions, especially for the Expo 2000, is the largest in the world. Hanover is of national importance because of its universities and medical school, its international airport and its large zoo. The city is also a major crossing point of railway lines and highways (Autobahnen), connecting European main lines in both the east-west (Berlin–Ruhr area) and north-south (Hamburg–Munich, etc.) directions. As an important railroad and road junction and production center, Hanover was a major target for strategic bombing during World War II, including the Oil Campaign. Targets included the AFA (Stöcken), the Deurag-Nerag refinery (Misburg), the Continental plants (Vahrenwald and Limmer), the United light metal works (VLW) in Ricklingen and Laatzen (today Hanover fairground), the Hanover/Limmer rubber reclamation plant, the Hanomag factory (Linden) and the tank factory M.N.H. Maschinenfabrik Niedersachsen (Badenstedt). Forced labourers were sometimes used from the Hannover-Misburg subcamp of the Neuengamme concentration camp. Residential areas were also targeted, and more than 6,000 civilians were killed by the Allied bombing raids. More than 90% of the city center was destroyed in a total of 88 bombing raids. After the war, the Aegidienkirche was not rebuilt and its ruins were left as a war memorial. The Great Garden is an important European baroque garden. The palace itself, however, was largely destroyed by Allied bombing but is currently under reconstruction.[citation needed] Some points of interest are the Grotto (the interior was designed by the French artist Niki de Saint-Phalle), the Gallery Building, the Orangerie and the two pavilions by Remy de la Fosse. The Great Garden consists of several parts. The most popular ones are the Great Ground and the Nouveau Jardin. At the centre of the Nouveau Jardin is Europe's highest garden fountain. The historic Garden Theatre inter alia hosted the musicals of the German rock musician Heinz Rudolf Kunze.[citation needed] Various industrial businesses are located in Hannover. The Volkswagen Commercial Vehicles Transporter (VWN) factory at Hannover-Stöcken is the biggest employer in the region and operates a huge plant at the northern edge of town adjoining the Mittellandkanal and Motorway A2. Jointly with a factory of German tire and automobile parts manufacturer Continental AG, they have a coal-burning power plant. Continental AG, founded in Hanover in 1871, is one of the city's major companies, as is Sennheiser. Since 2008 a take-over is in progress: the Schaeffler Group from Herzogenaurach (Bavaria) holds the majority of the stock but were required due to the financial crisis to deposit the options as securities at banks. TUI AG has its HQ in Hanover. Hanover is home to many insurance companies, many of which operate only in Germany. One major global reinsurance company is Hannover Re, whose headquarters are east of the city centre. Hanover was founded in medieval times on the east bank of the River Leine. Its original name Honovere may mean ""high (river)bank"", though this is debated (cf. das Hohe Ufer). Hanover was a small village of ferrymen and fishermen that became a comparatively large town in the 13th century due to its position at a natural crossroads. As overland travel was relatively difficult, its position on the upper navigable reaches of the river helped it to grow by increasing trade. It was connected to the Hanseatic League city of Bremen by the Leine, and was situated near the southern edge of the wide North German Plain and north-west of the Harz mountains, so that east-west traffic such as mule trains passed through it. Hanover was thus a gateway to the Rhine, Ruhr and Saar river valleys, their industrial areas which grew up to the southwest and the plains regions to the east and north, for overland traffic skirting the Harz between the Low Countries and Saxony or Thuringia."
History_of_science,"Perhaps the most prominent, controversial and far-reaching theory in all of science has been the theory of evolution by natural selection put forward by the British naturalist Charles Darwin in his book On the Origin of Species in 1859. Darwin proposed that the features of all living things, including humans, were shaped by natural processes over long periods of time. The theory of evolution in its current form affects almost all areas of biology. Implications of evolution on fields outside of pure science have led to both opposition and support from different parts of society, and profoundly influenced the popular understanding of ""man's place in the universe"". In the early 20th century, the study of heredity became a major investigation after the rediscovery in 1900 of the laws of inheritance developed by the Moravian monk Gregor Mendel in 1866. Mendel's laws provided the beginnings of the study of genetics, which became a major field of research for both scientific and industrial research. By 1953, James D. Watson, Francis Crick and Maurice Wilkins clarified the basic structure of DNA, the genetic material for expressing life in all its forms. In the late 20th century, the possibilities of genetic engineering became practical for the first time, and a massive international effort began in 1990 to map out an entire human genome (the Human Genome Project). The earliest Greek philosophers, known as the pre-Socratics, provided competing answers to the question found in the myths of their neighbors: ""How did the ordered cosmos in which we live come to be?"" The pre-Socratic philosopher Thales (640-546 BC), dubbed the ""father of science"", was the first to postulate non-supernatural explanations for natural phenomena, for example, that land floats on water and that earthquakes are caused by the agitation of the water upon which the land floats, rather than the god Poseidon. Thales' student Pythagoras of Samos founded the Pythagorean school, which investigated mathematics for its own sake, and was the first to postulate that the Earth is spherical in shape. Leucippus (5th century BC) introduced atomism, the theory that all matter is made of indivisible, imperishable units called atoms. This was greatly expanded by his pupil Democritus. The renewal of learning in Europe, that began with 12th century Scholasticism, came to an end about the time of the Black Death, and the initial period of the subsequent Italian Renaissance is sometimes seen as a lull in scientific activity. The Northern Renaissance, on the other hand, showed a decisive shift in focus from Aristoteleian natural philosophy to chemistry and the biological sciences (botany, anatomy, and medicine). Thus modern science in Europe was resumed in a period of great upheaval: the Protestant Reformation and Catholic Counter-Reformation; the discovery of the Americas by Christopher Columbus; the Fall of Constantinople; but also the re-discovery of Aristotle during the Scholastic period presaged large social and political changes. Thus, a suitable environment was created in which it became possible to question scientific doctrine, in much the same way that Martin Luther and John Calvin questioned religious doctrine. The works of Ptolemy (astronomy) and Galen (medicine) were found not always to match everyday observations. Work by Vesalius on human cadavers found problems with the Galenic view of anatomy. An intellectual revitalization of Europe started with the birth of medieval universities in the 12th century. The contact with the Islamic world in Spain and Sicily, and during the Reconquista and the Crusades, allowed Europeans access to scientific Greek and Arabic texts, including the works of Aristotle, Ptolemy, Jābir ibn Hayyān, al-Khwarizmi, Alhazen, Avicenna, and Averroes. European scholars had access to the translation programs of Raymond of Toledo, who sponsored the 12th century Toledo School of Translators from Arabic to Latin. Later translators like Michael Scotus would learn Arabic in order to study these texts directly. The European universities aided materially in the translation and propagation of these texts and started a new infrastructure which was needed for scientific communities. In fact, European university put many works about the natural world and the study of nature at the center of its curriculum, with the result that the ""medieval university laid far greater emphasis on science than does its modern counterpart and descendent."" The above ""history of economics"" reflects modern economic textbooks and this means that the last stage of a science is represented as the culmination of its history (Kuhn, 1962). The ""invisible hand"" mentioned in a lost page in the middle of a chapter in the middle of the to ""Wealth of Nations"", 1776, advances as Smith's central message.[clarification needed] It is played down that this ""invisible hand"" acts only ""frequently"" and that it is ""no part of his [the individual's] intentions"" because competition leads to lower prices by imitating ""his"" invention. That this ""invisible hand"" prefers ""the support of domestic to foreign industry"" is cleansed—often without indication that part of the citation is truncated. The opening passage of the ""Wealth"" containing Smith's message is never mentioned as it cannot be integrated into modern theory: ""Wealth"" depends on the division of labour which changes with market volume and on the proportion of productive to Unproductive labor. Ibn Sina (Avicenna) is regarded as the most influential philosopher of Islam. He pioneered the science of experimental medicine and was the first physician to conduct clinical trials. His two most notable works in medicine are the Kitāb al-shifāʾ (""Book of Healing"") and The Canon of Medicine, both of which were used as standard medicinal texts in both the Muslim world and in Europe well into the 17th century. Amongst his many contributions are the discovery of the contagious nature of infectious diseases, and the introduction of clinical pharmacology. Humboldtian science refers to the early 19th century approach of combining scientific field work with the age of Romanticism sensitivity, ethics and aesthetic ideals. It helped to install natural history as a separate field, gave base for ecology and was based on the role model of scientist, naturalist and explorer Alexander von Humboldt. The later 19th century positivism asserted that all authentic knowledge allows verification and that all authentic knowledge assumes that the only valid knowledge is scientific. In Western culture, the study of politics is first found in Ancient Greece. The antecedents of European politics trace their roots back even earlier than Plato and Aristotle, particularly in the works of Homer, Hesiod, Thucydides, Xenophon, and Euripides. Later, Plato analyzed political systems, abstracted their analysis from more literary- and history- oriented studies and applied an approach we would understand as closer to philosophy. Similarly, Aristotle built upon Plato's analysis to include historical empirical evidence in his analysis. Astronomy: Astronomical observations from China constitute the longest continuous sequence from any civilisation and include records of sunspots (112 records from 364 BC), supernovas (1054), lunar and solar eclipses. By the 12th century, they could reasonably accurately make predictions of eclipses, but the knowledge of this was lost during the Ming dynasty, so that the Jesuit Matteo Ricci gained much favour in 1601 by his predictions. By 635 Chinese astronomers had observed that the tails of comets always point away from the sun. The development of writing enabled knowledge to be stored and communicated across generations with much greater fidelity. Combined with the development of agriculture, which allowed for a surplus of food, it became possible for early civilizations to develop, because more time and effort could be devoted to tasks (other than food production) than hunter-gatherers or early subsistence farmers had available. This surplus allowed a community to support individuals who did things other than work towards bare survival. These other tasks included systematic studies of nature, study of written information gathered and recorded by others, and often of adding to that body of information. In 1847, Hungarian physician Ignác Fülöp Semmelweis dramatically reduced the occurrency of puerperal fever by simply requiring physicians to wash their hands before attending to women in childbirth. This discovery predated the germ theory of disease. However, Semmelweis' findings were not appreciated by his contemporaries and came into use only with discoveries by British surgeon Joseph Lister, who in 1865 proved the principles of antisepsis. Lister's work was based on the important findings by French biologist Louis Pasteur. Pasteur was able to link microorganisms with disease, revolutionizing medicine. He also devised one of the most important methods in preventive medicine, when in 1880 he produced a vaccine against rabies. Pasteur invented the process of pasteurization, to help prevent the spread of disease through milk and other foods. Political science is a late arrival in terms of social sciences[citation needed]. However, the discipline has a clear set of antecedents such as moral philosophy, political philosophy, political economy, history, and other fields concerned with normative determinations of what ought to be and with deducing the characteristics and functions of the ideal form of government. The roots of politics are in prehistory. In each historic period and in almost every geographic area, we can find someone studying politics and increasing political understanding. The end of the 19th century marks the start of psychology as a scientific enterprise. The year 1879 is commonly seen as the start of psychology as an independent field of study. In that year Wilhelm Wundt founded the first laboratory dedicated exclusively to psychological research (in Leipzig). Other important early contributors to the field include Hermann Ebbinghaus (a pioneer in memory studies), Ivan Pavlov (who discovered classical conditioning), William James, and Sigmund Freud. Freud's influence has been enormous, though more as cultural icon than a force in scientific psychology. From their beginnings in Sumer (now Iraq) around 3500 BC, the Mesopotamian people began to attempt to record some observations of the world with numerical data. But their observations and measurements were seemingly taken for purposes other than for elucidating scientific laws. A concrete instance of Pythagoras' law was recorded, as early as the 18th century BC: the Mesopotamian cuneiform tablet Plimpton 322 records a number of Pythagorean triplets (3,4,5) (5,12,13). ..., dated 1900 BC, possibly millennia before Pythagoras,  but an abstract formulation of the Pythagorean theorem was not. In Babylonian astronomy, records of the motions of the stars, planets, and the moon are left on thousands of clay tablets created by scribes. Even today, astronomical periods identified by Mesopotamian proto-scientists are still widely used in Western calendars such as the solar year and the lunar month. Using these data they developed arithmetical methods to compute the changing length of daylight in the course of the year and to predict the appearances and disappearances of the Moon and planets and eclipses of the Sun and Moon. Only a few astronomers' names are known, such as that of Kidinnu, a Chaldean astronomer and mathematician. Kiddinu's value for the solar year is in use for today's calendars. Babylonian astronomy was ""the first and highly successful attempt at giving a refined mathematical description of astronomical phenomena."" According to the historian A. Aaboe, ""all subsequent varieties of scientific astronomy, in the Hellenistic world, in India, in Islam, and in the West—if not indeed all subsequent endeavour in the exact sciences—depend upon Babylonian astronomy in decisive and fundamental ways."" Geology did not undergo systematic restructuring during the Scientific Revolution, but individual theorists made important contributions. Robert Hooke, for example, formulated a theory of earthquakes, and Nicholas Steno developed the theory of superposition and argued that fossils were the remains of once-living creatures. Beginning with Thomas Burnet's Sacred Theory of the Earth in 1681, natural philosophers began to explore the idea that the Earth had changed over time. Burnet and his contemporaries interpreted Earth's past in terms of events described in the Bible, but their work laid the intellectual foundations for secular interpretations of Earth history. Indian astronomer and mathematician Aryabhata (476-550), in his Aryabhatiya (499) introduced a number of trigonometric functions (including sine, versine, cosine and inverse sine), trigonometric tables, and techniques and algorithms of algebra. In 628 AD, Brahmagupta suggested that gravity was a force of attraction. He also lucidly explained the use of zero as both a placeholder and a decimal digit, along with the Hindu-Arabic numeral system now used universally throughout the world. Arabic translations of the two astronomers' texts were soon available in the Islamic world, introducing what would become Arabic numerals to the Islamic World by the 9th century. During the 14th–16th centuries, the Kerala school of astronomy and mathematics made significant advances in astronomy and especially mathematics, including fields such as trigonometry and analysis. In particular, Madhava of Sangamagrama is considered the ""founder of mathematical analysis"". In Hellenistic Egypt, the mathematician Euclid laid down the foundations of mathematical rigor and introduced the concepts of definition, axiom, theorem and proof still in use today in his Elements, considered the most influential textbook ever written. Archimedes, considered one of the greatest mathematicians of all time, is credited with using the method of exhaustion to calculate the area under the arc of a parabola with the summation of an infinite series, and gave a remarkably accurate approximation of Pi. He is also known in physics for laying the foundations of hydrostatics, statics, and the explanation of the principle of the lever. Ancient Egypt made significant advances in astronomy, mathematics and medicine. Their development of geometry was a necessary outgrowth of surveying to preserve the layout and ownership of farmland, which was flooded annually by the Nile river. The 3-4-5 right triangle and other rules of thumb were used to build rectilinear structures, and the post and lintel architecture of Egypt. Egypt was also a center of alchemy research for much of the Mediterranean.The Edwin Smith papyrus is one of the first medical documents still extant, and perhaps the earliest document that attempts to describe and analyse the brain: it might be seen as the very beginnings of modern neuroscience. However, while Egyptian medicine had some effective practices, it was not without its ineffective and sometimes harmful practices. Medical historians believe that ancient Egyptian pharmacology, for example, was largely ineffective. Nevertheless, it applies the following components to the treatment of disease: examination, diagnosis, treatment, and prognosis, which display strong parallels to the basic empirical method of science and according to G. E. R. Lloyd played a significant role in the development of this methodology. The Ebers papyrus (c. 1550 BC) also contains evidence of traditional empiricism. Further studies, e.g. Jerome Ravetz 1971 Scientific Knowledge and its Social Problems referred to the role of the scientific community, as a social construct, in accepting or rejecting (objective) scientific knowledge. The Science wars of the 1990 were about the influence of especially French philosophers, which denied the objectivity of science in general or seemed to do so. They described as well differences between the idealized model of a pure science and the actual scientific practice; while scientism, a revival of the positivism approach, saw in precise measurement and rigorous calculation the basis for finally settling enduring metaphysical and moral controversies. However, more recently some of the leading critical theorists have recognized that their postmodern deconstructions have at times been counter-productive, and are providing intellectual ammunition for reactionary interests. Bruno Latour noted that ""dangerous extremists are using the very same argument of social construction to destroy hard-won evidence that could save our lives. Was I wrong to participate in the invention of this field known as science studies? Is it enough to say that we did not really mean what we meant?"" Modern chemistry emerged from the sixteenth through the eighteenth centuries through the material practices and theories promoted by alchemy, medicine, manufacturing and mining. A decisive moment came when 'chymistry' was distinguished from alchemy by Robert Boyle in his work The Sceptical Chymist, in 1661; although the alchemical tradition continued for some time after his work. Other important steps included the gravimetric experimental practices of medical chemists like William Cullen, Joseph Black, Torbern Bergman and Pierre Macquer and through the work of Antoine Lavoisier (Father of Modern Chemistry) on oxygen and the law of conservation of mass, which refuted phlogiston theory. The theory that all matter is made of atoms, which are the smallest constituents of matter that cannot be broken down without losing the basic chemical and physical properties of that matter, was provided by John Dalton in 1803, although the question took a hundred years to settle as proven. Dalton also formulated the law of mass relationships. In 1869, Dmitri Mendeleev composed his periodic table of elements on the basis of Dalton's discoveries. The synthesis of urea by Friedrich Wöhler opened a new research field, organic chemistry, and by the end of the 19th century, scientists were able to synthesize hundreds of organic compounds. The later part of the 19th century saw the exploitation of the Earth's petrochemicals, after the exhaustion of the oil supply from whaling. By the 20th century, systematic production of refined materials provided a ready supply of products which provided not only energy, but also synthetic materials for clothing, medicine, and everyday disposable resources. Application of the techniques of organic chemistry to living organisms resulted in physiological chemistry, the precursor to biochemistry. The 20th century also saw the integration of physics and chemistry, with chemical properties explained as the result of the electronic structure of the atom. Linus Pauling's book on The Nature of the Chemical Bond used the principles of quantum mechanics to deduce bond angles in ever-more complicated molecules. Pauling's work culminated in the physical modelling of DNA, the secret of life (in the words of Francis Crick, 1953). In the same year, the Miller–Urey experiment demonstrated in a simulation of primordial processes, that basic constituents of proteins, simple amino acids, could themselves be built up from simpler molecules. The basis for classical economics forms Adam Smith's An Inquiry into the Nature and Causes of the Wealth of Nations, published in 1776. Smith criticized mercantilism, advocating a system of free trade with division of labour. He postulated an ""invisible hand"" that regulated economic systems made up of actors guided only by self-interest. Karl Marx developed an alternative economic theory, called Marxian economics. Marxian economics is based on the labor theory of value and assumes the value of good to be based on the amount of labor required to produce it. Under this assumption, capitalism was based on employers not paying the full value of workers labor to create profit. The Austrian school responded to Marxian economics by viewing entrepreneurship as driving force of economic development. This replaced the labor theory of value by a system of supply and demand. Much of the study of the history of science has been devoted to answering questions about what science is, how it functions, and whether it exhibits large-scale patterns and trends. The sociology of science in particular has focused on the ways in which scientists work, looking closely at the ways in which they ""produce"" and ""construct"" scientific knowledge. Since the 1960s, a common trend in science studies (the study of the sociology and history of science) has been to emphasize the ""human component"" of scientific knowledge, and to de-emphasize the view that scientific data are self-evident, value-free, and context-free. The field of Science and Technology Studies, an area that overlaps and often informs historical studies of science, focuses on the social context of science in both contemporary and historical periods. At the beginning of the 13th century, there were reasonably accurate Latin translations of the main works of almost all the intellectually crucial ancient authors, allowing a sound transfer of scientific ideas via both the universities and the monasteries. By then, the natural philosophy contained in these texts began to be extended by notable scholastics such as Robert Grosseteste, Roger Bacon, Albertus Magnus and Duns Scotus. Precursors of the modern scientific method, influenced by earlier contributions of the Islamic world, can be seen already in Grosseteste's emphasis on mathematics as a way to understand nature, and in the empirical approach admired by Bacon, particularly in his Opus Majus. Pierre Duhem's provocative thesis of the Catholic Church's Condemnation of 1277 led to the study of medieval science as a serious discipline, ""but no one in the field any longer endorses his view that modern science started in 1277"". However, many scholars agree with Duhem's view that the Middle Ages were a period of important scientific developments. The scientific revolution is a convenient boundary between ancient thought and classical physics. Nicolaus Copernicus revived the heliocentric model of the solar system described by Aristarchus of Samos. This was followed by the first known model of planetary motion given by Johannes Kepler in the early 17th century, which proposed that the planets follow elliptical orbits, with the Sun at one focus of the ellipse. Galileo (""Father of Modern Physics"") also made use of experiments to validate physical theories, a key element of the scientific method. An ancient Indian treatise on statecraft, economic policy and military strategy by Kautilya and Viṣhṇugupta, who are traditionally identified with Chāṇakya (c. 350–-283 BCE). In this treatise, the behaviors and relationships of the people, the King, the State, the Government Superintendents, Courtiers, Enemies, Invaders, and Corporations are analysed and documented. Roger Boesche describes the Arthaśāstra as ""a book of political realism, a book analysing how the political world does work and not very often stating how it ought to work, a book that frequently discloses to a king what calculating and sometimes brutal measures he must carry out to preserve the state and the common good."" American sociology in the 1940s and 1950s was dominated largely by Talcott Parsons, who argued that aspects of society that promoted structural integration were therefore ""functional"". This structural functionalism approach was questioned in the 1960s, when sociologists came to see this approach as merely a justification for inequalities present in the status quo. In reaction, conflict theory was developed, which was based in part on the philosophies of Karl Marx. Conflict theorists saw society as an arena in which different groups compete for control over resources. Symbolic interactionism also came to be regarded as central to sociological thinking. Erving Goffman saw social interactions as a stage performance, with individuals preparing ""backstage"" and attempting to control their audience through impression management. While these theories are currently prominent in sociological thought, other approaches exist, including feminist theory, post-structuralism, rational choice theory, and postmodernism. Astronomy: The first textual mention of astronomical concepts comes from the Vedas, religious literature of India. According to Sarma (2008): ""One finds in the Rigveda intelligent speculations about the genesis of the universe from nonexistence, the configuration of the universe, the spherical self-supporting earth, and the year of 360 days divided into 12 equal parts of 30 days each with a periodical intercalary month."". The first 12 chapters of the Siddhanta Shiromani, written by Bhāskara in the 12th century, cover topics such as: mean longitudes of the planets; true longitudes of the planets; the three problems of diurnal rotation; syzygies; lunar eclipses; solar eclipses; latitudes of the planets; risings and settings; the moon's crescent; conjunctions of the planets with each other; conjunctions of the planets with the fixed stars; and the patas of the sun and moon. The 13 chapters of the second part cover the nature of the sphere, as well as significant astronomical and trigonometric calculations based on it. Modern geology, like modern chemistry, gradually evolved during the 18th and early 19th centuries. Benoît de Maillet and the Comte de Buffon saw the Earth as much older than the 6,000 years envisioned by biblical scholars. Jean-Étienne Guettard and Nicolas Desmarest hiked central France and recorded their observations on some of the first geological maps. Aided by chemical experimentation, naturalists such as Scotland's John Walker, Sweden's Torbern Bergman, and Germany's Abraham Werner created comprehensive classification systems for rocks and minerals—a collective achievement that transformed geology into a cutting edge field by the end of the eighteenth century. These early geologists also proposed a generalized interpretations of Earth history that led James Hutton, Georges Cuvier and Alexandre Brongniart, following in the steps of Steno, to argue that layers of rock could be dated by the fossils they contained: a principle first applied to the geology of the Paris Basin. The use of index fossils became a powerful tool for making geological maps, because it allowed geologists to correlate the rocks in one locality with those of similar age in other, distant localities. Over the first half of the 19th century, geologists such as Charles Lyell, Adam Sedgwick, and Roderick Murchison applied the new technique to rocks throughout Europe and eastern North America, setting the stage for more detailed, government-funded mapping projects in later decades. Medicine: Findings from Neolithic graveyards in what is now Pakistan show evidence of proto-dentistry among an early farming culture. Ayurveda is a system of traditional medicine that originated in ancient India before 2500 BC, and is now practiced as a form of alternative medicine in other parts of the world. Its most famous text is the Suśrutasamhitā of Suśruta, which is notable for describing procedures on various forms of surgery, including rhinoplasty, the repair of torn ear lobes, perineal lithotomy, cataract surgery, and several other excisions and other surgical procedures. The beginning of the 20th century brought the start of a revolution in physics. The long-held theories of Newton were shown not to be correct in all circumstances. Beginning in 1900, Max Planck, Albert Einstein, Niels Bohr and others developed quantum theories to explain various anomalous experimental results, by introducing discrete energy levels. Not only did quantum mechanics show that the laws of motion did not hold on small scales, but even more disturbingly, the theory of general relativity, proposed by Einstein in 1915, showed that the fixed background of spacetime, on which both Newtonian mechanics and special relativity depended, could not exist. In 1925, Werner Heisenberg and Erwin Schrödinger formulated quantum mechanics, which explained the preceding quantum theories. The observation by Edwin Hubble in 1929 that the speed at which galaxies recede positively correlates with their distance, led to the understanding that the universe is expanding, and the formulation of the Big Bang theory by Georges Lemaître. The final decades of the 20th century have seen the rise of a new interdisciplinary approach to studying human psychology, known collectively as cognitive science. Cognitive science again considers the mind as a subject for investigation, using the tools of psychology, linguistics, computer science, philosophy, and neurobiology. New methods of visualizing the activity of the brain, such as PET scans and CAT scans, began to exert their influence as well, leading some researchers to investigate the mind by investigating the brain, rather than cognition. These new forms of investigation assume that a wide understanding of the human mind is possible, and that such an understanding may be applied to other research domains, such as artificial intelligence. Subsequently, Plato and Aristotle produced the first systematic discussions of natural philosophy, which did much to shape later investigations of nature. Their development of deductive reasoning was of particular importance and usefulness to later scientific inquiry. Plato founded the Platonic Academy in 387 BC, whose motto was ""Let none unversed in geometry enter here"", and turned out many notable philosophers. Plato's student Aristotle introduced empiricism and the notion that universal truths can be arrived at via observation and induction, thereby laying the foundations of the scientific method. Aristotle also produced many biological writings that were empirical in nature, focusing on biological causation and the diversity of life. He made countless observations of nature, especially the habits and attributes of plants and animals in the world around him, classified more than 540 animal species, and dissected at least 50. Aristotle's writings profoundly influenced subsequent Islamic and European scholarship, though they were eventually superseded in the Scientific Revolution. In the 1920s, John Maynard Keynes prompted a division between microeconomics and macroeconomics. Under Keynesian economics macroeconomic trends can overwhelm economic choices made by individuals. Governments should promote aggregate demand for goods as a means to encourage economic expansion. Following World War II, Milton Friedman created the concept of monetarism. Monetarism focuses on using the supply and demand of money as a method for controlling economic activity. In the 1970s, monetarism has adapted into supply-side economics which advocates reducing taxes as a means to increase the amount of money available for economic expansion. The discipline of ecology typically traces its origin to the synthesis of Darwinian evolution and Humboldtian biogeography, in the late 19th and early 20th centuries. Equally important in the rise of ecology, however, were microbiology and soil science—particularly the cycle of life concept, prominent in the work Louis Pasteur and Ferdinand Cohn. The word ecology was coined by Ernst Haeckel, whose particularly holistic view of nature in general (and Darwin's theory in particular) was important in the spread of ecological thinking. In the 1930s, Arthur Tansley and others began developing the field of ecosystem ecology, which combined experimental soil science with physiological concepts of energy and the techniques of field biology. The history of ecology in the 20th century is closely tied to that of environmentalism; the Gaia hypothesis, first formulated in the 1960s, and spreading in the 1970s, and more recently the scientific-religious movement of Deep Ecology have brought the two closer together. The astronomer Aristarchus of Samos was the first known person to propose a heliocentric model of the solar system, while the geographer Eratosthenes accurately calculated the circumference of the Earth. Hipparchus (c. 190 – c. 120 BC) produced the first systematic star catalog. The level of achievement in Hellenistic astronomy and engineering is impressively shown by the Antikythera mechanism (150-100 BC), an analog computer for calculating the position of planets. Technological artifacts of similar complexity did not reappear until the 14th century, when mechanical astronomical clocks appeared in Europe. The Jesuit China missions of the 16th and 17th centuries ""learned to appreciate the scientific achievements of this ancient culture and made them known in Europe. Through their correspondence European scientists first learned about the Chinese science and culture."" Western academic thought on the history of Chinese technology and science was galvanized by the work of Joseph Needham and the Needham Research Institute. Among the technological accomplishments of China were, according to the British scholar Needham, early seismological detectors (Zhang Heng in the 2nd century), the water-powered celestial globe (Zhang Heng), matches, the independent invention of the decimal system, dry docks, sliding calipers, the double-action piston pump, cast iron, the blast furnace, the iron plough, the multi-tube seed drill, the wheelbarrow, the suspension bridge, the winnowing machine, the rotary fan, the parachute, natural gas as fuel, the raised-relief map, the propeller, the crossbow, and a solid fuel rocket, the multistage rocket, the horse collar, along with contributions in logic, astronomy, medicine, and other fields. In 1938 Otto Hahn and Fritz Strassmann discovered nuclear fission with radiochemical methods, and in 1939 Lise Meitner and Otto Robert Frisch wrote the first theoretical interpretation of the fission process, which was later improved by Niels Bohr and John A. Wheeler. Further developments took place during World War II, which led to the practical application of radar and the development and use of the atomic bomb. Though the process had begun with the invention of the cyclotron by Ernest O. Lawrence in the 1930s, physics in the postwar period entered into a phase of what historians have called ""Big Science"", requiring massive machines, budgets, and laboratories in order to test their theories and move into new frontiers. The primary patron of physics became state governments, who recognized that the support of ""basic"" research could often lead to technologies useful to both military and industrial applications. Currently, general relativity and quantum mechanics are inconsistent with each other, and efforts are underway to unify the two. The mid 20th century saw a series of studies relying to the role of science in a social context, starting from Thomas Kuhn's The Structure of Scientific Revolutions in 1962. It opened the study of science to new disciplines by suggesting that the evolution of science was in part sociologically determined and that positivism did not explain the actual interactions and strategies of the human participants in science. As Thomas Kuhn put it, the history of science may be seen in more nuanced terms, such as that of competing paradigms or conceptual systems in a wider matrix that includes intellectual, cultural, economic and political themes outside of science. ""Partly by selection and partly by distortion, the scientists of earlier ages are implicitly presented as having worked upon the same set of fixed problems and in accordance with the same set of fixed canons that the most recent revolution in scientific theory and method made seem scientific."" In astronomy, Al-Battani improved the measurements of Hipparchus, preserved in the translation of Ptolemy's Hè Megalè Syntaxis (The great treatise) translated as Almagest. Al-Battani also improved the precision of the measurement of the precession of the Earth's axis. The corrections made to the geocentric model by al-Battani, Ibn al-Haytham, Averroes and the Maragha astronomers such as Nasir al-Din al-Tusi, Mo'ayyeduddin Urdi and Ibn al-Shatir are similar to Copernican heliocentric model. Heliocentric theories may have also been discussed by several other Muslim astronomers such as Ja'far ibn Muhammad Abu Ma'shar al-Balkhi, Abu-Rayhan Biruni, Abu Said al-Sijzi, Qutb al-Din al-Shirazi, and Najm al-Dīn al-Qazwīnī al-Kātibī. As an academic field, history of science began with the publication of William Whewell's History of the Inductive Sciences (first published in 1837). A more formal study of the history of science as an independent discipline was launched by George Sarton's publications, Introduction to the History of Science (1927) and the Isis journal (founded in 1912). Sarton exemplified the early 20th-century view of the history of science as the history of great men and great ideas. He shared with many of his contemporaries a Whiggish belief in history as a record of the advances and delays in the march of progress. The history of science was not a recognized subfield of American history in this period, and most of the work was carried out by interested scientists and physicians rather than professional historians. With the work of I. Bernard Cohen at Harvard, the history of science became an established subdiscipline of history after 1945. The first half of the 14th century saw much important scientific work being done, largely within the framework of scholastic commentaries on Aristotle's scientific writings. William of Ockham introduced the principle of parsimony: natural philosophers should not postulate unnecessary entities, so that motion is not a distinct thing but is only the moving object and an intermediary ""sensible species"" is not needed to transmit an image of an object to the eye. Scholars such as Jean Buridan and Nicole Oresme started to reinterpret elements of Aristotle's mechanics. In particular, Buridan developed the theory that impetus was the cause of the motion of projectiles, which was a first step towards the modern concept of inertia. The Oxford Calculators began to mathematically analyze the kinematics of motion, making this analysis without considering the causes of motion. With the division of the Roman Empire, the Western Roman Empire lost contact with much of its past. In the Middle East, Greek philosophy was able to find some support under the newly created Arab Empire. With the spread of Islam in the 7th and 8th centuries, a period of Muslim scholarship, known as the Islamic Golden Age, lasted until the 13th century. This scholarship was aided by several factors. The use of a single language, Arabic, allowed communication without need of a translator. Access to Greek texts from the Byzantine Empire, along with Indian sources of learning, provided Muslim scholars a knowledge base to build upon. In Classical Antiquity, the inquiry into the workings of the universe took place both in investigations aimed at such practical goals as establishing a reliable calendar or determining how to cure a variety of illnesses and in those abstract investigations known as natural philosophy. The ancient people who are considered the first scientists may have thought of themselves as natural philosophers, as practitioners of a skilled profession (for example, physicians), or as followers of a religious tradition (for example, temple healers). Muslim scientists placed far greater emphasis on experiment than had the Greeks. This led to an early scientific method being developed in the Muslim world, where significant progress in methodology was made, beginning with the experiments of Ibn al-Haytham (Alhazen) on optics from c. 1000, in his Book of Optics. The law of refraction of light was known to the Persians. The most important development of the scientific method was the use of experiments to distinguish between competing scientific theories set within a generally empirical orientation, which began among Muslim scientists. Ibn al-Haytham is also regarded as the father of optics, especially for his empirical proof of the intromission theory of light. Some have also described Ibn al-Haytham as the ""first scientist"" for his development of the modern scientific method. Theophrastus wrote some of the earliest descriptions of plants and animals, establishing the first taxonomy and looking at minerals in terms of their properties such as hardness. Pliny the Elder produced what is one of the largest encyclopedias of the natural world in 77 AD, and must be regarded as the rightful successor to Theophrastus. For example, he accurately describes the octahedral shape of the diamond, and proceeds to mention that diamond dust is used by engravers to cut and polish other gems owing to its great hardness. His recognition of the importance of crystal shape is a precursor to modern crystallography, while mention of numerous other minerals presages mineralogy. He also recognises that other minerals have characteristic crystal shapes, but in one example, confuses the crystal habit with the work of lapidaries. He was also the first to recognise that amber was a fossilized resin from pine trees because he had seen samples with trapped insects within them. The Age of Enlightenment was a European affair. The 17th century ""Age of Reason"" opened the avenues to the decisive steps towards modern science, which took place during the 18th century ""Age of Enlightenment"". Directly based on the works of Newton, Descartes, Pascal and Leibniz, the way was now clear to the development of modern mathematics, physics and technology by the generation of Benjamin Franklin (1706–1790), Leonhard Euler (1707–1783), Mikhail Lomonosov (1711–1765) and Jean le Rond d'Alembert (1717–1783), epitomized in the appearance of Denis Diderot's Encyclopédie between 1751 and 1772. The impact of this process was not limited to science and technology, but affected philosophy (Immanuel Kant, David Hume), religion (the increasingly significant impact of science upon religion), and society and politics in general (Adam Smith, Voltaire), the French Revolution of 1789 setting a bloody cesura indicating the beginning of political modernity[citation needed]. The early modern period is seen as a flowering of the European Renaissance, in what is often known as the Scientific Revolution, viewed as a foundation of modern science. The Romantic Movement of the early 19th century reshaped science by opening up new pursuits unexpected in the classical approaches of the Enlightenment. Major breakthroughs came in biology, especially in Darwin's theory of evolution, as well as physics (electromagnetism), mathematics (non-Euclidean geometry, group theory) and chemistry (organic chemistry). The decline of Romanticism occurred because a new movement, Positivism, began to take hold of the ideals of the intellectuals after 1840 and lasted until about 1880. Mathematics: The earliest traces of mathematical knowledge in the Indian subcontinent appear with the Indus Valley Civilization (c. 4th millennium BC ~ c. 3rd millennium BC). The people of this civilization made bricks whose dimensions were in the proportion 4:2:1, considered favorable for the stability of a brick structure. They also tried to standardize measurement of length to a high degree of accuracy. They designed a ruler—the Mohenjo-daro ruler—whose unit of length (approximately 1.32 inches or 3.4 centimetres) was divided into ten equal parts. Bricks manufactured in ancient Mohenjo-daro often had dimensions that were integral multiples of this unit of length. Mathematics: From the earliest the Chinese used a positional decimal system on counting boards in order to calculate. To express 10, a single rod is placed in the second box from the right. The spoken language uses a similar system to English: e.g. four thousand two hundred seven. No symbol was used for zero. By the 1st century BC, negative numbers and decimal fractions were in use and The Nine Chapters on the Mathematical Art included methods for extracting higher order roots by Horner's method and solving linear equations and by Pythagoras' theorem. Cubic equations were solved in the Tang dynasty and solutions of equations of order higher than 3 appeared in print in 1245 AD by Ch'in Chiu-shao. Pascal's triangle for binomial coefficients was described around 1100 by Jia Xian. Seismology: To better prepare for calamities, Zhang Heng invented a seismometer in 132 CE which provided instant alert to authorities in the capital Luoyang that an earthquake had occurred in a location indicated by a specific cardinal or ordinal direction. Although no tremors could be felt in the capital when Zhang told the court that an earthquake had just occurred in the northwest, a message came soon afterwards that an earthquake had indeed struck 400 km (248 mi) to 500 km (310 mi) northwest of Luoyang (in what is now modern Gansu). Zhang called his device the 'instrument for measuring the seasonal winds and the movements of the Earth' (Houfeng didong yi 候风地动仪), so-named because he and others thought that earthquakes were most likely caused by the enormous compression of trapped air. See Zhang's seismometer for further details. In mathematics, the Persian mathematician Muhammad ibn Musa al-Khwarizmi gave his name to the concept of the algorithm, while the term algebra is derived from al-jabr, the beginning of the title of one of his publications. What is now known as Arabic numerals originally came from India, but Muslim mathematicians did make several refinements to the number system, such as the introduction of decimal point notation. Sabian mathematician Al-Battani (850-929) contributed to astronomy and mathematics, while Persian scholar Al-Razi contributed to chemistry and medicine. In 1348, the Black Death and other disasters sealed a sudden end to the previous period of massive philosophic and scientific development. Yet, the rediscovery of ancient texts was improved after the Fall of Constantinople in 1453, when many Byzantine scholars had to seek refuge in the West. Meanwhile, the introduction of printing was to have great effect on European society. The facilitated dissemination of the printed word democratized learning and allowed a faster propagation of new ideas. New ideas also helped to influence the development of European science at this point: not least the introduction of Algebra. These developments paved the way for the Scientific Revolution, which may also be understood as a resumption of the process of scientific inquiry, halted at the start of the Black Death. The English word scientist is relatively recent—first coined by William Whewell in the 19th century. Previously, people investigating nature called themselves natural philosophers. While empirical investigations of the natural world have been described since classical antiquity (for example, by Thales, Aristotle, and others), and scientific methods have been employed since the Middle Ages (for example, by Ibn al-Haytham, and Roger Bacon), the dawn of modern science is often traced back to the early modern period and in particular to the scientific revolution that took place in 16th- and 17th-century Europe. Scientific methods are considered to be so fundamental to modern science that some consider earlier inquiries into nature to be pre-scientific. Traditionally, historians of science have defined science sufficiently broadly to include those inquiries. The willingness to question previously held truths and search for new answers resulted in a period of major scientific advancements, now known as the Scientific Revolution. The Scientific Revolution is traditionally held by most historians to have begun in 1543, when the books De humani corporis fabrica (On the Workings of the Human Body) by Andreas Vesalius, and also De Revolutionibus, by the astronomer Nicolaus Copernicus, were first printed. The thesis of Copernicus' book was that the Earth moved around the Sun. The period culminated with the publication of the Philosophiæ Naturalis Principia Mathematica in 1687 by Isaac Newton, representative of the unprecedented growth of scientific publications throughout Europe. Historical linguistics emerged as an independent field of study at the end of the 18th century. Sir William Jones proposed that Sanskrit, Persian, Greek, Latin, Gothic, and Celtic languages all shared a common base. After Jones, an effort to catalog all languages of the world was made throughout the 19th century and into the 20th century. Publication of Ferdinand de Saussure's Cours de linguistique générale created the development of descriptive linguistics. Descriptive linguistics, and the related structuralism movement caused linguistics to focus on how language changes over time, instead of just describing the differences between languages. Noam Chomsky further diversified linguistics with the development of generative linguistics in the 1950s. His effort is based upon a mathematical model of language that allows for the description and prediction of valid syntax. Additional specialties such as sociolinguistics, cognitive linguistics, and computational linguistics have emerged from collaboration between linguistics and other disciplines. Ibn Khaldun can be regarded as the earliest scientific systematic sociologist. The modern sociology, emerged in the early 19th century as the academic response to the modernization of the world. Among many early sociologists (e.g., Émile Durkheim), the aim of sociology was in structuralism, understanding the cohesion of social groups, and developing an ""antidote"" to social disintegration. Max Weber was concerned with the modernization of society through the concept of rationalization, which he believed would trap individuals in an ""iron cage"" of rational thought. Some sociologists, including Georg Simmel and W. E. B. Du Bois, utilized more microsociological, qualitative analyses. This microlevel approach played an important role in American sociology, with the theories of George Herbert Mead and his student Herbert Blumer resulting in the creation of the symbolic interactionism approach to sociology. Geology existed as a cloud of isolated, disconnected ideas about rocks, minerals, and landforms long before it became a coherent science. Theophrastus' work on rocks, Peri lithōn, remained authoritative for millennia: its interpretation of fossils was not overturned until after the Scientific Revolution. Chinese polymath Shen Kua (1031–1095) first formulated hypotheses for the process of land formation. Based on his observation of fossils in a geological stratum in a mountain hundreds of miles from the ocean, he deduced that the land was formed by erosion of the mountains and by deposition of silt. The history of science is the study of the development of science and scientific knowledge, including both the natural sciences and social sciences. (The history of the arts and humanities is termed as the history of scholarship.) Science is a body of empirical, theoretical, and practical knowledge about the natural world, produced by scientists who emphasize the observation, explanation, and prediction of real world phenomena. Historiography of science, in contrast, often draws on the historical methods of both intellectual history and social history. With the fall of the Western Roman Empire, there arose a more diffuse arena for political studies. The rise of monotheism and, particularly for the Western tradition, Christianity, brought to light a new space for politics and political action[citation needed]. During the Middle Ages, the study of politics was widespread in the churches and courts. Works such as Augustine of Hippo's The City of God synthesized current philosophies and political traditions with those of Christianity, redefining the borders between what was religious and what was political. Most of the political questions surrounding the relationship between Church and State were clarified and contested in this period. Computer science, built upon a foundation of theoretical linguistics, discrete mathematics, and electrical engineering, studies the nature and limits of computation. Subfields include computability, computational complexity, database design, computer networking, artificial intelligence, and the design of computer hardware. One area in which advances in computing have contributed to more general scientific development is by facilitating large-scale archiving of scientific data. Contemporary computer science typically distinguishes itself by emphasising mathematical 'theory' in contrast to the practical emphasis of software engineering. The important legacy of this period included substantial advances in factual knowledge, especially in anatomy, zoology, botany, mineralogy, geography, mathematics and astronomy; an awareness of the importance of certain scientific problems, especially those related to the problem of change and its causes; and a recognition of the methodological importance of applying mathematics to natural phenomena and of undertaking empirical research. In the Hellenistic age scholars frequently employed the principles developed in earlier Greek thought: the application of mathematics and deliberate empirical research, in their scientific investigations. Thus, clear unbroken lines of influence lead from ancient Greek and Hellenistic philosophers, to medieval Muslim philosophers and scientists, to the European Renaissance and Enlightenment, to the secular sciences of the modern day. Neither reason nor inquiry began with the Ancient Greeks, but the Socratic method did, along with the idea of Forms, great advances in geometry, logic, and the natural sciences. According to Benjamin Farrington, former Professor of Classics at Swansea University: Midway through the 19th century, the focus of geology shifted from description and classification to attempts to understand how the surface of the Earth had changed. The first comprehensive theories of mountain building were proposed during this period, as were the first modern theories of earthquakes and volcanoes. Louis Agassiz and others established the reality of continent-covering ice ages, and ""fluvialists"" like Andrew Crombie Ramsay argued that river valleys were formed, over millions of years by the rivers that flow through them. After the discovery of radioactivity, radiometric dating methods were developed, starting in the 20th century. Alfred Wegener's theory of ""continental drift"" was widely dismissed when he proposed it in the 1910s, but new data gathered in the 1950s and 1960s led to the theory of plate tectonics, which provided a plausible mechanism for it. Plate tectonics also provided a unified explanation for a wide range of seemingly unrelated geological phenomena. Since 1970 it has served as the unifying principle in geology. In 1687, Isaac Newton published the Principia Mathematica, detailing two comprehensive and successful physical theories: Newton's laws of motion, which led to classical mechanics; and Newton's Law of Gravitation, which describes the fundamental force of gravity. The behavior of electricity and magnetism was studied by Faraday, Ohm, and others during the early 19th century. These studies led to the unification of the two phenomena into a single theory of electromagnetism, by James Clerk Maxwell (known as Maxwell's equations). From the 18th century through late 20th century, the history of science, especially of the physical and biological sciences, was often presented in a progressive narrative in which true theories replaced false beliefs. More recent historical interpretations, such as those of Thomas Kuhn, tend to portray the history of science in different terms, such as that of competing paradigms or conceptual systems in a wider matrix that includes intellectual, cultural, economic and political themes outside of science. There are many notable contributors to the field of Chinese science throughout the ages. One of the best examples would be Shen Kuo (1031–1095), a polymath scientist and statesman who was the first to describe the magnetic-needle compass used for navigation, discovered the concept of true north, improved the design of the astronomical gnomon, armillary sphere, sight tube, and clepsydra, and described the use of drydocks to repair boats. After observing the natural process of the inundation of silt and the find of marine fossils in the Taihang Mountains (hundreds of miles from the Pacific Ocean), Shen Kuo devised a theory of land formation, or geomorphology. He also adopted a theory of gradual climate change in regions over time, after observing petrified bamboo found underground at Yan'an, Shaanxi province. If not for Shen Kuo's writing, the architectural works of Yu Hao would be little known, along with the inventor of movable type printing, Bi Sheng (990-1051). Shen's contemporary Su Song (1020–1101) was also a brilliant polymath, an astronomer who created a celestial atlas of star maps, wrote a pharmaceutical treatise with related subjects of botany, zoology, mineralogy, and metallurgy, and had erected a large astronomical clocktower in Kaifeng city in 1088. To operate the crowning armillary sphere, his clocktower featured an escapement mechanism and the world's oldest known use of an endless power-transmitting chain drive."
George_VI,"During George's reign the break-up of the British Empire and its transition into the Commonwealth of Nations accelerated. The parliament of the Irish Free State removed direct mention of the monarch from the country's constitution on the day of his accession. From 1939, the Empire and Commonwealth, except Ireland, was at war with Nazi Germany. War with Italy and Japan followed in 1940 and 1941, respectively. Though Britain and its allies were ultimately victorious in 1945, the United States and the Soviet Union rose as pre-eminent world powers and the British Empire declined. After the independence of India and Pakistan in 1947, George remained as king of both countries, but the title Emperor of India was abandoned in June 1948. Ireland formally declared itself a republic and left the Commonwealth in 1949, and India became a republic within the Commonwealth the following year. George adopted the new title of Head of the Commonwealth. He was beset by health problems in the later years of his reign. His elder daughter, Elizabeth, succeeded him. In September 1939, Britain and the self-governing Dominions, but not Ireland, declared war on Nazi Germany. George VI and his wife resolved to stay in London, despite German bombing raids. They officially stayed in Buckingham Palace throughout the war, although they usually spent nights at Windsor Castle. The first German raid on London, on 7 September 1940, killed about one thousand civilians, mostly in the East End. On 13 September, the King and Queen narrowly avoided death when two German bombs exploded in a courtyard at Buckingham Palace while they were there. In defiance, the Queen famously declared: ""I am glad we have been bombed. It makes me feel we can look the East End in the face"". The royal family were portrayed as sharing the same dangers and deprivations as the rest of the country. They were subject to rationing restrictions, and U.S. First Lady Eleanor Roosevelt remarked on the rationed food served and the limited bathwater that was permitted during a stay at the unheated and boarded-up Palace. In August 1942, the King's brother, Prince George, Duke of Kent, was killed on active service. George VI's reign saw the acceleration of the dissolution of the British Empire. The Statute of Westminster 1931 had already acknowledged the evolution of the Dominions into separate sovereign states. The process of transformation from an empire to a voluntary association of independent states, known as the Commonwealth, gathered pace after the Second World War. During the ministry of Clement Attlee, British India became the two independent dominions of India and Pakistan in 1947. George relinquished the title of Emperor of India, and became King of India and King of Pakistan instead. In 1950 he ceased to be King of India when it became a republic within the Commonwealth of Nations, but he remained King of Pakistan until his death and India recognised his new title of Head of the Commonwealth. Other countries left the Commonwealth, such as Burma in January 1948, Palestine (divided between Israel and the Arab states) in May 1948 and the Republic of Ireland in 1949. From 9 February for two days his coffin rested in St. Mary Magdalene Church, Sandringham, before lying in state at Westminster Hall from 11 February. His funeral took place at St. George's Chapel, Windsor Castle, on the 15th. He was interred initially in the Royal Vault until he was transferred to the King George VI Memorial Chapel inside St. George's on 26 March 1969. In 2002, fifty years after his death, the remains of his widow, Queen Elizabeth The Queen Mother, and the ashes of his younger daughter Princess Margaret, who both died that year, were interred in the chapel alongside him. Throughout the war, the King and Queen provided morale-boosting visits throughout the United Kingdom, visiting bomb sites, munitions factories, and troops. The King visited military forces abroad in France in December 1939, North Africa and Malta in June 1943, Normandy in June 1944, southern Italy in July 1944, and the Low Countries in October 1944. Their high public profile and apparently indefatigable determination secured their place as symbols of national resistance. At a social function in 1944, Chief of the Imperial General Staff Sir Alan Brooke, revealed that every time he met Field Marshal Bernard Montgomery he thought he was after his job. The King replied: ""You should worry, when I meet him, I always think he's after mine!"" In May and June 1939, the King and Queen toured Canada and the United States. From Ottawa, the royal couple were accompanied throughout by Canadian Prime Minister William Lyon Mackenzie King, to present themselves in North America as King and Queen of Canada. George was the first reigning monarch of Canada to visit North America, although he had been to Canada previously as Prince Albert and as Duke of York. Both Governor General of Canada Lord Tweedsmuir and Mackenzie King hoped that the King's presence in Canada would demonstrate the principles of the Statute of Westminster 1931, which gave full sovereignty to the British Dominions. On 19 May, George VI personally accepted and approved the Letter of Credence of the new U.S. Ambassador to Canada, Daniel Calhoun Roper; gave Royal Assent to nine parliamentary bills; and ratified two international treaties with the Great Seal of Canada. The official royal tour historian, Gustave Lanctot, wrote ""the Statute of Westminster had assumed full reality"" and George gave a speech emphasising ""the free and equal association of the nations of the Commonwealth"". The growing likelihood of war in Europe dominated the early reign of George VI. The King was constitutionally bound to support Prime Minister Neville Chamberlain's appeasement of Hitler. However, when the King and Queen greeted Chamberlain on his return from negotiating the Munich Agreement in 1938, they invited him to appear on the balcony of Buckingham Palace with them. This public association of the monarchy with a politician was exceptional, as balcony appearances were traditionally restricted to the royal family. While broadly popular among the general public, Chamberlain's policy towards Hitler was the subject of some opposition in the House of Commons, which led historian John Grigg to describe the King's behaviour in associating himself so prominently with a politician as ""the most unconstitutional act by a British sovereign in the present century"". In a time when royals were expected to marry fellow royals, it was unusual that Albert had a great deal of freedom in choosing a prospective wife. An infatuation with the already-married Australian socialite Sheila, Lady Loughborough, came to an end in April 1920 when the King, with the promise of the dukedom of York, persuaded Albert to stop seeing her. That year, he met for the first time since childhood Lady Elizabeth Bowes-Lyon, the youngest daughter of the Earl and Countess of Strathmore and Kinghorne. He became determined to marry her. She rejected his proposal twice, in 1921 and 1922, reportedly because she was reluctant to make the sacrifices necessary to become a member of the royal family. In the words of Lady Elizabeth's mother, Albert would be ""made or marred"" by his choice of wife. After a protracted courtship, Elizabeth agreed to marry him. Albert spent the first six months of 1913 on the training ship HMS Cumberland in the West Indies and on the east coast of Canada. He was rated as a midshipman aboard HMS Collingwood on 15 September 1913, and spent three months in the Mediterranean. His fellow officers gave him the nickname ""Mr. Johnson"". One year after his commission, he began service in the First World War. He was mentioned in despatches for his action as a turret officer aboard Collingwood in the Battle of Jutland (31 May – 1 June 1916), an indecisive engagement with the German navy that was the largest naval action of the war. He did not see further combat, largely because of ill health caused by a duodenal ulcer, for which he had an operation in November 1917. On the day of the abdication, the Oireachtas, the parliament of the Irish Free State, removed all direct mention of the monarch from the Irish constitution. The next day, it passed the External Relations Act, which gave the monarch limited authority (strictly on the advice of the government) to appoint diplomatic representatives for Ireland and to be involved in the making of foreign treaties. The two acts made the Irish Free State a republic in essence without removing its links to the Commonwealth. The stress of the war had taken its toll on the King's health, exacerbated by his heavy smoking and subsequent development of lung cancer among other ailments, including arteriosclerosis and thromboangiitis obliterans. A planned tour of Australia and New Zealand was postponed after the King suffered an arterial blockage in his right leg, which threatened the loss of the leg and was treated with a right lumbar sympathectomy in March 1949. His elder daughter Elizabeth, the heir presumptive, took on more royal duties as her father's health deteriorated. The delayed tour was re-organised, with Elizabeth and her husband, the Duke of Edinburgh, taking the place of the King and Queen. The King was well enough to open the Festival of Britain in May 1951, but on 23 September 1951, his left lung was removed by Clement Price Thomas after a malignant tumour was found. In October 1951, Princess Elizabeth and the Duke of Edinburgh went on a month-long tour of Canada; the trip had been delayed for a week due to the King's illness. At the State Opening of Parliament in November, the King's speech from the throne was read for him by the Lord Chancellor, Lord Simonds. His Christmas broadcast of 1951 was recorded in sections, and then edited together. His birthday (14 December 1895) was the 34th anniversary of the death of his great-grandfather, Prince Albert, the Prince Consort. Uncertain of how the Prince Consort's widow, Queen Victoria, would take the news of the birth, the Prince of Wales wrote to the Duke of York that the Queen had been ""rather distressed"". Two days later, he wrote again: ""I really think it would gratify her if you yourself proposed the name Albert to her"". Queen Victoria was mollified by the proposal to name the new baby Albert, and wrote to the Duchess of York: ""I am all impatience to see the new one, born on such a sad day but rather more dear to me, especially as he will be called by that dear name which is a byword for all that is great and good"". Consequently, he was baptised ""Albert Frederick Arthur George"" at St. Mary Magdalene's Church near Sandringham three months later.[a] As a great-grandson of Queen Victoria, he was known formally as His Highness Prince Albert of York from birth. Within the family, he was known informally as ""Bertie"". His maternal grandmother, the Duchess of Teck, did not like the first name the baby had been given, and she wrote prophetically that she hoped the last name ""may supplant the less favoured one"". In October 1919, Albert went up to Trinity College, Cambridge, where he studied history, economics and civics for a year. On 4 June 1920, he was created Duke of York, Earl of Inverness and Baron Killarney. He began to take on more royal duties. He represented his father, and toured coal mines, factories, and railyards. Through such visits he acquired the nickname of the ""Industrial Prince"". His stammer, and his embarrassment over it, together with his tendency to shyness, caused him to appear much less impressive than his older brother, Edward. However, he was physically active and enjoyed playing tennis. He played at Wimbledon in the Men's Doubles with Louis Greig in 1926, losing in the first round. He developed an interest in working conditions, and was President of the Industrial Welfare Society. His series of annual summer camps for boys between 1921 and 1939 brought together boys from different social backgrounds. The trip was intended to soften the strong isolationist tendencies among the North American public with regard to the developing tensions in Europe. Although the aim of the tour was mainly political, to shore up Atlantic support for the United Kingdom in any future war, the King and Queen were enthusiastically received by the public. The fear that George would be compared unfavourably to his predecessor, Edward VIII, was dispelled. They visited the 1939 New York World's Fair and stayed with President Franklin D. Roosevelt at the White House and at his private estate at Hyde Park, New York. A strong bond of friendship was forged between the King and Queen and the President during the tour, which had major significance in the relations between the United States and the United Kingdom through the ensuing war years. Because of his stammer, Albert dreaded public speaking. After his closing speech at the British Empire Exhibition at Wembley on 31 October 1925, one which was an ordeal for both him and his listeners, he began to see Lionel Logue, an Australian-born speech therapist. The Duke and Logue practised breathing exercises, and the Duchess rehearsed with him patiently. Subsequently, he was able to speak with less hesitation. With his delivery improved, the Duke opened the new Parliament House in Canberra, Australia, during a tour of the empire in 1927. His journey by sea to Australia, New Zealand and Fiji took him via Jamaica, where Albert played doubles tennis partnered with a black man, which was unusual at the time and taken locally as a display of equality between races. The Duke and Duchess of York had two children: Elizabeth (called ""Lilibet"" by the family), and Margaret. The Duke and Duchess and their two daughters lived a relatively sheltered life at their London residence, 145 Piccadilly. They were a close and loving family. One of the few stirs arose when the Canadian Prime Minister, R. B. Bennett, considered the Duke for Governor General of Canada in 1931—a proposal that King George V rejected on the advice of the Secretary of State for Dominion Affairs, J. H. Thomas. In 1940, Winston Churchill replaced Neville Chamberlain as Prime Minister, though personally George would have preferred to appoint Lord Halifax. After the King's initial dismay over Churchill's appointment of Lord Beaverbrook to the Cabinet, he and Churchill developed ""the closest personal relationship in modern British history between a monarch and a Prime Minister"". Every Tuesday for four and a half years from September 1940, the two men met privately for lunch to discuss the war in secret and with frankness. In February 1918, he was appointed Officer in Charge of Boys at the Royal Naval Air Service's training establishment at Cranwell. With the establishment of the Royal Air Force two months later and the transfer of Cranwell from Navy to Air Force control, he transferred from the Royal Navy to the Royal Air Force. He was appointed Officer Commanding Number 4 Squadron of the Boys' Wing at Cranwell until August 1918, before reporting to the RAF's Cadet School at St Leonards-on-Sea where he completed a fortnight's training and took command of a squadron on the Cadet Wing. He was the first member of the royal family to be certified as a fully qualified pilot. During the closing weeks of the war, he served on the staff of the RAF's Independent Air Force at its headquarters in Nancy, France. Following the disbanding of the Independent Air Force in November 1918, he remained on the Continent for two months as a staff officer with the Royal Air Force until posted back to Britain. He accompanied the Belgian monarch King Albert on his triumphal reentry into Brussels on 22 November. Prince Albert qualified as an RAF pilot on 31 July 1919 and gained a promotion to squadron leader on the following day. As Edward was unmarried and had no children, Albert was the heir presumptive to the throne. Less than a year later, on 11 December 1936, Edward VIII abdicated in order to marry his mistress, Wallis Simpson, who was divorced from her first husband and divorcing her second. Edward had been advised by British Prime Minister Stanley Baldwin that he could not remain king and marry a divorced woman with two living ex-husbands. Edward chose abdication in preference to abandoning his marriage plans. Thus Albert became king, a position he was reluctant to accept. The day before the abdication, he went to London to see his mother, Queen Mary. He wrote in his diary, ""When I told her what had happened, I broke down and sobbed like a child."" Albert assumed the regnal name ""George VI"" to emphasise continuity with his father and restore confidence in the monarchy. The beginning of George VI's reign was taken up by questions surrounding his predecessor and brother, whose titles, style and position were uncertain. He had been introduced as ""His Royal Highness Prince Edward"" for the abdication broadcast, but George VI felt that by abdicating and renouncing the succession Edward had lost the right to bear royal titles, including ""Royal Highness"". In settling the issue, George's first act as king was to confer upon his brother the title and style ""His Royal Highness The Duke of Windsor"", but the Letters Patent creating the dukedom prevented any wife or children from bearing royal styles. George VI was also forced to buy from Edward the royal residences of Balmoral Castle and Sandringham House, as these were private properties and did not pass to George VI automatically. Three days after his accession, on his 41st birthday, he invested his wife, the new queen consort, with the Order of the Garter. In the words of Labour Member of Parliament George Hardie, the abdication crisis of 1936 did ""more for republicanism than fifty years of propaganda"". George VI wrote to his brother Edward that in the aftermath of the abdication he had reluctantly assumed ""a rocking throne"", and tried ""to make it steady again"". He became king at a point when public faith in the monarchy was at a low ebb. During his reign his people endured the hardships of war, and imperial power was eroded. However, as a dutiful family man and by showing personal courage, he succeeded in restoring the popularity of the monarchy. In 1947, the King and his family toured Southern Africa. The Prime Minister of the Union of South Africa, Jan Smuts, was facing an election and hoped to make political capital out of the visit. George was appalled, however, when instructed by the South African government to shake hands only with whites, and referred to his South African bodyguards as ""the Gestapo"". Despite the tour, Smuts lost the election the following year, and the new government instituted a strict policy of racial segregation. George VI's coronation took place on 12 May 1937, the date previously intended for Edward's coronation. In a break with tradition, Queen Mary attended the ceremony in a show of support for her son. There was no Durbar held in Delhi for George VI, as had occurred for his father, as the cost would have been a burden to the government of India. Rising Indian nationalism made the welcome that the royal couple would have received likely to be muted at best, and a prolonged absence from Britain would have been undesirable in the tense period before the Second World War. Two overseas tours were undertaken, to France and to North America, both of which promised greater strategic advantages in the event of war."
Cardinal_(Catholicism),"In 1630, Pope Urban VIII decreed their title to be Eminence (previously, it had been ""illustrissimo"" and ""reverendissimo"")  and decreed that their secular rank would equate to Prince, making them secondary only to the Pope and crowned monarchs. A cardinal named in pectore is known only to the pope; not even the cardinal so named is necessarily aware of his elevation, and in any event cannot function as a cardinal while his appointment is in pectore. Today, cardinals are named in pectore to protect them or their congregations from reprisals if their identities were known. In early modern times, cardinals often had important roles in secular affairs. In some cases, they took on powerful positions in government. In Henry VIII's England, his chief minister was Cardinal Wolsey. Cardinal Richelieu's power was so great that he was for many years effectively the ruler of France. Richelieu successor was also a cardinal, Jules Mazarin. Guillaume Dubois and André-Hercule de Fleury complete the list of the ""four great"" cardinals to have ruled France.[citation needed] In Portugal, due to a succession crisis, one cardinal, Henry, King of Portugal, was crowned king, the only example of a cardinal-king. In early times, the privilege of papal election was not reserved to the cardinals, and for centuries the person elected was customarily a Roman priest and never a bishop from elsewhere. To preserve apostolic succession the rite of consecrating him a bishop had to be performed by someone who was already a bishop. The rule remains that, if the person elected Pope is not yet a bishop, he is consecrated by the Dean of the College of Cardinals, the Cardinal Bishop of Ostia. The cardinal protodeacon, the senior cardinal deacon in order of appointment to the College of Cardinals, has the privilege of announcing a new pope's election and name (once he has been ordained to the Episcopate) from the central balcony at the Basilica of Saint Peter in Vatican City State. In the past, during papal coronations, the proto-deacon also had the honor of bestowing the pallium on the new pope and crowning him with the papal tiara. However, in 1978 Pope John Paul I chose not to be crowned and opted for a simpler papal inauguration ceremony, and his three successors followed that example. As a result, the Cardinal protodeacon's privilege of crowning a new pope has effectively ceased although it could be revived if a future Pope were to restore a coronation ceremony. However, the proto-deacon still has the privilege of bestowing the pallium on a new pope at his papal inauguration. “Acting in the place of the Roman Pontiff, he also confers the pallium upon metropolitan bishops or gives the pallium to their proxies.” The current cardinal proto-deacon is Renato Raffaele Martino. To symbolize their bond with the papacy, the pope gives each newly appointed cardinal a gold ring, which is traditionally kissed by Catholics when greeting a cardinal (as with a bishop's episcopal ring). The pope chooses the image on the outside: under Pope Benedict XVI it was a modern depiction of the crucifixion of Jesus, with Mary and John to each side. The ring includes the pope's coat of arms on the inside.[citation needed] The cardinal who is the longest-serving member of the order of cardinal priests is titled cardinal protopriest. He had certain ceremonial duties in the conclave that have effectively ceased because he would generally have already reached age 80, at which cardinals are barred from the conclave. The current cardinal protopriest is Paulo Evaristo Arns of Brazil. Cardinals elevated to the diaconal order are mainly officials of the Roman Curia holding various posts in the church administration. Their number and influence has varied through the years. While historically predominantly Italian the group has become much more internationally diverse in later years. While in 1939 about half were Italian by 1994 the number was reduced to one third. Their influence in the election of the Pope has been considered important, they are better informed and connected than the dislocated cardinals but their level of unity has been varied. Under the 1587 decree of Pope Sixtus V, which fixed the maximum size of the College of Cardinals, there were 14 cardinal deacons. Later the number increased. As late as 1939 almost half of the cardinals were members of the curia. Pius XII reduced this percentage to 24 percent. John XXIII brought it back up to 37 percent but Paul VI brought it down to 27 percent where John Paul II has maintained this ratio. In cities other than Rome, the name cardinal began to be applied to certain church men as a mark of honour. The earliest example of this occurs in a letter sent by Pope Zacharias in 747 to Pippin III (the Short), ruler of the Franks, in which Zacharias applied the title to the priests of Paris to distinguish them from country clergy. This meaning of the word spread rapidly, and from the 9th century various episcopal cities had a special class among the clergy known as cardinals. The use of the title was reserved for the cardinals of Rome in 1567 by Pius V. In 1059, the right of electing the pope was reserved to the principal clergy of Rome and the bishops of the seven suburbicarian sees. In the 12th century the practice of appointing ecclesiastics from outside Rome as cardinals began, with each of them assigned a church in Rome as his titular church or linked with one of the suburbicarian dioceses, while still being incardinated in a diocese other than that of Rome.[citation needed] During the Western Schism, many cardinals were created by the contending popes. Beginning with the reign of Pope Martin V, cardinals were created without publishing their names until later, termed creati et reservati in pectore. In the year 1563 the influential Ecumenical Council of Trent, headed by Pope Pius IV, wrote about the importance of selecting good Cardinals. According to this historic council ""nothing is more necessary to the Church of God than that the holy Roman pontiff apply that solicitude which by the duty of his office he owes the universal Church in a very special way by associating with himself as cardinals the most select persons only, and appoint to each church most eminently upright and competent shepherds; and this the more so, because our Lord Jesus Christ will require at his hands the blood of the sheep of Christ that perish through the evil government of shepherds who are negligent and forgetful of their office."" At various times, there have been cardinals who had only received first tonsure and minor orders but not yet been ordained as deacons or priests. Though clerics, they were inaccurately called ""lay cardinals"" and were permitted to marry. Teodolfo Mertel was among the last of the lay cardinals. When he died in 1899 he was the last surviving cardinal who was not at least ordained a priest. With the revision of the Code of Canon Law promulgated in 1917 by Pope Benedict XV, only those who are already priests or bishops may be appointed cardinals. Since the time of Pope John XXIII a priest who is appointed a cardinal must be consecrated a bishop, unless he obtains a dispensation. Since 1962, the cardinal bishops have only a titular relationship with the suburbicarian sees, with no powers of governance over them. Each see has its own bishop, with the exception of Ostia, in which the Cardinal Vicar of the see of Rome is apostolic administrator. In 1965, Pope Paul VI decreed in his motu proprio Ad Purpuratorum Patrum that patriarchs of the Eastern Catholic Churches who were named cardinals (i.e., patriarch cardinals) would also be part of the episcopal order, ranking after the six cardinal bishops of the suburbicarian sees (who had been relieved of direct responsibilities for those sees by Pope John XXIII three years earlier). Patriarch cardinals do not receive title of a suburbicarian see, and as such they cannot elect the dean or become dean. There are currently three Eastern Patriarchs who are cardinal bishops: In previous times, at the consistory at which the pope named a new cardinal, he would bestow upon him a distinctive wide-brimmed hat called a galero. This custom was discontinued in 1969 and the investiture now takes place with the scarlet biretta. In ecclesiastical heraldry, however, the scarlet galero is still displayed on the cardinal's coat of arms. Cardinals had the right to display the galero in their cathedral, and when a cardinal died, it would be suspended from the ceiling above his tomb. Some cardinals will still have a galero made, even though it is not officially part of their apparel.[citation needed] While the cardinalate has long been expanded beyond the Roman pastoral clergy and Roman Curia, every cardinal priest has a titular church in Rome, though they may be bishops or archbishops elsewhere, just as cardinal bishops are given one of the suburbicarian dioceses around Rome. Pope Paul VI abolished all administrative rights cardinals had with regard to their titular churches, though the cardinal's name and coat of arms are still posted in the church, and they are expected to celebrate mass and preach there if convenient when they are in Rome. While the number of cardinals was small from the times of the Roman Empire to the Renaissance, and frequently smaller than the number of recognized churches entitled to a cardinal priest, in the 16th century the College expanded markedly. In 1587, Pope Sixtus V sought to arrest this growth by fixing the maximum size of the College at 70, including 50 cardinal priests, about twice the historical number. This limit was respected until 1958, and the list of titular churches modified only on rare occasions, generally when a building fell into disrepair. When Pope John XXIII abolished the limit, he began to add new churches to the list, which Popes Paul VI and John Paul II continued to do. Today there are close to 150 titular churches, out of over 300 churches in Rome. In modern times, the name ""cardinal priest"" is interpreted as meaning a cardinal who is of the order of priests. Originally, however, this referred to certain key priests of important churches of the Diocese of Rome, who were recognized as the cardinal priests, the important priests chosen by the pope to advise him in his duties as Bishop of Rome (the Latin cardo means ""hinge""). Certain clerics in many dioceses at the time, not just that of Rome, were said to be the key personnel — the term gradually became exclusive to Rome to indicate those entrusted with electing the bishop of Rome, the pope. In Latin, on the other hand, the [First name] Cardinal [Surname] order is used in the proclamation of the election of a new pope by the cardinal protodeacon: ""Annuntio vobis gaudium magnum; habemus Papam: Eminentissimum ac Reverendissimum Dominum, Dominum (first name) Sanctae Romanae Ecclesiae Cardinalem (last name), ..."" (Meaning: ""I announce to you a great joy; we have a Pope: The Most Eminent and Most Reverend Lord, Lord (first name) Cardinal of the Holy Roman Church (last name), ..."") This assumes that the new pope had been a cardinal just before becoming pope; the most recent election of a non-cardinal as pope was in 1378. As of 2005, there were over 50 churches recognized as cardinalatial deaconries, though there were only 30 cardinals of the order of deacons. Cardinal deacons have long enjoyed the right to ""opt for the order of cardinal priests"" (optazione) after they have been cardinal deacons for 10 years. They may on such elevation take a vacant ""title"" (a church allotted to a cardinal priest as the church in Rome with which he is associated) or their diaconal church may be temporarily elevated to a cardinal priest's ""title"" for that occasion. When elevated to cardinal priests, they take their precedence according to the day they were first made cardinal deacons (thus ranking above cardinal priests who were elevated to the college after them, regardless of order). The Cardinal Camerlengo of the Holy Roman Church, assisted by the Vice-Camerlengo and the other prelates of the office known as the Apostolic Camera, has functions that in essence are limited to a period of sede vacante of the papacy. He is to collate information about the financial situation of all administrations dependent on the Holy See and present the results to the College of Cardinals, as they gather for the papal conclave. Cardinal bishops (cardinals of the episcopal order) are among the most senior prelates of the Catholic Church. Though in modern times most cardinals are also bishops, the term ""cardinal bishop"" only refers to the cardinals who are titular bishops of one of the ""suburbicarian"" sees. When not celebrating Mass but still serving a liturgical function, such as the semiannual Urbi et Orbi papal blessing, some Papal Masses and some events at Ecumenical Councils, cardinal deacons can be recognized by the dalmatics they would don with the simple white mitre (so called mitra simplex). Until 1917, it was possible for someone who was not a priest, but only in minor orders, to become a cardinal (see ""lay cardinals"", below), but they were enrolled only in the order of cardinal deacons. For example, in the 16th century, Reginald Pole was a cardinal for 18 years before he was ordained a priest. In 1917 it was established that all cardinals, even cardinal deacons, had to be priests, and, in 1962, Pope John XXIII set the norm that all cardinals be ordained as bishops, even if they are only priests at the time of appointment. As a consequence of these two changes, canon 351 of the 1983 Code of Canon Law requires that a cardinal be at least in the order of priesthood at his appointment, and that those who are not already bishops must receive episcopal consecration. Several cardinals aged over 80 or close to it when appointed have obtained dispensation from the rule of having to be a bishop. These were all appointed cardinal-deacons, but one of them, Roberto Tucci, lived long enough to exercise the right of option and be promoted to the rank of cardinal-priest. A cardinal who is not a bishop is still entitled to wear and use the episcopal vestments and other pontificalia (episcopal regalia: mitre, crozier, zucchetto, pectoral cross and ring). Even if not a bishop, any cardinal has both actual and honorary precedence over non-cardinal patriarchs, as well as the archbishops and bishops who are not cardinals, but he cannot perform the functions reserved solely to bishops, such as ordination. The prominent priests who since 1962 were not ordained bishops on their elevation to the cardinalate were over the age of 80 or near to it, and so no cardinal who was not a bishop has participated in recent papal conclaves. Cardinals have in canon law a ""privilege of forum"" (i.e., exemption from being judged by ecclesiastical tribunals of ordinary rank): only the pope is competent to judge them in matters subject to ecclesiastical jurisdiction (cases that refer to matters that are spiritual or linked with the spiritual, or with regard to infringement of ecclesiastical laws and whatever contains an element of sin, where culpability must be determined and the appropriate ecclesiastical penalty imposed). The pope either decides the case himself or delegates the decision to a tribunal, usually one of the tribunals or congregations of the Roman Curia. Without such delegation, no ecclesiastical court, even the Roman Rota, is competent to judge a canon law case against a cardinal. Cardinals are, however, subject to the civil and criminal law like everybody else. If conditions change, so that the pope judges it safe to make the appointment public, he may do so at any time. The cardinal in question then ranks in precedence with those raised to the cardinalate at the time of his in pectore appointment. If a pope dies before revealing the identity of an in pectore cardinal, the cardinalate expires. Cardinal priests are the most numerous of the three orders of cardinals in the Catholic Church, ranking above the cardinal deacons and below the cardinal bishops. Those who are named cardinal priests today are generally bishops of important dioceses throughout the world, though some hold Curial positions. The earlier influence of temporal rulers, notably the French kings, reasserted itself through the influence of cardinals of certain nationalities or politically significant movements. Traditions even developed entitling certain monarchs, including those of Austria, Spain, and Portugal, to nominate one of their trusted clerical subjects to be created cardinal, a so-called crown-cardinal.[citation needed] A cardinal (Latin: sanctae romanae ecclesiae cardinalis, literally cardinal of the Holy Roman Church) is a senior ecclesiastical leader, an ecclesiastical prince, and usually (now always for those created when still within the voting age-range) an ordained bishop of the Roman Catholic Church. The cardinals of the Church are collectively known as the College of Cardinals. The duties of the cardinals include attending the meetings of the College and making themselves available individually or in groups to the Pope as requested. Most have additional duties, such as leading a diocese or archdiocese or managing a department of the Roman Curia. A cardinal's primary duty is electing the pope when the see becomes vacant. During the sede vacante (the period between a pope's death or resignation and the election of his successor), the day-to-day governance of the Holy See is in the hands of the College of Cardinals. The right to enter the conclave of cardinals where the pope is elected is limited to those who have not reached the age of 80 years by the day the vacancy occurs. There is disagreement about the origin of the term, but general consensus that ""cardinalis"" from the word cardo (meaning 'pivot' or 'hinge') was first used in late antiquity to designate a bishop or priest who was incorporated into a church for which he had not originally been ordained. In Rome the first persons to be called cardinals were the deacons of the seven regions of the city at the beginning of the 6th century, when the word began to mean “principal,” “eminent,” or ""superior."" The name was also given to the senior priest in each of the ""title"" churches (the parish churches) of Rome and to the bishops of the seven sees surrounding the city. By the 8th century the Roman cardinals constituted a privileged class among the Roman clergy. They took part in the administration of the church of Rome and in the papal liturgy. By decree of a synod of 769, only a cardinal was eligible to become pope. In 1059, during the pontificate of Nicholas II, cardinals were given the right to elect the pope under the Papal Bull In nomine Domini. For a time this power was assigned exclusively to the cardinal bishops, but the Third Lateran Council in 1179 gave back the right to the whole body of cardinals. Cardinals were granted the privilege of wearing the red hat by Pope Innocent IV in 1244. The cardinal deacons are the lowest-ranking cardinals. Cardinals elevated to the diaconal order are either officials of the Roman Curia or priests elevated after their 80th birthday. Bishops with diocesan responsibilities, however, are created cardinal priests. For a period ending in the mid-20th century, long-serving cardinal priests were entitled to fill vacancies that arose among the cardinal bishops, just as cardinal deacons of ten years' standing are still entitled to become cardinal priests. Since then, cardinals have been advanced to cardinal bishop exclusively by papal appointment. Cardinal deacons derive originally from the seven deacons in the Papal Household and the seven deacons who supervised the Church's works in the districts of Rome during the early Middle Ages, when church administration was effectively the government of Rome and provided all social services. Cardinal deacons are given title to one of these deaconries. Pope Sixtus V limited the number of cardinals to 70, comprising six cardinal bishops, 50 cardinal priests, and 14 cardinal deacons. Starting in the pontificate of Pope John XXIII, that limit has been exceeded. At the start of 1971, Pope Paul VI set the number of cardinal electors at a maximum of 120, but set no limit on the number of cardinals generally. He also established a maximum age of eighty years for electors. His action deprived twenty-five living cardinals, including the three living cardinals elevated by Pope Pius XI, of the right to participate in a conclave.[citation needed] Popes can dispense from church laws and have sometimes brought the number of cardinals under the age of 80 to more than 120. Pope Paul VI also increased the number of cardinal bishops by giving that rank to patriarchs of the Eastern Catholic Churches. In accordance with tradition, they sign by placing the title ""Cardinal"" (abbreviated Card.) after their personal name and before their surname as, for instance, ""John Card(inal) Doe"" or, in Latin, ""Ioannes Card(inalis) Cognomen"". Some writers, such as James-Charles Noonan, hold that, in the case of cardinals, the form used for signatures should be used also when referring to them in English. Official sources such as the Archdiocese of Milwaukee and Catholic News Service say that the correct form for referring to a cardinal in English is normally as ""Cardinal [First name] [Surname]"". This is the rule given also in stylebooks not associated with the Catholic Church. This style is also generally followed on the websites of the Holy See and episcopal conferences. Oriental Patriarchs who are created Cardinals customarily use ""Sanctae Ecclesiae Cardinalis"" as their full title, probably because they do not belong to the Roman clergy. The Dean of the College of Cardinals in addition to such a titular church also receives the titular bishopric of Ostia, the primary suburbicarian see. Cardinals governing a particular Church retain that church. There are seven suburbicarian sees: Ostia, Albano, Porto and Santa Rufina, Palestrina, Sabina and Mentana, Frascati and Velletri. Velletri was united with Ostia from 1150 until 1914, when Pope Pius X separated them again, but decreed that whatever cardinal bishop became Dean of the College of Cardinals would keep the suburbicarian see he already held, adding to it that of Ostia, with the result that there continued to be only six cardinal bishops. Eastern Catholic cardinals continue to wear the normal dress appropriate to their liturgical tradition, though some may line their cassocks with scarlet and wear scarlet fascias, or in some cases, wear Eastern-style cassocks entirely of scarlet. When in choir dress, a Latin-rite cardinal wears scarlet garments — the blood-like red symbolizes a cardinal's willingness to die for his faith. Excluding the rochet — which is always white — the scarlet garments include the cassock, mozzetta, and biretta (over the usual scarlet zucchetto). The biretta of a cardinal is distinctive not merely for its scarlet color, but also for the fact that it does not have a pompon or tassel on the top as do the birettas of other prelates. Until the 1460s, it was customary for cardinals to wear a violet or blue cape unless granted the privilege of wearing red when acting on papal business. His normal-wear cassock is black but has scarlet piping and a scarlet fascia (sash). Occasionally, a cardinal wears a scarlet ferraiolo which is a cape worn over the shoulders, tied at the neck in a bow by narrow strips of cloth in the front, without any 'trim' or piping on it. It is because of the scarlet color of cardinals' vesture that the bird of the same name has become known as such.[citation needed] The term cardinal at one time applied to any priest permanently assigned or incardinated to a church, or specifically to the senior priest of an important church, based on the Latin cardo (hinge), meaning ""principal"" or ""chief"". The term was applied in this sense as early as the ninth century to the priests of the tituli (parishes) of the diocese of Rome. The Church of England retains an instance of this origin of the title, which is held by the two senior members of the College of Minor Canons of St Paul's Cathedral. The Dean of the College of Cardinals, or Cardinal-dean, is the primus inter pares of the College of Cardinals, elected by the cardinal bishops holding suburbicarian sees from among their own number, an election, however, that must be approved by the Pope. Formerly the position of dean belonged by right to the longest-serving of the cardinal bishops. While the incumbents of some sees are regularly made cardinals, and some countries are entitled to at least one cardinal by concordate (usually earning its primate the cardinal's hat), no see carries an actual right to the cardinalate, not even if its bishop is a Patriarch. Each cardinal takes on a titular church, either a church in the city of Rome or one of the suburbicarian sees. The only exception is for patriarchs of Eastern Catholic Churches. Nevertheless, cardinals possess no power of governance nor are they to intervene in any way in matters which pertain to the administration of goods, discipline, or the service of their titular churches. They are allowed to celebrate Mass and hear confessions and lead visits and pilgrimages to their titular churches, in coordination with the staff of the church. They often support their churches monetarily, and many Cardinals do keep in contact with the pastoral staffs of their titular churches."
The_Legend_of_Zelda:_Twilight_Princess,"Following the discovery of a buffer overflow vulnerability in the Wii version of Twilight Princess, an exploit known as the ""Twilight Hack"" was developed, allowing the execution of custom code from a Secure Digital (SD) card on the console. A properly designed save file would cause the game to load unsigned code, which could include Executable and Linkable Format (ELF) programs and homebrew Wii applications. Versions 3.3 and 3.4 of the Wii Menu prevented copying exploited save files onto the console until circumvention methods were discovered, and version 4.0 of the Wii Menu patched the vulnerability. The Legend of Zelda: Twilight Princess (Japanese: ゼルダの伝説 トワイライトプリンセス, Hepburn: Zeruda no Densetsu: Towairaito Purinsesu?) is an action-adventure game developed and published by Nintendo for the GameCube and Wii home video game consoles. It is the thirteenth installment in the The Legend of Zelda series. Originally planned for release on the GameCube in November 2005, Twilight Princess was delayed by Nintendo to allow its developers to refine the game, add more content, and port it to the Wii. The Wii version was released alongside the console in North America in November 2006, and in Japan, Europe, and Australia the following month. The GameCube version was released worldwide in December 2006.[b] Special bundles of the game contain a Wolf Link Amiibo figurine, which unlocks a Wii U-exclusive dungeon called the ""Cave of Shadows"" and can carry data over to the upcoming 2016 Zelda game. Other Zelda-related Amiibo figurines have distinct functions: Link and Toon Link replenish arrows, Zelda and Sheik restore Link's health, and Ganondorf causes Link to take twice as much damage. The game's score was composed by Toru Minegishi and Asuka Ohta, with series regular Koji Kondo serving as the sound supervisor. Minegishi took charge of composition and sound design in Twilight Princess, providing all field and dungeon music under the supervision of Kondo. For the trailers, three pieces were written by different composers, two of which were created by Mahito Yokota and Kondo. Michiru Ōshima created orchestral arrangements for the three compositions, later to be performed by an ensemble conducted by Yasuzo Takemoto. Kondo's piece was later chosen as music for the E3 2005 trailer and for the demo movie after the game's title screen. Twilight Princess received the awards for Best Artistic Design, Best Original Score, and Best Use of Sound from IGN for its GameCube version. Both IGN and Nintendo Power gave Twilight Princess the awards for Best Graphics and Best Story. Twilight Princess received Game of the Year awards from GameTrailers, 1UP.com, Electronic Gaming Monthly, Game Informer, Games Radar, GameSpy, Spacey Awards, X-Play and Nintendo Power. It was also given awards for Best Adventure Game from the Game Critics Awards, X-Play, IGN, GameTrailers, 1UP.com, and Nintendo Power. The game was considered the Best Console Game by the Game Critics Awards and GameSpy. The game placed 16th in Official Nintendo Magazine's list of the 100 Greatest Nintendo Games of All Time. IGN ranked the game as the 4th-best Wii game. Nintendo Power ranked the game as the third-best game to be released on a Nintendo system in the 2000s decade. The context-sensitive button mechanic allows one button to serve a variety of functions, such as talking, opening doors, and pushing, pulling, and throwing objects.[e] The on-screen display shows what action, if any, the button will trigger, determined by the situation. For example, if Link is holding a rock, the context-sensitive button will cause Link to throw the rock if he is moving or targeting an object or enemy, or place the rock on the ground if he is standing still.[f] In the PAL region, which covers most of Africa, Asia, Europe, and Oceania, Twilight Princess is the best-selling entry in the Zelda series. During its first week, the game was sold with three of every four Wii purchases. The game had sold 5.82 million copies on the Wii as of March 31, 2011[update], and 1.32 million on the GameCube as of March 31, 2007[update]. Aonuma had anticipated creating a Zelda game for what would later be called the Wii, but had assumed that he would need to complete Twilight Princess first. His team began work developing a pointing-based interface for the bow and arrow, and Aonuma found that aiming directly at the screen gave the game a new feel, just like the DS control scheme for Phantom Hourglass. Aonuma felt confident this was the only way to proceed, but worried about consumers who had been anticipating a GameCube release. Developing two versions would mean delaying the previously announced 2005 release, still disappointing the consumer. Satoru Iwata felt that having both versions would satisfy users in the end, even though they would have to wait for the finished product. Aonuma then started working on both versions in parallel.[o] Media requests at the trade show prompted Kondo to consider using orchestral music for the other tracks in the game as well, a notion reinforced by his preference for live instruments. He originally envisioned a full 50-person orchestra for action sequences and a string quartet for more ""lyrical moments"", though the final product used sequenced music instead. Kondo later cited the lack of interactivity that comes with orchestral music as one of the main reasons for the decision. Both six- and seven-track versions of the game's soundtrack were released on November 19, 2006, as part of a Nintendo Power promotion and bundled with replicas of the Master Sword and the Hylian Shield. The story focuses on series protagonist Link, who tries to prevent Hyrule from being engulfed by a corrupted parallel dimension known as the Twilight Realm. To do so, he takes the form of both a Hylian and a wolf, and is assisted by a mysterious creature named Midna. The game takes place hundreds of years after Ocarina of Time and Majora's Mask, in an alternate timeline from The Wind Waker. There is very little voice acting in the game, as is the case in most Zelda titles to date. Link remains silent in conversation, but grunts when attacking or injured and gasps when surprised. His emotions and responses are largely indicated visually by nods and facial expressions. Other characters have similar language-independent verbalizations, including laughter, surprised or fearful exclamations, and screams. The character of Midna has the most voice acting—her on-screen dialog is often accompanied by a babble of pseudo-speech, which was produced by scrambling the phonemes of English phrases[better source needed] sampled by Japanese voice actress Akiko Kōmoto. During this time, Link also helps Midna find the Fused Shadows, fragments of a relic containing powerful dark magic. In return, she helps Link find Ordon Village's children while helping the monkeys of Faron, the Gorons of Eldin, and the Zoras of Lanayru. Once Link has restored the Light Spirits and Midna has all the Fused Shadows, they are ambushed by Zant. After he relieves Midna of the Fused Shadow fragments, she ridicules him for abusing his tribe's magic, but Zant reveals that his power comes from another source as he uses it to turn Link back into a wolf, and then leaves Midna in Hyrule to die from the world's light. Bringing a dying Midna to Zelda, Link learns he needs the Master Sword to return to human form. Zelda sacrifices herself to heal Midna with her power before vanishing mysteriously. Midna is moved by Zelda's sacrifice, and begins to care more about Link and the fate of the light world. Nintendo staff members reported that demo users complained about the difficulty of the control scheme. Aonuma realized that his team had implemented Wii controls under the mindset of ""forcing"" users to adapt, instead of making the system intuitive and easy to use. He began rethinking the controls with Miyamoto to focus on comfort and ease.[q] The camera movement was reworked and item controls were changed to avoid accidental button presses.[r] In addition, the new item system required use of the button that had previously been used for the sword. To solve this, sword controls were transferred back to gestures—something E3 attendees had commented they would like to see. This reintroduced the problem of using a right-handed swing to control a left-handed sword attack. The team did not have enough time before release to rework Link's character model, so they instead flipped the entire game—everything was made a mirror image.[s] Link was now right-handed, and references to ""east"" and ""west"" were switched around. The GameCube version, however, was left with the original orientation. The Twilight Princess player's guide focuses on the Wii version, but has a section in the back with mirror-image maps for GameCube users.[t] On release, Twilight Princess was considered to be the greatest Zelda game ever made by many critics including writers for 1UP.com, Computer and Video Games, Electronic Gaming Monthly, Game Informer, GamesRadar, IGN and The Washington Post. Game Informer called it ""so creative that it rivals the best that Hollywood has to offer"". GamesRadar praised Twilight Princess as ""a game that deserves nothing but the absolute highest recommendation"". Cubed3 hailed Twilight Princess as ""the single greatest videogame experience"". Twilight Princess's graphics were praised for the art style and animation, although the game was designed for the GameCube, which is technically lacking compared to the next generation consoles. Both IGN and GameSpy pointed out the existence of blurry textures and low-resolution characters. Despite these complaints, Computer and Video Games felt the game's atmosphere was superior to that of any previous Zelda game, and regarded Twilight Princess's Hyrule as the best version ever created. PALGN praised the game's cinematics, noting that ""the cutscenes are the best ever in Zelda games"". Regarding the Wii version, GameSpot's Jeff Gerstmann said the Wii controls felt ""tacked-on"", although 1UP.com said the remote-swinging sword attacks were ""the most impressive in the entire series"". Gaming Nexus considered Twilight Princess's soundtrack to be the best of this generation, though IGN criticized its MIDI-formatted songs for lacking ""the punch and crispness"" of their orchestrated counterparts. Hyper's Javier Glickman commended the game for its ""very long quests, superb Wii controls and being able to save anytime"". However, he criticised it for ""no voice acting, no orchestral score and slightly outdated graphics"". The team worked on a Wii control scheme, adapting camera control and the fighting mechanics to the new interface. A prototype was created that used a swinging gesture to control the sword from a first-person viewpoint, but was unable to show the variety of Link's movements. When the third-person view was restored, Aonuma thought it felt strange to swing the Wii Remote with the right hand to control the sword in Link's left hand, so the entire Wii version map was mirrored.[p] Details about Wii controls began to surface in December 2005 when British publication NGC Magazine claimed that when a GameCube copy of Twilight Princess was played on the Revolution, it would give the player the option of using the Revolution controller. Miyamoto confirmed the Revolution controller-functionality in an interview with Nintendo of Europe and Time reported this soon after. However, support for the Wii controller did not make it into the GameCube release. At E3 2006, Nintendo announced that both versions would be available at the Wii launch, and had a playable version of Twilight Princess for the Wii.[p] Later, the GameCube release was pushed back to a month after the launch of the Wii. Twilight Princess takes place several centuries after Ocarina of Time and Majora's Mask, and begins with a youth named Link who is working as a ranch hand in Ordon Village. One day, the village is attacked by Bulblins, who carry off the village's children with Link in pursuit before he encounters a wall of Twilight. A Shadow Beast pulls him beyond the wall into the Realm of Twilight, where he is transformed into a wolf and imprisoned. Link is soon freed by an imp-like Twilight being named Midna, who dislikes Link but agrees to help him if he obeys her unconditionally. She guides him to Princess Zelda. Zelda explains that Zant, the King of the Twilight, has stolen the light from three of the four Light Spirits and conquered Hyrule. In order to save Hyrule, Link must first restore the Light Spirits by entering the Twilight-covered areas and, as a wolf, recover the Spirits' lost light. He must do this by collecting the multiple ""Tears of Light""; once all the Tears of Light are collected for one area, he restores that area's Light Spirit. As he restores them, the Light Spirits return Link to his Hylian form. In four months, Aonuma's team managed to present realistic horseback riding,[l] which Nintendo later revealed to the public with a trailer at Electronic Entertainment Expo 2004. The game was scheduled to be released the next year, and was no longer a follow-up to The Wind Waker; a true sequel to it was released for the Nintendo DS in 2007, in the form of Phantom Hourglass. Miyamoto explained in interviews that the graphical style was chosen to satisfy demand, and that it better fit the theme of an older incarnation of Link. The game runs on a modified The Wind Waker engine. When Link enters the Twilight Realm, the void that corrupts parts of Hyrule, he transforms into a wolf.[h] He is eventually able to transform between his Hylian and wolf forms at will. As a wolf, Link loses the ability to use his sword, shield, or any secondary items; he instead attacks by biting, and defends primarily by dodging attacks. However, ""Wolf Link"" gains several key advantages in return—he moves faster than he does as a human (though riding Epona is still faster) and digs holes to create new passages and uncover buried items, and has improved senses, including the ability to follow scent trails.[i] He also carries Midna, a small imp-like creature who gives him hints, uses an energy field to attack enemies, helps him jump long distances, and eventually allows Link to ""warp"" to any of several preset locations throughout the overworld.[j] Using Link's wolf senses, the player can see and listen to the wandering spirits of those affected by the Twilight, as well as hunt for enemy ghosts named Poes.[k] Transferring GameCube development to the Wii was relatively simple, since the Wii was being created to be compatible with the GameCube.[o] At E3 2005, Nintendo released a small number of Nintendo DS game cards containing a preview trailer for Twilight Princess. They also announced that Zelda would appear on the Wii (then codenamed ""Revolution""), but it was not clear to the media if this meant Twilight Princess or a different game. The GameCube and Wii versions feature several minor differences in their controls. The Wii version of the game makes use of the motion sensors and built-in speaker of the Wii Remote. The speaker emits the sounds of a bowstring when shooting an arrow, Midna's laugh when she gives advice to Link, and the series' trademark ""chime"" when discovering secrets. The player controls Link's sword by swinging the Wii Remote. Other attacks are triggered using similar gestures with the Nunchuk. Unique to the GameCube version is the ability for the player to control the camera freely, without entering a special ""lookaround"" mode required by the Wii; however, in the GameCube version, only two of Link's secondary weapons can be equipped at a time, as opposed to four in the Wii version.[g] A high-definition remaster of the game, The Legend of Zelda: Twilight Princess HD, is being developed by Tantalus Media for the Wii U. Officially announced during a Nintendo Direct presentation on November 12, 2015, it features enhanced graphics and Amiibo functionality. The game will be released in North America and Europe on March 4, 2016; in Australia on March 5, 2016; and in Japan on March 10, 2016. A Japan-exclusive manga series based on Twilight Princess, penned and illustrated by Akira Himekawa, was first released on February 8, 2016. The series is available solely via publisher Shogakukan's MangaOne mobile application. While the manga adaptation began almost ten years after the initial release of the game on which it is based, it launched only a month before the release of the high-definition remake. At the time of its release, Twilight Princess was considered the greatest entry in the Zelda series by many critics, including writers for 1UP.com, Computer and Video Games, Electronic Gaming Monthly, Game Informer, GamesRadar, IGN, and The Washington Post. It received several Game of the Year awards, and was the most critically acclaimed game of 2006. In 2011, the Wii version was rereleased under the Nintendo Selects label. A high-definition port for the Wii U, The Legend of Zelda: Twilight Princess HD, will be released in March 2016. The Legend of Zelda: Twilight Princess is an action-adventure game focused on combat, exploration, and item collection. It uses the basic control scheme introduced in Ocarina of Time, including context-sensitive action buttons and L-targeting (Z-targeting on the Wii), a system that allows the player to keep Link's view focused on an enemy or important object while moving and attacking. Link can walk, run, and attack, and will automatically jump when running off of or reaching for a ledge.[c] Link uses a sword and shield in combat, complemented with secondary weapons and items, including a bow and arrows, a boomerang, bombs, and the Clawshot (similar to the Hookshot introduced earlier in the The Legend of Zelda series).[d] While L-targeting, projectile-based weapons can be fired at a target without the need for manual aiming.[c] Ganondorf then revives, and Midna teleports Link and Zelda outside the castle so she can hold him off with the Fused Shadows. However, as Hyrule Castle collapses, it is revealed that Ganondorf was victorious as he crushes Midna's helmet. Ganondorf engages Link on horseback, and, assisted by Zelda and the Light Spirits, Link eventually knocks Ganondorf off his horse and they duel on foot before Link strikes down Ganondorf and plunges the Master Sword into his chest. With Ganondorf dead, the Light Spirits not only bring Midna back to life, but restore her to her true form. After bidding farewell to Link and Zelda, Midna returns home before destroying the Mirror of Twilight with a tear to maintain balance between Hyrule and the Twilight Realm. Near the end, as Hyrule Castle is rebuilt, Link is shown leaving Ordon Village heading to parts unknown. Prior Zelda games have employed a theme of two separate, yet connected, worlds. In A Link to the Past, Link travels between a ""Light World"" and a ""Dark World""; in Ocarina of Time, as well as in Oracle of Ages, Link travels between two different time periods. The Zelda team sought to reuse this motif in the series' latest installment. It was suggested that Link transform into a wolf, much like he metamorphoses into a rabbit in the Dark World of A Link to the Past.[m] The story of the game was created by Aonuma, and later underwent several changes by scenario writers Mitsuhiro Takano and Aya Kyogoku. Takano created the script for the story scenes, while Kyogoku and Takayuki Ikkaku handled the actual in-game script. Aonuma left his team working on the new idea while he directed The Minish Cap for the Game Boy Advance. When he returned, he found the Twilight Princess team struggling. Emphasis on the parallel worlds and the wolf transformation had made Link's character unbelievable. Aonuma also felt the gameplay lacked the caliber of innovation found in Phantom Hourglass, which was being developed with touch controls for the Nintendo DS. At the same time, the Wii was under development with the code name ""Revolution"". Miyamoto thought that the Revolution's pointing device, the Wii Remote, was well suited for aiming arrows in Zelda, and suggested that Aonuma consider using it.[n] The artificial intelligence (AI) of enemies in Twilight Princess is more advanced than that of enemies in The Wind Waker. Enemies react to defeated companions and to arrows or slingshot pellets that pass by, and can detect Link from a greater distance than was possible in previous games. A CD containing 20 musical selections from the game was available as a GameStop preorder bonus in the United States; it is included in all bundles in Japan, Europe, and Australia.[citation needed] Twilight Princess was released to universal critical acclaim and commercial success. It received perfect scores from major publications such as 1UP.com, Computer and Video Games, Electronic Gaming Monthly, Game Informer, GamesRadar, and GameSpy. On the review aggregators GameRankings and Metacritic, Twilight Princess has average scores of 95% and 95 for the Wii version and scores of 95% and 96 for the GameCube version. GameTrailers in their review called it one of the greatest games ever created. In 2003, Nintendo announced that a new The Legend of Zelda game was in the works for the GameCube by the same team that had created the cel-shaded The Wind Waker. At the following year's Game Developers Conference, director Eiji Aonuma unintentionally revealed that the game's sequel was in development under the working title The Wind Waker 2; it was set to use a similar graphical style to that of its predecessor. Nintendo of America told Aonuma that North American sales of The Wind Waker were sluggish because its cartoon appearance created the impression that the game was designed for a young audience. Concerned that the sequel would have the same problem, Aonuma expressed to producer Shigeru Miyamoto that he wanted to create a realistic Zelda game that would appeal to the North American market. Miyamoto, hesitant about solely changing the game's presentation, suggested the team's focus should instead be on coming up with gameplay innovations. He advised that Aonuma should start by doing what could not be done in Ocarina of Time, particularly horseback combat.[l] After gaining the Master Sword, Link is cleansed of the magic that kept him in wolf form, obtaining the Shadow Crystal. Now able to use it to switch between both forms at will, Link is led by Midna to the Mirror of Twilight located deep within the Gerudo Desert, the only known gateway between the Twilight Realm and Hyrule. However, they discover that the mirror is broken. The Sages there explain that Zant tried to destroy it, but he was only able to shatter it into fragments; only the true ruler of the Twili can completely destroy the Mirror of Twilight. They also reveal that they used it a century ago to banish Ganondorf, the Gerudo leader who attempted to steal the Triforce, to the Twilight Realm when executing him failed. Assisted by an underground resistance group they meet in Castle Town, Link and Midna set out to retrieve the missing shards of the Mirror, defeating those they infected. Once the portal has been restored, Midna is revealed to be the true ruler of the Twilight Realm, usurped by Zant when he cursed her into her current form. Confronting Zant, Link and Midna learn that Zant's coup was made possible when he forged a pact with Ganondorf, who asked for Zant's assistance in conquering Hyrule. After Link defeats Zant, Midna recovers the Fused Shadows, but destroys Zant after learning that only Ganondorf's death can release her from her curse. Returning to Hyrule, Link and Midna find Ganondorf in Hyrule Castle, with a lifeless Zelda suspended above his head. Ganondorf fights Link by possessing Zelda's body and eventually by transforming into a beast, but Link defeats him and Midna is able to resurrect Zelda. The game features nine dungeons—large, contained areas where Link battles enemies, collects items, and solves puzzles. Link navigates these dungeons and fights a boss at the end in order to obtain an item or otherwise advance the plot. The dungeons are connected by a large overworld, across which Link can travel on foot; on his horse, Epona; or by teleporting."
Web_browser,"The most recent major entrant to the browser market is Chrome, first released in September 2008. Chrome's take-up has increased significantly year by year, by doubling its usage share from 8% to 16% by August 2011. This increase seems largely to be at the expense of Internet Explorer, whose share has tended to decrease from month to month. In December 2011, Chrome overtook Internet Explorer 8 as the most widely used web browser but still had lower usage than all versions of Internet Explorer combined. Chrome's user-base continued to grow and in May 2012, Chrome's usage passed the usage of all versions of Internet Explorer combined. By April 2014, Chrome's usage had hit 45%. Although browsers are primarily intended to use the World Wide Web, they can also be used to access information provided by web servers in private networks or files in file systems. In 1993, browser software was further innovated by Marc Andreessen with the release of Mosaic, ""the world's first popular browser"", which made the World Wide Web system easy to use and more accessible to the average person. Andreesen's browser sparked the internet boom of the 1990s. The introduction of Mosaic in 1993 – one of the first graphical web browsers – led to an explosion in web use. Andreessen, the leader of the Mosaic team at National Center for Supercomputing Applications (NCSA), soon started his own company, named Netscape, and released the Mosaic-influenced Netscape Navigator in 1994, which quickly became the world's most popular browser, accounting for 90% of all web use at its peak (see usage share of web browsers). In 1998, Netscape launched what was to become the Mozilla Foundation in an attempt to produce a competitive browser using the open source software model. That browser would eventually evolve into Firefox, which developed a respectable following while still in the beta stage of development; shortly after the release of Firefox 1.0 in late 2004, Firefox (all versions) accounted for 7% of browser use. As of August 2011, Firefox has a 28% usage share. A web browser (commonly referred to as a browser) is a software application for retrieving, presenting, and traversing information resources on the World Wide Web. An information resource is identified by a Uniform Resource Identifier (URI/URL) and may be a web page, image, video or other piece of content. Hyperlinks present in resources enable users easily to navigate their browsers to related resources. A browser extension is a computer program that extends the functionality of a web browser. Every major web browser supports the development of browser extensions. This process begins when the user inputs a Uniform Resource Locator (URL), for example http://en.wikipedia.org/, into the browser. The prefix of the URL, the Uniform Resource Identifier or URI, determines how the URL will be interpreted. The most commonly used kind of URI starts with http: and identifies a resource to be retrieved over the Hypertext Transfer Protocol (HTTP). Many browsers also support a variety of other prefixes, such as https: for HTTPS, ftp: for the File Transfer Protocol, and file: for local files. Prefixes that the web browser cannot directly handle are often handed off to another application entirely. For example, mailto: URIs are usually passed to the user's default e-mail application, and news: URIs are passed to the user's default newsgroup reader. Information resources may contain hyperlinks to other information resources. Each link contains the URI of a resource to go to. When a link is clicked, the browser navigates to the resource indicated by the link's target URI, and the process of bringing content to the user begins again. Early web browsers supported only a very simple version of HTML. The rapid development of proprietary web browsers led to the development of non-standard dialects of HTML, leading to problems with interoperability. Modern web browsers support a combination of standards-based and de facto HTML and XHTML, which should be rendered in the same way by all browsers. Opera debuted in 1996; it has never achieved widespread use, having less than 2% browser usage share as of February 2012 according to Net Applications. Its Opera-mini version has an additive share, in April 2011 amounting to 1.1% of overall browser use, but focused on the fast-growing mobile phone web browser market, being preinstalled on over 40 million phones. It is also available on several other embedded systems, including Nintendo's Wii video game console. In January 2009, the European Commission announced it would investigate the bundling of Internet Explorer with Windows operating systems from Microsoft, saying ""Microsoft's tying of Internet Explorer to the Windows operating system harms competition between web browsers, undermines product innovation and ultimately reduces consumer choice."" Microsoft Corp v Commission The primary purpose of a web browser is to bring information resources to the user (""retrieval"" or ""fetching""), allowing them to view the information (""display"", ""rendering""), and then access other information (""navigation"", ""following links""). Safari and Mobile Safari were likewise always included with OS X and iOS respectively, so, similarly, they were originally funded by sales of Apple computers and mobile devices, and formed part of the overall Apple experience to customers. Microsoft responded with its Internet Explorer in 1995, also heavily influenced by Mosaic, initiating the industry's first browser war. Bundled with Windows, Internet Explorer gained dominance in the web browser market; Internet Explorer usage share peaked at over 95% by 2002. Today, most commercial web browsers are paid by search engine companies to make their engine default, or to include them as another option. For example, Google pays Mozilla, the maker of Firefox, to make Google Search the default search engine in Firefox. Mozilla makes enough money from this deal that it does not need to charge users for Firefox. In addition, Google Search is also (as one would expect) the default search engine in Google Chrome. Users searching for websites or items on the Internet would be led to Google's search results page, increasing ad revenue and which funds development at Google and of Google Chrome. Web browsers consist of a user interface, layout engine, rendering engine, JavaScript interpreter, UI backend, networking component and data persistence component. These components achieve different functionalities of a web browser and together provide all capabilities of a web browser. The first web browser was invented in 1990 by Sir Tim Berners-Lee. Berners-Lee is the director of the World Wide Web Consortium (W3C), which oversees the Web's continued development, and is also the founder of the World Wide Web Foundation. His browser was called WorldWideWeb and later renamed Nexus. Available web browsers range in features from minimal, text-based user interfaces with bare-bones support for HTML to rich user interfaces supporting a wide variety of file formats and protocols. Browsers which include additional components to support e-mail, Usenet news, and Internet Relay Chat (IRC), are sometimes referred to as ""Internet suites"" rather than merely ""web browsers"". Most browsers support HTTP Secure and offer quick and easy ways to delete the web cache, download history, form and search history, cookies, and browsing history. For a comparison of the current security vulnerabilities of browsers, see comparison of web browsers. Apple's Safari had its first beta release in January 2003; as of April 2011, it had a dominant share of Apple-based web browsing, accounting for just over 7% of the entire browser market. Most web browsers can display a list of web pages that the user has bookmarked so that the user can quickly return to them. Bookmarks are also called ""Favorites"" in Internet Explorer. In addition, all major web browsers have some form of built-in web feed aggregator. In Firefox, web feeds are formatted as ""live bookmarks"" and behave like a folder of bookmarks corresponding to recent entries in the feed. In Opera, a more traditional feed reader is included which stores and displays the contents of the feed. All major web browsers allow the user to open multiple information resources at the same time, either in different browser windows or in different tabs of the same window. Major browsers also include pop-up blockers to prevent unwanted windows from ""popping up"" without the user's consent. Internet Explorer, on the other hand, was bundled free with the Windows operating system (and was also downloadable free), and therefore it was funded partly by the sales of Windows to computer manufacturers and direct to users. Internet Explorer also used to be available for the Mac. It is likely that releasing IE for the Mac was part of Microsoft's overall strategy to fight threats to its quasi-monopoly platform dominance - threats such as web standards and Java - by making some web developers, or at least their managers, assume that there was ""no need"" to develop for anything other than Internet Explorer. In this respect, IE may have contributed to Windows and Microsoft applications sales in another way, through ""lock-in"" to Microsoft's browser. In the case of http, https, file, and others, once the resource has been retrieved the web browser will display it. HTML and associated content (image files, formatting information such as CSS, etc.) is passed to the browser's layout engine to be transformed from markup to an interactive document, a process known as ""rendering"". Aside from HTML, web browsers can generally display any kind of content that can be part of a web page. Most browsers can display images, audio, video, and XML files, and often have plug-ins to support Flash applications and Java applets. Upon encountering a file of an unsupported type or a file that is set up to be downloaded rather than displayed, the browser prompts the user to save the file to disk."
Indigenous_peoples_of_the_Americas,"Morales began work on his ""indigenous autonomy"" policy, which he launched in the eastern lowlands department on August 3, 2009, making Bolivia the first country in the history of South America to affirm the right of indigenous people to govern themselves. Speaking in Santa Cruz Department, the President called it ""a historic day for the peasant and indigenous movement"", saying that, though he might make errors, he would ""never betray the fight started by our ancestors and the fight of the Bolivian people"". A vote on further autonomy will take place in referendums which are expected to be held in December 2009. The issue has divided the country. The Maya writing system (often called hieroglyphs from a superficial resemblance to the Ancient Egyptian writing) was a combination of phonetic symbols and logograms. It is most often classified as a logographic or (more properly) a logosyllabic writing system, in which syllabic signs play a significant role. It is the only pre-Columbian writing system known to represent completely the spoken language of its community. In total, the script has more than one thousand different glyphs although a few are variations of the same sign or meaning and many appear only rarely or are confined to particular localities. At any one time, no more than about five hundred glyphs were in use, some two hundred of which (including variations) had a phonetic or syllabic interpretation. Large numbers of Bolivian highland peasants retained indigenous language, culture, customs, and communal organization throughout the Spanish conquest and the post-independence period. They mobilized to resist various attempts at the dissolution of communal landholdings and used legal recognition of ""empowered caciques"" to further communal organization. Indigenous revolts took place frequently until 1953. While the National Revolutionary Movement government begun in 1952 discouraged self-identification as indigenous (reclassifying rural people as campesinos, or peasants), renewed ethnic and class militancy re-emerged in the Katarista movement beginning in the 1970s. Lowland indigenous peoples, mostly in the east, entered national politics through the 1990 March for Territory and Dignity organized by the CIDOB confederation. That march successfully pressured the national government to sign the ILO Convention 169 and to begin the still-ongoing process of recognizing and titling indigenous territories. The 1994 Law of Popular Participation granted ""grassroots territorial organizations"" that are recognized by the state certain rights to govern local areas. Many parts of the Americas are still populated by indigenous peoples; some countries have sizable populations, especially Belize, Bolivia, Chile, Ecuador, Greenland, Guatemala, Mexico, and Peru. At least a thousand different indigenous languages are spoken in the Americas. Some, such as the Quechuan languages, Aymara, Guaraní, Mayan languages, and Nahuatl, count their speakers in millions. Many also maintain aspects of indigenous cultural practices to varying degrees, including religion, social organization, and subsistence practices. Like most cultures, over time, cultures specific to many indigenous peoples have evolved to incorporate traditional aspects, but also cater to modern needs. Some indigenous peoples still live in relative isolation from Western culture and a few are still counted as uncontacted peoples. According to both indigenous American and European accounts and documents, American civilizations at the time of European encounter had achieved many accomplishments. For instance, the Aztecs built one of the largest cities in the world, Tenochtitlan, the ancient site of Mexico City, with an estimated population of 200,000. American civilizations also displayed impressive accomplishments in astronomy and mathematics. The domestication of maize or corn required thousands of years of selective breeding. Spanish mendicants in the sixteenth century taught indigenous scribes in their communities to write their languages in Latin letters and there is a large number of local-level documents in Nahuatl, Zapotec, Mixtec, and Yucatec Maya from the colonial era, many of which were part of lawsuits and other legal matters. Although Spaniards initially taught indigenous scribes alphabetic writing, the tradition became self-perpetuating at the local level. The Spanish crown gathered such documentation and contemporary Spanish translations were made for legal cases. Scholars have translated and analyzed these documents in what is called the New Philology to write histories of indigenous peoples from indigenous viewpoints. Native American music in North America is almost entirely monophonic, but there are notable exceptions. Traditional Native American music often centers around drumming. Rattles, clappersticks, and rasps were also popular percussive instruments. Flutes were made of rivercane, cedar, and other woods. The tuning of these flutes is not precise and depends on the length of the wood used and the hand span of the intended player, but the finger holes are most often around a whole step apart and, at least in Northern California, a flute was not used if it turned out to have an interval close to a half step. The Apache fiddle is a single stringed instrument. Although not without conflict, European/Canadian early interactions with First Nations and Inuit populations were relatively peaceful compared to the experience of native peoples in the United States. Combined with a late economic development in many regions, this relatively peaceful history has allowed Canadian Indigenous peoples to have a fairly strong influence on the early national culture while preserving their own identity. From the late 18th century, European Canadians encouraged Aboriginals to assimilate into their own culture, referred to as ""Canadian culture"". These attempts reached a climax in the late 19th and early 20th centuries with forced integration. National Aboriginal Day recognises the cultures and contributions of Aboriginal peoples of Canada. There are currently over 600 recognized First Nations governments or bands encompassing 1,172,790 2006 people spread across Canada with distinctive Aboriginal cultures, languages, art, and music. The music of the indigenous peoples of Central Mexico and Central America was often pentatonic. Before the arrival of the Spaniards and other Europeans, music was inseparable from religious festivities and included a large variety of percussion and wind instruments such as drums, flutes, sea snail shells (used as a trumpet) and ""rain"" tubes. No remnants of pre-Columbian stringed instruments were found until archaeologists discovered a jar in Guatemala, attributed to the Maya of the Late Classic Era (600–900 CE), which depicts a stringed musical instrument which has since been reproduced. This instrument is one of the very few stringed instruments known in the Americas prior to the introduction of European musical instruments; when played it produces a sound virtually identical to a jaguar's growl. Ecuador was the site of many indigenous cultures, and civilizations of different proportions. An early sedentary culture, known as the Valdivia culture, developed in the coastal region, while the Caras and the Quitus unified to form an elaborate civilization that ended at the birth of the Capital Quito. The Cañaris near Cuenca were the most advanced, and most feared by the Inca, due to their fierce resistance to the Incan expansion. Their architecture remains were later destroyed by Spaniards and the Incas. The first indigenous group encountered by Columbus were the 250,000 Taínos of Hispaniola who represented the dominant culture in the Greater Antilles and the Bahamas. Within thirty years about 70% of the Taínos had died. They had no immunity to European diseases, so outbreaks of measles and smallpox ravaged their population. Increasing punishment of the Taínos for revolting against forced labour, despite measures put in place by the encomienda, which included religious education and protection from warring tribes, eventually led to the last great Taíno rebellion. The Native American name controversy is an ongoing dispute over the acceptable ways to refer to the indigenous peoples of the Americas and to broad subsets thereof, such as those living in a specific country or sharing certain cultural attributes. When discussing broader subsets of peoples, naming may be based on shared language, region, or historical relationship. Many English exonyms have been used to refer to the indigenous peoples of the Americas. Some of these names were based on foreign-language terms used by earlier explorers and colonists, while others resulted from the colonists' attempt to translate endonyms from the native language into their own, and yet others were pejorative terms arising out of prejudice and fear, during periods of conflict. Genetic history of indigenous peoples of the Americas primarily focus on Human Y-chromosome DNA haplogroups and Human mitochondrial DNA haplogroups. ""Y-DNA"" is passed solely along the patrilineal line, from father to son, while ""mtDNA"" is passed down the matrilineal line, from mother to offspring of both sexes. Neither recombines, and thus Y-DNA and mtDNA change only by chance mutation at each generation with no intermixture between parents' genetic material. Autosomal ""atDNA"" markers are also used, but differ from mtDNA or Y-DNA in that they overlap significantly. AtDNA is generally used to measure the average continent-of-ancestry genetic admixture in the entire human genome and related isolated populations. About five percent of the population are of full-blooded indigenous descent, but upwards to eighty percent more or the majority of Hondurans are mestizo or part-indigenous with European admixture, and about ten percent are of indigenous or African descent. The main concentration of indigenous in Honduras are in the rural westernmost areas facing Guatemala and to the Caribbean Sea coastline, as well on the Nicaraguan border. The majority of indigenous people are Lencas, Miskitos to the east, Mayans, Pech, Sumos, and Tolupan. Indigenous peoples of Brazil make up 0.4% of Brazil's population, or about 700,000 people, even though millions of Brazilians have some indigenous ancestry. Indigenous peoples are found in the entire territory of Brazil, although the majority of them live in Indian reservations in the North and Center-Western part of the country. On January 18, 2007, FUNAI reported that it had confirmed the presence of 67 different uncontacted tribes in Brazil, up from 40 in 2005. With this addition Brazil has now overtaken the island of New Guinea as the country having the largest number of uncontacted tribes. Indigenous peoples in what is now the contiguous United States, including their descendants, are commonly called ""American Indians"", or simply ""Indians"" domestically, or ""Native Americans"" by the USCB. In Alaska, indigenous peoples belong to 11 cultures with 11 languages. These include the St. Lawrence Island Yupik, Iñupiat, Athabaskan, Yup'ik, Cup'ik, Unangax, Alutiiq, Eyak, Haida, Tsimshian, and Tlingit, who are collectively called Alaska Natives. Indigenous Polynesian peoples, which include Marshallese, Samoan, Tahitian, and Tongan, are politically considered Pacific Islands American but are geographically and culturally distinct from indigenous peoples of the Americas. However, since the 20th century, indigenous peoples in the Americas have been more vocal about the ways they wish to be referred to, pressing for the elimination of terms widely considered to be obsolete, inaccurate, or racist. During the latter half of the 20th century and the rise of the Indian rights movement, the United States government responded by proposing the use of the term ""Native American,"" to recognize the primacy of indigenous peoples' tenure in the nation, but this term was not fully accepted. Other naming conventions have been proposed and used, but none are accepted by all indigenous groups. While technically referring to the era before Christopher Columbus' voyages of 1492 to 1504, in practice the term usually includes the history of American indigenous cultures until Europeans either conquered or significantly influenced them, even if this happened decades or even centuries after Columbus' initial landing. ""Pre-Columbian"" is used especially often in the context of discussing the great indigenous civilizations of the Americas, such as those of Mesoamerica (the Olmec, the Toltec, the Teotihuacano, the Zapotec, the Mixtec, the Aztec, and the Maya civilizations) and those of the Andes (Inca Empire, Moche culture, Muisca Confederation, Cañaris). Application of the term ""Indian"" originated with Christopher Columbus, who, in his search for Asia, thought that he had arrived in the East Indies. The Americas came to be known as the ""West Indies"", a name still used to refer to the islands of the Caribbean Sea. This led to the names ""Indies"" and ""Indian"", which implied some kind of racial or cultural unity among the aboriginal peoples of the Americas. This unifying concept, codified in law, religion, and politics, was not originally accepted by indigenous peoples but has been embraced by many over the last two centuries.[citation needed] Even though the term ""Indian"" does not include the Aleuts, Inuit, or Yupik peoples, these groups are considered indigenous peoples of the Americas. About 5% of the Nicaraguan population are indigenous. The largest indigenous group in Nicaragua is the Miskito people. Their territory extended from Cape Camarón, Honduras, to Rio Grande, Nicaragua along the Mosquito Coast. There is a native Miskito language, but large groups speak Miskito Coast Creole, Spanish, Rama and other languages. The Creole English came about through frequent contact with the British who colonized the area. Many are Christians. Traditional Miskito society was highly structured with a defined political structure. There was a king, but he did not have total power. Instead, the power was split between himself, a governor, a general, and by the 1750s, an admiral. Historical information on kings is often obscured by the fact that many of the kings were semi-mythical. Another major group is the Mayangna (or Sumu) people, counting some 10,000 people. The territory of modern-day Mexico was home to numerous indigenous civilizations prior to the arrival of the Spanish conquistadores: The Olmecs, who flourished from between 1200 BCE to about 400 BCE in the coastal regions of the Gulf of Mexico; the Zapotecs and the Mixtecs, who held sway in the mountains of Oaxaca and the Isthmus of Tehuantepec; the Maya in the Yucatan (and into neighbouring areas of contemporary Central America); the Purépecha in present-day Michoacán and surrounding areas, and the Aztecs/Mexica, who, from their central capital at Tenochtitlan, dominated much of the centre and south of the country (and the non-Aztec inhabitants of those areas) when Hernán Cortés first landed at Veracruz. Native Americans in the United States make up 0.97% to 2% of the population. In the 2010 census, 2.9 million people self-identified as Native American, Native Hawaiian, and Alaska Native alone, and 5.2 million people identified as U.S. Native Americans, either alone or in combination with one or more ethnicity or other races. 1.8 million are recognized as enrolled tribal members.[citation needed] Tribes have established their own criteria for membership, which are often based on blood quantum, lineal descent, or residency. A minority of US Native Americans live in land units called Indian reservations. Some California and Southwestern tribes, such as the Kumeyaay, Cocopa, Pascua Yaqui and Apache span both sides of the US–Mexican border. Haudenosaunee people have the legal right to freely cross the US–Canadian border. Athabascan, Tlingit, Haida, Tsimshian, Iñupiat, Blackfeet, Nakota, Cree, Anishinaabe, Huron, Lenape, Mi'kmaq, Penobscot, and Haudenosaunee, among others live in both Canada and the US. The indigenous peoples of the Americas are the descendants of the pre-Columbian inhabitants of the Americas. Pueblos indígenas (indigenous peoples) is a common term in Spanish-speaking countries. Aborigen (aboriginal/native) is used in Argentina, whereas ""Amerindian"" is used in Quebec, The Guianas, and the English-speaking Caribbean. Indigenous peoples are commonly known in Canada as Aboriginal peoples, which include First Nations, Inuit, and Métis peoples. Indigenous peoples of the United States are commonly known as Native Americans or American Indians, and Alaska Natives. Indigenous population in Peru make up around 45%. Native Peruvian traditions and customs have shaped the way Peruvians live and see themselves today. Cultural citizenship—or what Renato Rosaldo has called, ""the right to be different and to belong, in a democratic, participatory sense"" (1996:243)—is not yet very well developed in Peru. This is perhaps no more apparent than in the country's Amazonian regions where indigenous societies continue to struggle against state-sponsored economic abuses, cultural discrimination, and pervasive violence. Genetic studies of mitochondrial DNA (mtDNA) of Amerindians and some Siberian and Central Asian peoples also revealed that the gene pool of the Turkic-speaking peoples of Siberia such as Altaians, Khakas, Shors and Soyots, living between the Altai and Lake Baikal along the Sayan mountains, are genetically closest to Amerindians.[citation needed] This view is shared by other researchers who argue that ""the ancestors of the American Indians were the first to separate from the great Asian population in the Middle Paleolithic. 2012 research found evidence for a recent common ancestry between Native Americans and indigenous Altaians based on mitochondrial DNA and Y-Chromosome analysis. The paternal lineages of Altaians mostly belong to the subclades of haplogroup P-M45 (xR1a 38-93%; xQ1a 4-32%). Aboriginal peoples in Canada comprise the First Nations, Inuit and Métis; the descriptors ""Indian"" and ""Eskimo"" are falling into disuse, and other than in neighboring Alaska. ""Eskimo"" is considered derogatory in many other places because it was given by non-Inuit people and was said to mean ""eater of raw meat."" Hundreds of Aboriginal nations evolved trade, spiritual and social hierarchies. The Métis culture of mixed blood originated in the mid-17th century when First Nation and native Inuit married European settlers. The Inuit had more limited interaction with European settlers during that early period. Various laws, treaties, and legislation have been enacted between European immigrants and First Nations across Canada. Aboriginal Right to Self-Government provides opportunity to manage historical, cultural, political, health care and economic control aspects within first people's communities. Following years of mistreatment, the Taínos began to adopt suicidal behaviors, with women aborting or killing their infants and men jumping from the cliffs or ingesting untreated cassava, a violent poison. Eventually, a Taíno Cacique named Enriquillo managed to hold out in the Baoruco Mountain Range for thirteen years, causing serious damage to the Spanish, Carib-held plantations and their Indian auxiliaries. Hearing of the seriousness of the revolt, Emperor Charles V (also King of Spain) sent captain Francisco Barrionuevo to negotiate a peace treaty with the ever-increasing number of rebels. Two months later, after consultation with the Audencia of Santo Domingo, Enriquillo was offered any part of the island to live in peace. The Spanish Empire and other Europeans brought horses to the Americas. Some of these animals escaped and began to breed and increase their numbers in the wild. The re-introduction of the horse, extinct in the Americas for over 7500 years, had a profound impact on Native American culture in the Great Plains of North America and of Patagonia in South America. By domesticating horses, some tribes had great success: horses enabled them to expand their territories, exchange more goods with neighboring tribes, and more easily capture game, especially bison. In recent years, there has been a rise of indigenous movements in the Americas (mainly South America). These are rights-driven groups that organize themselves in order to achieve some sort of self-determination and the preservation of their culture for their peoples. Organizations like the Coordinator of Indigenous Organizations of the Amazon River Basin and the Indian Council of South America are examples of movements that are breaking the barrier of borders in order to obtain rights for Amazonian indigenous populations everywhere. Similar movements for indigenous rights can also be seen in Canada and the United States, with movements like the International Indian Treaty Council and the accession of native Indian group into the Unrepresented Nations and Peoples Organization. In Bolivia, a 62% majority of residents over the age of 15 self-identify as belonging to an indigenous people, while another 3.7% grew up with an indigenous mother tongue yet do not self-identify as indigenous. Including both of these categories, and children under 15, some 66.4% of Bolivia's population was registered as indigenous in the 2001 Census. The largest indigenous ethnic groups are: Quechua, about 2.5 million people; Aymara, 2.0 million; Chiquitano, 181,000; Guaraní, 126,000; and Mojeño, 69,000. Some 124,000 belong to smaller indigenous groups. The Constitution of Bolivia, enacted in 2009, recognizes 36 cultures, each with its own language, as part of a plurinational state. Some groups, including CONAMAQ (the National Council of Ayllus and Markas of Qullasuyu) draw ethnic boundaries within the Quechua- and Aymara-speaking population, resulting in a total of fifty indigenous peoples native to Bolivia. Many crops first domesticated by indigenous Americans are now produced and used globally. Chief among these is maize or ""corn"", arguably the most important crop in the world. Other significant crops include cassava, chia, squash (pumpkins, zucchini, marrow, acorn squash, butternut squash), the pinto bean, Phaseolus beans including most common beans, tepary beans and lima beans, tomatoes, potatoes, avocados, peanuts, cocoa beans (used to make chocolate), vanilla, strawberries, pineapples, Peppers (species and varieties of Capsicum, including bell peppers, jalapeños, paprika and chili peppers) sunflower seeds, rubber, brazilwood, chicle, tobacco, coca, manioc and some species of cotton. Human settlement of the New World occurred in stages from the Bering sea coast line, with an initial 15,000 to 20,000-year layover on Beringia for the small founding population. The micro-satellite diversity and distributions of the Y lineage specific to South America indicates that certain indigenous peoples of the Americas populations have been isolated since the initial colonization of the region. The Na-Dené, Inuit and Indigenous Alaskan populations exhibit haplogroup Q (Y-DNA) mutations, however are distinct from other indigenous peoples of the Americas with various mtDNA and atDNA mutations. This suggests that the earliest migrants into the northern extremes of North America and Greenland derived from later migrant populations. Many pre-Columbian civilizations established characteristics and hallmarks which included permanent or urban settlements, agriculture, civic and monumental architecture, and complex societal hierarchies. Some of these civilizations had long faded by the time of the first significant European and African arrivals (ca. late 15th–early 16th centuries), and are known only through oral history and through archaeological investigations. Others were contemporary with this period, and are also known from historical accounts of the time. A few, such as the Mayan, Olmec, Mixtec, and Nahua peoples, had their own written records. However, the European colonists of the time worked to eliminate non-Christian beliefs, and Christian pyres destroyed many pre-Columbian written records. Only a few documents remained hidden and survived, leaving contemporary historians with glimpses of ancient culture and knowledge. The South American highlands were a center of early agriculture. Genetic testing of the wide variety of cultivars and wild species suggests that the potato has a single origin in the area of southern Peru, from a species in the Solanum brevicaule complex. Over 99% of all modern cultivated potatoes worldwide are descendants of a subspecies indigenous to south-central Chile, Solanum tuberosum ssp. tuberosum, where it was cultivated as long as 10,000 years ago. According to George Raudzens, ""It is clear that in pre-Columbian times some groups struggled to survive and often suffered food shortages and famines, while others enjoyed a varied and substantial diet."" The persistent drought around 850 AD coincided with the collapse of Classic Maya civilization, and the famine of One Rabbit (AD 1454) was a major catastrophe in Mexico. In 2005, Argentina's indigenous population (known as pueblos originarios) numbered about 600,329 (1.6% of total population); this figure includes 457,363 people who self-identified as belonging to an indigenous ethnic group and 142,966 who identified themselves as first-generation descendants of an indigenous people. The ten most populous indigenous peoples are the Mapuche (113,680 people), the Kolla (70,505), the Toba (69,452), the Guaraní (68,454), the Wichi (40,036), the Diaguita-Calchaquí (31,753), the Mocoví (15,837), the Huarpe (14,633), the Comechingón (10,863) and the Tehuelche (10,590). Minor but important peoples are the Quechua (6,739), the Charrúa (4,511), the Pilagá (4,465), the Chané (4,376), and the Chorote (2,613). The Selknam (Ona) people are now virtually extinct in its pure form. The languages of the Diaguita, Tehuelche, and Selknam nations have become extinct or virtually extinct: the Cacán language (spoken by Diaguitas) in the 18th century and the Selknam language in the 20th century; one Tehuelche language (Southern Tehuelche) is still spoken by a handful of elderly people. Over the course of thousands of years, American indigenous peoples domesticated, bred and cultivated a large array of plant species. These species now constitute 50–60% of all crops in cultivation worldwide. In certain cases, the indigenous peoples developed entirely new species and strains through artificial selection, as was the case in the domestication and breeding of maize from wild teosinte grasses in the valleys of southern Mexico. Numerous such agricultural products retain their native names in the English and Spanish lexicons. Much of El Salvador was home to the Pipil, the Lenca, Xinca, and Kakawira. The Pipil lived in western El Salvador, spoke Nawat, and had many settlements there, most noticeably Cuzcatlan. The Pipil had no precious mineral resources, but they did have rich and fertile land that was good for farming. The Spaniards were disappointed not to find gold or jewels in El Salvador as they had in other lands like Guatemala or Mexico, but upon learning of the fertile land in El Salvador, they attempted to conquer it. Noted Meso-American indigenous warriors to rise militarily against the Spanish included Princes Atonal and Atlacatl of the Pipil people in central El Salvador and Princess Antu Silan Ulap of the Lenca people in eastern El Salvador, who saw the Spanish not as gods but as barbaric invaders. After fierce battles, the Pipil successfully fought off the Spanish army led by Pedro de Alvarado along with their Mexican Indian allies (the Tlaxcalas), sending them back to Guatemala. After many other attacks with an army reinforced with Guatemalan Indian allies, the Spanish were able to conquer Cuzcatlan. After further attacks, the Spanish also conquered the Lenca people. Eventually, the Spaniards intermarried with Pipil and Lenca women, resulting in the Mestizo population which would become the majority of the Salvadoran people. Today many Pipil and other indigenous populations live in the many small towns of El Salvador like Izalco, Panchimalco, Sacacoyo, and Nahuizalco. The specifics of Paleo-Indian migration to and throughout the Americas, including the exact dates and routes traveled, provide the subject of ongoing research and discussion. According to archaeological and genetic evidence, North and South America were the last continents in the world with human habitation. During the Wisconsin glaciation, 50–17,000 years ago, falling sea levels allowed people to move across the land bridge of Beringia that joined Siberia to north west North America (Alaska). Alaska was a glacial refugia because it had low snowfall, allowing a small population to exist. The Laurentide Ice Sheet covered most of North America, blocking nomadic inhabitants and confining them to Alaska (East Beringia) for thousands of years. Approximately 96.4% of Ecuador's Indigenous population are Highland Quichuas living in the valleys of the Sierra region. Primarily consisting of the descendents of Incans, they are Kichwa speakers and include the Caranqui, the Otavalos, the Cayambi, the Quitu-Caras, the Panzaleo, the Chimbuelo, the Salasacan, the Tugua, the Puruhá, the Cañari, and the Saraguro. Linguistic evidence suggests that the Salascan and the Saraguro may have been the descendants of Bolivian ethnic groups transplanted to Ecuador as mitimaes. The development of writing is counted among the many achievements and innovations of pre-Columbian American cultures. Independent from the development of writing in other areas of the world, the Mesoamerican region produced several indigenous writing systems beginning in the 1st millennium BCE. What may be the earliest-known example in the Americas of an extensive text thought to be writing is by the Cascajal Block. The Olmec hieroglyphs tablet has been indirectly dated from ceramic shards found in the same context to approximately 900 BCE, around the time that Olmec occupation of San Lorenzo Tenochtitlán began to wane. Although some indigenous peoples of the Americas were traditionally hunter-gatherers—and many, especially in Amazonia, still are—many groups practiced aquaculture and agriculture. The impact of their agricultural endowment to the world is a testament to their time and work in reshaping and cultivating the flora indigenous to the Americas. Although some societies depended heavily on agriculture, others practiced a mix of farming, hunting, and gathering. In some regions the indigenous peoples created monumental architecture, large-scale organized cities, chiefdoms, states, and empires. Visual arts by indigenous peoples of the Americas comprise a major category in the world art collection. Contributions include pottery, paintings, jewellery, weavings, sculptures, basketry, carvings, and beadwork. Because too many artists were posing as Native Americans and Alaska Natives in order to profit from the caché of Indigenous art in the United States, the U.S. passed the Indian Arts and Crafts Act of 1990, requiring artists to prove that they are enrolled in a state or federally recognized tribe. To support the ongoing practice of American Indian, Alaska Native and Native Hawaiian arts and cultures in the United States, the Ford Foundation, arts advocates and American Indian tribes created an endowment seed fund and established a national Native Arts and Cultures Foundation in 2007. Most Venezuelans have some indigenous heritage, but the indigenous population make up only around 2% of the total population. They speak around 29 different languages and many more dialects, but some of the ethnic groups are very small and their languages are in danger of becoming extinct in the next decades. The most important indigenous groups are the Ye'kuana, the Wayuu, the Pemon and the Warao. The most advanced native people to have lived in present-day Venezuela is thought to have been the Timoto-cuicas, who mainly lived in the Venezuelan Andes. In total it is estimated that there were between 350 thousand and 500 thousand inhabitants, the most densely populated area being the Andean region (Timoto-cuicas), thanks to the advanced agricultural techniques used. A route through Beringia is seen as more likely than the Solutrean hypothesis. Kashani et al. 2012 state that ""The similarities in ages and geographical distributions for C4c and the previously analyzed X2a lineage provide support to the scenario of a dual origin for Paleo-Indians. Taking into account that C4c is deeply rooted in the Asian portion of the mtDNA phylogeny and is indubitably of Asian origin, the finding that C4c and X2a are characterized by parallel genetic histories definitively dismisses the controversial hypothesis of an Atlantic glacial entry route into North America."" Contact with European diseases such as smallpox and measles killed between 50 and 67 per cent of the Aboriginal population of North America in the first hundred years after the arrival of Europeans. Some 90 per cent of the native population near Massachusetts Bay Colony died of smallpox in an epidemic in 1617–1619. In 1633, in Plymouth, the Native Americans there were exposed to smallpox because of contact with Europeans. As it had done elsewhere, the virus wiped out entire population groups of Native Americans. It reached Lake Ontario in 1636, and the lands of the Iroquois by 1679. During the 1770s, smallpox killed at least 30% of the West Coast Native Americans. The 1775–82 North American smallpox epidemic and 1837 Great Plains smallpox epidemic brought devastation and drastic population depletion among the Plains Indians. In 1832, the federal government of the United States established a smallpox vaccination program for Native Americans (The Indian Vaccination Act of 1832). Indigenous genetic studies suggest that the first inhabitants of the Americas share a single ancestral population, one that developed in isolation, conjectured to be Beringia. The isolation of these peoples in Beringia might have lasted 10–20,000 years. Around 16,500 years ago, the glaciers began melting, allowing people to move south and east into Canada and beyond. These people are believed to have followed herds of now-extinct Pleistocene megafauna along ice-free corridors that stretched between the Laurentide and Cordilleran Ice Sheets. Various theories for the decline of the Native American populations emphasize epidemic diseases, conflicts with Europeans, and conflicts among warring tribes. Scholars now believe that, among the various contributing factors, epidemic disease was the overwhelming cause of the population decline of the American natives. Some believe that after first contacts with Europeans and Africans, Old World diseases caused the death of 90 to 95% of the native population of the New World in the following 150 years. Smallpox killed up to one third of the native population of Hispaniola in 1518. By killing the Incan ruler Huayna Capac, smallpox caused the Inca Civil War. Smallpox was only the first epidemic. Typhus (probably) in 1546, influenza and smallpox together in 1558, smallpox again in 1589, diphtheria in 1614, measles in 1618—all ravaged the remains of Inca culture. According to the prevailing theories of the settlement of the Americas, migrations of humans from Asia (in particular North Asia) to the Americas took place via Beringia, a land bridge which connected the two continents across what is now the Bering Strait. The majority of experts agree that the earliest pre-modern human migration via Beringia took place at least 13,500 years ago, with disputed evidence that people had migrated into the Americas much earlier, up to 40,000 years ago. These early Paleo-Indians spread throughout the Americas, diversifying into many hundreds of culturally distinct nations and tribes. According to the oral histories of many of the indigenous peoples of the Americas, they have been living there since their genesis, described by a wide range of creation myths. According to the 2002 Census, 4.6% of the Chilean population, including the Rapanui (a Polynesian people) of Easter Island, was indigenous, although most show varying degrees of mixed heritage. Many are descendants of the Mapuche, and live in Santiago, Araucanía and the lake district. The Mapuche successfully fought off defeat in the first 300–350 years of Spanish rule during the Arauco War. Relations with the new Chilean Republic were good until the Chilean state decided to occupy their lands. During the Occupation of Araucanía the Mapuche surrendered to the country's army in the 1880s. Their land was opened to settlement by Chileans and Europeans. Conflict over Mapuche land rights continues to the present. The ""General Law of Linguistic Rights of the Indigenous Peoples"" grants all indigenous languages spoken in Mexico, regardless of the number of speakers, the same validity as Spanish in all territories in which they are spoken, and indigenous peoples are entitled to request some public services and documents in their native languages. Along with Spanish, the law has granted them — more than 60 languages — the status of ""national languages"". The law includes all indigenous languages of the Americas regardless of origin; that is, it includes the indigenous languages of ethnic groups non-native to the territory. As such the National Commission for the Development of Indigenous Peoples recognizes the language of the Kickapoo, who immigrated from the United States, and recognizes the languages of the Guatemalan indigenous refugees. The Mexican government has promoted and established bilingual primary and secondary education in some indigenous rural communities. Nonetheless, of the indigenous peoples in Mexico, only about 67% of them (or 5.4% of the country's population) speak an indigenous language and about a sixth do not speak Spanish (1.2% of the country's population). Natives of North America began practicing farming approximately 4,000 years ago, late in the Archaic period of North American cultures. Technology had advanced to the point that pottery was becoming common and the small-scale felling of trees had become feasible. Concurrently, the Archaic Indians began using fire in a controlled manner. Intentional burning of vegetation was used to mimic the effects of natural fires that tended to clear forest understories. It made travel easier and facilitated the growth of herbs and berry-producing plants, which were important for both food and medicines. A 2013 study in Nature reported that DNA found in the 24,000-year-old remains of a young boy from the archaeological Mal'ta-Buret' culture suggest that up to one-third of the indigenous Americans may have ancestry that can be traced back to western Eurasians, who may have ""had a more north-easterly distribution 24,000 years ago than commonly thought"". ""We estimate that 14 to 38 percent of Native American ancestry may originate through gene flow from this ancient population,"" the authors wrote. Professor Kelly Graf said, The European colonization of the Americas forever changed the lives and cultures of the peoples of the continents. Although the exact pre-contact population of the Americas is unknown, scholars estimate that Native American populations diminished by between 80 and 90% within the first centuries of contact with Europeans. The leading cause was disease. The continent was ravaged by epidemics of diseases such as smallpox, measles, and cholera, which were brought from Europe by the early explorers and spread quickly into new areas even before later explorers and colonists reached them. Native Americans suffered high mortality rates due to their lack of prior exposure to these diseases. The loss of lives was exacerbated by conflict between colonists and indigenous people. Colonists also frequently perpetrated massacres on the indigenous groups and enslaved them. According to the U.S. Bureau of the Census (1894), the North American Indian Wars of the 19th century cost the lives of about 19,000 whites and 30,000 Native Americans. Representatives from indigenous and rural organizations from major South American countries, including Bolivia, Ecuador, Colombia, Chile and Brazil, started a forum in support of Morales' legal process of change. The meeting condemned plans by the European ""foreign power elite"" to destabilize the country. The forum also expressed solidarity with the Morales and his economic and social changes in the interest of historically marginalized majorities. Furthermore, in a cathartic blow to the US-backed elite, it questioned US interference through diplomats and NGOs. The forum was suspicious of plots against Bolivia and other countries, including Cuba, Venezuela, Ecuador, Paraguay and Nicaragua. Cultural practices in the Americas seem to have been shared mostly within geographical zones where unrelated peoples adopted similar technologies and social organizations. An example of such a cultural area is Mesoamerica, where millennia of coexistence and shared development among the peoples of the region produced a fairly homogeneous culture with complex agricultural and social patterns. Another well-known example is the North American plains where until the 19th century several peoples shared the traits of nomadic hunter-gatherers based primarily on buffalo hunting. The data indicate that the individual was from a population directly ancestral to present South American and Central American Native American populations, and closely related to present North American Native American populations. The implication is that there was an early divergence between North American and Central American plus South American populations. Hypotheses which posit that invasions subsequent to the Clovis culture overwhelmed or assimilated previous migrants into the Americas were ruled out."
Endangered_Species_Act,"A species can be listed in two ways. The United States Fish and Wildlife Service (FWS) or NOAA Fisheries (also called the National Marine Fisheries Service) can directly list a species through its candidate assessment program, or an individual or organizational petition may request that the FWS or NMFS list a species. A ""species"" under the act can be a true taxonomic species, a subspecies, or in the case of vertebrates, a ""distinct population segment."" The procedures are the same for both types except with the person/organization petition, there is a 90-day screening period. Fish and Wildlife Service (FWS) and National Marine Fisheries Service (NMFS) are required to create an Endangered Species Recovery Plan outlining the goals, tasks required, likely costs, and estimated timeline to recover endangered species (i.e., increase their numbers and improve their management to the point where they can be removed from the endangered list). The ESA does not specify when a recovery plan must be completed. The FWS has a policy specifying completion within three years of the species being listed, but the average time to completion is approximately six years. The annual rate of recovery plan completion increased steadily from the Ford administration (4) through Carter (9), Reagan (30), Bush I (44), and Clinton (72), but declined under Bush II (16 per year as of 9/1/06). Section 6 of the Endangered Species Act provided funding for development of programs for management of threatened and endangered species by state wildlife agencies. Subsequently, lists of endangered and threatened species within their boundaries have been prepared by each state. These state lists often include species which are considered endangered or threatened within a specific state but not within all states, and which therefore are not included on the national list of endangered and threatened species. Examples include Florida, Minnesota, Maine, and California. Opponents of the Endangered Species Act argue that with over 2,000 endangered species listed, and only 28 delisted due to recovery, the success rate of 1% over nearly three decades proves that there needs to be serious reform in their methods to actually help the endangered animals and plants. Others argue that the ESA may encourage preemptive habitat destruction by landowners who fear losing the use of their land because of the presence of an endangered species; known colloquially as ""Shoot, Shovel and Shut-Up."" One example of such perverse incentives is the case of a forest owner who, in response to ESA listing of the red-cockaded woodpecker, increased harvesting and shortened the age at which he harvests his trees to ensure that they do not become old enough to become suitable habitat. While no studies have shown that the Act's negative effects, in total, exceed the positive effects, many economists believe that finding a way to reduce such perverse incentives would lead to more effective protection of endangered species. One species in particular received widespread attention—the whooping crane. The species' historical range extended from central Canada South to Mexico, and from Utah to the Atlantic coast. Unregulated hunting and habitat loss contributed to a steady decline in the whooping crane population until, by 1890, it had disappeared from its primary breeding range in the north central United States. It would be another eight years before the first national law regulating wildlife commerce was signed, and another two years before the first version of the endangered species act was passed. The whooping crane population by 1941 was estimated at about only 16 birds still in the wild. The ""Safe Harbor"" agreement is a voluntary agreement between the private landowner and FWS. The landowner agrees to alter the property to benefit or even attract a listed or proposed species in exchange for assurances that the FWS will permit future ""takes"" above a pre-determined level. The policy relies on the ""enhancement of survival"" provision of Section §1539(a)(1)(A). A landowner can have either a ""Safe Harbor"" agreement or an Incidental Take Permit, or both. The policy was developed by the Clinton Administration in 1999. Green and the CPI further noted another exploit of the ESA in their discussion of the critically endangered cotton-top tamarin (Saguinus oedipus). Not only had they found documentation that 151 of these primates had inadvertently made their way from the Harvard-affiliated New England Regional Primate Research Center into the exotic pet trade through the aforementioned loophole, but in October 1976, over 800 cotton-top tamarins were imported into the United States in order to beat the official listing of the species under the ESA. Long before the exemption is considered by the Endangered Species Committee, the Forest Service, and either the FWS or the NMFS will have consulted on the biological implications of the timber harvest. The consultation can be informal, to determine if harm may occur; and then formal if the harm is believed to be likely. The questions to be answered in these consultations are whether the species will be harmed, whether the habitat will be harmed and if the action will aid or hinder the recovery of the listed species. More than half of habitat for listed species is on non-federal property, owned by citizens, states, local governments, tribal governments and private organizations. Before the law was amended in 1982, a listed species could be taken only for scientific or research purposes. The amendment created a permit process to circumvent the take prohibition called a Habitat Conservation Plan or HCP to give incentives to non-federal land managers and private landowners to help protect listed and unlisted species, while allowing economic development that may harm (""take"") the species. A reward will be paid to any person who furnishes information which leads to an arrest, conviction, or revocation of a license, so long as they are not a local, state, or federal employee in the performance of official duties. The Secretary may also provide reasonable and necessary costs incurred for the care of fish, wildlife, and forest service or plant pending the violation caused by the criminal. If the balance ever exceeds $500,000 the Secretary of the Treasury is required to deposit an amount equal to the excess into the cooperative endangered species conservation fund. The ESA requires that critical habitat be designated at the time of or within one year of a species being placed on the endangered list. In practice, most designations occur several years after listing. Between 1978 and 1986 the FWS regularly designated critical habitat. In 1986 the Reagan Administration issued a regulation limiting the protective status of critical habitat. As a result, few critical habitats were designated between 1986 and the late 1990s. In the late 1990s and early 2000s, a series of court orders invalidated the Reagan regulations and forced the FWS and NMFS to designate several hundred critical habitats, especially in Hawaii, California and other western states. Midwest and Eastern states received less critical habitat, primarily on rivers and coastlines. As of December, 2006, the Reagan regulation has not yet been replaced though its use has been suspended. Nonetheless, the agencies have generally changed course and since about 2005 have tried to designate critical habitat at or near the time of listing. The person or organization submits a HCP and if approved by the agency (FWS or NMFS), will be issued an Incidental Take Permit (ITP) which allows a certain number of ""takes"" of the listed species. The permit may be revoked at any time and can allow incidental takes for varying amounts of time. For instance, the San Bruno Habitat Conservation Plan/ Incidental Take Permit is good for 30 years and the Wal-Mart store (in Florida) permit expires after one year. Because the permit is issued by a federal agency to a private party, it is a federal action-which means other federal laws can apply, such as the National Environmental Policy Act or NEPA. A notice of the permit application action is published in the Federal Register and a public comment period of 30 to 90 days begins. The Candidate Conservation Agreement is closely related to the ""Safe Harbor"" agreement, the main difference is that the Candidate Conservation Agreements With Assurances(CCA) are meant to protect unlisted species by providing incentives to private landowners and land managing agencies to restore, enhance or maintain habitat of unlisted species which are declining and have the potential to become threatened or endangered if critical habitat is not protected. The FWS will then assure that if, in the future the unlisted species becomes listed, the landowner will not be required to do more than already agreed upon in the CCA. This first list is referred to as the ""Class of '67"" in The Endangered Species Act at Thirty, Volume 1, which concludes that habitat destruction, the biggest threat to those 78 species, is still the same threat to the currently listed species. It included only vertebrates because the Department of Interior's definition of ""fish and wildlife"" was limited to vertebrates. However, with time, researchers noticed that the animals on the endangered species list still were not getting enough protection, thus further threatening their extinction. The endangered species program was expanded by the Endangered Species Act of 1969. The Lacey Act of 1900 was the first federal law that regulated commercial animal markets. It prohibited interstate commerce of animals killed in violation of state game laws, and covered all fish and wildlife and their parts or products, as well as plants. Other legislation followed, including the Migratory Bird Conservation Act of 1929, a 1937 treaty prohibiting the hunting of right and gray whales, and the Bald Eagle Protection Act of 1940. These later laws had a low cost to society–the species were relatively rare–and little opposition was raised. According to research published in 1999 by Alan Green and the Center for Public Integrity (CPI), loopholes in the ESA are commonly exploited in the exotic pet trade. Although the legislation prohibits interstate and foreign transactions for list species, no provisions are made for in-state commerce, allowing these animals to be sold to roadside zoos and private collectors. Additionally, the ESA allows listed species to be shipped across state lines as long as they are not sold. According to Green and the CPI, this allows dealers to ""donate"" listed species through supposed ""breeding loans"" to anyone, and in return they can legally receive a reciprocal monetary ""donation"" from the receiving party. Furthermore, an interview with an endangered species specialist at the US Fish and Wildlife Service revealed that the agency does not have sufficient staff to perform undercover investigations, which would catch these false ""donations"" and other mislabeled transactions. The provision of the law in Section 4 that establishes critical habitat is a regulatory link between habitat protection and recovery goals, requiring the identification and protection of all lands, water and air necessary to recover endangered species. To determine what exactly is critical habitat, the needs of open space for individual and population growth, food, water, light or other nutritional requirements, breeding sites, seed germination and dispersal needs, and lack of disturbances are considered. The Endangered Species Conservation Act (P. L. 91-135), passed in December, 1969, amended the original law to provide additional protection to species in danger of ""worldwide extinction"" by prohibiting their importation and subsequent sale in the United States. It expanded the Lacey Act's ban on interstate commerce to include mammals, reptiles, amphibians, mollusks and crustaceans. Reptiles were added mainly to reduce the rampant poaching of alligators and crocodiles. This law was the first time that invertebrates were included for protection. Two examples of animal species recently delisted are: the Virginia northern flying squirrel (subspecies) on August, 2008, which had been listed since 1985, and the gray wolf (Northern Rocky Mountain DPS). On April 15, 2011, President Obama signed the Department of Defense and Full-Year Appropriations Act of 2011. A section of that Appropriations Act directed the Secretary of the Interior to reissue within 60 days of enactment the final rule published on April 2, 2009, that identified the Northern Rocky Mountain population of gray wolf (Canis lupus) as a distinct population segment (DPS) and to revise the List of Endangered and Threatened Wildlife by removing most of the gray wolves in the DPS. Growing scientific recognition of the role of private lands for endangered species recovery and the landmark 1981 court decision in Palila v. Hawaii Department of Land and Natural Resources both contributed to making Habitat Conservation Plans/ Incidental Take Permits ""a major force for wildlife conservation and a major headache to the development community"", wrote Robert D.Thornton in the 1991 Environmental Law article, Searching for Consensus and Predictability: Habitat Conservation Planning under the Endangered Species Act of 1973. The question to be answered is whether a listed species will be harmed by the action and, if so, how the harm can be minimized. If harm cannot be avoided, the project agency can seek an exemption from the Endangered Species Committee, an ad hoc panel composed of members from the executive branch and at least one appointee from the state where the project is to occur. Five of the seven committee members must vote for the exemption to allow taking (to harass, harm, pursue, hunt, shoot, wound, kill, trap, capture, or collect, or significant habitat modification, or to attempt to engage in any such conduct) of listed species. It authorized the Secretary of the Interior to list endangered domestic fish and wildlife and allowed the United States Fish and Wildlife Service to spend up to $15 million per year to buy habitats for listed species. It also directed federal land agencies to preserve habitat on their lands. The Act also consolidated and even expanded authority for the Secretary of the Interior to manage and administer the National Wildlife Refuge System. Other public agencies were encouraged, but not required, to protect species. The act did not address the commerce in endangered species and parts. The US Congress was urged to create the exemption by proponents of a conservation plan on San Bruno Mountain, California that was drafted in the early 1980s and is the first HCP in the nation. In the conference report on the 1982 amendments, Congress specified that it intended the San Bruno plan to act ""as a model"" for future conservation plans developed under the incidental take exemption provision and that ""the adequacy of similar conservation plans should be measured against the San Bruno plan"". Congress further noted that the San Bruno plan was based on ""an independent exhaustive biological study"" and protected at least 87% of the habitat of the listed butterflies that led to the development of the HCP. The ""No Surprises"" rule is meant to protect the landowner if ""unforeseen circumstances"" occur which make the landowner's efforts to prevent or mitigate harm to the species fall short. The ""No Surprises"" policy may be the most controversial of the recent reforms of the law, because once an Incidental Take Permit is granted, the Fish and Wildlife Service (FWS) loses much ability to further protect a species if the mitigation measures by the landowner prove insufficient. The landowner or permittee would not be required to set aside additional land or pay more in conservation money. The federal government would have to pay for additional protection measures. President Richard Nixon declared current species conservation efforts to be inadequate and called on the 93rd United States Congress to pass comprehensive endangered species legislation. Congress responded with a completely rewritten law, the Endangered Species Act of 1973 which was signed by Nixon on December 28, 1973 (Pub.L. 93–205). It was written by a team of lawyers and scientists, including the first appointed head of the Council on Environmental Quality (CEQ),an outgrowth of NEPA (The ""National Environmental Policy Act of 1969"") Dr. Russell E. Train. Dr. Train was assisted by a core group of staffers, including Dr. Earl Baysinger at EPA (currently Assistant Chief, Office of Endangered Species and International. Activities), Dick Gutting (U.S. Commerce Dept. lawyer, currently joined NOAA the previous year (1972), and Dr. Gerard A. ""Jerry"" Bertrand, a marine biologist (Ph.D, Oregon State University) by training, who had transferred from his post as the Scientific Adviser to the U.S Army Corps of Engineers, office of the Commandant of the Corp. to join the newly formed White House office. The staff, under Dr. Train's leadership, incorporated dozens of new principles and ideas into the landmark legislation; crafting a document that completely changed the direction of environmental conservation in the United States. Dr. Bertrand is credited with writing the most challenged section of the Act, the ""takings"" clause - Section 2. All federal agencies are prohibited from authorizing, funding or carrying out actions that ""destroy or adversely modify"" critical habitats (Section 7(a) (2)). While the regulatory aspect of critical habitat does not apply directly to private and other non-federal landowners, large-scale development, logging and mining projects on private and state land typically require a federal permit and thus become subject to critical habitat regulations. Outside or in parallel with regulatory processes, critical habitats also focus and encourage voluntary actions such as land purchases, grant making, restoration, and establishment of reserves. There have been six instances as of 2009 in which the exemption process was initiated. Of these six, one was granted, one was partially granted, one was denied and three were withdrawn. Donald Baur, in The Endangered Species Act: law, policy, and perspectives, concluded,"" ... the exemption provision is basically a nonfactor in the administration of the ESA. A major reason, of course, is that so few consultations result in jeopardy opinions, and those that do almost always result in the identification of reasonable and prudent alternatives to avoid jeopardy."" Public notice is given through legal notices in newspapers, and communicated to state and county agencies within the species' area. Foreign nations may also receive notice of a listing. A public hearing is mandatory if any person has requested one within 45 days of the published notice. ""The purpose of the notice and comment requirement is to provide for meaningful public participation in the rulemaking process."" summarized the Ninth Circuit court in the case of Idaho Farm Bureau Federation v. Babbitt. The Endangered Species Act of 1973 (ESA; 16 U.S.C. § 1531 et seq.) is one of the few dozens of United States environmental laws passed in the 1970s, and serves as the enacting legislation to carry out the provisions outlined in The Convention on International Trade in Endangered Species of Wild Fauna and Flora (CITES). The ESA was signed into law by President Richard Nixon on December 28, 1973, it was designed to protect critically imperiled species from extinction as a ""consequence of economic growth and development untempered by adequate concern and conservation."" The U.S. Supreme Court found that ""the plain intent of Congress in enacting"" the ESA ""was to halt and reverse the trend toward species extinction, whatever the cost."" The Act is administered by two federal agencies, the United States Fish and Wildlife Service (FWS) and the National Oceanic and Atmospheric Administration (NOAA). During the listing process, economic factors cannot be considered, but must be "" based solely on the best scientific and commercial data available."" The 1982 amendment to the ESA added the word ""solely"" to prevent any consideration other than the biological status of the species. Congress rejected President Ronald Reagan's Executive Order 12291 which required economic analysis of all government agency actions. The House committee's statement was ""that economic considerations have no relevance to determinations regarding the status of species."" As of September 2012, fifty-six species have been delisted; twenty-eight due to recovery, ten due to extinction (seven of which are believed to have been extinct prior to being listed), ten due to changes in taxonomic classification practices, six due to discovery of new populations, one due to an error in the listing rule, and one due to an amendment to the Endangered Species Act specifically requiring the species delisting. Twenty-five others have been down listed from ""endangered"" to ""threatened"" status."
Central_Intelligence_Agency,"Unlike the Federal Bureau of Investigation (FBI), which is a domestic security service, CIA has no law enforcement function and is mainly focused on overseas intelligence gathering, with only limited domestic collection. Though it is not the only U.S. government agency specializing in HUMINT, CIA serves as the national manager for coordination and deconfliction of HUMINT activities across the entire intelligence community. Moreover, CIA is the only agency authorized by law to carry out and oversee covert action on behalf of the President, unless the President determines that another agency is better suited for carrying out such action. It can, for example, exert foreign political influence through its tactical divisions, such as the Special Activities Division. Details of the overall United States intelligence budget are classified. Under the Central Intelligence Agency Act of 1949, the Director of Central Intelligence is the only federal government employee who can spend ""un-vouchered"" government money. The government has disclosed a total figure for all non-military intelligence spending since 2007; the fiscal 2013 figure is $52.6 billion. According to the 2013 mass surveillance disclosures, the CIA's fiscal 2013 budget is $14.7 billion, 28% of the total and almost 50% more than the budget of the National Security Agency. CIA's HUMINT budget is $2.3 billion, the SIGINT budget is $1.7 billion, and spending for security and logistics of CIA missions is $2.5 billion. ""Covert action programs"", including a variety of activities such as the CIA's drone fleet and anti-Iranian nuclear program activities, accounts for $2.6 billion. There were numerous previous attempts to obtain general information about the budget. As a result, it was revealed that CIA's annual budget in Fiscal Year 1963 was US $550 million (inflation-adjusted US$ 4.3 billion in 2016), and the overall intelligence budget in FY 1997 was US $26.6 billion (inflation-adjusted US$ 39.2 billion in 2016). There have been accidental disclosures; for instance, Mary Margaret Graham, a former CIA official and deputy director of national intelligence for collection in 2005, said that the annual intelligence budget was $44 billion, and in 1994 Congress accidentally published a budget of $43.4 billion (in 2012 dollars) in 1994 for the non-military National Intelligence Program, including $4.8 billion for the CIA. After the Marshall Plan was approved, appropriating $13.7 billion over five years, 5% of those funds or $685 million were made available to the CIA. US army general Hoyt Vandenberg, the CIG's second director, created the Office of Special Operations (OSO), as well as the Office of Reports and Estimates (ORE). Initially the OSO was tasked with spying and subversion overseas with a budget of $15 million, the largesse of a small number of patrons in congress. Vandenberg's goals were much like the ones set out by his predecessor; finding out ""everything about the Soviet forces in Eastern and Central Europe - their movements, their capabilities, and their intentions."" This task fell to the 228 overseas personnel covering Germany, Austria, Switzerland, Poland, Czechoslovakia, and Hungary. The Executive Office also supports the U.S. military by providing it with information it gathers, receiving information from military intelligence organizations, and cooperating on field activities. The Executive Director is in charge of the day to day operation of the CIA, and each branch of the service has its own Director. The Associate Director of military affairs, a senior military officer, manages the relationship between the CIA and the Unified Combatant Commands, who produce regional/operational intelligence and consume national intelligence. The early track record of the CIA was poor, with the agency unable to provide sufficient intelligence about the Soviet takeovers of Romania and Czechoslovakia, the Soviet blockade of Berlin, and the Soviet atomic bomb project. In particular, the agency failed to predict the Chinese entry into the Korean War with 300,000 troops. The famous double agent Kim Philby was the British liaison to American Central Intelligence. Through him the CIA coordinated hundreds of airdrops inside the iron curtain, all compromised by Philby. Arlington Hall, the nerve center of CIA cryptanalysisl was compromised by Bill Weisband, a Russian translator and Soviet spy. The CIA would reuse the tactic of dropping plant agents behind enemy lines by parachute again on China, and North Korea. This too would be fruitless. The Directorate of Analysis produces all-source intelligence investigation on key foreign and intercontinental issues relating to powerful and sometimes anti-government sensitive topics. It has four regional analytic groups, six groups for transnational issues, and three focus on policy, collection, and staff support. There is an office dedicated to Iraq, and regional analytical Offices covering the Near Eastern and South Asian Analysis, the Office of Russian and European Analysis, and the Office of Asian Pacific, Asian Pacific, Latin American, and African Analysis and African Analysis. The CIA established its first training facility, the Office of Training and Education, in 1950. Following the end of the Cold War, the CIA's training budget was slashed, which had a negative effect on employee retention. In response, Director of Central Intelligence George Tenet established CIA University in 2002. CIA University holds between 200 and 300 courses each year, training both new hires and experienced intelligence officers, as well as CIA support staff. The facility works in partnership with the National Intelligence University, and includes the Sherman Kent School for Intelligence Analysis, the Directorate of Analysis' component of the university. The CIA had different demands placed on it by the different bodies overseeing it. Truman wanted a centralized group to organize the information that reached him, the Department of Defense wanted military intelligence and covert action, and the State Department wanted to create global political change favorable to the US. Thus the two areas of responsibility for the CIA were covert action and covert intelligence. One of the main targets for intelligence gathering was the Soviet Union, which had also been a priority of the CIA's predecessors. The success of the British Commandos during World War II prompted U.S. President Franklin D. Roosevelt to authorize the creation of an intelligence service modeled after the British Secret Intelligence Service (MI6), and Special Operations Executive. This led to the creation of the Office of Strategic Services (OSS). On September 20, 1945, shortly after the end of World War II, Harry S. Truman signed an executive order dissolving the OSS, and by October 1945 its functions had been divided between the Departments of State and War. The division lasted only a few months. The first public mention of the ""Central Intelligence Agency"" appeared on a command-restructuring proposal presented by Jim Forrestal and Arthur Radford to the U.S. Senate Military Affairs Committee at the end of 1945. Despite opposition from the military establishment, the United States Department of State and the Federal Bureau of Investigation (FBI), Truman established the National Intelligence Authority in January 1946, which was the direct predecessor of the CIA. Its operational extension was known as the Central Intelligence Group (CIG) On 18 June 1948, the National Security Council issued Directive 10/2 calling for covert action against the USSR, and granting the authority to carry out covert operations against ""hostile foreign states or groups"" that could, if needed, be denied by the U.S. government. To this end, the Office of Policy Coordination was created inside the new CIA. The OPC was quite unique; Frank Wisner, the head of the OPC, answered not to the CIA Director, but to the secretaries of defense, state, and the NSC, and the OPC's actions were a secret even from the head of the CIA. Most CIA stations had two station chiefs, one working for the OSO, and one working for the OPC. The Directorate of Operations is responsible for collecting foreign intelligence, mainly from clandestine HUMINT sources, and covert action. The name reflects its role as the coordinator of human intelligence activities among other elements of the wider U.S. intelligence community with their own HUMINT operations. This Directorate was created in an attempt to end years of rivalry over influence, philosophy and budget between the United States Department of Defense (DOD) and the CIA. In spite of this, the Department of Defense recently organized its own global clandestine intelligence service, the Defense Clandestine Service (DCS), under the Defense Intelligence Agency (DIA). Lawrence Houston, head counsel of the SSU, CIG, and, later CIA, was a principle draftsman of the National Security Act of 1947 which dissolved the NIA and the CIG, and established both the National Security Council and the Central Intelligence Agency. In 1949, Houston would help draft the Central Intelligence Agency Act, (Public law 81-110) which authorized the agency to use confidential fiscal and administrative procedures, and exempted it from most limitations on the use of Federal funds. It also exempted the CIA from having to disclose its ""organization, functions, officials, titles, salaries, or numbers of personnel employed."" It created the program ""PL-110"", to handle defectors and other ""essential aliens"" who fell outside normal immigration procedures. The closest links of the U.S. IC to other foreign intelligence agencies are to Anglophone countries: Australia, Canada, New Zealand, and the United Kingdom. There is a special communications marking that signals that intelligence-related messages can be shared with these four countries. An indication of the United States' close operational cooperation is the creation of a new message distribution label within the main U.S. military communications network. Previously, the marking of NOFORN (i.e., No Foreign Nationals) required the originator to specify which, if any, non-U.S. countries could receive the information. A new handling caveat, USA/AUS/CAN/GBR/NZL Five Eyes, used primarily on intelligence messages, gives an easier way to indicate that the material can be shared with Australia, Canada, United Kingdom, and New Zealand. At the outset of the Korean War the CIA still only had a few thousand employees, a thousand of whom worked in analysis. Intelligence primarily came from the Office of Reports and Estimates, which drew its reports from a daily take of State Department telegrams, military dispatches, and other public documents. The CIA still lacked its own intelligence gathering abilities. On 21 August 1950, shortly after the invasion of South Korea, Truman announced Walter Bedell Smith as the new Director of the CIA to correct what was seen as a grave failure of Intelligence.[clarification needed] The role and functions of the CIA are roughly equivalent to those of the United Kingdom's Secret Intelligence Service (the SIS or MI6), the Australian Secret Intelligence Service (ASIS), the Egyptian General Intelligence Service, the Russian Foreign Intelligence Service (Sluzhba Vneshney Razvedki) (SVR), the Indian Research and Analysis Wing (RAW), the Pakistani Inter-Services Intelligence (ISI), the French foreign intelligence service Direction Générale de la Sécurité Extérieure (DGSE) and Israel's Mossad. While the preceding agencies both collect and analyze information, some like the U.S. State Department's Bureau of Intelligence and Research are purely analytical agencies.[citation needed]"
British_Empire,"At the end of the 16th century, England and the Netherlands began to challenge Portugal's monopoly of trade with Asia, forming private joint-stock companies to finance the voyages—the English, later British, East India Company and the Dutch East India Company, chartered in 1600 and 1602 respectively. The primary aim of these companies was to tap into the lucrative spice trade, an effort focused mainly on two regions; the East Indies archipelago, and an important hub in the trade network, India. There, they competed for trade supremacy with Portugal and with each other. Although England ultimately eclipsed the Netherlands as a colonial power, in the short term the Netherlands' more advanced financial system and the three Anglo-Dutch Wars of the 17th century left it with a stronger position in Asia. Hostilities ceased after the Glorious Revolution of 1688 when the Dutch William of Orange ascended the English throne, bringing peace between the Netherlands and England. A deal between the two nations left the spice trade of the East Indies archipelago to the Netherlands and the textiles industry of India to England, but textiles soon overtook spices in terms of profitability, and by 1720, in terms of sales, the British company had overtaken the Dutch. The British and French struggles in India became but one theatre of the global Seven Years' War (1756–1763) involving France, Britain and the other major European powers. The signing of the Treaty of Paris (1763) had important consequences for the future of the British Empire. In North America, France's future as a colonial power there was effectively ended with the recognition of British claims to Rupert's Land, and the ceding of New France to Britain (leaving a sizeable French-speaking population under British control) and Louisiana to Spain. Spain ceded Florida to Britain. Along with its victory over France in India, the Seven Years' War therefore left Britain as the world's most powerful maritime power. Since 1718, transportation to the American colonies had been a penalty for various criminal offences in Britain, with approximately one thousand convicts transported per year across the Atlantic. Forced to find an alternative location after the loss of the 13 Colonies in 1783, the British government turned to the newly discovered lands of Australia. The western coast of Australia had been discovered for Europeans by the Dutch explorer Willem Jansz in 1606 and was later named New Holland by the Dutch East India Company, but there was no attempt to colonise it. In 1770 James Cook discovered the eastern coast of Australia while on a scientific voyage to the South Pacific Ocean, claimed the continent for Britain, and named it New South Wales. In 1778, Joseph Banks, Cook's botanist on the voyage, presented evidence to the government on the suitability of Botany Bay for the establishment of a penal settlement, and in 1787 the first shipment of convicts set sail, arriving in 1788. Britain continued to transport convicts to New South Wales until 1840. The Australian colonies became profitable exporters of wool and gold, mainly because of gold rushes in the colony of Victoria, making its capital Melbourne the richest city in the world and the largest city after London in the British Empire. From its base in India, the Company had also been engaged in an increasingly profitable opium export trade to China since the 1730s. This trade, illegal since it was outlawed by the Qing dynasty in 1729, helped reverse the trade imbalances resulting from the British imports of tea, which saw large outflows of silver from Britain to China. In 1839, the confiscation by the Chinese authorities at Canton of 20,000 chests of opium led Britain to attack China in the First Opium War, and resulted in the seizure by Britain of Hong Kong Island, at that time a minor settlement. By the start of the 20th century, Germany and the United States challenged Britain's economic lead. Subsequent military and economic tensions between Britain and Germany were major causes of the First World War, during which Britain relied heavily upon its empire. The conflict placed enormous strain on the military, financial and manpower resources of Britain. Although the British Empire achieved its largest territorial extent immediately after World War I, Britain was no longer the world's pre-eminent industrial or military power. In the Second World War, Britain's colonies in South-East Asia were occupied by Imperial Japan. Despite the final victory of Britain and its allies, the damage to British prestige helped to accelerate the decline of the empire. India, Britain's most valuable and populous possession, achieved independence as part of a larger decolonisation movement in which Britain granted independence to most territories of the Empire. The transfer of Hong Kong to China in 1997 marked for many the end of the British Empire. Fourteen overseas territories remain under British sovereignty. After independence, many former British colonies joined the Commonwealth of Nations, a free association of independent states. The United Kingdom is now one of 16 Commonwealth nations, a grouping known informally as the Commonwealth realms, that share one monarch—Queen Elizabeth II. Between 1815 and 1914, a period referred to as Britain's ""imperial century"" by some historians, around 10,000,000 square miles (26,000,000 km2) of territory and roughly 400 million people were added to the British Empire. Victory over Napoleon left Britain without any serious international rival, other than Russia in central Asia. Unchallenged at sea, Britain adopted the role of global policeman, a state of affairs later known as the Pax Britannica, and a foreign policy of ""splendid isolation"". Alongside the formal control it exerted over its own colonies, Britain's dominant position in world trade meant that it effectively controlled the economies of many countries, such as China, Argentina and Siam, which has been characterised by some historians as ""Informal Empire"". A similar struggle began in India when the Government of India Act 1919 failed to satisfy demand for independence. Concerns over communist and foreign plots following the Ghadar Conspiracy ensured that war-time strictures were renewed by the Rowlatt Acts. This led to tension, particularly in the Punjab region, where repressive measures culminated in the Amritsar Massacre. In Britain public opinion was divided over the morality of the event, between those who saw it as having saved India from anarchy, and those who viewed it with revulsion. The subsequent Non-Co-Operation movement was called off in March 1922 following the Chauri Chaura incident, and discontent continued to simmer for the next 25 years. Under the terms of the concluding Treaty of Versailles signed in 1919, the empire reached its greatest extent with the addition of 1,800,000 square miles (4,700,000 km2) and 13 million new subjects. The colonies of Germany and the Ottoman Empire were distributed to the Allied powers as League of Nations mandates. Britain gained control of Palestine, Transjordan, Iraq, parts of Cameroon and Togo, and Tanganyika. The Dominions themselves also acquired mandates of their own: the Union of South Africa gained South-West Africa (modern-day Namibia), Australia gained German New Guinea, and New Zealand Western Samoa. Nauru was made a combined mandate of Britain and the two Pacific Dominions. Following the defeat of Japan in the Second World War, anti-Japanese resistance movements in Malaya turned their attention towards the British, who had moved to quickly retake control of the colony, valuing it as a source of rubber and tin. The fact that the guerrillas were primarily Malayan-Chinese Communists meant that the British attempt to quell the uprising was supported by the Muslim Malay majority, on the understanding that once the insurgency had been quelled, independence would be granted. The Malayan Emergency, as it was called, began in 1948 and lasted until 1960, but by 1957, Britain felt confident enough to grant independence to the Federation of Malaya within the Commonwealth. In 1963, the 11 states of the federation together with Singapore, Sarawak and North Borneo joined to form Malaysia, but in 1965 Chinese-majority Singapore was expelled from the union following tensions between the Malay and Chinese populations. Brunei, which had been a British protectorate since 1888, declined to join the union and maintained its status until independence in 1984. Britain's fears of war with Germany were realised in 1914 with the outbreak of the First World War. Britain quickly invaded and occupied most of Germany's overseas colonies in Africa. In the Pacific, Australia and New Zealand occupied German New Guinea and Samoa respectively. Plans for a post-war division of the Ottoman Empire, which had joined the war on Germany's side, were secretly drawn up by Britain and France under the 1916 Sykes–Picot Agreement. This agreement was not divulged to the Sharif of Mecca, who the British had been encouraging to launch an Arab revolt against their Ottoman rulers, giving the impression that Britain was supporting the creation of an independent Arab state. In 1695, the Scottish Parliament granted a charter to the Company of Scotland, which established a settlement in 1698 on the isthmus of Panama. Besieged by neighbouring Spanish colonists of New Granada, and afflicted by malaria, the colony was abandoned two years later. The Darien scheme was a financial disaster for Scotland—a quarter of Scottish capital was lost in the enterprise—and ended Scottish hopes of establishing its own overseas empire. The episode also had major political consequences, persuading the governments of both England and Scotland of the merits of a union of countries, rather than just crowns. This occurred in 1707 with the Treaty of Union, establishing the Kingdom of Great Britain. In 1919, the frustrations caused by delays to Irish home rule led members of Sinn Féin, a pro-independence party that had won a majority of the Irish seats at Westminster in the 1918 British general election, to establish an Irish assembly in Dublin, at which Irish independence was declared. The Irish Republican Army simultaneously began a guerrilla war against the British administration. The Anglo-Irish War ended in 1921 with a stalemate and the signing of the Anglo-Irish Treaty, creating the Irish Free State, a Dominion within the British Empire, with effective internal independence but still constitutionally linked with the British Crown. Northern Ireland, consisting of six of the 32 Irish counties which had been established as a devolved region under the 1920 Government of Ireland Act, immediately exercised its option under the treaty to retain its existing status within the United Kingdom. During the late 18th and early 19th centuries the British Crown began to assume an increasingly large role in the affairs of the Company. A series of Acts of Parliament were passed, including the Regulating Act of 1773, Pitt's India Act of 1784 and the Charter Act of 1813 which regulated the Company's affairs and established the sovereignty of the Crown over the territories that it had acquired. The Company's eventual end was precipitated by the Indian Rebellion, a conflict that had begun with the mutiny of sepoys, Indian troops under British officers and discipline. The rebellion took six months to suppress, with heavy loss of life on both sides. The following year the British government dissolved the Company and assumed direct control over India through the Government of India Act 1858, establishing the British Raj, where an appointed governor-general administered India and Queen Victoria was crowned the Empress of India. India became the empire's most valuable possession, ""the Jewel in the Crown"", and was the most important source of Britain's strength. Political boundaries drawn by the British did not always reflect homogeneous ethnicities or religions, contributing to conflicts in formerly colonised areas. The British Empire was also responsible for large migrations of peoples. Millions left the British Isles, with the founding settler populations of the United States, Canada, Australia and New Zealand coming mainly from Britain and Ireland. Tensions remain between the white settler populations of these countries and their indigenous minorities, and between white settler minorities and indigenous majorities in South Africa and Zimbabwe. Settlers in Ireland from Great Britain have left their mark in the form of divided nationalist and unionist communities in Northern Ireland. Millions of people moved to and from British colonies, with large numbers of Indians emigrating to other parts of the empire, such as Malaysia and Fiji, and Chinese people to Malaysia, Singapore and the Caribbean. The demographics of Britain itself was changed after the Second World War owing to immigration to Britain from its former colonies. The changing world order that the war had brought about, in particular the growth of the United States and Japan as naval powers, and the rise of independence movements in India and Ireland, caused a major reassessment of British imperial policy. Forced to choose between alignment with the United States or Japan, Britain opted not to renew its Japanese alliance and instead signed the 1922 Washington Naval Treaty, where Britain accepted naval parity with the United States. This decision was the source of much debate in Britain during the 1930s as militaristic governments took hold in Japan and Germany helped in part by the Great Depression, for it was feared that the empire could not survive a simultaneous attack by both nations. Although the issue of the empire's security was a serious concern in Britain, at the same time the empire was vital to the British economy. Most of the UK's Caribbean territories achieved independence after the departure in 1961 and 1962 of Jamaica and Trinidad from the West Indies Federation, established in 1958 in an attempt to unite the British Caribbean colonies under one government, but which collapsed following the loss of its two largest members. Barbados achieved independence in 1966 and the remainder of the eastern Caribbean islands in the 1970s and 1980s, but Anguilla and the Turks and Caicos Islands opted to revert to British rule after they had already started on the path to independence. The British Virgin Islands, Cayman Islands and Montserrat opted to retain ties with Britain, while Guyana achieved independence in 1966. Britain's last colony on the American mainland, British Honduras, became a self-governing colony in 1964 and was renamed Belize in 1973, achieving full independence in 1981. A dispute with Guatemala over claims to Belize was left unresolved. With support from the British abolitionist movement, Parliament enacted the Slave Trade Act in 1807, which abolished the slave trade in the empire. In 1808, Sierra Leone was designated an official British colony for freed slaves. The Slavery Abolition Act passed in 1833 abolished slavery in the British Empire on 1 August 1834 (with the exception of St. Helena, Ceylon and the territories administered by the East India Company, though these exclusions were later repealed). Under the Act, slaves were granted full emancipation after a period of 4 to 6 years of ""apprenticeship"". By the turn of the 20th century, fears had begun to grow in Britain that it would no longer be able to defend the metropole and the entirety of the empire while at the same time maintaining the policy of ""splendid isolation"". Germany was rapidly rising as a military and industrial power and was now seen as the most likely opponent in any future war. Recognising that it was overstretched in the Pacific and threatened at home by the Imperial German Navy, Britain formed an alliance with Japan in 1902 and with its old enemies France and Russia in 1904 and 1907, respectively. In 1980, Rhodesia, Britain's last African colony, became the independent nation of Zimbabwe. The New Hebrides achieved independence (as Vanuatu) in 1980, with Belize following suit in 1981. The passage of the British Nationality Act 1981, which reclassified the remaining Crown colonies as ""British Dependent Territories"" (renamed British Overseas Territories in 2002) meant that, aside from a scattering of islands and outposts the process of decolonisation that had begun after the Second World War was largely complete. In 1982, Britain's resolve in defending its remaining overseas territories was tested when Argentina invaded the Falkland Islands, acting on a long-standing claim that dated back to the Spanish Empire. Britain's ultimately successful military response to retake the islands during the ensuing Falklands War was viewed by many to have contributed to reversing the downward trend in Britain's status as a world power. The same year, the Canadian government severed its last legal link with Britain by patriating the Canadian constitution from Britain. The 1982 Canada Act passed by the British parliament ended the need for British involvement in changes to the Canadian constitution. Similarly, the Constitution Act 1986 reformed the constitution of New Zealand to sever its constitutional link with Britain, and the Australia Act 1986 severed the constitutional link between Britain and the Australian states. In 1984, Brunei, Britain's last remaining Asian protectorate, gained its independence. During the Age of Discovery in the 15th and 16th centuries, Portugal and Spain pioneered European exploration of the globe, and in the process established large overseas empires. Envious of the great wealth these empires generated, England, France, and the Netherlands began to establish colonies and trade networks of their own in the Americas and Asia. A series of wars in the 17th and 18th centuries with the Netherlands and France left England (and then, following union between England and Scotland in 1707, Great Britain) the dominant colonial power in North America and India. The loss of such a large portion of British America, at the time Britain's most populous overseas possession, is seen by some historians as the event defining the transition between the ""first"" and ""second"" empires, in which Britain shifted its attention away from the Americas to Asia, the Pacific and later Africa. Adam Smith's Wealth of Nations, published in 1776, had argued that colonies were redundant, and that free trade should replace the old mercantilist policies that had characterised the first period of colonial expansion, dating back to the protectionism of Spain and Portugal. The growth of trade between the newly independent United States and Britain after 1783 seemed to confirm Smith's view that political control was not necessary for economic success. In December 1941, Japan launched, in quick succession, attacks on British Malaya, the United States naval base at Pearl Harbor, and Hong Kong. Churchill's reaction to the entry of the United States into the war was that Britain was now assured of victory and the future of the empire was safe, but the manner in which British forces were rapidly defeated in the Far East irreversibly harmed Britain's standing and prestige as an imperial power. Most damaging of all was the fall of Singapore, which had previously been hailed as an impregnable fortress and the eastern equivalent of Gibraltar. The realisation that Britain could not defend its entire empire pushed Australia and New Zealand, which now appeared threatened by Japanese forces, into closer ties with the United States. This resulted in the 1951 ANZUS Pact between Australia, New Zealand and the United States of America. In 1578, Elizabeth I granted a patent to Humphrey Gilbert for discovery and overseas exploration. That year, Gilbert sailed for the West Indies with the intention of engaging in piracy and establishing a colony in North America, but the expedition was aborted before it had crossed the Atlantic. In 1583 he embarked on a second attempt, on this occasion to the island of Newfoundland whose harbour he formally claimed for England, although no settlers were left behind. Gilbert did not survive the return journey to England, and was succeeded by his half-brother, Walter Raleigh, who was granted his own patent by Elizabeth in 1584. Later that year, Raleigh founded the colony of Roanoke on the coast of present-day North Carolina, but lack of supplies caused the colony to fail. The Napoleonic Wars were therefore ones in which Britain invested large amounts of capital and resources to win. French ports were blockaded by the Royal Navy, which won a decisive victory over a Franco-Spanish fleet at Trafalgar in 1805. Overseas colonies were attacked and occupied, including those of the Netherlands, which was annexed by Napoleon in 1810. France was finally defeated by a coalition of European armies in 1815. Britain was again the beneficiary of peace treaties: France ceded the Ionian Islands, Malta (which it had occupied in 1797 and 1798 respectively), Mauritius, St Lucia, and Tobago; Spain ceded Trinidad; the Netherlands Guyana, and the Cape Colony. Britain returned Guadeloupe, Martinique, French Guiana, and Réunion to France, and Java and Suriname to the Netherlands, while gaining control of Ceylon (1795–1815). In July 1956, Nasser unilaterally nationalised the Suez Canal. The response of Anthony Eden, who had succeeded Churchill as Prime Minister, was to collude with France to engineer an Israeli attack on Egypt that would give Britain and France an excuse to intervene militarily and retake the canal. Eden infuriated US President Dwight D. Eisenhower, by his lack of consultation, and Eisenhower refused to back the invasion. Another of Eisenhower's concerns was the possibility of a wider war with the Soviet Union after it threatened to intervene on the Egyptian side. Eisenhower applied financial leverage by threatening to sell US reserves of the British pound and thereby precipitate a collapse of the British currency. Though the invasion force was militarily successful in its objectives, UN intervention and US pressure forced Britain into a humiliating withdrawal of its forces, and Eden resigned. During the middle decades of the 18th century, there were several outbreaks of military conflict on the Indian subcontinent, the Carnatic Wars, as the English East India Company (the Company) and its French counterpart, the Compagnie française des Indes orientales, struggled alongside local rulers to fill the vacuum that had been left by the decline of the Mughal Empire. The Battle of Plassey in 1757, in which the British, led by Robert Clive, defeated the Nawab of Bengal and his French allies, left the Company in control of Bengal and as the major military and political power in India. France was left control of its enclaves but with military restrictions and an obligation to support British client states, ending French hopes of controlling India. In the following decades the Company gradually increased the size of the territories under its control, either ruling directly or via local rulers under the threat of force from the British Indian Army, the vast majority of which was composed of Indian sepoys. After the German occupation of France in 1940, Britain and the empire stood alone against Germany, until the entry of the Soviet Union to the war in 1941. British Prime Minister Winston Churchill successfully lobbied President Franklin D. Roosevelt for military aid from the United States, but Roosevelt was not yet ready to ask Congress to commit the country to war. In August 1941, Churchill and Roosevelt met and signed the Atlantic Charter, which included the statement that ""the rights of all peoples to choose the form of government under which they live"" should be respected. This wording was ambiguous as to whether it referred to European countries invaded by Germany, or the peoples colonised by European nations, and would later be interpreted differently by the British, Americans, and nationalist movements. The British Empire comprised the dominions, colonies, protectorates, mandates and other territories ruled or administered by the United Kingdom. It originated with the overseas possessions and trading posts established by England between the late 16th and early 18th centuries. At its height, it was the largest empire in history and, for over a century, was the foremost global power. By 1922 the British Empire held sway over about 458 million people, one-fifth of the world's population at the time, and covered more than 13,000,000 sq mi (33,670,000 km2), almost a quarter of the Earth's total land area. As a result, its political, legal, linguistic and cultural legacy is widespread. At the peak of its power, the phrase ""the empire on which the sun never sets"" was often used to describe the British Empire, because its expanse around the globe meant that the sun was always shining on at least one of its territories. During his voyage, Cook also visited New Zealand, first discovered by Dutch explorer Abel Tasman in 1642, and claimed the North and South islands for the British crown in 1769 and 1770 respectively. Initially, interaction between the indigenous Māori population and Europeans was limited to the trading of goods. European settlement increased through the early decades of the 19th century, with numerous trading stations established, especially in the North. In 1839, the New Zealand Company announced plans to buy large tracts of land and establish colonies in New Zealand. On 6 February 1840, Captain William Hobson and around 40 Maori chiefs signed the Treaty of Waitangi. This treaty is considered by many to be New Zealand's founding document, but differing interpretations of the Maori and English versions of the text have meant that it continues to be a source of dispute. In 1951, the Conservative Party returned to power in Britain, under the leadership of Winston Churchill. Churchill and the Conservatives believed that Britain's position as a world power relied on the continued existence of the empire, with the base at the Suez Canal allowing Britain to maintain its pre-eminent position in the Middle East in spite of the loss of India. However, Churchill could not ignore Gamal Abdul Nasser's new revolutionary government of Egypt that had taken power in 1952, and the following year it was agreed that British troops would withdraw from the Suez Canal zone and that Sudan would be granted self-determination by 1955, with independence to follow. Sudan was granted independence on 1 January 1956. The path to independence for the white colonies of the British Empire began with the 1839 Durham Report, which proposed unification and self-government for Upper and Lower Canada, as a solution to political unrest there. This began with the passing of the Act of Union in 1840, which created the Province of Canada. Responsible government was first granted to Nova Scotia in 1848, and was soon extended to the other British North American colonies. With the passage of the British North America Act, 1867 by the British Parliament, Upper and Lower Canada, New Brunswick and Nova Scotia were formed into the Dominion of Canada, a confederation enjoying full self-government with the exception of international relations. Australia and New Zealand achieved similar levels of self-government after 1900, with the Australian colonies federating in 1901. The term ""dominion status"" was officially introduced at the Colonial Conference of 1907. The British declaration of war on Germany and its allies also committed the colonies and Dominions, which provided invaluable military, financial and material support. Over 2.5 million men served in the armies of the Dominions, as well as many thousands of volunteers from the Crown colonies. The contributions of Australian and New Zealand troops during the 1915 Gallipoli Campaign against the Ottoman Empire had a great impact on the national consciousness at home, and marked a watershed in the transition of Australia and New Zealand from colonies to nations in their own right. The countries continue to commemorate this occasion on Anzac Day. Canadians viewed the Battle of Vimy Ridge in a similar light. The important contribution of the Dominions to the war effort was recognised in 1917 by the British Prime Minister David Lloyd George when he invited each of the Dominion Prime Ministers to join an Imperial War Cabinet to co-ordinate imperial policy. No further attempts to establish English colonies in the Americas were made until well into the reign of Queen Elizabeth I, during the last decades of the 16th century. In the meantime the Protestant Reformation had turned England and Catholic Spain into implacable enemies . In 1562, the English Crown encouraged the privateers John Hawkins and Francis Drake to engage in slave-raiding attacks against Spanish and Portuguese ships off the coast of West Africa with the aim of breaking into the Atlantic trade system. This effort was rebuffed and later, as the Anglo-Spanish Wars intensified, Elizabeth I gave her blessing to further privateering raids against Spanish ports in the Americas and shipping that was returning across the Atlantic, laden with treasure from the New World. At the same time, influential writers such as Richard Hakluyt and John Dee (who was the first to use the term ""British Empire"") were beginning to press for the establishment of England's own empire. By this time, Spain had become the dominant power in the Americas and was exploring the Pacific ocean, Portugal had established trading posts and forts from the coasts of Africa and Brazil to China, and France had begun to settle the Saint Lawrence River area, later to become New France. Britain's remaining colonies in Africa, except for self-governing Southern Rhodesia, were all granted independence by 1968. British withdrawal from the southern and eastern parts of Africa was not a peaceful process. Kenyan independence was preceded by the eight-year Mau Mau Uprising. In Rhodesia, the 1965 Unilateral Declaration of Independence by the white minority resulted in a civil war that lasted until the Lancaster House Agreement of 1979, which set the terms for recognised independence in 1980, as the new nation of Zimbabwe. Events in America influenced British policy in Canada, where between 40,000 and 100,000 defeated Loyalists had migrated from America following independence. The 14,000 Loyalists who went to the Saint John and Saint Croix river valleys, then part of Nova Scotia, felt too far removed from the provincial government in Halifax, so London split off New Brunswick as a separate colony in 1784. The Constitutional Act of 1791 created the provinces of Upper Canada (mainly English-speaking) and Lower Canada (mainly French-speaking) to defuse tensions between the French and British communities, and implemented governmental systems similar to those employed in Britain, with the intention of asserting imperial authority and not allowing the sort of popular control of government that was perceived to have led to the American Revolution. Britain retains sovereignty over 14 territories outside the British Isles, which were renamed the British Overseas Territories in 2002. Some are uninhabited except for transient military or scientific personnel; the remainder are self-governing to varying degrees and are reliant on the UK for foreign relations and defence. The British government has stated its willingness to assist any Overseas Territory that wishes to proceed to independence, where that is an option. British sovereignty of several of the overseas territories is disputed by their geographical neighbours: Gibraltar is claimed by Spain, the Falkland Islands and South Georgia and the South Sandwich Islands are claimed by Argentina, and the British Indian Ocean Territory is claimed by Mauritius and Seychelles. The British Antarctic Territory is subject to overlapping claims by Argentina and Chile, while many countries do not recognise any territorial claims in Antarctica. The Suez Crisis very publicly exposed Britain's limitations to the world and confirmed Britain's decline on the world stage, demonstrating that henceforth it could no longer act without at least the acquiescence, if not the full support, of the United States. The events at Suez wounded British national pride, leading one MP to describe it as ""Britain's Waterloo"" and another to suggest that the country had become an ""American satellite"". Margaret Thatcher later described the mindset she believed had befallen the British political establishment as ""Suez syndrome"", from which Britain did not recover until the successful recapture of the Falkland Islands from Argentina in 1982. The British Mandate of Palestine, where an Arab majority lived alongside a Jewish minority, presented the British with a similar problem to that of India. The matter was complicated by large numbers of Jewish refugees seeking to be admitted to Palestine following the Holocaust, while Arabs were opposed to the creation of a Jewish state. Frustrated by the intractability of the problem, attacks by Jewish paramilitary organisations and the increasing cost of maintaining its military presence, Britain announced in 1947 that it would withdraw in 1948 and leave the matter to the United Nations to solve. The UN General Assembly subsequently voted for a plan to partition Palestine into a Jewish and an Arab state. The Dutch East India Company had founded the Cape Colony on the southern tip of Africa in 1652 as a way station for its ships travelling to and from its colonies in the East Indies. Britain formally acquired the colony, and its large Afrikaner (or Boer) population in 1806, having occupied it in 1795 to prevent its falling into French hands, following the invasion of the Netherlands by France. British immigration began to rise after 1820, and pushed thousands of Boers, resentful of British rule, northwards to found their own—mostly short-lived—independent republics, during the Great Trek of the late 1830s and early 1840s. In the process the Voortrekkers clashed repeatedly with the British, who had their own agenda with regard to colonial expansion in South Africa and with several African polities, including those of the Sotho and the Zulu nations. Eventually the Boers established two republics which had a longer lifespan: the South African Republic or Transvaal Republic (1852–77; 1881–1902) and the Orange Free State (1854–1902). In 1902 Britain occupied both republics, concluding a treaty with the two Boer Republics following the Second Boer War (1899–1902). In 1603, James VI, King of Scots, ascended (as James I) to the English throne and in 1604 negotiated the Treaty of London, ending hostilities with Spain. Now at peace with its main rival, English attention shifted from preying on other nations' colonial infrastructures to the business of establishing its own overseas colonies. The British Empire began to take shape during the early 17th century, with the English settlement of North America and the smaller islands of the Caribbean, and the establishment of private companies, most notably the English East India Company, to administer colonies and overseas trade. This period, until the loss of the Thirteen Colonies after the American War of Independence towards the end of the 18th century, has subsequently been referred to by some historians as the ""First British Empire"". The foundations of the British Empire were laid when England and Scotland were separate kingdoms. In 1496 King Henry VII of England, following the successes of Spain and Portugal in overseas exploration, commissioned John Cabot to lead a voyage to discover a route to Asia via the North Atlantic. Cabot sailed in 1497, five years after the European discovery of America, and although he successfully made landfall on the coast of Newfoundland (mistakenly believing, like Christopher Columbus, that he had reached Asia), there was no attempt to found a colony. Cabot led another voyage to the Americas the following year but nothing was heard of his ships again. During the 1760s and early 1770s, relations between the Thirteen Colonies and Britain became increasingly strained, primarily because of resentment of the British Parliament's attempts to govern and tax American colonists without their consent. This was summarised at the time by the slogan ""No taxation without representation"", a perceived violation of the guaranteed Rights of Englishmen. The American Revolution began with rejection of Parliamentary authority and moves towards self-government. In response Britain sent troops to reimpose direct rule, leading to the outbreak of war in 1775. The following year, in 1776, the United States declared independence. The entry of France to the war in 1778 tipped the military balance in the Americans' favour and after a decisive defeat at Yorktown in 1781, Britain began negotiating peace terms. American independence was acknowledged at the Peace of Paris in 1783. The last decades of the 19th century saw concerted political campaigns for Irish home rule. Ireland had been united with Britain into the United Kingdom of Great Britain and Ireland with the Act of Union 1800 after the Irish Rebellion of 1798, and had suffered a severe famine between 1845 and 1852. Home rule was supported by the British Prime minister, William Gladstone, who hoped that Ireland might follow in Canada's footsteps as a Dominion within the empire, but his 1886 Home Rule bill was defeated in Parliament. Although the bill, if passed, would have granted Ireland less autonomy within the UK than the Canadian provinces had within their own federation, many MPs feared that a partially independent Ireland might pose a security threat to Great Britain or mark the beginning of the break-up of the empire. A second Home Rule bill was also defeated for similar reasons. A third bill was passed by Parliament in 1914, but not implemented because of the outbreak of the First World War leading to the 1916 Easter Rising. At the concluding Treaty of Utrecht, Philip renounced his and his descendants' right to the French throne and Spain lost its empire in Europe. The British Empire was territorially enlarged: from France, Britain gained Newfoundland and Acadia, and from Spain, Gibraltar and Minorca. Gibraltar became a critical naval base and allowed Britain to control the Atlantic entry and exit point to the Mediterranean. Spain also ceded the rights to the lucrative asiento (permission to sell slaves in Spanish America) to Britain. The ability of the Dominions to set their own foreign policy, independent of Britain, was recognised at the 1923 Imperial Conference. Britain's request for military assistance from the Dominions at the outbreak of the Chanak Crisis the previous year had been turned down by Canada and South Africa, and Canada had refused to be bound by the 1923 Treaty of Lausanne. After pressure from Ireland and South Africa, the 1926 Imperial Conference issued the Balfour Declaration, declaring the Dominions to be ""autonomous Communities within the British Empire, equal in status, in no way subordinate one to another"" within a ""British Commonwealth of Nations"". This declaration was given legal substance under the 1931 Statute of Westminster. The parliaments of Canada, Australia, New Zealand, the Union of South Africa, the Irish Free State and Newfoundland were now independent of British legislative control, they could nullify British laws and Britain could no longer pass laws for them without their consent. Newfoundland reverted to colonial status in 1933, suffering from financial difficulties during the Great Depression. Ireland distanced itself further from Britain with the introduction of a new constitution in 1937, making it a republic in all but name. In September 1982, Prime minister Margaret Thatcher travelled to Beijing to negotiate with the Chinese government on the future of Britain's last major and most populous overseas territory, Hong Kong. Under the terms of the 1842 Treaty of Nanking, Hong Kong Island itself had been ceded to Britain in perpetuity, but the vast majority of the colony was constituted by the New Territories, which had been acquired under a 99-year lease in 1898, due to expire in 1997. Thatcher, seeing parallels with the Falkland Islands, initially wished to hold Hong Kong and proposed British administration with Chinese sovereignty, though this was rejected by China. A deal was reached in 1984—under the terms of the Sino-British Joint Declaration, Hong Kong would become a special administrative region of the People's Republic of China, maintaining its way of life for at least 50 years. The handover ceremony in 1997 marked for many, including Charles, Prince of Wales, who was in attendance, ""the end of Empire"". During the 19th century, Britain and the Russian Empire vied to fill the power vacuums that had been left by the declining Ottoman Empire, Qajar dynasty and Qing Dynasty. This rivalry in Eurasia came to be known as the ""Great Game"". As far as Britain was concerned, defeats inflicted by Russia on Persia and Turkey demonstrated its imperial ambitions and capabilities and stoked fears in Britain of an overland invasion of India. In 1839, Britain moved to pre-empt this by invading Afghanistan, but the First Anglo-Afghan War was a disaster for Britain. While the Suez Crisis caused British power in the Middle East to weaken, it did not collapse. Britain again deployed its armed forces to the region, intervening in Oman (1957), Jordan (1958) and Kuwait (1961), though on these occasions with American approval, as the new Prime Minister Harold Macmillan's foreign policy was to remain firmly aligned with the United States. Britain maintained a military presence in the Middle East for another decade. In January 1968, a few weeks after the devaluation of the pound, Prime Minister Harold Wilson and his Defence Secretary Denis Healey announced that British troops would be withdrawn from major military bases East of Suez, which included the ones in the Middle East, and primarily from Malaysia and Singapore. The British withdrew from Aden in 1967, Bahrain in 1971, and Maldives in 1976. The independence of the Thirteen Colonies in North America in 1783 after the American War of Independence caused Britain to lose some of its oldest and most populous colonies. British attention soon turned towards Asia, Africa, and the Pacific. After the defeat of France in the Revolutionary and Napoleonic Wars (1792–1815), Britain emerged as the principal naval and imperial power of the 19th century (with London the largest city in the world from about 1830). Unchallenged at sea, British dominance was later described as Pax Britannica (""British Peace""), a period of relative peace in Europe and the world (1815–1914) during which the British Empire became the global hegemon and adopted the role of global policeman. In the early 19th century, the Industrial Revolution began to transform Britain; by the time of the Great Exhibition in 1851 the country was described as the ""workshop of the world"". The British Empire expanded to include India, large parts of Africa and many other territories throughout the world. Alongside the formal control it exerted over its own colonies, British dominance of much of world trade meant that it effectively controlled the economies of many regions, such as Asia and Latin America. Domestically, political attitudes favoured free trade and laissez-faire policies and a gradual widening of the voting franchise. During this century, the population increased at a dramatic rate, accompanied by rapid urbanisation, causing significant social and economic stresses. To seek new markets and sources of raw materials, the Conservative Party under Disraeli launched a period of imperialist expansion in Egypt, South Africa, and elsewhere. Canada, Australia, and New Zealand became self-governing dominions. The Caribbean initially provided England's most important and lucrative colonies, but not before several attempts at colonisation failed. An attempt to establish a colony in Guiana in 1604 lasted only two years, and failed in its main objective to find gold deposits. Colonies in St Lucia (1605) and Grenada (1609) also rapidly folded, but settlements were successfully established in St. Kitts (1624), Barbados (1627) and Nevis (1628). The colonies soon adopted the system of sugar plantations successfully used by the Portuguese in Brazil, which depended on slave labour, and—at first—Dutch ships, to sell the slaves and buy the sugar. To ensure that the increasingly healthy profits of this trade remained in English hands, Parliament decreed in 1651 that only English ships would be able to ply their trade in English colonies. This led to hostilities with the United Dutch Provinces—a series of Anglo-Dutch Wars—which would eventually strengthen England's position in the Americas at the expense of the Dutch. In 1655, England annexed the island of Jamaica from the Spanish, and in 1666 succeeded in colonising the Bahamas. When Russia invaded the Turkish Balkans in 1853, fears of Russian dominance in the Mediterranean and Middle East led Britain and France to invade the Crimean Peninsula to destroy Russian naval capabilities. The ensuing Crimean War (1854–56), which involved new techniques of modern warfare, and was the only global war fought between Britain and another imperial power during the Pax Britannica, was a resounding defeat for Russia. The situation remained unresolved in Central Asia for two more decades, with Britain annexing Baluchistan in 1876 and Russia annexing Kirghizia, Kazakhstan, and Turkmenistan. For a while it appeared that another war would be inevitable, but the two countries reached an agreement on their respective spheres of influence in the region in 1878 and on all outstanding matters in 1907 with the signing of the Anglo-Russian Entente. The destruction of the Russian Navy by the Japanese at the Battle of Port Arthur during the Russo-Japanese War of 1904–05 also limited its threat to the British. England's first permanent settlement in the Americas was founded in 1607 in Jamestown, led by Captain John Smith and managed by the Virginia Company. Bermuda was settled and claimed by England as a result of the 1609 shipwreck there of the Virginia Company's flagship, and in 1615 was turned over to the newly formed Somers Isles Company. The Virginia Company's charter was revoked in 1624 and direct control of Virginia was assumed by the crown, thereby founding the Colony of Virginia. The London and Bristol Company was created in 1610 with the aim of creating a permanent settlement on Newfoundland, but was largely unsuccessful. In 1620, Plymouth was founded as a haven for puritan religious separatists, later known as the Pilgrims. Fleeing from religious persecution would become the motive of many English would-be colonists to risk the arduous trans-Atlantic voyage: Maryland was founded as a haven for Roman Catholics (1634), Rhode Island (1636) as a colony tolerant of all religions and Connecticut (1639) for Congregationalists. The Province of Carolina was founded in 1663. With the surrender of Fort Amsterdam in 1664, England gained control of the Dutch colony of New Netherland, renaming it New York. This was formalised in negotiations following the Second Anglo-Dutch War, in exchange for Suriname. In 1681, the colony of Pennsylvania was founded by William Penn. The American colonies were less financially successful than those of the Caribbean, but had large areas of good agricultural land and attracted far larger numbers of English emigrants who preferred their temperate climates. In 1869 the Suez Canal opened under Napoleon III, linking the Mediterranean with the Indian Ocean. Initially the Canal was opposed by the British; but once opened, its strategic value was quickly recognised and became the ""jugular vein of the Empire"". In 1875, the Conservative government of Benjamin Disraeli bought the indebted Egyptian ruler Isma'il Pasha's 44 percent shareholding in the Suez Canal for £4 million (£340 million in 2013). Although this did not grant outright control of the strategic waterway, it did give Britain leverage. Joint Anglo-French financial control over Egypt ended in outright British occupation in 1882. The French were still majority shareholders and attempted to weaken the British position, but a compromise was reached with the 1888 Convention of Constantinople, which made the Canal officially neutral territory. With French, Belgian and Portuguese activity in the lower Congo River region undermining orderly incursion of tropical Africa, the Berlin Conference of 1884–85 was held to regulate the competition between the European powers in what was called the ""Scramble for Africa"" by defining ""effective occupation"" as the criterion for international recognition of territorial claims. The scramble continued into the 1890s, and caused Britain to reconsider its decision in 1885 to withdraw from Sudan. A joint force of British and Egyptian troops defeated the Mahdist Army in 1896, and rebuffed a French attempted invasion at Fashoda in 1898. Sudan was nominally made an Anglo-Egyptian Condominium, but a British colony in reality. Though Britain and the empire emerged victorious from the Second World War, the effects of the conflict were profound, both at home and abroad. Much of Europe, a continent that had dominated the world for several centuries, was in ruins, and host to the armies of the United States and the Soviet Union, who now held the balance of global power. Britain was left essentially bankrupt, with insolvency only averted in 1946 after the negotiation of a $US 4.33 billion loan (US$56 billion in 2012) from the United States, the last instalment of which was repaid in 2006. At the same time, anti-colonial movements were on the rise in the colonies of European nations. The situation was complicated further by the increasing Cold War rivalry of the United States and the Soviet Union. In principle, both nations were opposed to European colonialism. In practice, however, American anti-communism prevailed over anti-imperialism, and therefore the United States supported the continued existence of the British Empire to keep Communist expansion in check. The ""wind of change"" ultimately meant that the British Empire's days were numbered, and on the whole, Britain adopted a policy of peaceful disengagement from its colonies once stable, non-Communist governments were available to transfer power to. This was in contrast to other European powers such as France and Portugal, which waged costly and ultimately unsuccessful wars to keep their empires intact. Between 1945 and 1965, the number of people under British rule outside the UK itself fell from 700 million to five million, three million of whom were in Hong Kong. The pro-decolonisation Labour government, elected at the 1945 general election and led by Clement Attlee, moved quickly to tackle the most pressing issue facing the empire: that of Indian independence. India's two major political parties—the Indian National Congress and the Muslim League—had been campaigning for independence for decades, but disagreed as to how it should be implemented. Congress favoured a unified secular Indian state, whereas the League, fearing domination by the Hindu majority, desired a separate Islamic state for Muslim-majority regions. Increasing civil unrest and the mutiny of the Royal Indian Navy during 1946 led Attlee to promise independence no later than 1948. When the urgency of the situation and risk of civil war became apparent, the newly appointed (and last) Viceroy, Lord Mountbatten, hastily brought forward the date to 15 August 1947. The borders drawn by the British to broadly partition India into Hindu and Muslim areas left tens of millions as minorities in the newly independent states of India and Pakistan. Millions of Muslims subsequently crossed from India to Pakistan and Hindus vice versa, and violence between the two communities cost hundreds of thousands of lives. Burma, which had been administered as part of the British Raj, and Sri Lanka gained their independence the following year in 1948. India, Pakistan and Sri Lanka became members of the Commonwealth, while Burma chose not to join. In 1922, Egypt, which had been declared a British protectorate at the outbreak of the First World War, was granted formal independence, though it continued to be a British client state until 1954. British troops remained stationed in Egypt until the signing of the Anglo-Egyptian Treaty in 1936, under which it was agreed that the troops would withdraw but continue to occupy and defend the Suez Canal zone. In return, Egypt was assisted to join the League of Nations. Iraq, a British mandate since 1920, also gained membership of the League in its own right after achieving independence from Britain in 1932. In Palestine, Britain was presented with the problem of mediating between the Arab and Jewish communities. The 1917 Balfour Declaration, which had been incorporated into the terms of the mandate, stated that a national home for the Jewish people would be established in Palestine, and Jewish immigration allowed up to a limit that would be determined by the mandatory power. This led to increasing conflict with the Arab population, who openly revolted in 1936. As the threat of war with Germany increased during the 1930s, Britain judged the support of the Arab population in the Middle East as more important than the establishment of a Jewish homeland, and shifted to a pro-Arab stance, limiting Jewish immigration and in turn triggering a Jewish insurgency. Peace between England and the Netherlands in 1688 meant that the two countries entered the Nine Years' War as allies, but the conflict—waged in Europe and overseas between France, Spain and the Anglo-Dutch alliance—left the English a stronger colonial power than the Dutch, who were forced to devote a larger proportion of their military budget on the costly land war in Europe. The 18th century saw England (after 1707, Britain) rise to be the world's dominant colonial power, and France becoming its main rival on the imperial stage. Most former British colonies and protectorates are among the 53 member states of the Commonwealth of Nations, a non-political, voluntary association of equal members, comprising a population of around 2.2 billion people. Sixteen Commonwealth realms voluntarily continue to share the British monarch, Queen Elizabeth II, as their head of state. These sixteen nations are distinct and equal legal entities – the United Kingdom, Australia, Canada, New Zealand, Papua New Guinea, Antigua and Barbuda, The Bahamas, Barbados, Belize, Grenada, Jamaica, Saint Kitts and Nevis, Saint Lucia, Saint Vincent and the Grenadines, Solomon Islands and Tuvalu. Two years later, the Royal African Company was inaugurated, receiving from King Charles a monopoly of the trade to supply slaves to the British colonies of the Caribbean. From the outset, slavery was the basis of the British Empire in the West Indies. Until the abolition of the slave trade in 1807, Britain was responsible for the transportation of 3.5 million African slaves to the Americas, a third of all slaves transported across the Atlantic. To facilitate this trade, forts were established on the coast of West Africa, such as James Island, Accra and Bunce Island. In the British Caribbean, the percentage of the population of African descent rose from 25 percent in 1650 to around 80 percent in 1780, and in the 13 Colonies from 10 percent to 40 percent over the same period (the majority in the southern colonies). For the slave traders, the trade was extremely profitable, and became a major economic mainstay for such western British cities as Bristol and Liverpool, which formed the third corner of the so-called triangular trade with Africa and the Americas. For the transported, harsh and unhygienic conditions on the slaving ships and poor diets meant that the average mortality rate during the Middle Passage was one in seven."
Miami,"Black labor played a crucial role in Miami's early development. During the beginning of the 20th century, migrants from the Bahamas and African-Americans constituted 40 percent of the city's population. Whatever their role in the city's growth, their community's growth was limited to a small space. When landlords began to rent homes to African-Americans in neighborhoods close to Avenue J (what would later become NW Fifth Avenue), a gang of white man with torches visited the renting families and warned them to move or be bombed. The northern side of Miami includes Midtown, a district with a great mix of diversity with many West Indians, Hispanics, European Americans, bohemians, and artists. Edgewater, and Wynwood, are neighborhoods of Midtown and are made up mostly of high-rise residential towers and are home to the Adrienne Arsht Center for the Performing Arts. The wealthier residents usually live in the northeastern part, in Midtown, the Design District, and the Upper East Side, with many sought after 1920s homes and home of the MiMo Historic District, a style of architecture originated in Miami in the 1950s. The northern side of Miami also has notable African American and Caribbean immigrant communities such as Little Haiti, Overtown (home of the Lyric Theater), and Liberty City. During the early 20th century, northerners were attracted to the city, and Miami prospered during the 1920s with an increase in population and infrastructure. The legacy of Jim Crow was embedded in these developments. Miami's chief of police, H. Leslie Quigg, did not hide the fact that he, like many other white Miami police officers, was a member of the Ku Klux Klan. Unsurprisingly, these officers enforced social codes far beyond the written law. Quigg, for example, ""personally and publicly beat a colored bellboy to death for speaking directly to a white woman."" Miami is the southern terminus of Amtrak's Atlantic Coast services, running two lines, the Silver Meteor and the Silver Star, both terminating in New York City. The Miami Amtrak Station is located in the suburb of Hialeah near the Tri-Rail/Metrorail Station on NW 79 St and NW 38 Ave. Current construction of the Miami Central Station will move all Amtrak operations from its current out-of-the-way location to a centralized location with Metrorail, MIA Mover, Tri-Rail, Miami International Airport, and the Miami Intermodal Center all within the same station closer to Downtown. The station was expected to be completed by 2012, but experienced several delays and was later expected to be completed in late 2014, again pushed back to early 2015. Miami is a major television production center, and the most important city in the U.S. for Spanish language media. Univisión, Telemundo and UniMÁS have their headquarters in Miami, along with their production studios. The Telemundo Television Studios produces much of the original programming for Telemundo, such as their telenovelas and talk shows. In 2011, 85% of Telemundo's original programming was filmed in Miami. Miami is also a major music recording center, with the Sony Music Latin and Universal Music Latin Entertainment headquarters in the city, along with many other smaller record labels. The city also attracts many artists for music video and film shootings. Miami has six major causeways that span over Biscayne Bay connecting the western mainland, with the eastern barrier islands along the Atlantic Ocean. The Rickenbacker Causeway is the southernmost causeway and connects Brickell to Virginia Key and Key Biscayne. The Venetian Causeway and MacArthur Causeway connect Downtown with South Beach. The Julia Tuttle Causeway connects Midtown and Miami Beach. The 79th Street Causeway connects the Upper East Side with North Beach. The northernmost causeway, the Broad Causeway, is the smallest of Miami's six causeways, and connects North Miami with Bal Harbour. Cuban immigrants in the 1960s brought the Cuban sandwich, medianoche, Cuban espresso, and croquetas, all of which have grown in popularity to all Miamians, and have become symbols of the city's varied cuisine. Today, these are part of the local culture, and can be found throughout the city in window cafés, particularly outside of supermarkets and restaurants. Restaurants such as Versailles restaurant in Little Havana is a landmark eatery of Miami. Located on the Atlantic Ocean, and with a long history as a seaport, Miami is also known for its seafood, with many seafood restaurants located along the Miami River, and in and around Biscayne Bay. Miami is also the home of restaurant chains such as Burger King, Tony Roma's and Benihana. Miami is also considered a ""hot spot"" for dance music, Freestyle, a style of dance music popular in the 80's and 90's heavily influenced by Electro, hip-hop, and disco. Many popular Freestyle acts such as Pretty Tony, Debbie Deb, Stevie B, and Exposé, originated in Miami. Indie/folk acts Cat Power and Iron & Wine are based in the city, while alternative hip hop artist Sage Francis, electro artist Uffie, and the electroclash duo Avenue D were born in Miami, but musically based elsewhere. Also, ska punk band Against All Authority is from Miami, and rock/metal bands Nonpoint and Marilyn Manson each formed in neighboring Fort Lauderdale. Cuban American female recording artist, Ana Cristina, was born in Miami in 1985. Miami is home to one of the largest ports in the United States, the PortMiami. It is the largest cruise ship port in the world. The port is often called the ""Cruise Capital of the World"" and the ""Cargo Gateway of the Americas"". It has retained its status as the number one cruise/passenger port in the world for well over a decade accommodating the largest cruise ships and the major cruise lines. In 2007, the port served 3,787,410 passengers. Additionally, the port is one of the nation's busiest cargo ports, importing 7.8 million tons of cargo in 2007. Among North American ports, it ranks second only to the Port of South Louisiana in New Orleans in terms of cargo tonnage imported/exported from Latin America. The port is on 518 acres (2 km2) and has 7 passenger terminals. China is the port's number one import country, and Honduras is the number one export country. Miami has the world's largest amount of cruise line headquarters, home to: Carnival Cruise Lines, Celebrity Cruises, Norwegian Cruise Line, Oceania Cruises, and Royal Caribbean International. In 2014, the Port of Miami Tunnel was completed and will serve the PortMiami. Tourism is also an important industry in Miami. Along with finance and business, the beaches, conventions, festivals and events draw over 38 million visitors annually into the city, from across the country and around the world, spending $17.1 billion. The Art Deco District in South Beach, is reputed as one of the most glamorous in the world for its nightclubs, beaches, historical buildings, and shopping. Annual events such as the Sony Ericsson Open, Art Basel, Winter Music Conference, South Beach Wine & Food Festival, and Mercedes-Benz Fashion Week Miami attract millions to the metropolis every year. In 1960, non-Hispanic whites represented 80% of Miami-Dade county's population. In 1970, the Census Bureau reported Miami's population as 45.3% Hispanic, 32.9% non-Hispanic White, and 22.7% Black. Miami's explosive population growth has been driven by internal migration from other parts of the country, primarily up until the 1980s, as well as by immigration, primarily from the 1960s to the 1990s. Today, immigration to Miami has slowed significantly and Miami's growth today is attributed greatly to its fast urbanization and high-rise construction, which has increased its inner city neighborhood population densities, such as in Downtown, Brickell, and Edgewater, where one area in Downtown alone saw a 2,069% increase in population in the 2010 Census. Miami is regarded as more of a multicultural mosaic, than it is a melting pot, with residents still maintaining much of, or some of their cultural traits. The overall culture of Miami is heavily influenced by its large population of Hispanics and blacks mainly from the Caribbean islands. Miami is a major center, and a leader in finance, commerce, culture, media, entertainment, the arts, and international trade. In 2012, Miami was classified as an Alpha−World City in the World Cities Study Group's inventory. In 2010, Miami ranked seventh in the United States in terms of finance, commerce, culture, entertainment, fashion, education, and other sectors. It ranked 33rd among global cities. In 2008, Forbes magazine ranked Miami ""America's Cleanest City"", for its year-round good air quality, vast green spaces, clean drinking water, clean streets, and city-wide recycling programs. According to a 2009 UBS study of 73 world cities, Miami was ranked as the richest city in the United States, and the world's fifth-richest city in terms of purchasing power. Miami is nicknamed the ""Capital of Latin America"", is the second largest U.S. city with a Spanish-speaking majority, and the largest city with a Cuban-American plurality. After Fidel Castro rose to power in Cuba in 1959, many wealthy Cubans sought refuge in Miami, further increasing the population. The city developed businesses and cultural amenities as part of the New South. In the 1980s and 1990s, South Florida weathered social problems related to drug wars, immigration from Haiti and Latin America, and the widespread destruction of Hurricane Andrew. Racial and cultural tensions were sometimes sparked, but the city developed in the latter half of the 20th century as a major international, financial, and cultural center. It is the second-largest U.S. city (after El Paso, Texas) with a Spanish-speaking majority, and the largest city with a Cuban-American plurality. Miami's main four sports teams are the Miami Dolphins of the National Football League, the Miami Heat of the National Basketball Association, the Miami Marlins of Major League Baseball, and the Florida Panthers of the National Hockey League. As well as having all four major professional teams, Miami is also home to the Major League Soccer expansion team led by David Beckham, Sony Ericsson Open for professional tennis, numerous greyhound racing tracks, marinas, jai alai venues, and golf courses. The city streets has hosted professional auto races, the Miami Indy Challenge and later the Grand Prix Americas. The Homestead-Miami Speedway oval hosts NASCAR national races. Construction is currently underway on the Miami Intermodal Center and Miami Central Station, a massive transportation hub servicing Metrorail, Amtrak, Tri-Rail, Metrobus, Greyhound Lines, taxis, rental cars, MIA Mover, private automobiles, bicycles and pedestrians adjacent to Miami International Airport. Completion of the Miami Intermodal Center is expected to be completed by winter 2011, and will serve over 150,000 commuters and travelers in the Miami area. Phase I of Miami Central Station is scheduled to begin service in the spring of 2012, and Phase II in 2013. The government of the City of Miami (proper) uses the mayor-commissioner type of system. The city commission consists of five commissioners which are elected from single member districts. The city commission constitutes the governing body with powers to pass ordinances, adopt regulations, and exercise all powers conferred upon the city in the city charter. The mayor is elected at large and appoints a city manager. The City of Miami is governed by Mayor Tomás Regalado and 5 City commissioners which oversee the five districts in the City. The commission's regular meetings are held at Miami City Hall, which is located at 3500 Pan American Drive on Dinner Key in the neighborhood of Coconut Grove . Florida High Speed Rail was a proposed government backed high-speed rail system that would have connected Miami, Orlando, and Tampa. The first phase was planned to connect Orlando and Tampa and was offered federal funding, but it was turned down by Governor Rick Scott in 2011. The second phase of the line was envisioned to connect Miami. By 2014, a private project known as All Aboard Florida by a company of the historic Florida East Coast Railway began construction of a higher-speed rail line in South Florida that is planned to eventually terminate at Orlando International Airport. Miami is partitioned into many different sections, roughly into North, South, West and Downtown. The heart of the city is Downtown Miami and is technically on the eastern side of the city. This area includes Brickell, Virginia Key, Watson Island, and PortMiami. Downtown is South Florida's central business district, and Florida's largest and most influential central business district. Downtown has the largest concentration of international banks in the U.S. along Brickell Avenue. Downtown is home to many major banks, courthouses, financial headquarters, cultural and tourist attractions, schools, parks and a large residential population. East of Downtown, across Biscayne Bay is South Beach. Just northwest of Downtown, is the Civic Center, which is Miami's center for hospitals, research institutes and biotechnology with hospitals such as Jackson Memorial Hospital, Miami VA Hospital, and the University of Miami's Leonard M. Miller School of Medicine. Miami has a tropical monsoon climate (Köppen climate classification Am) with hot and humid summers and short, warm winters, with a marked drier season in the winter. Its sea-level elevation, coastal location, position just above the Tropic of Cancer, and proximity to the Gulf Stream shapes its climate. With January averaging 67.2 °F (19.6 °C), winter features mild to warm temperatures; cool air usually settles after the passage of a cold front, which produces much of the little amount of rainfall during the season. Lows occasionally fall below 50 °F (10 °C), but very rarely below 35 °F (2 °C). Highs generally range between 70–77 °F (21–25 °C). This was also a period of alternatives to nightclubs, the warehouse party, acid house, rave and outdoor festival scenes of the late 1980s and early 1990s were havens for the latest trends in electronic dance music, especially house and its ever-more hypnotic, synthetic offspring techno and trance, in clubs like the infamous Warsaw Ballroom better known as Warsaw and The Mix where DJs like david padilla (who was the resident DJ for both) and radio. The new sound fed back into mainstream clubs across the country. The scene in SoBe, along with a bustling secondhand market for electronic instruments and turntables, had a strong democratizing effect, offering amateur, ""bedroom"" DJs the opportunity to become proficient and popular as both music players and producers, regardless of the whims of the professional music and club industries. Some of these notable DJs are John Benetiz (better known as JellyBean Benetiz), Danny Tenaglia, and David Padilla. Miami and its suburbs are located on a broad plain between the Florida Everglades to the west and Biscayne Bay to the east, which also extends from Florida Bay north to Lake Okeechobee. The elevation of the area never rises above 40 ft (12 m) and averages at around 6 ft (1.8 m) above mean sea level in most neighborhoods, especially near the coast. The highest undulations are found along the coastal Miami Rock Ridge, whose substrate underlies most of the eastern Miami metropolitan region. The main portion of the city lies on the shores of Biscayne Bay which contains several hundred natural and artificially created barrier islands, the largest of which contains Miami Beach and South Beach. The Gulf Stream, a warm ocean current, runs northward just 15 miles (24 km) off the coast, allowing the city's climate to stay warm and mild all year. In the early 1970s, the Miami disco sound came to life with TK Records, featuring the music of KC and the Sunshine Band, with such hits as ""Get Down Tonight"", ""(Shake, Shake, Shake) Shake Your Booty"" and ""That's the Way (I Like It)""; and the Latin-American disco group, Foxy (band), with their hit singles ""Get Off"" and ""Hot Number"". Miami-area natives George McCrae and Teri DeSario were also popular music artists during the 1970s disco era. The Bee Gees moved to Miami in 1975 and have lived here ever since then. Miami-influenced, Gloria Estefan and the Miami Sound Machine, hit the popular music scene with their Cuban-oriented sound and had hits in the 1980s with ""Conga"" and ""Bad Boys"". According to the U.S. Census Bureau, in 2004, Miami had the third highest incidence of family incomes below the federal poverty line in the United States, making it the third poorest city in the USA, behind only Detroit, Michigan (ranked #1) and El Paso, Texas (ranked #2). Miami is also one of the very few cities where its local government went bankrupt, in 2001. However, since that time, Miami has experienced a revival: in 2008, Miami was ranked as ""America's Cleanest City"" according to Forbes for its year-round good air quality, vast green spaces, clean drinking water, clean streets and city-wide recycling programs. In a 2009 UBS study of 73 world cities, Miami was ranked as the richest city in the United States (of four U.S. cities included in the survey) and the world's fifth-richest city, in terms of purchasing power. The city proper is home to less than one-thirteenth of the population of South Florida. Miami is the 42nd-most populous city in the United States. The Miami metropolitan area, which includes Miami-Dade, Broward and Palm Beach counties, had a combined population of more than 5.5 million people, ranked seventh largest in the United States, and is the largest metropolitan area in the Southeastern United States. As of 2008[update], the United Nations estimates that the Miami Urban Agglomeration is the 44th-largest in the world. The Miami area has a unique dialect, (commonly called the ""Miami accent"") which is widely spoken. The dialect developed among second- or third-generation Hispanics, including Cuban-Americans, whose first language was English (though some non-Hispanic white, black, and other races who were born and raised the Miami area tend to adopt it as well.) It is based on a fairly standard American accent but with some changes very similar to dialects in the Mid-Atlantic (especially the New York area dialect, Northern New Jersey English, and New York Latino English.) Unlike Virginia Piedmont, Coastal Southern American, and Northeast American dialects and Florida Cracker dialect (see section below), ""Miami accent"" is rhotic; it also incorporates a rhythm and pronunciation heavily influenced by Spanish (wherein rhythm is syllable-timed). However, this is a native dialect of English, not learner English or interlanguage; it is possible to differentiate this variety from an interlanguage spoken by second-language speakers in that ""Miami accent"" does not generally display the following features: there is no addition of /ɛ/ before initial consonant clusters with /s/, speakers do not confuse of /dʒ/ with /j/, (e.g., Yale with jail), and /r/ and /rr/ are pronounced as alveolar approximant [ɹ] instead of alveolar tap [ɾ] or alveolar trill [r] in Spanish. Other major newspapers include Miami Today, headquartered in Brickell, Miami New Times, headquartered in Midtown, Miami Sun Post, South Florida Business Journal, Miami Times, and Biscayne Boulevard Times. An additional Spanish-language newspapers, Diario Las Americas also serve Miami. The Miami Herald is Miami's primary newspaper with over a million readers and is headquartered in Downtown in Herald Plaza. Several other student newspapers from the local universities, such as the oldest, the University of Miami's The Miami Hurricane, Florida International University's The Beacon, Miami-Dade College's The Metropolis, Barry University's The Buccaneer, amongst others. Many neighborhoods and neighboring areas also have their own local newspapers such as the Aventura News, Coral Gables Tribune, Biscayne Bay Tribune, and the Palmetto Bay News. Miami (/maɪˈæmi/; Spanish pronunciation: [maiˈami]) is a city located on the Atlantic coast in southeastern Florida and the seat of Miami-Dade County. The 44th-most populated city proper in the United States, with a population of 430,332, it is the principal, central, and most populous city of the Miami metropolitan area, and the second most populous metropolis in the Southeastern United States after Washington, D.C. According to the U.S. Census Bureau, Miami's metro area is the eighth-most populous and fourth-largest urban area in the United States, with a population of around 5.5 million. Miami International Airport and PortMiami are among the nation's busiest ports of entry, especially for cargo from South America and the Caribbean. The Port of Miami is the world's busiest cruise port, and MIA is the busiest airport in Florida, and the largest gateway between the United States and Latin America. Additionally, the city has the largest concentration of international banks in the country, primarily along Brickell Avenue in Brickell, Miami's financial district. Due to its strength in international business, finance and trade, many international banks have offices in Downtown such as Espírito Santo Financial Group, which has its U.S. headquarters in Miami. Miami was also the host city of the 2003 Free Trade Area of the Americas negotiations, and is one of the leading candidates to become the trading bloc's headquarters. Since 2001, Miami has been undergoing a large building boom with more than 50 skyscrapers rising over 400 feet (122 m) built or currently under construction in the city. Miami's skyline is ranked third-most impressive in the U.S., behind New York City and Chicago, and 19th in the world according to the Almanac of Architecture and Design. The city currently has the eight tallest (as well as thirteen of the fourteen tallest) skyscrapers in the state of Florida, with the tallest being the 789-foot (240 m) Four Seasons Hotel & Tower. The southern side of Miami includes Coral Way, The Roads and Coconut Grove. Coral Way is a historic residential neighborhood built in 1922 connecting Downtown with Coral Gables, and is home to many old homes and tree-lined streets. Coconut Grove was established in 1825 and is the location of Miami's City Hall in Dinner Key, the Coconut Grove Playhouse, CocoWalk, many nightclubs, bars, restaurants and bohemian shops, and as such, is very popular with local college students. It is a historic neighborhood with narrow, winding roads, and a heavy tree canopy. Coconut Grove has many parks and gardens such as Villa Vizcaya, The Kampong, The Barnacle Historic State Park, and is the home of the Coconut Grove Convention Center and numerous historic homes and estates. Miami has one of the largest television markets in the nation and the second largest in the state of Florida. Miami has several major newspapers, the main and largest newspaper being The Miami Herald. El Nuevo Herald is the major and largest Spanish-language newspaper. The Miami Herald and El Nuevo Herald are Miami's and South Florida's main, major and largest newspapers. The papers left their longtime home in downtown Miami in 2013. The newspapers are now headquartered at the former home of U.S. Southern Command in Doral. Beneath the plain lies the Biscayne Aquifer, a natural underground source of fresh water that extends from southern Palm Beach County to Florida Bay, with its highest point peaking around the cities of Miami Springs and Hialeah. Most of the Miami metropolitan area obtains its drinking water from this aquifer. As a result of the aquifer, it is not possible to dig more than 15 to 20 ft (5 to 6 m) beneath the city without hitting water, which impedes underground construction, though some underground parking garages exist. For this reason, the mass transit systems in and around Miami are elevated or at-grade.[citation needed] The surface bedrock under the Miami area is called Miami oolite or Miami limestone. This bedrock is covered by a thin layer of soil, and is no more than 50 feet (15 m) thick. Miami limestone formed as the result of the drastic changes in sea level associated with recent glaciations or ice ages. Beginning some 130,000 years ago the Sangamonian Stage raised sea levels to approximately 25 feet (8 m) above the current level. All of southern Florida was covered by a shallow sea. Several parallel lines of reef formed along the edge of the submerged Florida plateau, stretching from the present Miami area to what is now the Dry Tortugas. The area behind this reef line was in effect a large lagoon, and the Miami limestone formed throughout the area from the deposition of oolites and the shells of bryozoans. Starting about 100,000 years ago the Wisconsin glaciation began lowering sea levels, exposing the floor of the lagoon. By 15,000 years ago, the sea level had dropped to 300 to 350 feet (90 to 110 m) below the contemporary level. The sea level rose quickly after that, stabilizing at the current level about 4000 years ago, leaving the mainland of South Florida just above sea level. Miami's road system is based along the numerical ""Miami Grid"" where Flagler Street forms the east-west baseline and Miami Avenue forms the north-south meridian. The corner of Flagler Street and Miami Avenue is in the middle of Downtown in front of the Downtown Macy's (formerly the Burdine's headquarters). The Miami grid is primarily numerical so that, for example, all street addresses north of Flagler Street and west of Miami Avenue have ""NW"" in their address. Because its point of origin is in Downtown, which is close to the coast, therefore, the ""NW"" and ""SW"" quadrants are much larger than the ""SE"" and ""NE"" quadrants. Many roads, especially major ones, are also named (e.g., Tamiami Trail/SW 8th St), although, with exceptions, the number is in more common usage among locals. Miami is noted as ""the only major city in the United States conceived by a woman, Julia Tuttle"", a local citrus grower and a wealthy Cleveland native. The Miami area was better known as ""Biscayne Bay Country"" in the early years of its growth. In the late 19th century, reports described the area as a promising wilderness. The area was also characterized as ""one of the finest building sites in Florida."" The Great Freeze of 1894–95 hastened Miami's growth, as the crops of the Miami area were the only ones in Florida that survived. Julia Tuttle subsequently convinced Henry Flagler, a railroad tycoon, to expand his Florida East Coast Railway to the region, for which she became known as ""the mother of Miami."" Miami was officially incorporated as a city on July 28, 1896 with a population of just over 300. It was named for the nearby Miami River, derived from Mayaimi, the historic name of Lake Okeechobee. Miami International Airport serves as the primary international airport of the Greater Miami Area. One of the busiest international airports in the world, Miami International Airport caters to over 35 million passengers a year. The airport is a major hub and the single largest international gateway for American Airlines. Miami International is the busiest airport in Florida, and is the United States' second-largest international port of entry for foreign air passengers behind New York's John F. Kennedy International Airport, and is the seventh-largest such gateway in the world. The airport's extensive international route network includes non-stop flights to over seventy international cities in North and South America, Europe, Asia, and the Middle East. Several large companies are headquartered in or around Miami, including but not limited to: Akerman Senterfitt, Alienware, Arquitectonica, Arrow Air, Bacardi, Benihana, Brightstar Corporation, Burger King, Celebrity Cruises, Carnival Corporation, Carnival Cruise Lines, Crispin Porter + Bogusky, Duany Plater-Zyberk & Company, Espírito Santo Financial Group, Fizber.com, Greenberg Traurig, Holland & Knight, Inktel Direct, Interval International, Lennar, Navarro Discount Pharmacies, Norwegian Cruise Lines, Oceania Cruises, Perry Ellis International, RCTV International, Royal Caribbean Cruise Lines, Ryder Systems, Seabourn Cruise Line, Sedano's, Telefónica USA, UniMÁS, Telemundo, Univision, U.S. Century Bank, Vector Group and World Fuel Services. Because of its proximity to Latin America, Miami serves as the headquarters of Latin American operations for more than 1400 multinational corporations, including AIG, American Airlines, Cisco, Disney, Exxon, FedEx, Kraft Foods, LEO Pharma Americas, Microsoft, Yahoo, Oracle, SBC Communications, Sony, Symantec, Visa International, and Wal-Mart. In recent years the city government, under Mayor Manny Diaz, has taken an ambitious stance in support of bicycling in Miami for both recreation and commuting. Every month, the city hosts ""Bike Miami"", where major streets in Downtown and Brickell are closed to automobiles, but left open for pedestrians and bicyclists. The event began in November 2008, and has doubled in popularity from 1,500 participants to about 3,000 in the October 2009 Bike Miami. This is the longest-running such event in the US. In October 2009, the city also approved an extensive 20-year plan for bike routes and paths around the city. The city has begun construction of bike routes as of late 2009, and ordinances requiring bike parking in all future construction in the city became mandatory as of October 2009. Miami's tropical weather allows for year-round outdoors activities. The city has numerous marinas, rivers, bays, canals, and the Atlantic Ocean, which make boating, sailing, and fishing popular outdoors activities. Biscayne Bay has numerous coral reefs which make snorkeling and scuba diving popular. There are over 80 parks and gardens in the city. The largest and most popular parks are Bayfront Park and Bicentennial Park (located in the heart of Downtown and the location of the American Airlines Arena and Bayside Marketplace), Tropical Park, Peacock Park, Morningside Park, Virginia Key, and Watson Island. During the mid-2000s, the city witnessed its largest real estate boom since the Florida land boom of the 1920s. During this period, the city had well over a hundred approved high-rise construction projects in which 50 were actually built. In 2007, however, the housing market crashed causing lots of foreclosures on houses. This rapid high-rise construction, has led to fast population growth in the city's inner neighborhoods, primarily in Downtown, Brickell and Edgewater, with these neighborhoods becoming the fastest-growing areas in the city. The Miami area ranks 8th in the nation in foreclosures. In 2011, Forbes magazine named Miami the second-most miserable city in the United States due to its high foreclosure rate and past decade of corruption among public officials. In 2012, Forbes magazine named Miami the most miserable city in the United States because of a crippling housing crisis that has cost multitudes of residents their homes and jobs. The metro area has one of the highest violent crime rates in the country and workers face lengthy daily commutes. Downtown Miami is home to the largest concentration of international banks in the United States, and many large national and international companies. The Civic Center is a major center for hospitals, research institutes, medical centers, and biotechnology industries. For more than two decades, the Port of Miami, known as the ""Cruise Capital of the World"", has been the number one cruise passenger port in the world. It accommodates some of the world's largest cruise ships and operations, and is the busiest port in both passenger traffic and cruise lines. In addition to such annual festivals like Calle Ocho Festival and Carnaval Miami, Miami is home to many entertainment venues, theaters, museums, parks and performing arts centers. The newest addition to the Miami arts scene is the Adrienne Arsht Center for the Performing Arts, the second-largest performing arts center in the United States after the Lincoln Center in New York City, and is the home of the Florida Grand Opera. Within it are the Ziff Ballet Opera House, the center's largest venue, the Knight Concert Hall, the Carnival Studio Theater and the Peacock Rehearsal Studio. The center attracts many large-scale operas, ballets, concerts, and musicals from around the world and is Florida's grandest performing arts center. Other performing arts venues in Miami include the Gusman Center for the Performing Arts, Coconut Grove Playhouse, Colony Theatre, Lincoln Theatre, New World Center, Actor's Playhouse at the Miracle Theatre, Jackie Gleason Theatre, Manuel Artime Theater, Ring Theatre, Playground Theatre, Wertheim Performing Arts Center, the Fair Expo Center and the Bayfront Park Amphitheater for outdoor music events. Miami's heavy-rail rapid transit system, Metrorail, is an elevated system comprising two lines and 23 stations on a 24.4-mile (39.3 km)-long line. Metrorail connects the urban western suburbs of Hialeah, Medley, and inner-city Miami with suburban The Roads, Coconut Grove, Coral Gables, South Miami and urban Kendall via the central business districts of Miami International Airport, the Civic Center, and Downtown. A free, elevated people mover, Metromover, operates 21 stations on three different lines in greater Downtown Miami, with a station at roughly every two blocks of Downtown and Brickell. Several expansion projects are being funded by a transit development sales tax surcharge throughout Miami-Dade County. Miami is also the headquarters and main production city of many of the world's largest television networks, record label companies, broadcasting companies and production facilities, such as Telemundo, TeleFutura, Galavisión, Mega TV, Univisión, Univision Communications, Inc., Universal Music Latin Entertainment, RCTV International and Sunbeam Television. In 2009, Univisión announced plans to build a new production studio in Miami, dubbed 'Univisión Studios'. Univisión Studios is currently headquartered in Miami, and will produce programming for all of Univisión Communications' television networks. The wet season begins some time in May, ending in mid-October. During this period, temperatures are in the mid 80s to low 90s (29–35 °C), accompanied by high humidity, though the heat is often relieved by afternoon thunderstorms or a sea breeze that develops off the Atlantic Ocean, which then allow lower temperatures, but conditions still remain very muggy. Much of the year's 55.9 inches (1,420 mm) of rainfall occurs during this period. Dewpoints in the warm months range from 71.9 °F (22.2 °C) in June to 73.7 °F (23.2 °C) in August."
Film_speed,"Upon exposure, the amount of light energy that reaches the film determines the effect upon the emulsion. If the brightness of the light is multiplied by a factor and the exposure of the film decreased by the same factor by varying the camera's shutter speed and aperture, so that the energy received is the same, the film will be developed to the same density. This rule is called reciprocity. The systems for determining the sensitivity for an emulsion are possible because reciprocity holds. In practice, reciprocity works reasonably well for normal photographic films for the range of exposures between 1/1000 second to 1/2 second. However, this relationship breaks down outside these limits, a phenomenon known as reciprocity failure. For digital photo cameras (""digital still cameras""), an exposure index (EI) rating—commonly called ISO setting—is specified by the manufacturer such that the sRGB image files produced by the camera will have a lightness similar to what would be obtained with film of the same EI rating at the same exposure. The usual design is that the camera's parameters for interpreting the sensor data values into sRGB values are fixed, and a number of different EI choices are accommodated by varying the sensor's signal gain in the analog realm, prior to conversion to digital. Some camera designs provide at least some EI choices by adjusting the sensor's signal gain in the digital realm. A few camera designs also provide EI adjustment through a choice of lightness parameters for the interpretation of sensor data values into sRGB; this variation allows different tradeoffs between the range of highlights that can be captured and the amount of noise introduced into the shadow areas of the photo. where  is the maximum possible exposure that does not lead to a clipped or bloomed camera output. Typically, the lower limit of the saturation speed is determined by the sensor itself, but with the gain of the amplifier between the sensor and the analog-to-digital converter, the saturation speed can be increased. The factor 78 is chosen such that exposure settings based on a standard light meter and an 18-percent reflective surface will result in an image with a grey level of 18%/√2 = 12.7% of saturation. The factor √2 indicates that there is half a stop of headroom to deal with specular reflections that would appear brighter than a 100% reflecting white surface. As in the Scheiner system, speeds were expressed in 'degrees'. Originally the sensitivity was written as a fraction with 'tenths' (for example ""18/10° DIN""), where the resultant value 1.8 represented the relative base 10 logarithm of the speed. 'Tenths' were later abandoned with DIN 4512:1957-11, and the example above would be written as ""18° DIN"". The degree symbol was finally dropped with DIN 4512:1961-10. This revision also saw significant changes in the definition of film speeds in order to accommodate then-recent changes in the American ASA PH2.5-1960 standard, so that film speeds of black-and-white negative film effectively would become doubled, that is, a film previously marked as ""18° DIN"" would now be labeled as ""21 DIN"" without emulsion changes. The Warnerke Standard Sensitometer consisted of a frame holding an opaque screen with an array of typically 25 numbered, gradually pigmented squares brought into contact with the photographic plate during a timed test exposure under a phosphorescent tablet excited before by the light of a burning Magnesium ribbon. The speed of the emulsion was then expressed in 'degrees' Warnerke (sometimes seen as Warn. or °W.) corresponding with the last number visible on the exposed plate after development and fixation. Each number represented an increase of 1/3 in speed, typical plate speeds were between 10° and 25° Warnerke at the time. The Recommended Exposure Index (REI) technique, new in the 2006 version of the standard, allows the manufacturer to specify a camera model’s EI choices arbitrarily. The choices are based solely on the manufacturer’s opinion of what EI values produce well-exposed sRGB images at the various sensor sensitivity settings. This is the only technique available under the standard for output formats that are not in the sRGB color space. This is also the only technique available under the standard when multi-zone metering (also called pattern metering) is used. The ASA standard underwent a major revision in 1960 with ASA PH2.5-1960, when the method to determine film speed was refined and previously applied safety factors against under-exposure were abandoned, effectively doubling the nominal speed of many black-and-white negative films. For example, an Ilford HP3 that had been rated at 200 ASA before 1960 was labeled 400 ASA afterwards without any change to the emulsion. Similar changes were applied to the DIN system with DIN 4512:1961-10 and the BS system with BS 1380:1963 in the following years. Digital cameras have far surpassed film in terms of sensitivity to light, with ISO equivalent speeds of up to 409,600, a number that is unfathomable in the realm of conventional film photography. Faster processors, as well as advances in software noise reduction techniques allow this type of processing to be executed the moment the photo is captured, allowing photographers to store images that have a higher level of refinement and would have been prohibitively time consuming to process with earlier generations of digital camera hardware. Despite these detailed standard definitions, cameras typically do not clearly indicate whether the user ""ISO"" setting refers to the noise-based speed, saturation-based speed, or the specified output sensitivity, or even some made-up number for marketing purposes. Because the 1998 version of ISO 12232 did not permit measurement of camera output that had lossy compression, it was not possible to correctly apply any of those measurements to cameras that did not produce sRGB files in an uncompressed format such as TIFF. Following the publication of CIPA DC-004 in 2006, Japanese manufacturers of digital still cameras are required to specify whether a sensitivity rating is REI or SOS.[citation needed] The system was later extended to cover larger ranges and some of its practical shortcomings were addressed by the Austrian scientist Josef Maria Eder (1855–1944) and Flemish-born botanist Walter Hecht (de) (1896–1960), (who, in 1919/1920, jointly developed their Eder–Hecht neutral wedge sensitometer measuring emulsion speeds in Eder–Hecht grades). Still, it remained difficult for manufactures to reliably determine film speeds, often only by comparing with competing products, so that an increasing number of modified semi-Scheiner-based systems started to spread, which no longer followed Scheiner's original procedures and thereby defeated the idea of comparability. The ISO standard ISO 12232:2006 gives digital still camera manufacturers a choice of five different techniques for determining the exposure index rating at each sensitivity setting provided by a particular camera model. Three of the techniques in ISO 12232:2006 are carried over from the 1998 version of the standard, while two new techniques allowing for measurement of JPEG output files are introduced from CIPA DC-004. Depending on the technique selected, the exposure index rating can depend on the sensor sensitivity, the sensor noise, and the appearance of the resulting image. The standard specifies the measurement of light sensitivity of the entire digital camera system and not of individual components such as digital sensors, although Kodak has reported using a variation to characterize the sensitivity of two of their sensors in 2001. The noise-based speed is defined as the exposure that will lead to a given signal-to-noise ratio on individual pixels. Two ratios are used, the 40:1 (""excellent image quality"") and the 10:1 (""acceptable image quality"") ratio. These ratios have been subjectively determined based on a resolution of 70 pixels per cm (178 DPI) when viewed at 25 cm (9.8 inch) distance. The signal-to-noise ratio is defined as the standard deviation of a weighted average of the luminance and color of individual pixels. The noise-based speed is mostly determined by the properties of the sensor and somewhat affected by the noise in the electronic gain and AD converter. General Electric switched to use the ASA scale in 1946. Meters manufactured since February 1946 were equipped with the ASA scale (labeled ""Exposure Index"") already. For some of the older meters with scales in ""Film Speed"" or ""Film Value"" (e.g. models DW-48, DW-49 as well as early DW-58 and GW-68 variants), replaceable hoods with ASA scales were available from the manufacturer. The company continued to publish recommended film values after that date, however, they were now aligned to the ASA scale. Based on earlier research work by Loyd Ancile Jones (1884–1954) of Kodak and inspired by the systems of Weston film speed ratings and General Electric film values, the American Standards Association (now named ANSI) defined a new method to determine and specify film speeds of black-and-white negative films in 1943. ASA Z38.2.1-1943 was revised in 1946 and 1947 before the standard grew into ASA PH2.5-1954. Originally, ASA values were frequently referred to as American standard speed numbers or ASA exposure-index numbers. (See also: Exposure Index (EI).) On an international level the German DIN 4512 system has been effectively superseded in the 1980s by ISO 6:1974, ISO 2240:1982, and ISO 5800:1979 where the same sensitivity is written in linear and logarithmic form as ""ISO 100/21°"" (now again with degree symbol). These ISO standards were subsequently adopted by DIN as well. Finally, the latest DIN 4512 revisions were replaced by corresponding ISO standards, DIN 4512-1:1993-05 by DIN ISO 6:1996-02 in September 2000, DIN 4512-4:1985-08 by DIN ISO 2240:1998-06 and DIN 4512-5:1990-11 by DIN ISO 5800:1998-06 both in July 2002. The CIPA DC-004 standard requires that Japanese manufacturers of digital still cameras use either the REI or SOS techniques, and DC-008 updates the Exif specification to differentiate between these values. Consequently, the three EI techniques carried over from ISO 12232:1998 are not widely used in recent camera models (approximately 2007 and later). As those earlier techniques did not allow for measurement from images produced with lossy compression, they cannot be used at all on cameras that produce images only in JPEG format. The standard specifies how speed ratings should be reported by the camera. If the noise-based speed (40:1) is higher than the saturation-based speed, the noise-based speed should be reported, rounded downwards to a standard value (e.g. 200, 250, 320, or 400). The rationale is that exposure according to the lower saturation-based speed would not result in a visibly better image. In addition, an exposure latitude can be specified, ranging from the saturation-based speed to the 10:1 noise-based speed. If the noise-based speed (40:1) is lower than the saturation-based speed, or undefined because of high noise, the saturation-based speed is specified, rounded upwards to a standard value, because using the noise-based speed would lead to overexposed images. The camera may also report the SOS-based speed (explicitly as being an SOS speed), rounded to the nearest standard speed rating. The Weston Cadet (model 852 introduced in 1949), Direct Reading (model 853 introduced 1954) and Master III (models 737 and S141.3 introduced in 1956) were the first in their line of exposure meters to switch and utilize the meanwhile established ASA scale instead. Other models used the original Weston scale up until ca. 1955. The company continued to publish Weston film ratings after 1955, but while their recommended values often differed slightly from the ASA film speeds found on film boxes, these newer Weston values were based on the ASA system and had to be converted for use with older Weston meters by subtracting 1/3 exposure stop as per Weston's recommendation. Vice versa, ""old"" Weston film speed ratings could be converted into ""new"" Westons and the ASA scale by adding the same amount, that is, a film rating of 100 Weston (up to 1955) corresponded with 125 ASA (as per ASA PH2.5-1954 and before). This conversion was not necessary on Weston meters manufactured and Weston film ratings published since 1956 due to their inherent use of the ASA system; however the changes of the ASA PH2.5-1960 revision may be taken into account when comparing with newer ASA or ISO values. Some high-speed black-and-white films, such as Ilford Delta 3200 and Kodak T-MAX P3200, are marketed with film speeds in excess of their true ISO speed as determined using the ISO testing method. For example, the Ilford product is actually an ISO 1000 film, according to its data sheet. The manufacturers do not indicate that the 3200 number is an ISO rating on their packaging. Kodak and Fuji also marketed E6 films designed for pushing (hence the ""P"" prefix), such as Ektachrome P800/1600 and Fujichrome P1600, both with a base speed of ISO 400. The DIN system, officially DIN standard 4512 by Deutsches Institut für Normung (but still named Deutscher Normenausschuß (DNA) at this time), was published in January 1934. It grew out of drafts for a standardized method of sensitometry put forward by Deutscher Normenausschuß für Phototechnik as proposed by the committee for sensitometry of the Deutsche Gesellschaft für photographische Forschung since 1930 and presented by Robert Luther (de) (1868–1945) and Emanuel Goldberg (1881–1970) at the influential VIII. International Congress of Photography (German: Internationaler Kongreß für wissenschaftliche und angewandte Photographie) held in Dresden from August 3 to 8, 1931. The Standard Output Sensitivity (SOS) technique, also new in the 2006 version of the standard, effectively specifies that the average level in the sRGB image must be 18% gray plus or minus 1/3 stop when the exposure is controlled by an automatic exposure control system calibrated per ISO 2721 and set to the EI with no exposure compensation. Because the output level is measured in the sRGB output from the camera, it is only applicable to sRGB images—typically JPEG—and not to output files in raw image format. It is not applicable when multi-zone metering is used. Film speed is found from a plot of optical density vs. log of exposure for the film, known as the D–log H curve or Hurter–Driffield curve. There typically are five regions in the curve: the base + fog, the toe, the linear region, the shoulder, and the overexposed region. For black-and-white negative film, the “speed point” m is the point on the curve where density exceeds the base + fog density by 0.1 when the negative is developed so that a point n where the log of exposure is 1.3 units greater than the exposure at point m has a density 0.8 greater than the density at point m. The exposure Hm, in lux-s, is that for point m when the specified contrast condition is satisfied. The ISO arithmetic speed is determined from: Before the advent of the ASA system, the system of Weston film speed ratings was introduced by Edward Faraday Weston (1878–1971) and his father Dr. Edward Weston (1850–1936), a British-born electrical engineer, industrialist and founder of the US-based Weston Electrical Instrument Corporation, with the Weston model 617, one of the earliest photo-electric exposure meters, in August 1932. The meter and film rating system were invented by William Nelson Goodwin, Jr., who worked for them and later received a Howard N. Potts Medal for his contributions to engineering. Film speed is used in the exposure equations to find the appropriate exposure parameters. Four variables are available to the photographer to obtain the desired effect: lighting, film speed, f-number (aperture size), and shutter speed (exposure time). The equation may be expressed as ratios, or, by taking the logarithm (base 2) of both sides, by addition, using the APEX system, in which every increment of 1 is a doubling of exposure; this increment is commonly known as a ""stop"". The effective f-number is proportional to the ratio between the lens focal length and aperture diameter, the diameter itself being proportional to the square root of the aperture area. Thus, a lens set to f/1.4 allows twice as much light to strike the focal plane as a lens set to f/2. Therefore, each f-number factor of the square root of two (approximately 1.4) is also a stop, so lenses are typically marked in that progression: f/1.4, 2, 2.8, 4, 5.6, 8, 11, 16, 22, 32, etc. Relatively insensitive film, with a correspondingly lower speed index, requires more exposure to light to produce the same image density as a more sensitive film, and is thus commonly termed a slow film. Highly sensitive films are correspondingly termed fast films. In both digital and film photography, the reduction of exposure corresponding to use of higher sensitivities generally leads to reduced image quality (via coarser film grain or higher image noise of other types). In short, the higher the sensitivity, the grainier the image will be. Ultimately sensitivity is limited by the quantum efficiency of the film or sensor. The Scheinergrade (Sch.) system was devised by the German astronomer Julius Scheiner (1858–1913) in 1894 originally as a method of comparing the speeds of plates used for astronomical photography. Scheiner's system rated the speed of a plate by the least exposure to produce a visible darkening upon development. Speed was expressed in degrees Scheiner, originally ranging from 1° Sch. to 20° Sch., where an increment of 19° Sch. corresponded to a hundredfold increase in sensitivity, which meant that an increment of 3° Sch. came close to a doubling of sensitivity."
Normans,"Before Rollo's arrival, its populations did not differ from Picardy or the Île-de-France, which were considered ""Frankish"". Earlier Viking settlers had begun arriving in the 880s, but were divided between colonies in the east (Roumois and Pays de Caux) around the low Seine valley and in the west in the Cotentin Peninsula, and were separated by traditional pagii, where the population remained about the same with almost no foreign settlers. Rollo's contingents who raided and ultimately settled Normandy and parts of the Atlantic coast included Danes, Norwegians, Norse–Gaels, Orkney Vikings, possibly Swedes, and Anglo-Danes from the English Danelaw under Norse control. The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (""Norman"" comes from ""Norseman"") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries. Between 1402 and 1405, the expedition led by the Norman noble Jean de Bethencourt and the Poitevine Gadifer de la Salle conquered the Canarian islands of Lanzarote, Fuerteventura and El Hierro off the Atlantic coast of Africa. Their troops were gathered in Normandy, Gascony and were later reinforced by Castilian colonists. The English name ""Normans"" comes from the French words Normans/Normanz, plural of Normant, modern French normand, which is itself borrowed from Old Low Franconian Nortmann ""Northman"" or directly from Old Norse Norðmaðr, Latinized variously as Nortmannus, Normannus, or Nordmannus (recorded in Medieval Latin, 9th century) to mean ""Norseman, Viking"". The Normans had a profound effect on Irish culture and history after their invasion at Bannow Bay in 1169. Initially the Normans maintained a distinct culture and ethnicity. Yet, with time, they came to be subsumed into Irish culture to the point that it has been said that they became ""more Irish than the Irish themselves."" The Normans settled mostly in an area in the east of Ireland, later known as the Pale, and also built many fine castles and settlements, including Trim Castle and Dublin Castle. Both cultures intermixed, borrowing from each other's language, culture and outlook. Norman descendants today can be recognised by their surnames. Names such as French, (De) Roche, Devereux, D'Arcy, Treacy and Lacy are particularly common in the southeast of Ireland, especially in the southern part of County Wexford where the first Norman settlements were established. Other Norman names such as Furlong predominate there. Another common Norman-Irish name was Morell (Murrell) derived from the French Norman name Morel. Other names beginning with Fitz (from the Norman for son) indicate Norman ancestry. These included Fitzgerald, FitzGibbons (Gibbons) dynasty, Fitzmaurice. Other families bearing such surnames as Barry (de Barra) and De Búrca (Burke) are also of Norman extraction. The customary law of Normandy was developed between the 10th and 13th centuries and survives today through the legal systems of Jersey and Guernsey in the Channel Islands. Norman customary law was transcribed in two customaries in Latin by two judges for use by them and their colleagues: These are the Très ancien coutumier (Very ancient customary), authored between 1200 and 1245; and the Grand coutumier de Normandie (Great customary of Normandy, originally Summa de legibus Normanniae in curia laïcali), authored between 1235 and 1245. In the visual arts, the Normans did not have the rich and distinctive traditions of the cultures they conquered. However, in the early 11th century the dukes began a programme of church reform, encouraging the Cluniac reform of monasteries and patronising intellectual pursuits, especially the proliferation of scriptoria and the reconstitution of a compilation of lost illuminated manuscripts. The church was utilised by the dukes as a unifying force for their disparate duchy. The chief monasteries taking part in this ""renaissance"" of Norman art and scholarship were Mont-Saint-Michel, Fécamp, Jumièges, Bec, Saint-Ouen, Saint-Evroul, and Saint-Wandrille. These centres were in contact with the so-called ""Winchester school"", which channeled a pure Carolingian artistic tradition to Normandy. In the final decade of the 11th and first of the 12th century, Normandy experienced a golden age of illustrated manuscripts, but it was brief and the major scriptoria of Normandy ceased to function after the midpoint of the century. The legendary religious zeal of the Normans was exercised in religious wars long before the First Crusade carved out a Norman principality in Antioch. They were major foreign participants in the Reconquista in Iberia. In 1018, Roger de Tosny travelled to the Iberian Peninsula to carve out a state for himself from Moorish lands, but failed. In 1064, during the War of Barbastro, William of Montreuil led the papal army and took a huge booty. Bethencourt took the title of King of the Canary Islands, as vassal to Henry III of Castile. In 1418, Jean's nephew Maciot de Bethencourt sold the rights to the islands to Enrique Pérez de Guzmán, 2nd Count de Niebla. In April 1191 Richard the Lion-hearted left Messina with a large fleet in order to reach Acre. But a storm dispersed the fleet. After some searching, it was discovered that the boat carrying his sister and his fiancée Berengaria was anchored on the south coast of Cyprus, together with the wrecks of several other ships, including the treasure ship. Survivors of the wrecks had been taken prisoner by the island's despot Isaac Komnenos. On 1 May 1191, Richard's fleet arrived in the port of Limassol on Cyprus. He ordered Isaac to release the prisoners and the treasure. Isaac refused, so Richard landed his troops and took Limassol. The further decline of Byzantine state-of-affairs paved the road to a third attack in 1185, when a large Norman army invaded Dyrrachium, owing to the betrayal of high Byzantine officials. Some time later, Dyrrachium—one of the most important naval bases of the Adriatic—fell again to Byzantine hands. Various princes of the Holy Land arrived in Limassol at the same time, in particular Guy de Lusignan. All declared their support for Richard provided that he support Guy against his rival Conrad of Montferrat. The local barons abandoned Isaac, who considered making peace with Richard, joining him on the crusade, and offering his daughter in marriage to the person named by Richard. But Isaac changed his mind and tried to escape. Richard then proceeded to conquer the whole island, his troops being led by Guy de Lusignan. Isaac surrendered and was confined with silver chains, because Richard had promised that he would not place him in irons. By 1 June, Richard had conquered the whole island. His exploit was well publicized and contributed to his reputation; he also derived significant financial gains from the conquest of the island. Richard left for Acre on 5 June, with his allies. Before his departure, he named two of his Norman generals, Richard de Camville and Robert de Thornham, as governors of Cyprus. Robert Guiscard, an other Norman adventurer previously elevated to the dignity of count of Apulia as the result of his military successes, ultimately drove the Byzantines out of southern Italy. Having obtained the consent of pope Gregory VII and acting as his vassal, Robert continued his campaign conquering the Balkan peninsula as a foothold for western feudal lords and the Catholic Church. After allying himself with Croatia and the Catholic cities of Dalmatia, in 1081 he led an army of 30,000 men in 300 ships landing on the southern shores of Albania, capturing Valona, Kanina, Jericho (Orikumi), and reaching Butrint after numerous pillages. They joined the fleet that had previously conquered Corfu and attacked Dyrrachium from land and sea, devastating everything along the way. Under these harsh circumstances, the locals accepted the call of emperor Alexius I Comnenus to join forces with the Byzantines against the Normans. The Albanian forces could not take part in the ensuing battle because it had started before their arrival. Immediately before the battle, the Venetian fleet had secured a victory in the coast surrounding the city. Forced to retreat, Alexius ceded the command to a high Albanian official named Comiscortes in the service of Byzantium. The city's garrison resisted until February 1082, when Dyrrachium was betrayed to the Normans by the Venetian and Amalfitan merchants who had settled there. The Normans were now free to penetrate into the hinterland; they took Ioannina and some minor cities in southwestern Macedonia and Thessaly before appearing at the gates of Thessalonica. Dissension among the high ranks coerced the Normans to retreat to Italy. They lost Dyrrachium, Valona, and Butrint in 1085, after the death of Robert. Eventually, the Normans merged with the natives, combining languages and traditions. In the course of the Hundred Years' War, the Norman aristocracy often identified themselves as English. The Anglo-Norman language became distinct from the Latin language, something that was the subject of some humour by Geoffrey Chaucer. The Anglo-Norman language was eventually absorbed into the Anglo-Saxon language of their subjects (see Old English) and influenced it, helping (along with the Norse language of the earlier Anglo-Norse settlers and the Latin used by the church) in the development of Middle English. It in turn evolved into Modern English. The Normans thereafter adopted the growing feudal doctrines of the rest of France, and worked them into a functional hierarchical system in both Normandy and in England. The new Norman rulers were culturally and ethnically distinct from the old French aristocracy, most of whom traced their lineage to Franks of the Carolingian dynasty. Most Norman knights remained poor and land-hungry, and by 1066 Normandy had been exporting fighting horsemen for more than a generation. Many Normans of Italy, France and England eventually served as avid Crusaders under the Italo-Norman prince Bohemund I and the Anglo-Norman king Richard the Lion-Heart. At Saint Evroul, a tradition of singing had developed and the choir achieved fame in Normandy. Under the Norman abbot Robert de Grantmesnil, several monks of Saint-Evroul fled to southern Italy, where they were patronised by Robert Guiscard and established a Latin monastery at Sant'Eufemia. There they continued the tradition of singing. The Normans were in contact with England from an early date. Not only were their original Viking brethren still ravaging the English coasts, they occupied most of the important ports opposite England across the English Channel. This relationship eventually produced closer ties of blood through the marriage of Emma, sister of Duke Richard II of Normandy, and King Ethelred II of England. Because of this, Ethelred fled to Normandy in 1013, when he was forced from his kingdom by Sweyn Forkbeard. His stay in Normandy (until 1016) influenced him and his sons by Emma, who stayed in Normandy after Cnut the Great's conquest of the isle. Normandy was the site of several important developments in the history of classical music in the 11th century. Fécamp Abbey and Saint-Evroul Abbey were centres of musical production and education. At Fécamp, under two Italian abbots, William of Volpiano and John of Ravenna, the system of denoting notes by letters was developed and taught. It is still the most common form of pitch representation in English- and German-speaking countries today. Also at Fécamp, the staff, around which neumes were oriented, was first developed and taught in the 11th century. Under the German abbot Isembard, La Trinité-du-Mont became a centre of musical composition. In England, the period of Norman architecture immediately succeeds that of the Anglo-Saxon and precedes the Early Gothic. In southern Italy, the Normans incorporated elements of Islamic, Lombard, and Byzantine building techniques into their own, initiating a unique style known as Norman-Arab architecture within the Kingdom of Sicily. The French Wars of Religion in the 16th century and French Revolution in the 18th successively destroyed much of what existed in the way of the architectural and artistic remnant of this Norman creativity. The former, with their violence, caused the wanton destruction of many Norman edifices; the latter, with its assault on religion, caused the purposeful destruction of religious objects of any type, and its destabilisation of society resulted in rampant pillaging. Some Normans joined Turkish forces to aid in the destruction of the Armenians vassal-states of Sassoun and Taron in far eastern Anatolia. Later, many took up service with the Armenian state further south in Cilicia and the Taurus Mountains. A Norman named Oursel led a force of ""Franks"" into the upper Euphrates valley in northern Syria. From 1073 to 1074, 8,000 of the 20,000 troops of the Armenian general Philaretus Brachamius were Normans—formerly of Oursel—led by Raimbaud. They even lent their ethnicity to the name of their castle: Afranji, meaning ""Franks."" The known trade between Amalfi and Antioch and between Bari and Tarsus may be related to the presence of Italo-Normans in those cities while Amalfi and Bari were under Norman rule in Italy. The conquest of Cyprus by the Anglo-Norman forces of the Third Crusade opened a new chapter in the history of the island, which would be under Western European domination for the following 380 years. Although not part of a planned operation, the conquest had much more permanent results than initially expected. Normans came into Scotland, building castles and founding noble families who would provide some future kings, such as Robert the Bruce, as well as founding a considerable number of the Scottish clans. King David I of Scotland, whose elder brother Alexander I had married Sybilla of Normandy, was instrumental in introducing Normans and Norman culture to Scotland, part of the process some scholars call the ""Davidian Revolution"". Having spent time at the court of Henry I of England (married to David's sister Maud of Scotland), and needing them to wrestle the kingdom from his half-brother Máel Coluim mac Alaxandair, David had to reward many with lands. The process was continued under David's successors, most intensely of all under William the Lion. The Norman-derived feudal system was applied in varying degrees to most of Scotland. Scottish families of the names Bruce, Gray, Ramsay, Fraser, Ogilvie, Montgomery, Sinclair, Pollock, Burnard, Douglas and Gordon to name but a few, and including the later royal House of Stewart, can all be traced back to Norman ancestry. In 1066, Duke William II of Normandy conquered England killing King Harold II at the Battle of Hastings. The invading Normans and their descendants replaced the Anglo-Saxons as the ruling class of England. The nobility of England were part of a single Normans culture and many had lands on both sides of the channel. Early Norman kings of England, as Dukes of Normandy, owed homage to the King of France for their land on the continent. They considered England to be their most important holding (it brought with it the title of King—an important status symbol). Even before the Norman Conquest of England, the Normans had come into contact with Wales. Edward the Confessor had set up the aforementioned Ralph as earl of Hereford and charged him with defending the Marches and warring with the Welsh. In these original ventures, the Normans failed to make any headway into Wales. When finally Edward the Confessor returned from his father's refuge in 1041, at the invitation of his half-brother Harthacnut, he brought with him a Norman-educated mind. He also brought many Norman counsellors and fighters, some of whom established an English cavalry force. This concept never really took root, but it is a typical example of the attitudes of Edward. He appointed Robert of Jumièges archbishop of Canterbury and made Ralph the Timid earl of Hereford. He invited his brother-in-law Eustace II, Count of Boulogne to his court in 1051, an event which resulted in the greatest of early conflicts between Saxon and Norman and ultimately resulted in the exile of Earl Godwin of Wessex. Subsequent to the Conquest, however, the Marches came completely under the dominance of William's most trusted Norman barons, including Bernard de Neufmarché, Roger of Montgomery in Shropshire and Hugh Lupus in Cheshire. These Normans began a long period of slow conquest during which almost all of Wales was at some point subject to Norman interference. Norman words, such as baron (barwn), first entered Welsh at that time. By far the most famous work of Norman art is the Bayeux Tapestry, which is not a tapestry but a work of embroidery. It was commissioned by Odo, the Bishop of Bayeux and first Earl of Kent, employing natives from Kent who were learned in the Nordic traditions imported in the previous half century by the Danish Vikings. One of the first Norman mercenaries to serve as a Byzantine general was Hervé in the 1050s. By then however, there were already Norman mercenaries serving as far away as Trebizond and Georgia. They were based at Malatya and Edessa, under the Byzantine duke of Antioch, Isaac Komnenos. In the 1060s, Robert Crispin led the Normans of Edessa against the Turks. Roussel de Bailleul even tried to carve out an independent state in Asia Minor with support from the local population, but he was stopped by the Byzantine general Alexius Komnenos. In Britain, Norman art primarily survives as stonework or metalwork, such as capitals and baptismal fonts. In southern Italy, however, Norman artwork survives plentifully in forms strongly influenced by its Greek, Lombard, and Arab forebears. Of the royal regalia preserved in Palermo, the crown is Byzantine in style and the coronation cloak is of Arab craftsmanship with Arabic inscriptions. Many churches preserve sculptured fonts, capitals, and more importantly mosaics, which were common in Norman Italy and drew heavily on the Greek heritage. Lombard Salerno was a centre of ivorywork in the 11th century and this continued under Norman domination. Finally should be noted the intercourse between French Crusaders traveling to the Holy Land who brought with them French artefacts with which to gift the churches at which they stopped in southern Italy amongst their Norman cousins. For this reason many south Italian churches preserve works from France alongside their native pieces. In the course of the 10th century, the initially destructive incursions of Norse war bands into the rivers of France evolved into more permanent encampments that included local women and personal property. The Duchy of Normandy, which began in 911 as a fiefdom, was established by the treaty of Saint-Clair-sur-Epte between King Charles III of West Francia and the famed Viking ruler Rollo, and was situated in the former Frankish kingdom of Neustria. The treaty offered Rollo and his men the French lands between the river Epte and the Atlantic coast in exchange for their protection against further Viking incursions. The area corresponded to the northern part of present-day Upper Normandy down to the river Seine, but the Duchy would eventually extend west beyond the Seine. The territory was roughly equivalent to the old province of Rouen, and reproduced the Roman administrative structure of Gallia Lugdunensis II (part of the former Gallia Lugdunensis). The descendants of Rollo's Vikings and their Frankish wives would replace the Norse religion and Old Norse language with Catholicism (Christianity) and the Gallo-Romance language of the local people, blending their maternal Frankish heritage with Old Norse traditions and customs to synthesize a unique ""Norman"" culture in the north of France. The Norman language was forged by the adoption of the indigenous langue d'oïl branch of Romance by a Norse-speaking ruling class, and it developed into the regional language that survives today. Soon after the Normans began to enter Italy, they entered the Byzantine Empire and then Armenia, fighting against the Pechenegs, the Bulgars, and especially the Seljuk Turks. Norman mercenaries were first encouraged to come to the south by the Lombards to act against the Byzantines, but they soon fought in Byzantine service in Sicily. They were prominent alongside Varangian and Lombard contingents in the Sicilian campaign of George Maniaces in 1038–40. There is debate whether the Normans in Greek service actually were from Norman Italy, and it now seems likely only a few came from there. It is also unknown how many of the ""Franks"", as the Byzantines called them, were Normans and not other Frenchmen. One of the claimants of the English throne opposing William the Conqueror, Edgar Atheling, eventually fled to Scotland. King Malcolm III of Scotland married Edgar's sister Margaret, and came into opposition to William who had already disputed Scotland's southern borders. William invaded Scotland in 1072, riding as far as Abernethy where he met up with his fleet of ships. Malcolm submitted, paid homage to William and surrendered his son Duncan as a hostage, beginning a series of arguments as to whether the Scottish Crown owed allegiance to the King of England. The Norman dynasty had a major political, cultural and military impact on medieval Europe and even the Near East. The Normans were famed for their martial spirit and eventually for their Christian piety, becoming exponents of the Catholic orthodoxy into which they assimilated. They adopted the Gallo-Romance language of the Frankish land they settled, their dialect becoming known as Norman, Normaund or Norman French, an important literary language. The Duchy of Normandy, which they formed by treaty with the French crown, was a great fief of medieval France, and under Richard I of Normandy was forged into a cohesive and formidable principality in feudal tenure. The Normans are noted both for their culture, such as their unique Romanesque architecture and musical traditions, and for their significant military accomplishments and innovations. Norman adventurers founded the Kingdom of Sicily under Roger II after conquering southern Italy on the Saracens and Byzantines, and an expedition on behalf of their duke, William the Conqueror, led to the Norman conquest of England at the Battle of Hastings in 1066. Norman cultural and military influence spread from these new European centres to the Crusader states of the Near East, where their prince Bohemond I founded the Principality of Antioch in the Levant, to Scotland and Wales in Great Britain, to Ireland, and to the coasts of north Africa and the Canary Islands. In 1096, Crusaders passing by the siege of Amalfi were joined by Bohemond of Taranto and his nephew Tancred with an army of Italo-Normans. Bohemond was the de facto leader of the Crusade during its passage through Asia Minor. After the successful Siege of Antioch in 1097, Bohemond began carving out an independent principality around that city. Tancred was instrumental in the conquest of Jerusalem and he worked for the expansion of the Crusader kingdom in Transjordan and the region of Galilee.[citation needed] Norman architecture typically stands out as a new stage in the architectural history of the regions they subdued. They spread a unique Romanesque idiom to England and Italy, and the encastellation of these regions with keeps in their north French style fundamentally altered the military landscape. Their style was characterised by rounded arches, particularly over windows and doorways, and massive proportions. A few years after the First Crusade, in 1107, the Normans under the command of Bohemond, Robert's son, landed in Valona and besieged Dyrrachium using the most sophisticated military equipment of the time, but to no avail. Meanwhile, they occupied Petrela, the citadel of Mili at the banks of the river Deabolis, Gllavenica (Ballsh), Kanina and Jericho. This time, the Albanians sided with the Normans, dissatisfied by the heavy taxes the Byzantines had imposed upon them. With their help, the Normans secured the Arbanon passes and opened their way to Dibra. The lack of supplies, disease and Byzantine resistance forced Bohemond to retreat from his campaign and sign a peace treaty with the Byzantines in the city of Deabolis. Several families of Byzantine Greece were of Norman mercenary origin during the period of the Comnenian Restoration, when Byzantine emperors were seeking out western European warriors. The Raoulii were descended from an Italo-Norman named Raoul, the Petraliphae were descended from a Pierre d'Aulps, and that group of Albanian clans known as the Maniakates were descended from Normans who served under George Maniaces in the Sicilian expedition of 1038."
Canon_law,"In Presbyterian and Reformed churches, canon law is known as ""practice and procedure"" or ""church order"", and includes the church's laws respecting its government, discipline, legal practice and worship. It is a fully developed legal system, with all the necessary elements: courts, lawyers, judges, a fully articulated legal code principles of legal interpretation, and coercive penalties, though it lacks civilly-binding force in most secular jurisdictions. The academic degrees in canon law are the J.C.B. (Juris Canonici Baccalaureatus, Bachelor of Canon Law, normally taken as a graduate degree), J.C.L. (Juris Canonici Licentiatus, Licentiate of Canon Law) and the J.C.D. (Juris Canonici Doctor, Doctor of Canon Law). Because of its specialized nature, advanced degrees in civil law or theology are normal prerequisites for the study of canon law. Canonical jurisprudential theory generally follows the principles of Aristotelian-Thomistic legal philosophy. While the term ""law"" is never explicitly defined in the Code, the Catechism of the Catholic Church cites Aquinas in defining law as ""...an ordinance of reason for the common good, promulgated by the one who is in charge of the community"" and reformulates it as ""...a rule of conduct enacted by competent authority for the sake of the common good."" The Greek-speaking Orthodox have collected canons and commentaries upon them in a work known as the Pēdálion (Greek: Πηδάλιον, ""Rudder""), so named because it is meant to ""steer"" the Church. The Orthodox Christian tradition in general treats its canons more as guidelines than as laws, the bishops adjusting them to cultural and other local circumstances. Some Orthodox canon scholars point out that, had the Ecumenical Councils (which deliberated in Greek) meant for the canons to be used as laws, they would have called them nómoi/νόμοι (laws) rather than kanónes/κανόνες (rules), but almost all Orthodox conform to them. The dogmatic decisions of the Councils, though, are to be obeyed rather than to be treated as guidelines, since they are essential for the Church's unity. The institutions and practices of canon law paralleled the legal development of much of Europe, and consequently both modern civil law and common law (legal system) bear the influences of canon law. Edson Luiz Sampel, a Brazilian expert in canon law, says that canon law is contained in the genesis of various institutes of civil law, such as the law in continental Europe and Latin American countries. Sampel explains that canon law has significant influence in contemporary society. In the Church of England, the ecclesiastical courts that formerly decided many matters such as disputes relating to marriage, divorce, wills, and defamation, still have jurisdiction of certain church-related matters (e.g. discipline of clergy, alteration of church property, and issues related to churchyards). Their separate status dates back to the 12th century when the Normans split them off from the mixed secular/religious county and local courts used by the Saxons. In contrast to the other courts of England the law used in ecclesiastical matters is at least partially a civil law system, not common law, although heavily governed by parliamentary statutes. Since the Reformation, ecclesiastical courts in England have been royal courts. The teaching of canon law at the Universities of Oxford and Cambridge was abrogated by Henry VIII; thereafter practitioners in the ecclesiastical courts were trained in civil law, receiving a Doctor of Civil Law (D.C.L.) degree from Oxford, or a Doctor of Laws (LL.D.) degree from Cambridge. Such lawyers (called ""doctors"" and ""civilians"") were centered at ""Doctors Commons"", a few streets south of St Paul's Cathedral in London, where they monopolized probate, matrimonial, and admiralty cases until their jurisdiction was removed to the common law courts in the mid-19th century. The Roman Catholic Church canon law also includes the main five rites (groups) of churches which are in full union with the Roman Catholic Church and the Supreme Pontiff: Canon law is the body of laws and regulations made by ecclesiastical authority (Church leadership), for the government of a Christian organization or church and its members. It is the internal ecclesiastical law governing the Catholic Church (both Latin Church and Eastern Catholic Churches), the Eastern and Oriental Orthodox churches, and the individual national churches within the Anglican Communion. The way that such church law is legislated, interpreted and at times adjudicated varies widely among these three bodies of churches. In all three traditions, a canon was originally a rule adopted by a church council; these canons formed the foundation of canon law. The first Code of Canon Law, 1917, was mostly for the Roman Rite, with limited application to the Eastern Churches. After the Second Vatican Council, (1962 - 1965), another edition was published specifically for the Roman Rite in 1983. Most recently, 1990, the Vatican produced the Code of Canons of the Eastern Churches which became the 1st code of Eastern Catholic Canon Law. Currently, (2004), there are principles of canon law common to the churches within the Anglican Communion; their existence can be factually established; each province or church contributes through its own legal system to the principles of canon law common within the Communion; these principles have a strong persuasive authority and are fundamental to the self-understanding of each of the churches of the Communion; these principles have a living force, and contain in themselves the possibility of further development; and the existence of these principles both demonstrates unity and promotes unity within the Anglican Communion.  The law of the Eastern Catholic Churches in full union with Rome was in much the same state as that of the Latin or Western Church before 1917; much more diversity in legislation existed in the various Eastern Catholic Churches. Each had its own special law, in which custom still played an important part. In 1929 Pius XI informed the Eastern Churches of his intention to work out a Code for the whole of the Eastern Church. The publication of these Codes for the Eastern Churches regarding the law of persons was made between 1949 through 1958 but finalized nearly 30 years later. The Book of Concord is the historic doctrinal statement of the Lutheran Church, consisting of ten credal documents recognized as authoritative in Lutheranism since the 16th century. However, the Book of Concord is a confessional document (stating orthodox belief) rather than a book of ecclesiastical rules or discipline, like canon law. Each Lutheran national church establishes its own system of church order and discipline, though these are referred to as ""canons."" Greek kanon / Ancient Greek: κανών, Arabic Qanun / قانون, Hebrew kaneh / קנה, ""straight""; a rule, code, standard, or measure; the root meaning in all these languages is ""reed"" (cf. the Romance-language ancestors of the English word ""cane""). Much of the legislative style was adapted from the Roman Law Code of Justinian. As a result, Roman ecclesiastical courts tend to follow the Roman Law style of continental Europe with some variation, featuring collegiate panels of judges and an investigative form of proceeding, called ""inquisitorial"", from the Latin ""inquirere"", to enquire. This is in contrast to the adversarial form of proceeding found in the common law system of English and U.S. law, which features such things as juries and single judges. The Apostolic Canons or Ecclesiastical Canons of the Same Holy Apostles is a collection of ancient ecclesiastical decrees (eighty-five in the Eastern, fifty in the Western Church) concerning the government and discipline of the Early Christian Church, incorporated with the Apostolic Constitutions which are part of the Ante-Nicene Fathers In the fourth century the First Council of Nicaea (325) calls canons the disciplinary measures of the Church: the term canon, κανὠν, means in Greek, a rule. There is a very early distinction between the rules enacted by the Church and the legislative measures taken by the State called leges, Latin for laws. Other churches in the Anglican Communion around the world (e.g., the Episcopal Church in the United States, and the Anglican Church of Canada) still function under their own private systems of canon law. In the Catholic Church, canon law is the system of laws and legal principles made and enforced by the Church's hierarchical authorities to regulate its external organization and government and to order and direct the activities of Catholics toward the mission of the Church. In the Roman Church, universal positive ecclesiastical laws, based upon either immutable divine and natural law, or changeable circumstantial and merely positive law, derive formal authority and promulgation from the office of pope, who as Supreme Pontiff possesses the totality of legislative, executive, and judicial power in his person. The actual subject material of the canons is not just doctrinal or moral in nature, but all-encompassing of the human condition. The history of Latin canon law can be divided into four periods: the jus antiquum, the jus novum, the jus novissimum and the Code of Canon Law. In relation to the Code, history can be divided into the jus vetus (all law before the Code) and the jus novum (the law of the Code, or jus codicis). The Catholic Church has what is claimed to be the oldest continuously functioning internal legal system in Western Europe, much later than Roman law but predating the evolution of modern European civil law traditions. What began with rules (""canons"") adopted by the Apostles at the Council of Jerusalem in the first century has developed into a highly complex legal system encapsulating not just norms of the New Testament, but some elements of the Hebrew (Old Testament), Roman, Visigothic, Saxon, and Celtic legal traditions. Roman canon law had been criticized by the Presbyterians as early as 1572 in the Admonition to Parliament. The protest centered on the standard defense that canon law could be retained so long as it did not contradict the civil law. According to Polly Ha, the Reformed Church Government refuted this claiming that the bishops had been enforcing canon law for 1500 years. The canon law of the Eastern Catholic Churches, which had developed some different disciplines and practices, underwent its own process of codification, resulting in the Code of Canons of the Eastern Churches promulgated in 1990 by Pope John Paul II."
Royal_Institute_of_British_Architects,"In the nineteenth and twentieth centuries the RIBA and its members had a leading part in the promotion of architectural education in the United Kingdom, including the establishment of the Architects' Registration Council of the United Kingdom (ARCUK) and the Board of Architectural Education under the Architects (Registration) Acts, 1931 to 1938. A member of the RIBA, Lionel Bailey Budden, then Associate Professor in the Liverpool University School of Architecture, had contributed the article on Architectural Education published in the fourteenth edition of the Encyclopædia Britannica (1929). His School, Liverpool, was one of the twenty schools named for the purpose of constituting the statutory Board of Architectural Education when the 1931 Act was passed. In 2007, RIBA called for minimum space standards in newly built British houses after research was published suggesting that British houses were falling behind other European countries. ""The average new home sold to people today is significantly smaller than that built in the 1920s... We're way behind the rest of Europe—even densely populated Holland has better proportioned houses than are being built in the country. So let's see minimum space standards for all new homes,"" said RIBA president Jack Pringle. It was granted its Royal Charter in 1837 under King William IV. Supplemental Charters of 1887, 1909 and 1925 were replaced by a single Charter in 1971, and there have been minor amendments since then. The Institute also maintains a dozen regional offices around the United Kingdom, it opened its first regional office for the East of England at Cambridge in 1966. Since 2004, through the V&A + RIBA Architecture Partnership, the RIBA and V&A have worked together to promote the understanding and enjoyment of architecture. Soon after the passing of the 1931 Act, in the book published on the occasion of the Institute's centenary celebration in 1934, Harry Barnes, FRIBA, Chairman of the Registration Committee, mentioned that ARCUK could not be a rival of any architectural association, least of all the RIBA, given the way ARCUK was constituted. Barnes commented that the Act's purpose was not protecting the architectural profession, and that the legitimate interests of the profession were best served by the (then) architectural associations in which some 80 per cent of those practising architecture were to be found. The content of the acts, particularly section 1 (1) of the amending act of 1938, shows the importance which was then attached to giving architects the responsibility of superintending or supervising the building works of local authorities (for housing and other projects), rather than persons professionally qualified only as municipal or other engineers. By the 1970s another issue had emerged affecting education for qualification and registration for practice as an architect, due to the obligation imposed on the United Kingdom and other European governments to comply with European Union Directives concerning mutual recognition of professional qualifications in favour of equal standards across borders, in furtherance of the policy for a single market of the European Union. This led to proposals for reconstituting ARCUK. Eventually, in the 1990s, before proceeding, the government issued a consultation paper ""Reform of Architects Registration"" (1994). The change of name to ""Architects Registration Board"" was one of the proposals which was later enacted in the Housing Grants, Construction and Regeneration Act 1996 and reenacted as the Architects Act 1997; another was the abolition of the ARCUK Board of Architectural Education. Its services include RIBA Insight, RIBA Appointments, and RIBA Publishing. It publishes the RIBA Product Selector and RIBA Journal. In Newcastle is the NBS, the National Building Specification, which has 130 staff and deals with the building regulations and the Construction Information Service. RIBA Bookshops, which operates online and at 66 Portland Place, is also part of RIBA Enterprises. RIBA Visiting Boards continue to assess courses for exemption from the RIBA's examinations in architecture. Under arrangements made in 2011 the validation criteria are jointly held by the RIBA and the Architects Registration Board, but unlike the ARB, the RIBA also validates courses outside the UK. The design of the Institute's Mycenean lions medal and the motto ‘Usui civium, decori urbium' has been attributed to Thomas Leverton Donaldson, who had been honorary secretary until 1839. The RIBA Guide to its Archive and History (Angela Mace,1986) records that the first official version of this badge was used as a bookplate for the Institute's library and publications from 1835 to 1891, when it was redesigned by J.H.Metcalfe. It was again redesigned in 1931 by Eric Gill and in 1960 by Joan Hassall. The description in the 1837 by-laws was: ""gules, two lions rampant guardant or, supporting a column marked with lines chevron, proper, all standing on a base of the same; a garter surrounding the whole with the inscription Institute of British Architects, anno salutis MDCCCXXXIV; above a mural crown proper, and beneath the motto Usui civium decori urbium "". The library is based at two public sites: the Reading Room at the RIBA's headquarters, 66 Portland Place, London; and the RIBA Architecture Study Rooms in the Henry Cole Wing of the V&A. The Reading Room, designed by the building's architect George Grey Wornum and his wife Miriam, retains its original 1934 Art Deco interior with open bookshelves, original furniture and double-height central space. The study rooms, opened in 2004, were designed by Wright & Wright. The library is funded entirely by the RIBA but it is open to the public without charge. It operates a free education programme aimed at students, education groups and families, and an information service for RIBA members and the public through the RIBA Information Centre. RIBA runs many awards including the Stirling Prize for the best new building of the year, the Royal Gold Medal (first awarded in 1848), which honours a distinguished body of work, and the Stephen Lawrence Prize for projects with a construction budget of less than £500,000. The RIBA also awards the President's Medals for student work, which are regarded as the most prestigious awards in architectural education, and the RIBA President's Awards for Research. The RIBA European Award was inaugurated in 2005 for work in the European Union, outside the UK. The RIBA National Award and the RIBA International Award were established in 2007. Since 1966, the RIBA also judges regional awards which are presented locally in the UK regions (East, East Midlands, London, North East, North West, Northern Ireland, Scotland, South/South East, South West/Wessex, Wales, West Midlands and Yorkshire). In addition to the Architects Registration Board, the RIBA provides accreditation to architecture schools in the UK under a course validation procedure. It also provides validation to international courses without input from the ARB. After the grant of the royal charter it had become known as the Royal Institute of British Architects in London, eventually dropping the reference to London in 1892. In 1934, it moved to its current headquarters on Portland Place, with the building being opened by King George V and Queen Mary. The RIBA is a member organisation, with 44,000 members. Chartered Members are entitled to call themselves chartered architects and to append the post-nominals RIBA after their name; Student Members are not permitted to do so. Formerly, fellowships of the institute were granted, although no longer; those who continue to hold this title instead add FRIBA. The Royal Institute of British Architects (RIBA) is a professional body for architects primarily in the United Kingdom, but also internationally, founded for the advancement of architecture under its charter granted in 1837 and Supplemental Charter granted in 1971. The RIBA Guide to its Archive and History (1986) has a section on the ""Statutory registration of architects"" with a bibliography extending from a draft bill of 1887 to one of 1969. The Guide's section on ""Education"" records the setting up in 1904 of the RIBA Board of Architectural Education, and the system by which any school which applied for recognition, whose syllabus was approved by the Board and whose examinations were conducted by an approved external examiner, and whose standard of attainment was guaranteed by periodical inspections by a ""Visiting Board"" from the BAE, could be placed on the list of ""recognized schools"" and its successful students could qualify for exemption from RIBA examinations. The original Charter of 1837 set out the purpose of the Royal Institute to be: '… the general advancement of Civil Architecture, and for promoting and facilitating the acquirement of the knowledge of the various arts and sciences connected therewith…' The overcrowded conditions of the library was one of the reasons why the RIBA moved from 9 Conduit Street to larger premises at 66 Portland Place in 1934. The library remained open throughout World War Two and was able to shelter the archives of Modernist architect Adolf Loos during the war. The British Architectural Library, sometimes referred to as the RIBA Library, was established in 1834 upon the founding of the institute with donations from members. Now, with over four million items, it is one of the three largest architectural libraries in the world and the largest in Europe. Some items from the collections are on permanent display at the Victoria and Albert Museum (V&A) in the V&A + RIBA Architecture Gallery and included in temporary exhibitions at the RIBA and across Europe and North America. Its collections include: The operational framework is provided by the Byelaws, which are more frequently updated than the Charter. Any revisions to the Charter or Byelaws require the Privy Council's approval.  RIBA is based at 66 Portland Place, London—a 1930s Grade II* listed building designed by architect George Grey Wornum with sculptures by Edward Bainbridge Copnall and James Woodford. Parts of the London building are open to the public, including the Library. It has a large architectural bookshop, a café, restaurant and lecture theatres. Rooms are hired out for events. In 2004, the two institutions created the Architecture Gallery (Room 128) at the V&A showing artefacts from the collections of both institutions, this was the first permanent gallery devoted to architecture in the UK. The adjacent Architecture Exhibition Space (Room 128a) is used for temporary displays related to architecture. Both spaces were designed by Gareth Hoskins Architects. At the same time the RIBA Library Drawing and Archives Collections moved from 21 Portman Place to new facilities in the Henry Cole Wing at the V&A. Under the Partnership new study rooms were opened where members of the public could view items from the RIBA and V&A architectural collections under the supervision of curatorial staff. These and the nearby education room were designed by Wright & Wright Architects. The RIBA has three parts to the education process: Part I which is generally a three-year first degree, a year-out of at least one year work experience in an architectural practice precedes the Part II which is generally a two-year post graduate diploma or masters. A further year out must be taken before the RIBA Part III professional exams can be taken. Overall it takes a minimum of seven years before an architecture student can seek chartered status. RIBA Enterprises is the commercial arm of RIBA, with a registered office in Newcastle upon Tyne, a base at 15 Bonhill Street in London, and an office in Newark. It employs over 250 staff, approximately 180 of whom are based in Newcastle. Originally named the Institute of British Architects in London, it was formed in 1834 by several prominent architects, including Philip Hardwick, Thomas Allom, William Donthorne, Thomas Leverton Donaldson, William Adams Nicholson, John Buonarotti Papworth, and Thomas de Grey, 2nd Earl de Grey. Architectural design competitions are used by an organisation that plans to build a new building or refurbish an existing building. They can be used for buildings, engineering work, structures, landscape design projects or public realm artworks. A competition typically asks for architects and/or designers to submit a design proposal in response to a given Brief. The winning design will then be selected by an independent jury panel of design professionals and client representatives. The independence of the jury is vital to the fair conduct of a competition."
Pope_John_XXIII,"On 25 May 1963, the pope suffered another haemorrhage and required several blood transfusions, but the cancer had perforated the stomach wall and peritonitis soon set in. The doctors conferred in a decision regarding this matter and John XXIII's aide Loris F. Capovilla broke the news to him saying that the cancer had done its work and nothing could be done for him. Around this time, his remaining siblings arrived to be with him. By 31 May, it had become clear that the cancer had overcome the resistance of John XXIII – it had left him confined to his bed. The Roman Catholic Church celebrates his feast day not on the date of his death, June 3, as is usual, nor even on the day of his papal inauguration (as is sometimes done with Popes who are Saints, such as with John Paul II) but on 11 October, the day of the first session of the Second Vatican Council. This is understandable, since he was the one who had had the idea for it and had convened it. On Thursday, 11 September 2014, Pope Francis added his optional memorial to the worldwide General Roman Calendar of saints' feast days, in response to global requests. He is commemorated on the date of his death, 3 June, by the Evangelical Lutheran Church in America and on the following day, 4 June, by the Anglican Church of Canada and the Episcopal Church (United States). Far from being a mere ""stopgap"" pope, to great excitement, John XXIII called for an ecumenical council fewer than ninety years after the First Vatican Council (Vatican I's predecessor, the Council of Trent, had been held in the 16th century). This decision was announced on 29 January 1959 at the Basilica of Saint Paul Outside the Walls. Cardinal Giovanni Battista Montini, who later became Pope Paul VI, remarked to Giulio Bevilacqua that ""this holy old boy doesn't realise what a hornet's nest he's stirring up"". From the Second Vatican Council came changes that reshaped the face of Catholicism: a comprehensively revised liturgy, a stronger emphasis on ecumenism, and a new approach to the world. Following the death of Pope Pius XII on 9 October 1958, Roncalli watched the live funeral on his last full day in Venice on 11 October. His journal was specifically concerned with the funeral and the abused state of the late pontiff's corpse. Roncalli left Venice for the conclave in Rome well aware that he was papabile,[b] and after eleven ballots, was elected to succeed the late Pius XII, so it came as no surprise to him, though he had arrived at the Vatican with a return train ticket to Venice.[citation needed] Pope John XXIII offered to mediate between US President John F. Kennedy and Nikita Khrushchev during the Cuban Missile Crisis in October 1962. Both men applauded the pope for his deep commitment to peace. Khrushchev would later send a message via Norman Cousins and the letter expressed his best wishes for the pontiff's ailing health. John XXIII personally typed and sent a message back to him, thanking him for his letter. Cousins, meanwhile, travelled to New York City and ensured that John would become Time magazine's 'Man of the Year'. John XXIII became the first Pope to receive the title, followed by John Paul II in 1994 and Francis in 2013. John XXIII died of peritonitis caused by a perforated stomach at 19:49 local time on 3 June 1963 at the age of 81, ending a historic pontificate of four years and seven months. He died just as a Mass for him finished in Saint Peter's Square below, celebrated by Luigi Traglia. After he died, his brow was ritually tapped to see if he was dead, and those with him in the room said prayers. Then the room was illuminated, thus informing the people of what had happened. He was buried on 6 June in the Vatican grottos. Two wreaths, placed on the two sides of his tomb, were donated by the prisoners of the Regina Coeli prison and the Mantova jail in Verona. On 22 June 1963, one day after his friend and successor Pope Paul VI was elected, the latter prayed at his tomb. The 50th anniversary of his death was celebrated on 3 June 2013 by Pope Francis, who visited his tomb and prayed there, then addressing the gathered crowd and spoke about the late pope. The people that gathered there at the tomb were from Bergamo, the province where the late pope came from. A month later, on 5 July 2013, Francis approved Pope John XXIII for canonization, along with Pope John Paul II without the traditional second miracle required. Instead, Francis based this decision on John XXIII's merits for the Second Vatican Council. On Sunday, 27 April 2014, John XXIII and Pope John Paul II were declared saints on Divine Mercy Sunday. Roncalli was elected pope on 28 October 1958 at age 76 after 11 ballots. His selection was unexpected, and Roncalli himself had come to Rome with a return train ticket to Venice. He was the first pope to take the pontifical name of ""John"" upon election in more than 500 years, and his choice settled the complicated question of official numbering attached to this papal name due to the antipope of this name. Pope John XXIII surprised those who expected him to be a caretaker pope by calling the historic Second Vatican Council (1962–65), the first session opening on 11 October 1962. His passionate views on equality were summed up in his famous statement, ""We were all made in God's image, and thus, we are all Godly alike."" John XXIII made many passionate speeches during his pontificate, one of which was on the day that he opened the Second Vatican Council in the middle of the night to the crowd gathered in St. Peter's Square: ""Dear children, returning home, you will find children: give your children a hug and say: This is a hug from the Pope!"" ""At 11 am Petrus Canisius Van Lierde as Papal Sacristan was at the bedside of the dying pope, ready to anoint him. The pope began to speak for the very last time: ""I had the great grace to be born into a Christian family, modest and poor, but with the fear of the Lord. My time on earth is drawing to a close. But Christ lives on and continues his work in the Church. Souls, souls, ut omnes unum sint.""[c] Van Lierde then anointed his eyes, ears, mouth, hands and feet. Overcome by emotion, Van Lierde forgot the right order of anointing. John XXIII gently helped him before bidding those present a last farewell. Pope John XXIII did not live to see the Vatican Council to completion. He died of stomach cancer on 3 June 1963, four and a half years after his election and two months after the completion of his final and famed encyclical, Pacem in terris. He was buried in the Vatican grottoes beneath Saint Peter's Basilica on 6 June 1963 and his cause for canonization was opened on 18 November 1965 by his successor, Pope Paul VI, who declared him a Servant of God. In addition to being named Venerable on 20 December 1999, he was beatified on 3 September 2000 by Pope John Paul II alongside Pope Pius IX and three others. Following his beatification, his body was moved on 3 June 2001 from its original place to the altar of Saint Jerome where it could be seen by the faithful. On 5 July 2013, Pope Francis – bypassing the traditionally required second miracle – declared John XXIII a saint, after unanimous agreement by a consistory, or meeting, of the College of Cardinals, based on the fact that he was considered to have lived a virtuous, model lifestyle, and because of the good for the Church which had come from his having opened the Second Vatican Council. He was canonised alongside Pope Saint John Paul II on 27 April 2014. John XXIII today is affectionately known as the ""Good Pope"" and in Italian, ""il Papa buono"". Pope Saint John XXIII (Latin: Ioannes XXIII; Italian: Giovanni XXIII) born Angelo Giuseppe Roncalli,[a] Italian pronunciation: [ˈandʒelo dʒuˈzɛppe roŋˈkalli]; 25 November 1881 – 3 June 1963) reigned as Pope from 28 October 1958 to his death in 1963 and was canonized on 27 April 2014. Angelo Giuseppe Roncalli was the fourth of fourteen children born to a family of sharecroppers who lived in a village in Lombardy. He was ordained to the priesthood on 10 August 1904 and served in a number of posts, including papal nuncio in France and a delegate to Bulgaria, Greece and Turkey. In a consistory on 12 January 1953 Pope Pius XII made Roncalli a cardinal as the Cardinal-Priest of Santa Prisca in addition to naming him as the Patriarch of Venice. Maintaining continuity with his predecessors, John XXIII continued the gradual reform of the Roman liturgy, and published changes that resulted in the 1962 Roman Missal, the last typical edition containing the Tridentine Mass established in 1570 by Pope Pius V at the request of the Council of Trent and whose continued use Pope Benedict XVI authorized in 2007, under the conditions indicated in his motu proprio Summorum Pontificum. In response to the directives of the Second Vatican Council, later editions of the Roman Missal present the 1970 form of the Roman Rite. In February 1925, the Cardinal Secretary of State Pietro Gasparri summoned him to the Vatican and informed him of Pope Pius XI's decision to appoint him as the Apostolic Visitor to Bulgaria (1925–35). On 3 March, Pius XI also named him for consecration as titular archbishop of Areopolis, Jordan. Roncalli was initially reluctant about a mission to Bulgaria, but he would soon relent. His nomination as apostolic visitor was made official on 19 March. Roncalli was consecrated by Giovanni Tacci Porcelli in the church of San Carlo alla Corso in Rome. After he was consecrated, he introduced his family to Pope Pius XI. He chose as his episcopal motto Obedientia et Pax (""Obedience and Peace""), which became his guiding motto. While he was in Bulgaria, an earthquake struck in a town not too far from where he was. Unaffected, he wrote to his sisters Ancilla and Maria and told them both that he was fine. The first session ended in a solemn ceremony on 8 December 1962 with the next session scheduled to occur in 1963 from 12 May to 29 June – this was announced on 12 November 1962. John XXIII's closing speech made subtle references to Pope Pius IX, and he had expressed the desire to see Pius IX beatified and eventually canonized. In his journal in 1959 during a spiritual retreat, John XXIII made this remark: ""I always think of Pius IX of holy and glorious memory, and by imitating him in his sacrifices, I would like to be worthy to celebrate his canonization"". On 3 December 1963, US President Lyndon B. Johnson posthumously awarded him the Presidential Medal of Freedom, the United States' highest civilian award, in recognition of the good relationship between Pope John XXIII and the United States of America. In his speech on 6 December 1963, Johnson said: ""I have also determined to confer the Presidential Medal of Freedom posthumously on another noble man whose death we mourned 6 months ago: His Holiness, Pope John XXIII. He was a man of simple origins, of simple faith, of simple charity. In this exalted office he was still the gentle pastor. He believed in discussion and persuasion. He profoundly respected the dignity of man. He gave the world immortal statements of the rights of man, of the obligations of men to each other, of their duty to strive for a world community in which all can live in peace and fraternal friendship. His goodness reached across temporal boundaries to warm the hearts of men of all nations and of all faiths"". On 12 January 1953, he was appointed Patriarch of Venice and, accordingly, raised to the rank of Cardinal-Priest of Santa Prisca by Pope Pius XII. Roncalli left France for Venice on 23 February 1953 stopping briefly in Milan and then to Rome. On 15 March 1953, he took possession of his new diocese in Venice. As a sign of his esteem, the President of France, Vincent Auriol, claimed the ancient privilege possessed by French monarchs and bestowed the red biretta on Roncalli at a ceremony in the Élysée Palace. It was around this time that he, with the aid of Monsignor Bruno Heim, formed his coat of arms with a lion of Saint Mark on a white ground. Auriol also awarded Roncalli three months later with the award of Commander of the Legion of Honour. On 10 May 1963, John XXIII received the Balzan Prize in private at the Vatican but deflected achievements of himself to the five popes of his lifetime, Pope Leo XIII to Pius XII. On 11 May, the Italian President Antonio Segni officially awarded Pope John XXIII with the Balzan Prize for his engagement for peace. While in the car en route to the official ceremony, he suffered great stomach pains but insisted on meeting with Segni to receive the award in the Quirinal Palace, refusing to do so within the Vatican. He stated that it would have been an insult to honour a pontiff on the remains of the crucified Saint Peter. It was the pope's last public appearance. Roncalli was summoned to the final ballot of the conclave at 4:00 pm. He was elected pope at 4:30 pm with a total of 38 votes. After the long pontificate of Pope Pius XII, the cardinals chose a man who – it was presumed because of his advanced age – would be a short-term or ""stop-gap"" pope. They wished to choose a candidate who would do little during the new pontificate. Upon his election, Cardinal Eugene Tisserant asked him the ritual questions of whether he would accept and if so, what name he would take for himself. Roncalli gave the first of his many surprises when he chose ""John"" as his regnal name. Roncalli's exact words were ""I will be called John"". This was the first time in over 500 years that this name had been chosen; previous popes had avoided its use since the time of the Antipope John XXIII during the Western Schism several centuries before. On 11 October 1962, the first session of the Second Vatican Council was held in the Vatican. He gave the Gaudet Mater Ecclesia speech, which served as the opening address for the council. The day was basically electing members for several council commissions that would work on the issues presented in the council. On that same night following the conclusion of the first session, the people in Saint Peter's Square chanted and yelled with the sole objective of getting John XXIII to appear at the window to address them. Many had considered Giovanni Battista Montini, the Archbishop of Milan, a possible candidate, but, although he was the archbishop of one of the most ancient and prominent sees in Italy, he had not yet been made a cardinal. Though his absence from the 1958 conclave did not make him ineligible – under Canon Law any Catholic male who is capable of receiving priestly ordination and episcopal consecration may be elected – the College of Cardinals usually chose the new pontiff from among the Cardinals who head archdioceses or departments of the Roman Curia that attend the papal conclave. At the time, as opposed to contemporary practice, the participating Cardinals did not have to be below age 80 to vote, there were few Eastern-rite Cardinals, and no Cardinals who were just priests at the time of their elevation. He was known affectionately as ""Good Pope John"". His cause for canonization was opened under Pope Paul VI during the final session of the Second Vatican Council on 18 November 1965, along with the cause of Pope Pius XII. On 3 September 2000, John XXIII was declared ""Blessed"" alongside Pope Pius IX by Pope John Paul II, the penultimate step on the road to sainthood after a miracle of curing an ill woman was discovered. He was the first pope since Pope Pius X to receive this honour. Following his beatification, his body was moved from its original burial place in the grottoes below the Vatican to the altar of St. Jerome and displayed for the veneration of the faithful.[citation needed] In February 1939, he received news from his sisters that his mother was dying. On 10 February 1939, Pope Pius XI died. Roncalli was unable to see his mother for the end as the death of a pontiff meant that he would have to stay at his post until the election of a new pontiff. Unfortunately, she died on 20 February 1939, during the nine days of mourning for the late Pius XI. He was sent a letter by Cardinal Eugenio Pacelli, and Roncalli later recalled that it was probably the last letter Pacelli sent until his election as Pope Pius XII on 2 March 1939. Roncalli expressed happiness that Pacelli was elected, and, on radio, listened to the coronation of the new pontiff.  John XXIII was an advocate for human rights which included the unborn and the elderly. He wrote about human rights in his Pacem in terris. He wrote, ""Man has the right to live. He has the right to bodily integrity and to the means necessary for the proper development of life, particularly food, clothing, shelter, medical care, rest, and, finally, the necessary social services. In consequence, he has the right to be looked after in the event of ill health; disability stemming from his work; widowhood; old age; enforced unemployment; or whenever through no fault of his own he is deprived of the means of livelihood."" His sister Ancilla would soon be diagnosed with stomach cancer in the early 1950s. Roncalli's last letter to her was dated on 8 November 1953 where he promised to visit her within the next week. He could not keep that promise, as Ancilla died on 11 November 1953 at the time when he was consecrating a new church in Venice. He attended her funeral back in his hometown. In his will around this time, he mentioned that he wished to be buried in the crypt of Saint Mark's in Venice with some of his predecessors rather than with the family in Sotto il Monte. On 30 November 1934, he was appointed Apostolic Delegate to Turkey and Greece and titular archbishop of Mesembria, Bulgaria. Thus, he is known as ""the Turcophile Pope,"" by the Turkish society which is predominantly Muslim. Roncalli took up this post in 1935 and used his office to help the Jewish underground in saving thousands of refugees in Europe, leading some to consider him to be a Righteous Gentile (see Pope John XXIII and Judaism). In October 1935, he led Bulgarian pilgrims to Rome and introduced them to Pope Pius XI on 14 October."
Southampton,"According to Hampshire Constabulary figures, Southampton is currently safer than it has ever been before, with dramatic reductions in violent crime year on year for the last three years. Data from the Southampton Safer City Partnership shows there has been a reduction in all crimes in recent years and an increase in crime detection rates. According to government figures Southampton has a higher crime rate than the national average. There is some controversy regarding comparative crime statisitics due to inconsistencies between different police forces recording methodologies. For example, in Hampshire all reported incidents are recorded and all records then retained. However, in neighbouring Dorset crimes reports withdrawn or shown to be false are not recorded, reducing apparent crime figures. In the violence against the person category, the national average is 16.7 per 1000 population while Southampton is 42.4 per 1000 population. In the theft from a vehicle category, the national average is 7.6 per 1000 compared to Southampton's 28.4 per 1000. Overall, for every 1,000 people in the city, 202 crimes are recorded. Hampshire Constabulary's figures for 2009/10 show fewer incidents of recorded crime in Southampton than the previous year. The geography of Southampton is influenced by the sea and rivers. The city lies at the northern tip of the Southampton Water, a deep water estuary, which is a ria formed at the end of the last Ice Age. Here, the rivers Test and Itchen converge. The Test—which has salt marsh that makes it ideal for salmon fishing—runs along the western edge of the city, while the Itchen splits Southampton in two—east and west. The city centre is located between the two rivers. The two local Sunday Leagues in the Southampton area are the City of Southampton Sunday Football League and the Southampton and District Sunday Football League. At certain times of the year, The Queen Mary 2, Queen Elizabeth and Queen Victoria may all visit Southampton at the same time, in an event commonly called 'Arrival of the Three Queens'. The city has undergone many changes to its governance over the centuries and once again became administratively independent from Hampshire County as it was made into a unitary authority in a local government reorganisation on 1 April 1997, a result of the 1992 Local Government Act. The district remains part of the Hampshire ceremonial county. Local train services operate in the central, southern and eastern sections of the city and are operated by South West Trains, with stations at Swaythling, St Denys, Millbrook, Redbridge, Bitterne, Sholing and Woolston. Plans were announced by Hampshire County Council in July 2009 for the introduction of tram-train running from Hythe (on what is now a freight-only line to Fawley) via Totton to Southampton Central Station and on to Fareham via St. Denys, and Swanwick. The proposal follows a failed plan to bring light rail to the Portsmouth and Gosport areas in 2005. The city is also well provided for in amateur men's and women's rugby with a number of teams in and around the city, the oldest of which is Trojans RFC who were promoted to London South West 2 division in 2008/9. A notable former player is Anthony Allen, who played with Leicester Tigers as a centre. Tottonians are also in London South West division 2 and Southampton RFC are in Hampshire division 1 in 2009/10, alongside Millbrook RFC and Eastleigh RFC. Many of the sides run mini and midi teams from under sevens up to under sixteens for both boys and girls. Southampton's strong economy is promoting redevelopment, and major projects are proposed, including the city's first skyscrapers on the waterfront. The three towers proposed will stand 23 storeys high and will be surrounded by smaller apartment blocks, office blocks and shops. There are also plans for a 15-storey hotel at the Ocean Village marina, and a 21-storey hotel on the north eastern corner of the city centre, as part of a £100m development. Many of the world's largest cruise ships can regularly be seen in Southampton water, including record-breaking vessels from Royal Caribbean and Carnival Corporation & plc. The latter has headquarters in Southampton, with its brands including Princess Cruises, P&O Cruises and Cunard Line. Southampton Solent University has 17,000 students and its strengths are in the training, design, consultancy, research and other services undertaken for business and industry. It is also host to the Warsash Maritime Academy, which provides training and certification for the international shipping and off-shore oil industries. The city provides for yachting and water sports, with a number of marinas. From 1977 to 2001 the Whitbread Around the World Yacht Race, which is now known as the Volvo Ocean Race was based in Southampton's Ocean Village marina. Pockets of Georgian architecture survived the war, but much of the city was levelled. There has been extensive redevelopment since World War II. Increasing traffic congestion in the 1920s led to partial demolition of medieval walls around the Bargate in 1932 and 1938. However a large portion of those walls remain. The River Test runs along the western border of the city, separating it from the New Forest. There are bridges over the Test from Southampton, including the road and rail bridges at Redbridge in the south and the M27 motorway to the north. The River Itchen runs through the middle of the city and is bridged in several places. The northernmost bridge, and the first to be built, is at Mansbridge, where the A27 road crosses the Itchen. The original bridge is closed to road traffic, but is still standing and open to pedestrians and cyclists. The river is bridged again at Swaythling, where Woodmill Bridge separates the tidal and non tidal sections of the river. Further south is Cobden Bridge which is notable as it was opened as a free bridge (it was originally named the Cobden Free Bridge), and was never a toll bridge. Downstream of the Cobden Bridge is the Northam Railway Bridge, then the Northam Road Bridge, which was the first major pre-stressed concrete bridge to be constructed in the United Kingdom. The southernmost, and newest, bridge on the Itchen is the Itchen Bridge, which is a toll bridge. The town was sacked in 1338 by French, Genoese and Monegasque ships (under Charles Grimaldi, who used the plunder to help found the principality of Monaco). On visiting Southampton in 1339, Edward III ordered that walls be built to 'close the town'. The extensive rebuilding—part of the walls dates from 1175—culminated in the completion of the western walls in 1380. Roughly half of the walls, 13 of the original towers, and six gates survive. The city hockey club, Southampton Hockey Club, founded in 1938, is now one of the largest and highly regarded clubs in Hampshire, fielding 7 senior men's and 5 senior ladies teams on a weekly basis along with boys’ and girls’ teams from 6 upwards. In addition to school sixth forms at St Anne's and King Edward's there are two sixth form colleges: Itchen College and Richard Taunton Sixth Form College. A number of Southampton pupils will travel outside the city, for example to Barton Peveril College. Southampton City College is a further education college serving the city. The college offers a range of vocational courses for school leavers, as well as ESOL programmes and Access courses for adult learners. Over 40 per cent of school pupils in the city that responded to a survey claimed to have been the victim of bullying. More than 2,000 took part and said that verbal bullying was the most common form, although physical bullying was a close second for boys. Just over a quarter of the jobs available in the city are in the health and education sector. A further 19 per cent are property and other business and the third largest sector is wholesale and retail, which accounts for 16.2 percent. Between 1995 and 2004, the number of jobs in Southampton has increased by 18.5 per cent. Significant employers in Southampton include The University of Southampton, Southampton Solent University, Southampton Airport, Ordnance Survey, BBC South, the NHS, ABP and Carnival UK. Southampton is noted for its association with the RMS Titanic, the Spitfire and more generally in the World War II narrative as one of the departure points for D-Day, and more recently as the home port of a number of the largest cruise ships in the world. Southampton has a large shopping centre and retail park called WestQuay. In October 2014, the City Council approved a follow-up from the WestQuay park, called WestQuay Watermark. Construction by Sir Robert McAlpine commenced in January 2015. Hammerson, the owners of the retail park, aim to have at least 1,550 people employed on its premises at year-end 2016. Surviving remains of 12th century merchants' houses such as King John's House and Canute's Palace are evidence of the wealth that existed in the town at this time. In 1348, the Black Death reached England via merchant vessels calling at Southampton. Southampton is home to Southampton Football Club—nicknamed ""The Saints""—who play in the Premier League at St Mary's Stadium, having relocated in 2001 from their 103-year-old former stadium, ""The Dell"". They reached the top flight of English football (First Division) for the first time in 1966, staying there for eight years. They lifted the FA Cup with a shock victory over Manchester United in 1976, returned to the top flight two years later, and stayed there for 27 years (becoming founder members of the Premier League in 1992) before they were relegated in 2005. The club was promoted back to the Premier League in 2012 following a brief spell in the third-tier and severe financial difficulties. In 2015, ""The Saints"" finished 7th in the Premier League, their highest league finish in 30 years, after a remarkable season under new manager Ronald Koeman. Their highest league position came in 1984 when they were runners-up in the old First Division. They were also runners-up in the 1979 Football League Cup final and 2003 FA Cup final. Notable former managers include Ted Bates, Lawrie McMenemy, Chris Nicholl, Ian Branfoot and Gordon Strachan. There is a strong rivalry between Portsmouth F.C. (""South Coast derby"") which is located only about 30 km (19 mi) away. University Hospital Southampton NHS Foundation Trust is one of the city's largest employers. It provides local hospital services to 500,000 people in the Southampton area and specialist regional services to more than 3 million people across the South of England. The Trust owns and manages Southampton General Hospital, the Princess Anne Hospital and a palliative care service at Countess Mountbatten House, part of the Moorgreen Hospital site in the village of West End, just outside the city. Southampton's largest retail centre, and 35th largest in the UK, is the WestQuay Shopping Centre, which opened in September 2000 and hosts major high street stores including John Lewis and Marks and Spencer. The centre was Phase Two of the West Quay development of the former Pirelli undersea cables factory; the first phase of this was the West Quay Retail Park, while the third phase (Watermark Westquay) was put on hold due to the recession. Work is has resumed in 2015, with plans for this third stage including shops, housing, an hotel and a public piazza alongside the Town Walls on Western Esplanade. Southampton has also been granted a licence for a large casino. A further part of the redevelopment of the West Quay site resulted in a new store, opened on 12 February 2009, for Swedish home products retailer IKEA. Marlands is a smaller shopping centre, built in the 1990s and located close to the northern side of WestQuay. Southampton currently has two disused shopping centres: the 1970s Eaststreet mall, and the 1980s Bargate centre. Neither of these were ever commercially successful; the former has been earmarked for redevelopment as a Morrison's supermarket, while the future of the latter is uncertain. There is also the East Street area which has been designated for speciality shopping, with the aim of promoting smaller retailers, alongside the chain store Debenhams. In 2007, Southampton was ranked 13th for shopping in the UK. The town experienced major expansion during the Victorian era. The Southampton Docks company had been formed in 1835. In October 1838 the foundation stone of the docks was laid and the first dock opened in 1842. The structural and economic development of docks continued for the next few decades. The railway link to London was fully opened in May 1840. Southampton subsequently became known as The Gateway to the Empire. Southampton became a spa town in 1740. It had also become a popular site for sea bathing by the 1760s, despite the lack of a good quality beach. Innovative buildings specifically for this purpose were built at West Quay, with baths that were filled and emptied by the flow of the tide. Southampton is also home to one of the most successful College American Football teams in the UK, the Southampton Stags, who play at the Wide Lane Sports Facility in Eastleigh. In his 1854 book ""The Cruise of the Steam Yacht North Star"" John Choules described Southampton thus: ""I hardly know a town that can show a more beautiful Main Street than Southampton, except it be Oxford. The High Street opens from the quay, and under various names it winds in a gently sweeping line for one mile and a half, and is of very handsome width. The variety of style and color of material in the buildings affords an exhibition of outline, light and color, that I think is seldom equalled. The shops are very elegant, and the streets are kept exceedingly clean."" After the establishment of Hampshire County Council, following the act in 1888, Southampton became a county borough within the county of Hampshire, which meant that it had many features of a county, but governance was now shared between the Corporation in Southampton and the new county council. There is a great source of confusion in the fact that the ancient shire county, along with its associated assizes, was known as the County of Southampton or Southamptonshire. This was officially changed to Hampshire in 1959 although the county had been commonly known as Hampshire or Hantscire for centuries. Southampton became a non-metropolitan district in 1974. Southampton is a major UK port which has good transport links with the rest of the country. The M27 motorway, linking places along the south coast of England, runs just to the north of the city. The M3 motorway links the city to London and also, via a link to the A34 (part of the European route E05) at Winchester, with the Midlands and North. The M271 motorway is a spur of the M27, linking it with the Western Docks and city centre. Southampton Airport is a regional airport located in the town of Eastleigh, just north of the city. It offers flights to UK and near European destinations, and is connected to the city by a frequent rail service from Southampton Airport (Parkway) railway station, and by bus services. Archaeological finds suggest that the area has been inhabited since the stone age. Following the Roman invasion of Britain in AD 43 and the conquering of the local Britons in 70 AD the fortress settlement of Clausentum was established. It was an important trading port and defensive outpost of Winchester, at the site of modern Bitterne Manor. Clausentum was defended by a wall and two ditches and is thought to have contained a bath house. Clausentum was not abandoned until around 410. Following the Norman Conquest in 1066, Southampton became the major port of transit between the then capital of England, Winchester, and Normandy. Southampton Castle was built in the 12th century and by the 13th century Southampton had become a leading port, particularly involved in the import of French wine in exchange for English cloth and wool. Other major employers in the city include Ordnance Survey, the UK's national mapping agency, whose headquarters is located in a new building on the outskirts of the city, opened in February 2011. The Lloyd's Register Group has announced plans to move its London marine operations to a specially developed site at the University of Southampton. The area of Swaythling is home to Ford's Southampton Assembly Plant, where the majority of their Transit models are manufactured. Closure of the plant in 2013 was announced in 2012, with the loss of hundreds of jobs. Southampton had an estimated 236,900 people living within the city boundary in 2011. There is a sizeable Polish population in the city, with estimates as high as 20,000. Southampton also has 2 community FM radio stations, the Queens Award winning Unity 101 Community Radio (www.unity101.org) broadcasting full-time on 101.1 FM since 2006 to the Asian and Ethnic communities, and Voice FM (http://www.voicefmradio.co.uk) located in St Mary's, which has been broadcasting full-time on 103.9 FM since September 2011, playing a wide range of music from Rock to Dance music and Top 40. A third station, Awaaz FM (www.awaazfm.co.uk), is an internet only radio stations also catering for Asian and Ethnic community. In the 2001 census Southampton and Portsmouth were recorded as being parts of separate urban areas, however by the time of the 2011 census they had merged to become the sixth largest built-up area in England with a population of 855,569. This built-up area is part of the metropolitan area known as South Hampshire, which is also known as Solent City, particularly in the media when discussing local governance organisational changes. With a population of over 1.5 million this makes the region one of the United Kingdom's most populous metropolitan areas. In 1642, during the English Civil War, a Parliamentary garrison moved into Southampton. The Royalists advanced as far as Redbridge, Southampton, in March 1644 but were prevented from taking the town. At the 2001 Census, 92.4 per cent of the city's populace was White—including one per cent White Irish—3.8 per cent were South Asian, 1.0 per cent Black, 1.3 per cent Chinese or other ethnic groups, and 1.5 per cent were of Mixed Race. Southampton City Council has developed twinning links with Le Havre in France (since 1973), Rems-Murr-Kreis in Germany (since 1991), Trieste in Italy (since 2002); Hampton, Virginia in USA, Qingdao in China (since 1998), and Busan in South Korea (since 1978). Southampton's police service is provided by Hampshire Constabulary. The main base of the Southampton operation is a new, eight storey purpose-built building which cost £30 million to construct. The building, located on Southern Road, opened in 2011 and is near to Southampton Central railway station. Previously, the central Southampton operation was located within the west wing of the Civic Centre, however the ageing facilities and the plans of constructing a new museum in the old police station and magistrates court necessitated the move. There are additional police stations at Portswood, Banister Park, Bitterne, and Shirley as well as a British Transport Police station at Southampton Central railway station. In March 2007 there were 120,305 jobs in Southampton, and 3,570 people claiming job seeker's allowance, approximately 2.4 per cent of the city's population. This compares with an average of 2.5 per cent for England as a whole. A Royal Charter in 1952 upgraded University College at Highfield to the University of Southampton. Southampton acquired city status, becoming the City of Southampton in 1964. Council estates are in the Weston, Thornhill and Townhill Park districts. The city is ranked 96th most deprived out of all 354 Local Authorities in England. During the latter half of the 20th century, a more diverse range of industry also came to the city, including aircraft and car manufacture, cables, electrical engineering products, and petrochemicals. These now exist alongside the city's older industries of the docks, grain milling, and tobacco processing. Between 1996 and 2004, the population of the city increased by 4.9 per cent—the tenth biggest increase in England. In 2005 the Government Statistics stated that Southampton was the third most densely populated city in the country after London and Portsmouth respectively. Hampshire County Council expects the city's population to grow by around a further two per cent between 2006 and 2013, adding around another 4,200 to the total number of residents. The highest increases are expected among the elderly. Southampton has two large live music venues, the Mayflower Theatre (formerly the Gaumont Theatre) and the Guildhall. The Guildhall has seen concerts from a wide range of popular artists including Pink Floyd, David Bowie, Delirious?, Manic Street Preachers, The Killers, The Kaiser Chiefs, Amy Winehouse, Lostprophets, The Midnight Beast, Modestep, and All Time Low. It also hosts classical concerts presented by the Bournemouth Symphony Orchestra, City of Southampton Orchestra, Southampton Concert Orchestra, Southampton Philharmonic Choir and Southampton Choral Society. The Anglo-Saxons formed a new, larger, settlement across the Itchen centred on what is now the St Mary's area of the city. The settlement was known as Hamwic, which evolved into Hamtun and then Hampton. Archaeological excavations of this site have uncovered one of the best collections of Saxon artefacts in Europe. It is from this town that the county of Hampshire gets its name. The city is home to the longest surviving stretch of medieval walls in England, as well as a number of museums such as Tudor House Museum, reopened on 30 July 2011 after undergoing extensive restoration and improvement; Southampton Maritime Museum; God's House Tower, an archaeology museum about the city's heritage and located in one of the tower walls; the Medieval Merchant's House; and Solent Sky, which focuses on aviation. The SeaCity Museum is located in the west wing of the civic centre, formerly occupied by Hampshire Constabulary and the Magistrates' Court, and focuses on Southampton's trading history and on the RMS Titanic. The museum received half a million pounds from the National Lottery in addition to interest from numerous private investors and is budgeted at £28 million. Southampton is also served by the rail network, which is used both by freight services to and from the docks and passenger services as part of the national rail system. The main station in the city is Southampton Central. Rail routes run east towards Portsmouth, north to Winchester, the Midlands and London, and westwards to Bournemouth, Poole, Dorchester, Weymouth, Salisbury, Bristol and Cardiff. The route to London was opened in 1840 by what was to become the London and South Western Railway Company. Both this and its successor the Southern Railway (UK) played a significant role in the creation of the modern port following their purchase and development of the town's docks. The Supermarine Spitfire was designed and developed in Southampton, evolving from the Schneider trophy-winning seaplanes of the 1920s and 1930s. Its designer, R J Mitchell, lived in the Portswood area of Southampton, and his house is today marked with a blue plaque. Heavy bombing of the factory in September 1940 destroyed it as well as homes in the vicinity, killing civilians and workers. World War II hit Southampton particularly hard because of its strategic importance as a major commercial port and industrial area. Prior to the Invasion of Europe, components for a Mulberry harbour were built here. After D-Day, Southampton docks handled military cargo to help keep the Allied forces supplied, making it a key target of Luftwaffe bombing raids until late 1944. Southampton docks was featured in the television show 24: Live Another Day in Day 9: 9:00 p.m. – 10:00 p.m. The largest theatre in the city is the 2,300 capacity Mayflower Theatre (formerly known as the Gaumont), which, as the largest theatre in Southern England outside London, has hosted West End shows such as Les Misérables, The Rocky Horror Show and Chitty Chitty Bang Bang, as well as regular visits from Welsh National Opera and English National Ballet. There is also the Nuffield Theatre based at the University of Southampton's Highfield campus, which is the city's primary producing theatre. It was awarded The Stage Award for Best Regional Theatre in 2015. It also hosts touring companies and local performing societies (such as Southampton Operatic Society, the Maskers and the University Players). Southampton (i/saʊθˈæmptən, -hæmptən/) is the largest city in the ceremonial county of Hampshire on the south coast of England, and is situated 75 miles (121 km) south-west of London and 19 miles (31 km) north-west of Portsmouth. Southampton is a major port and the closest city to the New Forest. It lies at the northernmost point of Southampton Water at the confluence of the River Test and River Itchen, with the River Hamble joining to the south of the urban area. The city, which is a unitary authority, has an estimated population of 253,651. The city's name is sometimes abbreviated in writing to ""So'ton"" or ""Soton"", and a resident of Southampton is called a Sotonian. Southampton Water has the benefit of a double high tide, with two high tide peaks, making the movement of large ships easier. This is not caused as popularly supposed by the presence of the Isle of Wight, but is a function of the shape and depth of the English Channel. In this area the general water flow is distorted by more local conditions reaching across to France. Southampton as a Port and city has had a long history of administrative independence of the surrounding County; as far back as the reign of King John the town and its port were removed from the writ of the King's Sheriff in Hampshire and the rights of custom and toll were granted by the King to the burgesses of Southampton over the port of Southampton and the Port of Portsmouth; this tax farm was granted for an annual fee of £200 in the charter dated at Orival on 29 June 1199. The definition of the port of Southampton was apparently broader than today and embraced all of the area between Lymington and Langstone. The corporation had resident representatives in Newport, Lymington and Portsmouth. By a charter of Henry VI, granted on 9 March 1446/7 (25+26 Hen. VI, m. 32), the mayor, bailiffs and burgesses of the towns and ports of Southampton and Portsmouth became a County incorporate and separate from Hampshire. The city has a strong higher education sector. The University of Southampton and Southampton Solent University together have a student population of over 40,000. The importance of Southampton to the cruise industry was indicated by P&O Cruises's 175th anniversary celebrations, which included all seven of the company's liners visiting Southampton in a single day. Adonia, Arcadia, Aurora, Azura, Oceana, Oriana and Ventura all left the city in a procession on 3 July 2012. Elsewhere, remnants of the medieval water supply system devised by the friars can still be seen today. Constructed in 1290, the system carried water from Conduit Head (remnants of which survive near Hill Lane, Shirley) some 1.7 kilometres to the site of the friary inside the town walls. The friars granted use of the water to the town in 1310 and passed on ownership of the water supply system itself in 1420. Further remains can be observed at Conduit House on Commercial Road. The city is home or birthplace to a number of contemporary musicians such as R'n'B singer Craig David, Coldplay drummer Will Champion, former Holloways singer Rob Skipper as well as 1980s popstar Howard Jones. Several rock bands were formed in Southampton, including Band of Skulls, The Delays, Bury Tomorrow, Heart in Hand, Thomas Tantrum (disbanded in 2011) and Kids Can't Fly (disbanded in 2014). James Zabiela, a highly regarded and recognised name in dance music, is also from Southampton. The annual Southampton Boat Show is held in September each year, with over 600 exhibitors present. It runs for just over a week at Mayflower Park on the city's waterfront, where it has been held since 1968. The Boat Show itself is the climax of Sea City, which runs from April to September each year to celebrate Southampton's links with the sea. On the other hand, many of the medieval buildings once situated within the town walls are now in ruins or have disappeared altogether. From successive incarnations of the motte and bailey castle, only a section of the bailey wall remains today, lying just off Castle Way. The last remains of the Franciscan friary in Southampton, founded circa 1233 and dissolved in 1538, were swept away in the 1940s. The site is now occupied by Friary House. The port was the point of departure for the Pilgrim Fathers aboard Mayflower in 1620. In 1912, the RMS Titanic sailed from Southampton. Four in five of the crew on board the vessel were Sotonians, with about a third of those who perished in the tragedy hailing from the city. Southampton was subsequently the home port for the transatlantic passenger services operated by Cunard with their Blue Riband liner RMS Queen Mary and her running mate RMS Queen Elizabeth. In 1938, Southampton docks also became home to the flying boats of Imperial Airways. Southampton Container Terminals first opened in 1968 and has continued to expand. Southampton was named ""fittest city in the UK"" in 2006 by Men's Fitness magazine. The results were based on the incidence of heart disease, the amount of junk food and alcohol consumed, and the level of gym membership. In 2007, it had slipped one place behind London, but was still ranked first when it came to the parks and green spaces available for exercise and the amount of television watched by Sotonians was the lowest in the country. Speedway racing took place at Banister Court Stadium in the pre-war era. It returned in the 1940s after WW2 and the Saints operated until the stadium closed down at the end of 1963. A training track operated in the 1950s in the Hamble area. There are 119,500 males within the city and 117,400 females. The 20–24 age range is the most populous, with an estimated 32,300 people falling in this age range. Next largest is the 25–29 range with 24,700 people and then 30–34 years with 17,800. By population, Southampton is the largest monocentric city in the South East England region and the second largest on the South Coast after Plymouth. During the Middle Ages, shipbuilding became an important industry for the town. Henry V's famous warship HMS Grace Dieu was built in Southampton. Walter Taylor's 18th century mechanisation of the block-making process was a significant step in the Industrial Revolution. From 1904 to 2004, the Thornycroft shipbuilding yard was a major employer in Southampton, building and repairing ships used in the two World Wars. While Southampton is no longer the base for any cross-channel ferries, it is the terminus for three internal ferry services, all of which operate from terminals at Town Quay. Two of these, a car ferry service and a fast catamaran passenger ferry service, provide links to East Cowes and Cowes respectively on the Isle of Wight and are operated by Red Funnel. The third ferry is the Hythe Ferry, providing a passenger service to Hythe on the other side of Southampton Water. Buses now provide the majority of local public transport. The main bus operators are First Southampton and Bluestar. Other operators include Brijan Tours, Stagecoach and Xelabus. The other large service provider is the Uni-link bus service (running from early in the morning to midnight), which was commissioned by the University of Southampton to provide transport from the university to the town. Previously run by Enterprise, it is now run by Bluestar. Free buses are provided by City-link'. The City-link runs from the Red Funnel ferry terminal at Town Quay to Central station via WestQuay and is operated by Bluestar. There is also a door-to-door minibus service called Southampton Dial a Ride, for residents who cannot access public transport. This is funded by the council and operated by SCA Support Services. As with the rest of the UK, Southampton experiences an oceanic climate (Köppen Cfb). Its southerly, low lying and sheltered location ensures it is among the warmer, sunnier cities in the UK. It has held the record for the highest temperature in the UK for June at 35.6 °C (96.1 °F) since 1976. Southampton has been used for military embarkation, including during 18th-century wars with the French, the Crimean war, and the Boer War. Southampton was designated No. 1 Military Embarkation port during the Great War and became a major centre for treating the returning wounded and POWs. It was also central to the preparations for the Invasion of Europe in 1944. The city also has several smaller music venues, including the Brook, The Talking Heads, The Soul Cellar, The Joiners and Turner Sims, as well as smaller ""club circuit"" venues like Hampton's and Lennon's, and a number of public houses including the Platform tavern, the Dolphin, the Blue Keys and many others. The Joiners has played host to such acts as Oasis, Radiohead, Green Day, Suede, PJ Harvey, the Manic Street Preachers, Coldplay, the Verve, the Libertines and Franz Ferdinand, while Hampton's and Lennon's have hosted early appearances by Kate Nash, Scouting for Girls and Band of Skulls. The nightclub, Junk, has been nominated for the UK's best small nightclub, and plays host to a range of dance music's top acts. Southampton has always been a port, and the docks have long been a major employer in the city. In particular, it is a port for cruise ships; its heyday was the first half of the 20th century, and in particular the inter-war years, when it handled almost half the passenger traffic of the UK. Today it remains home to luxury cruise ships, as well as being the largest freight port on the Channel coast and fourth largest UK port by tonnage, with several container terminals. Unlike some other ports, such as Liverpool, London, and Bristol, where industry and docks have largely moved out of the city centres leaving room for redevelopment, Southampton retains much of its inner-city industry. Despite the still active and expanding docklands to the west of the city centre, further enhanced with the opening of a fourth cruise terminal in 2009, parts of the eastern docks have been redeveloped; the Ocean Village development, which included a local marina and small entertainment complex, is a good example. Southampton is home to the headquarters of both the Maritime and Coastguard Agency and the Marine Accident Investigation Branch of the Department for Transport in addition to cruise operator Carnival UK. The status of the town was changed by a later charter of Charles I by at once the formal separation from Portsmouth and the recognition of Southampton as a county, In the charter dated 27 June 1640 the formal title of the town became 'The Town and County of the Town of Southampton'. These charters and Royal Grants, of which there were many, also set out the governance and regulation of the town and port which remained the 'constitution' of the town until the local government organisation of the later Victorian period which from about 1888 saw the setting up of County Councils across England and Wales and including Hampshire County Council who now took on some of the function of Government in Southampton Town. In this regime, The Town and County of the Town of Southampton also became a county borough with shared responsibility for aspects of local government. On 24 February 1964 the status changed again by a Charter of Elizabeth II, creating the City and County of the City of Southampton. The city also has the Southampton Sports Centre which is the focal point for the public's sporting and outdoor activities and includes an Alpine Centre, theme park and athletics centre which is used by professional athletes. With the addition of 11 other additional leisure venures which are currently operate by the Council leisure executives. However these have been sold the operating rights to ""Park Wood Leisure."" 630 people lost their lives as a result of the air raids on Southampton and nearly 2,000 more were injured, not to mention the thousands of buildings damaged or destroyed. In January 2007, the average annual salary in the city was £22,267. This was £1,700 lower than the national average and £3,800 less than the average for the South East. Hampshire County Cricket Club play close to the city, at the Rose Bowl in West End, after previously playing at the County Cricket Ground and the Antelope Ground, both near the city centre. There is also the Southampton Evening Cricket League. Prior to King Henry's departure for the Battle of Agincourt in 1415, the ringleaders of the ""Southampton Plot""—Richard, Earl of Cambridge, Henry Scrope, 3rd Baron Scrope of Masham, and Sir Thomas Grey of Heton—were accused of high treason and tried at what is now the Red Lion public house in the High Street. They were found guilty and summarily executed outside the Bargate. Local media include the Southern Daily Echo newspaper based in Redbridge and BBC South, which has its regional headquarters in the city centre opposite the civic centre. From there the BBC broadcasts South Today, the local television news bulletin and BBC Radio Solent. The local ITV franchise is Meridian, which has its headquarters in Whiteley, around nine miles (14 km) from the city. Until December 2004, the station's studios were located in the Northam area of the city on land reclaimed from the River Itchen. That's Solent is an local television channel that began broadcasting in November 2014, which will be based in and serve Southampton and Portsmouth. Southampton's fire cover is provided by Hampshire Fire and Rescue Service. There are three fire stations within the city boundaries at St Mary's, Hightown and Redbridge. There are two main termini for bus services. As the biggest operator, First uses stops around Pound Tree Road. This leaves the other terminal of West Quay available for other operators. Uni-link passes West Quay in both directions, and Wilts & Dorset drop passengers off and pick them up there, terminating at a series of bus stands along the road. Certain Bluestar services also do this, while others stop at Bargate and some loop round West Quay, stopping at Hanover Buildings. There was a tram system from 1879 to 1949. The town was the subject of an attempt by a separate company, the Didcot, Newbury and Southampton Railway, to open another rail route to the North in the 1880s and some building work, including a surviving embankment, was undertaken in the Hill Lane area. The city walls include God's House Tower, built in 1417, the first purpose-built artillery fortification in England. Over the years it has been used as home to the city's gunner, the Town Gaol and even as storage for the Southampton Harbour Board. Until September 2011, it housed the Museum of Archaeology. The walls were completed in the 15th century, but later development of several new fortifications along Southampton Water and the Solent by Henry VIII meant that Southampton was no longer dependent upon its fortifications. The city has a Mayor and is one of the 16 cities and towns in England and Wales to have a ceremonial sheriff who acts as a deputy for the Mayor. The current and 793rd Mayor of Southampton is Linda Norris. Catherine McEwing is the current and 578th sherriff. The town crier from 2004 until his death in 2014 was John Melody, who acted as master of ceremonies in the city and who possessed a cry of 104 decibels. There are many innovative art galleries in the city. The Southampton City Art Gallery at the Civic Centre is one of the best known and as well as a nationally important Designated Collection, houses several permanent and travelling exhibitions. The Millais Gallery at Southampton Solent University, the John Hansard Gallery at Southampton University as well as smaller galleries including the Art House in Above Bar Street provide a different view. The city's Bargate is also an art gallery run by the arts organisation ""a space"". A space also run the Art Vaults project, which creatively uses several of Southampton's medieval vaults, halls and cellars as venues for contemporary art installations. Viking raids from 840 onwards contributed to the decline of Hamwic in the 9th century, and by the 10th century a fortified settlement, which became medieval Southampton, had been established. There are three members of parliament for the city: Royston Smith (Conservative) for Southampton Itchen, the constituency covering the east of the city; Dr. Alan Whitehead (Labour) for Southampton Test, which covers the west of the city; and Caroline Nokes (Conservative) for Romsey and Southampton North, which includes a northern portion of the city. Southampton City Council consists of 48 councillors, 3 for each of the 16 wards. Council elections are held in early May for one third of the seats (one councillor for each ward), elected for a four-year term, so there are elections three years out of four. Since the 2015 council elections, the composition of the council is: The centre of Southampton is located above a large hot water aquifer that provides geothermal power to some of the city's buildings. This energy is processed at a plant in the West Quay region in Southampton city centre, the only geothermal power station in the UK. The plant provides private electricity for the Port of Southampton and hot water to the Southampton District Energy Scheme used by many buildings including the WestQuay shopping centre. In a 2006 survey of carbon emissions in major UK cities conducted by British Gas, Southampton was ranked as being one of the lowest carbon emitting cities in the United Kingdom. The city has a particular connection to Cunard Line and their fleet of ships. This was particularly evident on 11 November 2008 when the Cunard liner RMS Queen Elizabeth 2 departed the city for the final time amid a spectacular fireworks display after a full day of celebrations. Cunard ships are regularly launched in the city, for example Queen Victoria was named by HRH The Duchess of Cornwall in December 2007, and the Queen named Queen Elizabeth in the city during October 2011. The Duchess of Cambridge performed the naming ceremony of Royal Princess on 13 June 2013. Commercial radio stations broadcasting to the city include The Breeze, previously The Saint and currently broadcasting Hot adult contemporary music, Capital, previously Power FM and Galaxy and broadcasting popular music, Wave 105 and Heart Hampshire, the latter previously Ocean FM and both broadcasting adult contemporary music, and 106 Jack FM (www.jackradio.com), previously The Coast 106. In addition, Southampton University has a radio station called SURGE, broadcasting on AM band as well as through the web. According to 2004 figures, Southampton contributes around £4.2 bn to the regional economy annually. The vast majority of this is from the service sector, with the remainder coming from industry in the city. This figure has almost doubled since 1995. The University of Southampton, which was founded in 1862 and received its Royal Charter as a university in 1952, has over 22,000 students. The university is ranked in the top 100 research universities in the world in the Academic Ranking of World Universities 2010. In 2010, the THES - QS World University Rankings positioned the University of Southampton in the top 80 universities in the world. The university considers itself one of the top 5 research universities in the UK. The university has a global reputation for research into engineering sciences, oceanography, chemistry, cancer sciences, sound and vibration research, computer science and electronics, optoelectronics and textile conservation at the Textile Conservation Centre (which is due to close in October 2009.) It is also home to the National Oceanography Centre, Southampton (NOCS), the focus of Natural Environment Research Council-funded marine research. Southampton used to be home to a number of ferry services to the continent, with destinations such as San Sebastian, Lisbon, Tangier and Casablanca. A ferry port was built during the 1960s. However, a number of these relocated to Portsmouth and by 1996, there were no longer any car ferries operating from Southampton with the exception of services to the Isle of Wight. The land used for Southampton Ferry Port was sold off and a retail and housing development was built on the site. The Princess Alexandra Dock was converted into a marina. Reception areas for new cars now fill the Eastern Docks where passengers, dry docks and trains used to be. It has been revealed that Southampton has the worst behaved secondary schools within the UK. With suspension rates three times the national average, the suspension rate is approximately 1 in every 14 children, the highest in the country for physical or verbal assaults against staff. Southampton is divided into council wards, suburbs, constituencies, ecclesiastical parishes, and other less formal areas. It has a number of parks and green spaces, the largest being the 148 hectare Southampton Common, parts of which are used to host the annual summer festivals, circuses and fun fairs. The Common includes Hawthorns Urban Wildlife Centre on the former site of Southampton Zoo, a paddling pool and several lakes and ponds. Town Quay is the original public quay, and dates from the 13th century. Today's Eastern Docks were created in the 1830s by land reclamation of the mud flats between the Itchen & Test estuaries. The Western Docks date from the 1930s when the Southern Railway Company commissioned a major land reclamation and dredging programme. Most of the material used for reclamation came from dredging of Southampton Water, to ensure that the port can continue to handle large ships."
Neolithic,"In southeast Europe agrarian societies first appeared in the 7th millennium BC, attested by one of the earliest farming sites of Europe, discovered in Vashtëmi, southeastern Albania and dating back to 6,500 BC. Anthropomorphic figurines have been found in the Balkans from 6000 BC, and in Central Europe by c. 5800 BC (La Hoguette). Among the earliest cultural complexes of this area are the Sesklo culture in Thessaly, which later expanded in the Balkans giving rise to Starčevo-Körös (Cris), Linearbandkeramik, and Vinča. Through a combination of cultural diffusion and migration of peoples, the Neolithic traditions spread west and northwards to reach northwestern Europe by around 4500 BC. The Vinča culture may have created the earliest system of writing, the Vinča signs, though archaeologist Shan Winn believes they most likely represented pictograms and ideograms rather than a truly developed form of writing. The Cucuteni-Trypillian culture built enormous settlements in Romania, Moldova and Ukraine from 5300 to 2300 BC. The megalithic temple complexes of Ġgantija on the Mediterranean island of Gozo (in the Maltese archipelago) and of Mnajdra (Malta) are notable for their gigantic Neolithic structures, the oldest of which date back to c. 3600 BC. The Hypogeum of Ħal-Saflieni, Paola, Malta, is a subterranean structure excavated c. 2500 BC; originally a sanctuary, it became a necropolis, the only prehistoric underground temple in the world, and showing a degree of artistry in stone sculpture unique in prehistory to the Maltese islands. After 2500 BC, the Maltese Islands were depopulated for several decades until the arrival of a new influx of Bronze Age immigrants, a culture that cremated its dead and introduced smaller megalithic structures called dolmens to Malta. In most cases there are small chambers here, with the cover made of a large slab placed on upright stones. They are claimed to belong to a population certainly different from that which built the previous megalithic temples. It is presumed the population arrived from Sicily because of the similarity of Maltese dolmens to some small constructions found in the largest island of the Mediterranean sea. Families and households were still largely independent economically, and the household was probably the center of life. However, excavations in Central Europe have revealed that early Neolithic Linear Ceramic cultures (""Linearbandkeramik"") were building large arrangements of circular ditches between 4800 BC and 4600 BC. These structures (and their later counterparts such as causewayed enclosures, burial mounds, and henge) required considerable time and labour to construct, which suggests that some influential individuals were able to organise and direct human labour — though non-hierarchical and voluntary work remain possibilities. Neolithic peoples in the Levant, Anatolia, Syria, northern Mesopotamia and Central Asia were also accomplished builders, utilizing mud-brick to construct houses and villages. At Çatal höyük, houses were plastered and painted with elaborate scenes of humans and animals. In Europe, long houses built from wattle and daub were constructed. Elaborate tombs were built for the dead. These tombs are particularly numerous in Ireland, where there are many thousand still in existence. Neolithic people in the British Isles built long barrows and chamber tombs for their dead and causewayed camps, henges, flint mines and cursus monuments. It was also important to figure out ways of preserving food for future months, such as fashioning relatively airtight containers, and using substances like salt as preservatives. Most clothing appears to have been made of animal skins, as indicated by finds of large numbers of bone and antler pins which are ideal for fastening leather. Wool cloth and linen might have become available during the later Neolithic, as suggested by finds of perforated stones which (depending on size) may have served as spindle whorls or loom weights. The clothing worn in the Neolithic Age might be similar to that worn by Ötzi the Iceman, although he was not Neolithic (since he belonged to the later Copper age). The shelter of the early people changed dramatically from the paleolithic to the neolithic era. In the paleolithic, people did not normally live in permanent constructions. In the neolithic, mud brick houses started appearing that were coated with plaster. The growth of agriculture made permanent houses possible. Doorways were made on the roof, with ladders positioned both on the inside and outside of the houses. The roof was supported by beams from the inside. The rough ground was covered by platforms, mats, and skins on which residents slept. Stilt-houses settlements were common in the Alpine and Pianura Padana (Terramare) region. Remains have been found at the Ljubljana Marshes in Slovenia and at the Mondsee and Attersee lakes in Upper Austria, for example. The Neolithic 2 (PPNB) began around 8,800 BCE according to the ASPRO chronology in the Levant (Jericho, Israel). As with the PPNA dates, there are two versions from the same laboratories noted above. This system of terminology, however, is not convenient for southeast Anatolia and settlements of the middle Anatolia basin. This era was before the Mesolithic era.[citation needed] A settlement of 3,000 inhabitants was found in the outskirts of Amman, Jordan. Considered to be one of the largest prehistoric settlements in the Near East, called 'Ain Ghazal, it was continuously inhabited from approximately 7,250 – 5,000 B. In 1981 a team of researchers from the Maison de l'Orient et de la Méditerranée, including Jacques Cauvin and Oliver Aurenche divided Near East neolithic chronology into ten periods (0 to 9) based on social, economic and cultural characteristics. In 2002 Danielle Stordeur and Frédéric Abbès advanced this system with a division into five periods. Natufian (1) between 12,000 and 10,200 BC, Khiamian (2) between 10,200-8,800 BC, PPNA: Sultanian (Jericho), Mureybetian, early PPNB (PPNB ancien) (3) between 8,800-7,600 BC, middle PPNB (PPNB moyen) 7,600-6,900 BC, late PPNB (PPNB récent) (4) between 7,500 and 7,000 BC and a PPNB (sometimes called PPNC) transitional stage (PPNB final) (5) where Halaf and dark faced burnished ware begin to emerge between 6,900-6,400 BC. They also advanced the idea of a transitional stage between the PPNA and PPNB between 8,800 and 8,600 BC at sites like Jerf el Ahmar and Tell Aswad. Traditionally considered the last part of the Stone Age, the Neolithic followed the terminal Holocene Epipaleolithic period and commenced with the beginning of farming, which produced the ""Neolithic Revolution"". It ended when metal tools became widespread (in the Copper Age or Bronze Age; or, in some geographical regions, in the Iron Age). The Neolithic is a progression of behavioral and cultural characteristics and changes, including the use of wild and domestic crops and of domesticated animals. Domestication of sheep and goats reached Egypt from the Near East possibly as early as 6,000 BC. Graeme Barker states ""The first indisputable evidence for domestic plants and animals in the Nile valley is not until the early fifth millennium bc in northern Egypt and a thousand years later further south, in both cases as part of strategies that still relied heavily on fishing, hunting, and the gathering of wild plants"" and suggests that these subsistence changes were not due to farmers migrating from the Near East but was an indigenous development, with cereals either indigenous or obtained through exchange. Other scholars argue that the primary stimulus for agriculture and domesticated animals (as well as mud-brick architecture and other Neolithic cultural features) in Egypt was from the Middle East. Not all of these cultural elements characteristic of the Neolithic appeared everywhere in the same order: the earliest farming societies in the Near East did not use pottery. In other parts of the world, such as Africa, South Asia and Southeast Asia, independent domestication events led to their own regionally distinctive Neolithic cultures that arose completely independent of those in Europe and Southwest Asia. Early Japanese societies and other East Asian cultures used pottery before developing agriculture. In Mesoamerica, a similar set of events (i.e., crop domestication and sedentary lifestyles) occurred by around 4500 BC, but possibly as early as 11,000–10,000 BC. These cultures are usually not referred to as belonging to the Neolithic; in America different terms are used such as Formative stage instead of mid-late Neolithic, Archaic Era instead of Early Neolithic and Paleo-Indian for the preceding period. The Formative stage is equivalent to the Neolithic Revolution period in Europe, Asia, and Africa. In the Southwestern United States it occurred from 500 to 1200 C.E. when there was a dramatic increase in population and development of large villages supported by agriculture based on dryland farming of maize, and later, beans, squash, and domesticated turkeys. During this period the bow and arrow and ceramic pottery were also introduced. Neolithic people were skilled farmers, manufacturing a range of tools necessary for the tending, harvesting and processing of crops (such as sickle blades and grinding stones) and food production (e.g. pottery, bone implements). They were also skilled manufacturers of a range of other types of stone tools and ornaments, including projectile points, beads, and statuettes. But what allowed forest clearance on a large scale was the polished stone axe above all other tools. Together with the adze, fashioning wood for shelter, structures and canoes for example, this enabled them to exploit their newly won farmland. Control of labour and inter-group conflict is characteristic of corporate-level or 'tribal' groups, headed by a charismatic individual; whether a 'big man' or a proto-chief, functioning as a lineage-group head. Whether a non-hierarchical system of organization existed is debatable, and there is no evidence that explicitly suggests that Neolithic societies functioned under any dominating class or individual, as was the case in the chiefdoms of the European Early Bronze Age. Theories to explain the apparent implied egalitarianism of Neolithic (and Paleolithic) societies have arisen, notably the Marxist concept of primitive communism. In 2012, news was released about a new farming site discovered in Munam-ri, Goseong, Gangwon Province, South Korea, which may be the earliest farmland known to date in east Asia. ""No remains of an agricultural field from the Neolithic period have been found in any East Asian country before, the institute said, adding that the discovery reveals that the history of agricultural cultivation at least began during the period on the Korean Peninsula"". The farm was dated between 3600 and 3000 B.C. Pottery, stone projectile points, and possible houses were also found. ""In 2002, researchers discovered prehistoric earthenware, jade earrings, among other items in the area"". The research team will perform accelerator mass spectrometry (AMS) dating to retrieve a more precise date for the site. The domestication of large animals (c. 8000 BC) resulted in a dramatic increase in social inequality in most of the areas where it occurred; New Guinea being a notable exception. Possession of livestock allowed competition between households and resulted in inherited inequalities of wealth. Neolithic pastoralists who controlled large herds gradually acquired more livestock, and this made economic inequalities more pronounced. However, evidence of social inequality is still disputed, as settlements such as Catal Huyuk reveal a striking lack of difference in the size of homes and burial sites, suggesting a more egalitarian society with no evidence of the concept of capital, although some homes do appear slightly larger or more elaborately decorated than others. Another significant change undergone by many of these newly agrarian communities was one of diet. Pre-agrarian diets varied by region, season, available local plant and animal resources and degree of pastoralism and hunting. Post-agrarian diet was restricted to a limited package of successfully cultivated cereal grains, plants and to a variable extent domesticated animals and animal products. Supplementation of diet by hunting and gathering was to variable degrees precluded by the increase in population above the carrying capacity of the land and a high sedentary local population concentration. In some cultures, there would have been a significant shift toward increased starch and plant protein. The relative nutritional benefits and drawbacks of these dietary changes and their overall impact on early societal development is still debated. There is a large body of evidence for fortified settlements at Linearbandkeramik sites along the Rhine, as at least some villages were fortified for some time with a palisade and an outer ditch. Settlements with palisades and weapon-traumatized bones have been discovered, such as at the Talheim Death Pit demonstrates ""...systematic violence between groups"" and warfare was probably much more common during the Neolithic than in the preceding Paleolithic period. This supplanted an earlier view of the Linear Pottery Culture as living a ""peaceful, unfortified lifestyle"". Around 10,200 BC the first fully developed Neolithic cultures belonging to the phase Pre-Pottery Neolithic A (PPNA) appeared in the fertile crescent. Around 10,700 to 9,400 BC a settlement was established in Tell Qaramel, 10 miles north of Aleppo. The settlement included 2 temples dating back to 9,650. Around 9000 BC during the PPNA, one of the world's first towns, Jericho, appeared in the Levant. It was surrounded by a stone and marble wall and contained a population of 2000–3000 people and a massive stone tower. Around 6,400 BC the Halaf culture appeared in Lebanon, Israel and Palestine, Syria, Anatolia, and Northern Mesopotamia and subsisted on dryland agriculture. A significant and far-reaching shift in human subsistence and lifestyle was to be brought about in areas where crop farming and cultivation were first developed: the previous reliance on an essentially nomadic hunter-gatherer subsistence technique or pastoral transhumance was at first supplemented, and then increasingly replaced by, a reliance upon the foods produced from cultivated lands. These developments are also believed to have greatly encouraged the growth of settlements, since it may be supposed that the increased need to spend more time and labor in tending crop fields required more localized dwellings. This trend would continue into the Bronze Age, eventually giving rise to permanently settled farming towns, and later cities and states whose larger populations could be sustained by the increased productivity from cultivated lands. The beginning of the Neolithic culture is considered to be in the Levant (Jericho, modern-day West Bank) about 10,200 – 8,800 BC. It developed directly from the Epipaleolithic Natufian culture in the region, whose people pioneered the use of wild cereals, which then evolved into true farming. The Natufian period was between 12,000 and 10,200 BC, and the so-called ""proto-Neolithic"" is now included in the Pre-Pottery Neolithic (PPNA) between 10,200 and 8,800 BC. As the Natufians had become dependent on wild cereals in their diet, and a sedentary way of life had begun among them, the climatic changes associated with the Younger Dryas are thought to have forced people to develop farming. However, early farmers were also adversely affected in times of famine, such as may be caused by drought or pests. In instances where agriculture had become the predominant way of life, the sensitivity to these shortages could be particularly acute, affecting agrarian populations to an extent that otherwise may not have been routinely experienced by prior hunter-gatherer communities. Nevertheless, agrarian communities generally proved successful, and their growth and the expansion of territory under cultivation continued. During most of the Neolithic age of Eurasia, people lived in small tribes composed of multiple bands or lineages. There is little scientific evidence of developed social stratification in most Neolithic societies; social stratification is more associated with the later Bronze Age. Although some late Eurasian Neolithic societies formed complex stratified chiefdoms or even states, states evolved in Eurasia only with the rise of metallurgy, and most Neolithic societies on the whole were relatively simple and egalitarian. Beyond Eurasia, however, states were formed during the local Neolithic in three areas, namely in the Preceramic Andes with the Norte Chico Civilization, Formative Mesoamerica and Ancient Hawaiʻi. However, most Neolithic societies were noticeably more hierarchical than the Paleolithic cultures that preceded them and hunter-gatherer cultures in general. The Neolithic 1 (PPNA) period began roughly 10,000 years ago in the Levant. A temple area in southeastern Turkey at Göbekli Tepe dated around 9,500 BC may be regarded as the beginning of the period. This site was developed by nomadic hunter-gatherer tribes, evidenced by the lack of permanent housing in the vicinity and may be the oldest known human-made place of worship. At least seven stone circles, covering 25 acres (10 ha), contain limestone pillars carved with animals, insects, and birds. Stone tools were used by perhaps as many as hundreds of people to create the pillars, which might have supported roofs. Other early PPNA sites dating to around 9,500 to 9,000 BCE have been found in Jericho, Israel (notably Ain Mallaha, Nahal Oren, and Kfar HaHoresh), Gilgal in the Jordan Valley, and Byblos, Lebanon. The start of Neolithic 1 overlaps the Tahunian and Heavy Neolithic periods to some degree.[citation needed]"
Yale_University,"Yale's central campus in downtown New Haven covers 260 acres (1.1 km2) and comprises its main, historic campus and a medical campus adjacent to the Yale-New Haven Hospital. In western New Haven, the university holds 500 acres (2.0 km2) of athletic facilities, including the Yale Golf Course. In 2008, Yale purchased the 136-acre (0.55 km2) former Bayer Pharmaceutical campus in West Haven, Connecticut, the buildings of which are now used as laboratory and research space. Yale also owns seven forests in Connecticut, Vermont, and New Hampshire—the largest of which is the 7,840-acre (31.7 km2) Yale-Myers Forest in Connecticut's Quiet Corner—and nature preserves including Horse Island. Yale seniors at graduation smash clay pipes underfoot to symbolize passage from their ""bright college years,"" though in recent history the pipes have been replaced with ""bubble pipes"". (""Bright College Years,"" the University's alma mater, was penned in 1881 by Henry Durand, Class of 1881, to the tune of Die Wacht am Rhein.) Yale's student tour guides tell visitors that students consider it good luck to rub the toe of the statue of Theodore Dwight Woolsey on Old Campus. Actual students rarely do so. In the second half of the 20th century Bladderball, a campus-wide game played with a large inflatable ball, became a popular tradition but was banned by administration due to safety concerns. In spite of administration opposition, students revived the game in 2009, 2011, and 2014, but its future remains uncertain. The Revolutionary War soldier Nathan Hale (Yale 1773) was the prototype of the Yale ideal in the early 19th century: a manly yet aristocratic scholar, equally well-versed in knowledge and sports, and a patriot who ""regretted"" that he ""had but one life to lose"" for his country. Western painter Frederic Remington (Yale 1900) was an artist whose heroes gloried in combat and tests of strength in the Wild West. The fictional, turn-of-the-20th-century Yale man Frank Merriwell embodied the heroic ideal without racial prejudice, and his fictional successor Frank Stover in the novel Stover at Yale (1911) questioned the business mentality that had become prevalent at the school. Increasingly the students turned to athletic stars as their heroes, especially since winning the big game became the goal of the student body, and the alumni, as well as the team itself. Yale has numerous athletic facilities, including the Yale Bowl (the nation's first natural ""bowl"" stadium, and prototype for such stadiums as the Los Angeles Memorial Coliseum and the Rose Bowl), located at The Walter Camp Field athletic complex, and the Payne Whitney Gymnasium, the second-largest indoor athletic complex in the world. October 21, 2000, marked the dedication of Yale's fourth new boathouse in 157 years of collegiate rowing. The Richard Gilder Boathouse is named to honor former Olympic rower Virginia Gilder '79 and her father Richard Gilder '54, who gave $4 million towards the $7.5 million project. Yale also maintains the Gales Ferry site where the heavyweight men's team trains for the Yale-Harvard Boat Race. Several campus safety strategies have been pioneered at Yale. The first campus police force was founded at Yale in 1894, when the university contracted city police officers to exclusively cover the campus. Later hired by the university, the officers were originally brought in to quell unrest between students and city residents and curb destructive student behavior. In addition to the Yale Police Department, a variety of safety services are available including blue phones, a safety escort, and 24-hour shuttle service. Other examples of the Gothic (also called neo-Gothic and collegiate Gothic) style are on Old Campus by such architects as Henry Austin, Charles C. Haight and Russell Sturgis. Several are associated with members of the Vanderbilt family, including Vanderbilt Hall, Phelps Hall, St. Anthony Hall (a commission for member Frederick William Vanderbilt), the Mason, Sloane and Osborn laboratories, dormitories for the Sheffield Scientific School (the engineering and sciences school at Yale until 1956) and elements of Silliman College, the largest residential college. Yale's secret society buildings (some of which are called ""tombs"") were built both to be private yet unmistakable. A diversity of architectural styles is represented: Berzelius, Donn Barber in an austere cube with classical detailing (erected in 1908 or 1910); Book and Snake, Louis R. Metcalfe in a Greek Ionic style (erected in 1901); Elihu, architect unknown but built in a Colonial style (constructed on an early 17th-century foundation although the building is from the 18th century); Mace and Chain, in a late colonial, early Victorian style (built in 1823). Interior moulding is said to have belonged to Benedict Arnold; Manuscript Society, King Lui-Wu with Dan Kniley responsible for landscaping and Josef Albers for the brickwork intaglio mural. Building constructed in a mid-century modern style; Scroll and Key, Richard Morris Hunt in a Moorish- or Islamic-inspired Beaux-Arts style (erected 1869–70); Skull and Bones, possibly Alexander Jackson Davis or Henry Austin in an Egypto-Doric style utilizing Brownstone (in 1856 the first wing was completed, in 1903 the second wing, 1911 the Neo-Gothic towers in rear garden were completed); St. Elmo, (former tomb) Kenneth M. Murchison, 1912, designs inspired by Elizabethan manor. Current location, brick colonial; Shabtai, 1882, the Anderson Mansion built in the Second Empire architectural style; and Wolf's Head, Bertram Grosvenor Goodhue (erected 1923-4). Much of Yale University's staff, including most maintenance staff, dining hall employees, and administrative staff, are unionized. Clerical and technical employees are represented by Local 34 of UNITE HERE and service and maintenance workers by Local 35 of the same international. Together with the Graduate Employees and Students Organization (GESO), an unrecognized union of graduate employees, Locals 34 and 35 make up the Federation of Hospital and University Employees. Also included in FHUE are the dietary workers at Yale-New Haven Hospital, who are members of 1199 SEIU. In addition to these unions, officers of the Yale University Police Department are members of the Yale Police Benevolent Association, which affiliated in 2005 with the Connecticut Organization for Public Safety Employees. Finally, Yale security officers voted to join the International Union of Security, Police and Fire Professionals of America in fall 2010 after the National Labor Relations Board ruled they could not join AFSCME; the Yale administration contested the election. Yale's Office of Sustainability develops and implements sustainability practices at Yale. Yale is committed to reduce its greenhouse gas emissions 10% below 1990 levels by the year 2020. As part of this commitment, the university allocates renewable energy credits to offset some of the energy used by residential colleges. Eleven campus buildings are candidates for LEED design and certification. Yale Sustainable Food Project initiated the introduction of local, organic vegetables, fruits, and beef to all residential college dining halls. Yale was listed as a Campus Sustainability Leader on the Sustainable Endowments Institute’s College Sustainability Report Card 2008, and received a ""B+"" grade overall. Serious American students of theology and divinity, particularly in New England, regarded Hebrew as a classical language, along with Greek and Latin, and essential for study of the Old Testament in the original words. The Reverend Ezra Stiles, president of the College from 1778 to 1795, brought with him his interest in the Hebrew language as a vehicle for studying ancient Biblical texts in their original language (as was common in other schools), requiring all freshmen to study Hebrew (in contrast to Harvard, where only upperclassmen were required to study the language) and is responsible for the Hebrew phrase אורים ותמים (Urim and Thummim) on the Yale seal. Stiles' greatest challenge occurred in July 1779 when hostile British forces occupied New Haven and threatened to raze the College. However, Yale graduate Edmund Fanning, Secretary to the British General in command of the occupation, interceded and the College was saved. Fanning later was granted an honorary degree LL.D., at 1803, for his efforts. During the 1988 presidential election, George H. W. Bush (Yale '48) derided Michael Dukakis for having ""foreign-policy views born in Harvard Yard's boutique"". When challenged on the distinction between Dukakis's Harvard connection and his own Yale background, he said that, unlike Harvard, Yale's reputation was ""so diffuse, there isn't a symbol, I don't think, in the Yale situation, any symbolism in it"" and said Yale did not share Harvard's reputation for ""liberalism and elitism"". In 2004 Howard Dean stated, ""In some ways, I consider myself separate from the other three (Yale) candidates of 2004. Yale changed so much between the class of '68 and the class of '71. My class was the first class to have women in it; it was the first class to have a significant effort to recruit African Americans. It was an extraordinary time, and in that span of time is the change of an entire generation"". In the wake of the racially-motivated"" church shooting in Charleston, South Carolina, Yale was under criticism again in the summer of 2015 for Calhoun College, one of 12 residential colleges, which was named after John C. Calhoun, a slave-owner and strong slavery supporter in the nineteenth century. In July 2015 students signed a petition calling for the name change. They argued in the petition that—while Calhoun was respected in the 19th century as an ""extraordinary American statesman""—he was ""one of the most prolific defenders of slavery and white supremacy"" in the history of the United States. In August 2015 Yale President Peter Salovey addressed the Freshman Class of 2019 in which he responded to the racial tensions but explained why the college would not be renamed. He described Calhoun as a ""a notable political theorist, a vice president to two different U.S. presidents, a secretary of war and of state, and a congressman and senator representing South Carolina."" He acknowledged that Calhoun also ""believed that the highest forms of civilization depend on involuntary servitude. Not only that, but he also believed that the races he thought to be inferior, black people in particular, ought to be subjected to it for the sake of their own best interests."" Racial tensions increased in the fall of 2015 centering on comments by Nicholas A. Christakis and his wife Erika regarding freedom of speech. In April 2016 Salovey announced that ""despite decades of vigorous alumni and student protests,"" Calhoun's name will remain on the Yale residential college explaining that it is preferable for Yale students to live in Calhoun's ""shadow"" so they will be ""better prepared to rise to the challenges of the present and the future."" He claimed that if they removed Calhoun's name, it would ""obscure"" his ""legacy of slavery rather than addressing it."" ""Yale is part of that history"" and ""We cannot erase American history, but we can confront it, teach it and learn from it."" One change that will be issued is the title of “master” for faculty members who serve as residential college leaders will be renamed to “head of college” due to its connotation of slavery. Several explanations have been offered for Yale’s representation in national elections since the end of the Vietnam War. Various sources note the spirit of campus activism that has existed at Yale since the 1960s, and the intellectual influence of Reverend William Sloane Coffin on many of the future candidates. Yale President Richard Levin attributes the run to Yale’s focus on creating ""a laboratory for future leaders,"" an institutional priority that began during the tenure of Yale Presidents Alfred Whitney Griswold and Kingman Brewster. Richard H. Brodhead, former dean of Yale College and now president of Duke University, stated: ""We do give very significant attention to orientation to the community in our admissions, and there is a very strong tradition of volunteerism at Yale."" Yale historian Gaddis Smith notes ""an ethos of organized activity"" at Yale during the 20th century that led John Kerry to lead the Yale Political Union's Liberal Party, George Pataki the Conservative Party, and Joseph Lieberman to manage the Yale Daily News. Camille Paglia points to a history of networking and elitism: ""It has to do with a web of friendships and affiliations built up in school."" CNN suggests that George W. Bush benefited from preferential admissions policies for the ""son and grandson of alumni"", and for a ""member of a politically influential family."" New York Times correspondent Elisabeth Bumiller and The Atlantic Monthly correspondent James Fallows credit the culture of community and cooperation that exists between students, faculty, and administration, which downplays self-interest and reinforces commitment to others. Alumnus Eero Saarinen, Finnish-American architect of such notable structures as the Gateway Arch in St. Louis, Washington Dulles International Airport main terminal, Bell Labs Holmdel Complex and the CBS Building in Manhattan, designed Ingalls Rink at Yale and the newest residential colleges of Ezra Stiles and Morse. These latter were modelled after the medieval Italian hilltown of San Gimignano – a prototype chosen for the town's pedestrian-friendly milieu and fortress-like stone towers. These tower forms at Yale act in counterpoint to the college's many Gothic spires and Georgian cupolas. In 1909–10, football faced a crisis resulting from the failure of the previous reforms of 1905–06 to solve the problem of serious injuries. There was a mood of alarm and mistrust, and, while the crisis was developing, the presidents of Harvard, Yale, and Princeton developed a project to reform the sport and forestall possible radical changes forced by government upon the sport. President Arthur Hadley of Yale, A. Lawrence Lowell of Harvard, and Woodrow Wilson of Princeton worked to develop moderate changes to reduce injuries. Their attempts, however, were reduced by rebellion against the rules committee and formation of the Intercollegiate Athletic Association. The big three had tried to operate independently of the majority, but changes did reduce injuries. A decade into co-education, rampant student assault and harassment by faculty became the impetus for the trailblazing lawsuit Alexander v. Yale. While unsuccessful in the courts, the legal reasoning behind the case changed the landscape of sex discrimination law and resulted in the establishment of Yale's Grievance Board and the Yale Women's Center. In March 2011 a Title IX complaint was filed against Yale by students and recent graduates, including editors of Yale's feminist magazine Broad Recognition, alleging that the university had a hostile sexual climate. In response, the university formed a Title IX steering committee to address complaints of sexual misconduct. In 1718, at the behest of either Rector Samuel Andrew or the colony's Governor Gurdon Saltonstall, Cotton Mather contacted a successful businessman named Elihu Yale, who lived in Wales but had been born in Boston and whose father, David, had been one of the original settlers in New Haven, to ask him for financial help in constructing a new building for the college. Through the persuasion of Jeremiah Dummer, Yale, who had made a fortune through trade while living in Madras as a representative of the East India Company, donated nine bales of goods, which were sold for more than £560, a substantial sum at the time. Cotton Mather suggested that the school change its name to Yale College. Meanwhile, a Harvard graduate working in England convinced some 180 prominent intellectuals that they should donate books to Yale. The 1714 shipment of 500 books represented the best of modern English literature, science, philosophy and theology. It had a profound effect on intellectuals at Yale. Undergraduate Jonathan Edwards discovered John Locke's works and developed his original theology known as the ""new divinity."" In 1722 the Rector and six of his friends, who had a study group to discuss the new ideas, announced that they had given up Calvinism, become Arminians, and joined the Church of England. They were ordained in England and returned to the colonies as missionaries for the Anglican faith. Thomas Clapp became president in 1745, and struggled to return the college to Calvinist orthodoxy; but he did not close the library. Other students found Deist books in the library. Yale has a complicated relationship with its home city; for example, thousands of students volunteer every year in a myriad of community organizations, but city officials, who decry Yale's exemption from local property taxes, have long pressed the university to do more to help. Under President Levin, Yale has financially supported many of New Haven's efforts to reinvigorate the city. Evidence suggests that the town and gown relationships are mutually beneficial. Still, the economic power of the university increased dramatically with its financial success amid a decline in the local economy. Yale has produced alumni distinguished in their respective fields. Among the best-known are U.S. Presidents William Howard Taft, Gerald Ford, George H. W. Bush, Bill Clinton and George W. Bush; royals Crown Princess Victoria Bernadotte, Prince Rostislav Romanov and Prince Akiiki Hosea Nyabongo; heads of state, including Italian prime minister Mario Monti, Turkish prime minister Tansu Çiller, Mexican president Ernesto Zedillo, German president Karl Carstens, and Philippines president José Paciano Laurel; U.S. Supreme Court Justices Sonia Sotomayor, Samuel Alito and Clarence Thomas; U.S. Secretaries of State John Kerry, Hillary Clinton, Cyrus Vance, and Dean Acheson; authors Sinclair Lewis, Stephen Vincent Benét, and Tom Wolfe; lexicographer Noah Webster; inventors Samuel F. B. Morse and Eli Whitney; patriot and ""first spy"" Nathan Hale; theologian Jonathan Edwards; actors, directors and producers Paul Newman, Henry Winkler, Vincent Price, Meryl Streep, Sigourney Weaver, Jodie Foster, Angela Bassett, Patricia Clarkson, Courtney Vance, Frances McDormand, Elia Kazan, George Roy Hill, Edward Norton, Lupita Nyong'o, Allison Williams, Oliver Stone, Sam Waterston, and Michael Cimino; ""Father of American football"" Walter Camp, James Franco, ""The perfect oarsman"" Rusty Wailes; baseball players Ron Darling, Bill Hutchinson, and Craig Breslow; basketball player Chris Dudley; football players Gary Fencik, and Calvin Hill; hockey players Chris Higgins and Mike Richter; figure skater Sarah Hughes; swimmer Don Schollander; skier Ryan Max Riley; runner Frank Shorter; composers Charles Ives, Douglas Moore and Cole Porter; Peace Corps founder Sargent Shriver; child psychologist Benjamin Spock; architects Eero Saarinen and Norman Foster; sculptor Richard Serra; film critic Gene Siskel; television commentators Dick Cavett and Anderson Cooper; New York Times journalist David Gonzalez; pundits William F. Buckley, Jr., and Fareed Zakaria; economists Irving Fischer, Mahbub ul Haq, and Paul Krugman; cyclotron inventor and Nobel laureate in Physics, Ernest Lawrence; Human Genome Project director Francis S. Collins; mathematician and chemist Josiah Willard Gibbs; and businesspeople, including Time Magazine co-founder Henry Luce, Morgan Stanley founder Harold Stanley, Boeing CEO James McNerney, FedEx founder Frederick W. Smith, Time Warner president Jeffrey Bewkes, Electronic Arts co-founder Bing Gordon, and investor/philanthropist Sir John Templeton; pioneer in electrical applications Austin Cornelius Dunham. In 2009, former British Prime Minister Tony Blair picked Yale as one location – the others are Britain's Durham University and Universiti Teknologi Mara – for the Tony Blair Faith Foundation's United States Faith and Globalization Initiative. As of 2009, former Mexican President Ernesto Zedillo is the director of the Yale Center for the Study of Globalization and teaches an undergraduate seminar, ""Debating Globalization"". As of 2009, former presidential candidate and DNC chair Howard Dean teaches a residential college seminar, ""Understanding Politics and Politicians."" Also in 2009, an alliance was formed among Yale, University College London, and both schools’ affiliated hospital complexes to conduct research focused on the direct improvement of patient care—a growing field known as translational medicine. President Richard Levin noted that Yale has hundreds of other partnerships across the world, but ""no existing collaboration matches the scale of the new partnership with UCL"". Rare books are found in several Yale collections. The Beinecke Rare Book Library has a large collection of rare books and manuscripts. The Harvey Cushing/John Hay Whitney Medical Library includes important historical medical texts, including an impressive collection of rare books, as well as historical medical instruments. The Lewis Walpole Library contains the largest collection of 18th‑century British literary works. The Elizabethan Club, technically a private organization, makes its Elizabethan folios and first editions available to qualified researchers through Yale. Milton Winternitz led the Yale Medical School as its dean from 1920 to 1935. Dedicated to the new scientific medicine established in Germany, he was equally fervent about ""social medicine"" and the study of humans in their culture and environment. He established the ""Yale System"" of teaching, with few lectures and fewer exams, and strengthened the full-time faculty system; he also created the graduate-level Yale School of Nursing and the Psychiatry Department, and built numerous new buildings. Progress toward his plans for an Institute of Human Relations, envisioned as a refuge where social scientists would collaborate with biological scientists in a holistic study of humankind, unfortunately lasted for only a few years before the opposition of resentful anti-Semitic colleagues drove him to resign. Many of Yale's buildings were constructed in the Collegiate Gothic architecture style from 1917 to 1931, financed largely by Edward S. Harkness Stone sculpture built into the walls of the buildings portray contemporary college personalities such as a writer, an athlete, a tea-drinking socialite, and a student who has fallen asleep while reading. Similarly, the decorative friezes on the buildings depict contemporary scenes such as policemen chasing a robber and arresting a prostitute (on the wall of the Law School), or a student relaxing with a mug of beer and a cigarette. The architect, James Gamble Rogers, faux-aged these buildings by splashing the walls with acid, deliberately breaking their leaded glass windows and repairing them in the style of the Middle Ages, and creating niches for decorative statuary but leaving them empty to simulate loss or theft over the ages. In fact, the buildings merely simulate Middle Ages architecture, for though they appear to be constructed of solid stone blocks in the authentic manner, most actually have steel framing as was commonly used in 1930. One exception is Harkness Tower, 216 feet (66 m) tall, which was originally a free-standing stone structure. It was reinforced in 1964 to allow the installation of the Yale Memorial Carillon. Between 1892, when Harvard and Yale met in one of the first intercollegiate debates, and 1909, the year of the first Triangular Debate of Harvard, Yale, and Princeton, the rhetoric, symbolism, and metaphors used in athletics were used to frame these early debates. Debates were covered on front pages of college newspapers and emphasized in yearbooks, and team members even received the equivalent of athletic letters for their jackets. There even were rallies sending off the debating teams to matches. Yet, the debates never attained the broad appeal that athletics enjoyed. One reason may be that debates do not have a clear winner, as is the case in sports, and that scoring is subjective. In addition, with late 19th-century concerns about the impact of modern life on the human body, athletics offered hope that neither the individual nor the society was coming apart. Yale expanded gradually, establishing the Yale School of Medicine (1810), Yale Divinity School (1822), Yale Law School (1843), Yale Graduate School of Arts and Sciences (1847), the Sheffield Scientific School (1847), and the Yale School of Fine Arts (1869). In 1887, as the college continued to grow under the presidency of Timothy Dwight V, Yale College was renamed Yale University. The university would later add the Yale School of Music (1894), the Yale School of Forestry & Environmental Studies (founded by Gifford Pinchot in 1900), the Yale School of Public Health (1915), the Yale School of Nursing (1923), the Yale School of Drama (1955), the Yale Physician Associate Program (1973), and the Yale School of Management (1976). It would also reorganize its relationship with the Sheffield Scientific School. Expansion caused controversy about Yale's new roles. Noah Porter, moral philosopher, was president from 1871 to 1886. During an age of tremendous expansion in higher education, Porter resisted the rise of the new research university, claiming that an eager embrace of its ideals would corrupt undergraduate education. Many of Porter's contemporaries criticized his administration, and historians since have disparaged his leadership. Levesque argues Porter was not a simple-minded reactionary, uncritically committed to tradition, but a principled and selective conservative. He did not endorse everything old or reject everything new; rather, he sought to apply long-established ethical and pedagogical principles to a rapidly changing culture. He may have misunderstood some of the challenges of his time, but he correctly anticipated the enduring tensions that have accompanied the emergence and growth of the modern university. Yale University, one of the oldest universities in the United States, is a cultural referent as an institution that produces some of the most elite members of society and its grounds, alumni, and students have been prominently portrayed in fiction and U.S. popular culture. For example, Owen Johnson's novel, Stover at Yale, follows the college career of Dink Stover and Frank Merriwell, the model for all later juvenile sports fiction, plays football, baseball, crew, and track at Yale while solving mysteries and righting wrongs. Yale University also is featured in F. Scott Fitzgerald's novel ""The Great Gatsby"". The narrator, Nick Carraway, wrote a series of editorials for the Yale News, and Tom Buchanan was ""one of the most powerful ends that ever played football"" for Yale. Yale has a history of difficult and prolonged labor negotiations, often culminating in strikes. There have been at least eight strikes since 1968, and The New York Times wrote that Yale has a reputation as having the worst record of labor tension of any university in the U.S. Yale's unusually large endowment exacerbates the tension over wages. Moreover, Yale has been accused of failing to treat workers with respect. In a 2003 strike, however, the university claimed that more union employees were working than striking. Professor David Graeber was 'retired' after he came to the defense of a student who was involved in campus labor issues. Through its program of need-based financial aid, Yale commits to meet the full demonstrated financial need of all applicants. Most financial aid is in the form of grants and scholarships that do not need to be paid back to the university, and the average need-based aid grant for the Class of 2017 was $46,395. 15% of Yale College students are expected to have no parental contribution, and about 50% receive some form of financial aid. About 16% of the Class of 2013 had some form of student loan debt at graduation, with an average debt of $13,000 among borrowers. In 1966, Yale began discussions with its sister school Vassar College about merging to foster coeducation at the undergraduate level. Vassar, then all-female and part of the Seven Sisters—elite higher education schools that historically served as sister institutions to the Ivy League when the Ivy League still only admitted men—tentatively accepted, but then declined the invitation. Both schools introduced coeducation independently in 1969. Amy Solomon was the first woman to register as a Yale undergraduate; she was also the first woman at Yale to join an undergraduate society, St. Anthony Hall. The undergraduate class of 1973 was the first class to have women starting from freshman year; at the time, all undergraduate women were housed in Vanderbilt Hall at the south end of Old Campus.[citation needed] Slack (2003) compares three groups that conducted biological research at Yale during overlapping periods between 1910 and 1970. Yale proved important as a site for this research. The leaders of these groups were Ross Granville Harrison, Grace E. Pickford, and G. Evelyn Hutchinson, and their members included both graduate students and more experienced scientists. All produced innovative research, including the opening of new subfields in embryology, endocrinology, and ecology, respectively, over a long period of time. Harrison's group is shown to have been a classic research school; Pickford's and Hutchinson's were not. Pickford's group was successful in spite of her lack of departmental or institutional position or power. Hutchinson and his graduate and postgraduate students were extremely productive, but in diverse areas of ecology rather than one focused area of research or the use of one set of research tools. Hutchinson's example shows that new models for research groups are needed, especially for those that include extensive field research. Yale's residential college system was established in 1933 by Edward S. Harkness, who admired the social intimacy of Oxford and Cambridge and donated significant funds to found similar colleges at Yale and Harvard. Though Yale's colleges resemble their English precursors organizationally and architecturally, they are dependent entities of Yale College and have limited autonomy. The colleges are led by a master and an academic dean, who reside in the college, and university faculty and affiliates comprise each college's fellowship. Colleges offer their own seminars, social events, and speaking engagements known as ""Master's Teas,"" but do not contain programs of study or academic departments. Instead, all undergraduate courses are taught by the Faculty of Arts and Sciences and are open to members of any college. Yale traces its beginnings to ""An Act for Liberty to Erect a Collegiate School,"" passed by the General Court of the Colony of Connecticut on October 9, 1701, while meeting in New Haven. The Act was an effort to create an institution to train ministers and lay leadership for Connecticut. Soon thereafter, a group of ten Congregationalist ministers: Samuel Andrew, Thomas Buckingham, Israel Chauncy, Samuel Mather, Rev. James Noyes II (son of James Noyes), James Pierpont, Abraham Pierson, Noadiah Russell, Joseph Webb and Timothy Woodbridge, all alumni of Harvard, met in the study of Reverend Samuel Russell in Branford, Connecticut, to pool their books to form the school's library. The group, led by James Pierpont, is now known as ""The Founders"".[citation needed] While Harkness' original colleges were Georgian Revival or Collegiate Gothic in style, two colleges constructed in the 1960s, Morse and Ezra Stiles Colleges, have modernist designs. All twelve college quadrangles are organized around a courtyard, and each has a dining hall, courtyard, library, common room, and a range of student facilities. The twelve colleges are named for important alumni or significant places in university history. In 2017, the university expects to open two new colleges near Science Hill. The university hosts a variety of student journals, magazines, and newspapers. Established in 1872, The Yale Record is the world's oldest humor magazine. Newspapers include the Yale Daily News, which was first published in 1878, and the weekly Yale Herald, which was first published in 1986. Dwight Hall, an independent, non-profit community service organization, oversees more than 2,000 Yale undergraduates working on more than 70 community service initiatives in New Haven. The Yale College Council runs several agencies that oversee campus wide activities and student services. The Yale Dramatic Association and Bulldog Productions cater to the theater and film communities, respectively. In addition, the Yale Drama Coalition serves to coordinate between and provide resources for the various Sudler Fund sponsored theater productions which run each weekend. WYBC Yale Radio is the campus's radio station, owned and operated by students. While students used to broadcast on AM & FM frequencies, they now have an Internet-only stream. Yale's English and Comparative Literature departments were part of the New Criticism movement. Of the New Critics, Robert Penn Warren, W.K. Wimsatt, and Cleanth Brooks were all Yale faculty. Later, the Yale Comparative literature department became a center of American deconstruction. Jacques Derrida, the father of deconstruction, taught at the Department of Comparative Literature from the late seventies to mid-1980s. Several other Yale faculty members were also associated with deconstruction, forming the so-called ""Yale School"". These included Paul de Man who taught in the Departments of Comparative Literature and French, J. Hillis Miller, Geoffrey Hartman (both taught in the Departments of English and Comparative Literature), and Harold Bloom (English), whose theoretical position was always somewhat specific, and who ultimately took a very different path from the rest of this group. Yale's history department has also originated important intellectual trends. Historians C. Vann Woodward and David Brion Davis are credited with beginning in the 1960s and 1970s an important stream of southern historians; likewise, David Montgomery, a labor historian, advised many of the current generation of labor historians in the country. Yale's Music School and Department fostered the growth of Music Theory in the latter half of the 20th century. The Journal of Music Theory was founded there in 1957; Allen Forte and David Lewin were influential teachers and scholars. Yale University is an American private Ivy League research university in New Haven, Connecticut. Founded in 1701 in Saybrook Colony as the Collegiate School, the University is the third-oldest institution of higher education in the United States. The school was renamed Yale College in 1718 in recognition of a gift from Elihu Yale, who was governor of the British East India Company. Established to train Congregationalist ministers in theology and sacred languages, by 1777 the school's curriculum began to incorporate humanities and sciences. In the 19th century the school incorporated graduate and professional instruction, awarding the first Ph.D. in the United States in 1861 and organizing as a university in 1887. Yale's museum collections are also of international stature. The Yale University Art Gallery, the country's first university-affiliated art museum, contains more than 180,000 works, including Old Masters and important collections of modern art, in the Swartout and Kahn buildings. The latter, Louis Kahn's first large-scale American work (1953), was renovated and reopened in December 2006. The Yale Center for British Art, the largest collection of British art outside of the UK, grew from a gift of Paul Mellon and is housed in another Kahn-designed building. The Yale Provost's Office has launched several women into prominent university presidencies. In 1977 Hanna Holborn Gray was appointed acting President of Yale from this position, and went on to become President of the University of Chicago, the first woman to be full president of a major university. In 1994 Yale Provost Judith Rodin became the first female president of an Ivy League institution at the University of Pennsylvania. In 2002 Provost Alison Richard became the Vice Chancellor of the University of Cambridge. In 2004, Provost Susan Hockfield became the President of the Massachusetts Institute of Technology. In 2007 Deputy Provost Kim Bottomly was named President of Wellesley College. In 2003, the Dean of the Divinity School, Rebecca Chopp, was appointed president of Colgate University and now heads Swarthmore College. Between 1925 and 1940, philanthropic foundations, especially ones connected with the Rockefellers, contributed about $7 million to support the Yale Institute of Human Relations and the affiliated Yerkes Laboratories of Primate Biology. The money went toward behavioral science research, which was supported by foundation officers who aimed to ""improve mankind"" under an informal, loosely defined human engineering effort. The behavioral scientists at Yale, led by President James R. Angell and psychobiologist Robert M. Yerkes, tapped into foundation largesse by crafting research programs aimed to investigate, then suggest, ways to control, sexual and social behavior. For example, Yerkes analyzed chimpanzee sexual behavior in hopes of illuminating the evolutionary underpinnings of human development and providing information that could ameliorate dysfunction. Ultimately, the behavioral-science results disappointed foundation officers, who shifted their human-engineering funds toward biological sciences. Yale is organized into fourteen constituent schools: the original undergraduate college, the Yale Graduate School of Arts and Sciences, and twelve professional schools. While the university is governed by the Yale Corporation, each school's faculty oversees its curriculum and degree programs. In addition to a central campus in downtown New Haven, the University owns athletic facilities in western New Haven, including the Yale Bowl, a campus in West Haven, Connecticut, and forest and nature preserves throughout New England. The university's assets include an endowment valued at $25.6 billion as of September 2015, the second largest of any educational institution.The Yale University Library, serving all constituent schools, holds more than 15 million volumes and is the third-largest academic library in the United States. The Yale Report of 1828 was a dogmatic defense of the Latin and Greek curriculum against critics who wanted more courses in modern languages, mathematics, and science. Unlike higher education in Europe, there was no national curriculum for colleges and universities in the United States. In the competition for students and financial support, college leaders strove to keep current with demands for innovation. At the same time, they realized that a significant portion of their students and prospective students demanded a classical background. The Yale report meant the classics would not be abandoned. All institutions experimented with changes in the curriculum, often resulting in a dual track. In the decentralized environment of higher education in the United States, balancing change with tradition was a common challenge because no one could afford to be completely modern or completely classical. A group of professors at Yale and New Haven Congregationalist ministers articulated a conservative response to the changes brought about by the Victorian culture. They concentrated on developing a whole man possessed of religious values sufficiently strong to resist temptations from within, yet flexible enough to adjust to the 'isms' (professionalism, materialism, individualism, and consumerism) tempting him from without. William Graham Sumner, professor from 1872 to 1909, taught in the emerging disciplines of economics and sociology to overflowing classrooms. He bested President Noah Porter, who disliked social science and wanted Yale to lock into its traditions of classical education. Porter objected to Sumner's use of a textbook by Herbert Spencer that espoused agnostic materialism because it might harm students. Yale has had many financial supporters, but some stand out by the magnitude or timeliness of their contributions. Among those who have made large donations commemorated at the university are: Elihu Yale; Jeremiah Dummer; the Harkness family (Edward, Anna, and William); the Beinecke family (Edwin, Frederick, and Walter); John William Sterling; Payne Whitney; Joseph E. Sheffield, Paul Mellon, Charles B. G. Murphy and William K. Lanman. The Yale Class of 1954, led by Richard Gilder, donated $70 million in commemoration of their 50th reunion. Charles B. Johnson, a 1954 graduate of Yale College, pledged a $250 million gift in 2013 to support of the construction of two new residential colleges. The Boston Globe wrote that ""if there's one school that can lay claim to educating the nation's top national leaders over the past three decades, it's Yale."" Yale alumni were represented on the Democratic or Republican ticket in every U.S. Presidential election between 1972 and 2004. Yale-educated Presidents since the end of the Vietnam War include Gerald Ford, George H.W. Bush, Bill Clinton, and George W. Bush, and major-party nominees during this period include John Kerry (2004), Joseph Lieberman (Vice President, 2000), and Sargent Shriver (Vice President, 1972). Other Yale alumni who made serious bids for the Presidency during this period include Hillary Clinton (2008), Howard Dean (2004), Gary Hart (1984 and 1988), Paul Tsongas (1992), Pat Robertson (1988) and Jerry Brown (1976, 1980, 1992). Yale is noted for its largely Collegiate Gothic campus as well as for several iconic modern buildings commonly discussed in architectural history survey courses: Louis Kahn's Yale Art Gallery and Center for British Art, Eero Saarinen's Ingalls Rink and Ezra Stiles and Morse Colleges, and Paul Rudolph's Art & Architecture Building. Yale also owns and has restored many noteworthy 19th-century mansions along Hillhouse Avenue, which was considered the most beautiful street in America by Charles Dickens when he visited the United States in the 1840s. In 2011, Travel+Leisure listed the Yale campus as one of the most beautiful in the United States. The American studies program reflected the worldwide anti-Communist ideological struggle. Norman Holmes Pearson, who worked for the Office of Strategic Studies in London during World War II, returned to Yale and headed the new American studies program, in which scholarship quickly became an instrument of promoting liberty. Popular among undergraduates, the program sought to instruct them in the fundamentals of American civilization and thereby instill a sense of nationalism and national purpose. Also during the 1940s and 1950s, Wyoming millionaire William Robertson Coe made large contributions to the American studies programs at Yale University and at the University of Wyoming. Coe was concerned to celebrate the 'values' of the Western United States in order to meet the ""threat of communism."""
