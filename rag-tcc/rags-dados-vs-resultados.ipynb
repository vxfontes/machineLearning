{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: haystack-ai in c:\\users\\nessa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (2.9.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\nessa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (2.2.2)\n",
      "Requirement already satisfied: datasets in c:\\users\\nessa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (3.2.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\nessa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (3.10.0)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\nessa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (1.0.1)\n",
      "Requirement already satisfied: qdrant-client in c:\\users\\nessa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (1.13.2)\n",
      "Requirement already satisfied: langchain-openai in c:\\users\\nessa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (0.3.5)\n",
      "Requirement already satisfied: langchain_community in c:\\users\\nessa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (0.3.17)\n",
      "Collecting streamlit\n",
      "  Using cached streamlit-1.42.0-py2.py3-none-any.whl.metadata (8.9 kB)\n",
      "Collecting sentence-transformers\n",
      "  Using cached sentence_transformers-3.4.1-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting langchain_experimental\n",
      "  Using cached langchain_experimental-0.3.4-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: ollama in c:\\users\\nessa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (0.4.7)\n",
      "Requirement already satisfied: haystack-experimental in c:\\users\\nessa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from haystack-ai) (0.6.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\nessa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from haystack-ai) (3.1.5)\n",
      "Requirement already satisfied: lazy-imports in c:\\users\\nessa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from haystack-ai) (0.4.0)\n",
      "Requirement already satisfied: more-itertools in c:\\users\\nessa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from haystack-ai) (10.6.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\nessa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from haystack-ai) (3.4.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\nessa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from haystack-ai) (1.26.4)\n",
      "Requirement already satisfied: openai>=1.56.1 in c:\\users\\nessa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from haystack-ai) (1.61.1)\n",
      "Requirement already satisfied: posthog in c:\\users\\nessa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from haystack-ai) (3.12.0)\n",
      "Requirement already satisfied: pydantic in c:\\users\\nessa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from haystack-ai) (2.10.6)\n",
      "Requirement already satisfied: python-dateutil in c:\\users\\nessa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from haystack-ai) (2.9.0.post0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\nessa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from haystack-ai) (6.0.2)\n",
      "Requirement already satisfied: requests in c:\\users\\nessa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from haystack-ai) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0 in c:\\users\\nessa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from haystack-ai) (9.0.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\nessa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from haystack-ai) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\nessa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from haystack-ai) (4.12.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\nessa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\nessa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\nessa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from datasets) (3.17.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\nessa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from datasets) (19.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\nessa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: xxhash in c:\\users\\nessa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\nessa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in c:\\users\\nessa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\nessa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from datasets) (3.11.12)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in c:\\users\\nessa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\nessa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from datasets) (24.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\nessa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\nessa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\nessa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\nessa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\nessa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\nessa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib) (3.2.1)\n",
      "Requirement already satisfied: grpcio>=1.41.0 in c:\\users\\nessa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from qdrant-client) (1.70.0)\n",
      "Requirement already satisfied: grpcio-tools>=1.41.0 in c:\\users\\nessa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from qdrant-client) (1.70.0)\n",
      "Requirement already satisfied: httpx>=0.20.0 in c:\\users\\nessa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from httpx[http2]>=0.20.0->qdrant-client) (0.28.1)\n",
      "Requirement already satisfied: portalocker<3.0.0,>=2.7.0 in c:\\users\\nessa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from qdrant-client) (2.10.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.26.14 in c:\\users\\nessa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from qdrant-client) (2.2.1)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.34 in c:\\users\\nessa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain-openai) (0.3.34)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in c:\\users\\nessa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain-openai) (0.8.0)\n",
      "Requirement already satisfied: langchain<1.0.0,>=0.3.18 in c:\\users\\nessa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain_community) (0.3.18)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\nessa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain_community) (2.0.38)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\nessa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain_community) (0.6.6)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in c:\\users\\nessa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain_community) (2.7.1)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in c:\\users\\nessa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain_community) (0.3.8)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in c:\\users\\nessa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain_community) (0.4.0)\n",
      "Collecting altair<6,>=4.0 (from streamlit)\n",
      "  Using cached altair-5.5.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting blinker<2,>=1.0.0 (from streamlit)\n",
      "  Using cached blinker-1.9.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting cachetools<6,>=4.0 (from streamlit)\n",
      "  Using cached cachetools-5.5.1-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: click<9,>=7.0 in c:\\users\\nessa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from streamlit) (8.1.7)\n",
      "Requirement already satisfied: protobuf<6,>=3.20 in c:\\users\\nessa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from streamlit) (5.29.3)\n",
      "Requirement already satisfied: rich<14,>=10.14.0 in c:\\users\\nessa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from streamlit) (13.7.1)\n",
      "Collecting toml<2,>=0.10.1 (from streamlit)\n",
      "  Using cached toml-0.10.2-py2.py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting watchdog<7,>=2.1.5 (from streamlit)\n",
      "  Using cached watchdog-6.0.0-py3-none-win_amd64.whl.metadata (44 kB)\n",
      "Collecting gitpython!=3.1.19,<4,>=3.0.7 (from streamlit)\n",
      "  Using cached GitPython-3.1.44-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
      "  Using cached pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: tornado<7,>=6.0.3 in c:\\users\\nessa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from streamlit) (6.4)\n",
      "Collecting transformers<5.0.0,>=4.41.0 (from sentence-transformers)\n",
      "  Using cached transformers-4.48.3-py3-none-any.whl.metadata (44 kB)\n",
      "Collecting torch>=1.11.0 (from sentence-transformers)\n",
      "  Downloading torch-2.6.0-cp311-cp311-win_amd64.whl.metadata (28 kB)\n",
      "Collecting scikit-learn (from sentence-transformers)\n",
      "  Downloading scikit_learn-1.6.1-cp311-cp311-win_amd64.whl.metadata (15 kB)\n",
      "Collecting scipy (from sentence-transformers)\n",
      "  Downloading scipy-1.15.1-cp311-cp311-win_amd64.whl.metadata (60 kB)\n",
      "     ---------------------------------------- 0.0/60.8 kB ? eta -:--:--\n",
      "     ---------------------------------------- 60.8/60.8 kB 1.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\nessa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp->datasets) (2.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\nessa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\nessa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp->datasets) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\nessa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\nessa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\nessa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp->datasets) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\nessa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Collecting jsonschema>=3.0 (from altair<6,>=4.0->streamlit)\n",
      "  Using cached jsonschema-4.23.0-py3-none-any.whl.metadata (7.9 kB)\n",
      "Collecting narwhals>=1.14.2 (from altair<6,>=4.0->streamlit)\n",
      "  Using cached narwhals-1.26.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\nessa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from click<9,>=7.0->streamlit) (0.4.6)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\nessa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.21.2)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\nessa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
      "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
      "  Using cached gitdb-4.0.12-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\nessa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from grpcio-tools>=1.41.0->qdrant-client) (70.0.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\nessa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (4.8.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\nessa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\nessa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (1.0.7)\n",
      "Requirement already satisfied: idna in c:\\users\\nessa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (3.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\nessa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from httpcore==1.*->httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (0.14.0)\n",
      "Requirement already satisfied: h2<5,>=3 in c:\\users\\nessa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from httpx[http2]>=0.20.0->qdrant-client) (4.2.0)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.6 in c:\\users\\nessa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain<1.0.0,>=0.3.18->langchain_community) (0.3.6)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\nessa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain-core<1.0.0,>=0.3.34->langchain-openai) (1.33)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\nessa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langsmith<0.4,>=0.1.125->langchain_community) (3.10.15)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\nessa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langsmith<0.4,>=0.1.125->langchain_community) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\users\\nessa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langsmith<0.4,>=0.1.125->langchain_community) (0.23.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\nessa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from openai>=1.56.1->haystack-ai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\nessa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from openai>=1.56.1->haystack-ai) (0.8.2)\n",
      "Requirement already satisfied: sniffio in c:\\users\\nessa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from openai>=1.56.1->haystack-ai) (1.3.1)\n",
      "Requirement already satisfied: pywin32>=226 in c:\\users\\nessa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from portalocker<3.0.0,>=2.7.0->qdrant-client) (306)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\nessa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pydantic->haystack-ai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\nessa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pydantic->haystack-ai) (2.27.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\nessa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from jinja2->haystack-ai) (2.1.5)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\nessa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from python-dateutil->haystack-ai) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\nessa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests->haystack-ai) (3.3.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\nessa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\nessa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from rich<14,>=10.14.0->streamlit) (2.17.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\nessa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.1.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\nessa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.5.15)\n",
      "Collecting sympy==1.13.1 (from torch>=1.11.0->sentence-transformers)\n",
      "  Using cached sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy==1.13.1->torch>=1.11.0->sentence-transformers)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Using cached tokenizers-0.21.0-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Using cached safetensors-0.5.2-cp38-abi3-win_amd64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: monotonic>=1.5 in c:\\users\\nessa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from posthog->haystack-ai) (1.6)\n",
      "Requirement already satisfied: backoff>=1.10.0 in c:\\users\\nessa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from posthog->haystack-ai) (2.2.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\nessa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn->sentence-transformers)\n",
      "  Using cached threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
      "  Using cached smmap-5.0.2-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: hyperframe<7,>=6.1 in c:\\users\\nessa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client) (6.1.0)\n",
      "Requirement already satisfied: hpack<5,>=4.1 in c:\\users\\nessa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client) (4.1.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\nessa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.34->langchain-openai) (3.0.0)\n",
      "Collecting jsonschema-specifications>=2023.03.6 (from jsonschema>=3.0->altair<6,>=4.0->streamlit)\n",
      "  Using cached jsonschema_specifications-2024.10.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting referencing>=0.28.4 (from jsonschema>=3.0->altair<6,>=4.0->streamlit)\n",
      "  Using cached referencing-0.36.2-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting rpds-py>=0.7.1 (from jsonschema>=3.0->altair<6,>=4.0->streamlit)\n",
      "  Downloading rpds_py-0.22.3-cp311-cp311-win_amd64.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\nessa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\nessa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.0.0)\n",
      "Using cached streamlit-1.42.0-py2.py3-none-any.whl (9.6 MB)\n",
      "Using cached sentence_transformers-3.4.1-py3-none-any.whl (275 kB)\n",
      "Using cached langchain_experimental-0.3.4-py3-none-any.whl (209 kB)\n",
      "Using cached altair-5.5.0-py3-none-any.whl (731 kB)\n",
      "Using cached blinker-1.9.0-py3-none-any.whl (8.5 kB)\n",
      "Using cached cachetools-5.5.1-py3-none-any.whl (9.5 kB)\n",
      "Using cached GitPython-3.1.44-py3-none-any.whl (207 kB)\n",
      "Using cached pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
      "Using cached toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Downloading torch-2.6.0-cp311-cp311-win_amd64.whl (204.2 MB)\n",
      "   ---------------------------------------- 0.0/204.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.1/204.2 MB 6.4 MB/s eta 0:00:32\n",
      "   ---------------------------------------- 0.4/204.2 MB 4.5 MB/s eta 0:00:46\n",
      "   ---------------------------------------- 0.8/204.2 MB 6.6 MB/s eta 0:00:32\n",
      "   ---------------------------------------- 1.9/204.2 MB 11.0 MB/s eta 0:00:19\n",
      "    --------------------------------------- 4.0/204.2 MB 18.2 MB/s eta 0:00:12\n",
      "   - -------------------------------------- 7.3/204.2 MB 27.5 MB/s eta 0:00:08\n",
      "   -- ------------------------------------- 10.3/204.2 MB 34.4 MB/s eta 0:00:06\n",
      "   -- ------------------------------------- 10.3/204.2 MB 34.4 MB/s eta 0:00:06\n",
      "   -- ------------------------------------- 10.4/204.2 MB 28.5 MB/s eta 0:00:07\n",
      "   -- ------------------------------------- 10.5/204.2 MB 26.2 MB/s eta 0:00:08\n",
      "   -- ------------------------------------- 10.7/204.2 MB 27.3 MB/s eta 0:00:08\n",
      "   -- ------------------------------------- 10.9/204.2 MB 25.2 MB/s eta 0:00:08\n",
      "   -- ------------------------------------- 11.2/204.2 MB 23.4 MB/s eta 0:00:09\n",
      "   -- ------------------------------------- 11.4/204.2 MB 21.8 MB/s eta 0:00:09\n",
      "   -- ------------------------------------- 11.7/204.2 MB 20.5 MB/s eta 0:00:10\n",
      "   -- ------------------------------------- 12.0/204.2 MB 18.7 MB/s eta 0:00:11\n",
      "   -- ------------------------------------- 12.3/204.2 MB 18.2 MB/s eta 0:00:11\n",
      "   -- ------------------------------------- 12.7/204.2 MB 16.8 MB/s eta 0:00:12\n",
      "   -- ------------------------------------- 13.1/204.2 MB 16.0 MB/s eta 0:00:12\n",
      "   -- ------------------------------------- 13.5/204.2 MB 15.2 MB/s eta 0:00:13\n",
      "   -- ------------------------------------- 14.0/204.2 MB 14.2 MB/s eta 0:00:14\n",
      "   -- ------------------------------------- 14.6/204.2 MB 13.9 MB/s eta 0:00:14\n",
      "   --- ------------------------------------ 15.5/204.2 MB 12.4 MB/s eta 0:00:16\n",
      "   --- ------------------------------------ 16.3/204.2 MB 11.9 MB/s eta 0:00:16\n",
      "   --- ------------------------------------ 17.3/204.2 MB 11.3 MB/s eta 0:00:17\n",
      "   --- ------------------------------------ 18.1/204.2 MB 11.1 MB/s eta 0:00:17\n",
      "   --- ------------------------------------ 19.1/204.2 MB 10.7 MB/s eta 0:00:18\n",
      "   --- ------------------------------------ 19.9/204.2 MB 10.2 MB/s eta 0:00:19\n",
      "   ---- ----------------------------------- 20.5/204.2 MB 10.9 MB/s eta 0:00:17\n",
      "   ---- ----------------------------------- 20.8/204.2 MB 11.3 MB/s eta 0:00:17\n",
      "   ---- ----------------------------------- 22.9/204.2 MB 16.0 MB/s eta 0:00:12\n",
      "   ---- ----------------------------------- 23.1/204.2 MB 16.8 MB/s eta 0:00:11\n",
      "   ---- ----------------------------------- 23.6/204.2 MB 15.6 MB/s eta 0:00:12\n",
      "   ---- ----------------------------------- 23.9/204.2 MB 16.0 MB/s eta 0:00:12\n",
      "   ---- ----------------------------------- 24.2/204.2 MB 14.9 MB/s eta 0:00:13\n",
      "   ---- ----------------------------------- 24.6/204.2 MB 14.9 MB/s eta 0:00:13\n",
      "   ---- ----------------------------------- 25.0/204.2 MB 14.9 MB/s eta 0:00:13\n",
      "   ----- ---------------------------------- 25.7/204.2 MB 14.9 MB/s eta 0:00:13\n",
      "   ----- ---------------------------------- 26.4/204.2 MB 14.9 MB/s eta 0:00:12\n",
      "   ----- ---------------------------------- 27.2/204.2 MB 14.5 MB/s eta 0:00:13\n",
      "   ----- ---------------------------------- 28.3/204.2 MB 14.6 MB/s eta 0:00:13\n",
      "   ----- ---------------------------------- 29.5/204.2 MB 15.6 MB/s eta 0:00:12\n",
      "   ----- ---------------------------------- 30.2/204.2 MB 15.2 MB/s eta 0:00:12\n",
      "   ----- ---------------------------------- 30.5/204.2 MB 14.9 MB/s eta 0:00:12\n",
      "   ------ --------------------------------- 34.2/204.2 MB 21.1 MB/s eta 0:00:09\n",
      "   ------- -------------------------------- 39.6/204.2 MB 50.4 MB/s eta 0:00:04\n",
      "   -------- ------------------------------ 44.6/204.2 MB 110.0 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 47.8/204.2 MB 93.9 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 51.0/204.2 MB 81.8 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 54.3/204.2 MB 72.6 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 57.5/204.2 MB 72.6 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 60.6/204.2 MB 72.6 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 63.9/204.2 MB 72.6 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 67.0/204.2 MB 73.1 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 70.1/204.2 MB 73.1 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 73.3/204.2 MB 73.1 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 76.5/204.2 MB 72.6 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 79.7/204.2 MB 72.6 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 82.9/204.2 MB 72.6 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 86.1/204.2 MB 72.6 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 89.3/204.2 MB 72.6 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 92.5/204.2 MB 73.1 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 95.7/204.2 MB 73.1 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 98.9/204.2 MB 73.1 MB/s eta 0:00:02\n",
      "   ------------------- ------------------- 102.1/204.2 MB 72.6 MB/s eta 0:00:02\n",
      "   -------------------- ------------------ 105.3/204.2 MB 72.6 MB/s eta 0:00:02\n",
      "   -------------------- ------------------ 108.5/204.2 MB 72.6 MB/s eta 0:00:02\n",
      "   --------------------- ----------------- 111.7/204.2 MB 65.6 MB/s eta 0:00:02\n",
      "   --------------------- ----------------- 114.9/204.2 MB 65.6 MB/s eta 0:00:02\n",
      "   ---------------------- ---------------- 118.1/204.2 MB 65.6 MB/s eta 0:00:02\n",
      "   ----------------------- --------------- 121.3/204.2 MB 65.6 MB/s eta 0:00:02\n",
      "   ----------------------- --------------- 124.5/204.2 MB 65.6 MB/s eta 0:00:02\n",
      "   ------------------------ -------------- 127.8/204.2 MB 65.2 MB/s eta 0:00:02\n",
      "   ------------------------- ------------- 131.0/204.2 MB 65.2 MB/s eta 0:00:02\n",
      "   ------------------------- ------------- 134.2/204.2 MB 65.6 MB/s eta 0:00:02\n",
      "   -------------------------- ------------ 137.4/204.2 MB 65.6 MB/s eta 0:00:02\n",
      "   -------------------------- ------------ 140.5/204.2 MB 65.6 MB/s eta 0:00:01\n",
      "   --------------------------- ----------- 143.8/204.2 MB 65.6 MB/s eta 0:00:01\n",
      "   ---------------------------- ---------- 147.0/204.2 MB 65.6 MB/s eta 0:00:01\n",
      "   ---------------------------- ---------- 150.2/204.2 MB 65.6 MB/s eta 0:00:01\n",
      "   ----------------------------- --------- 153.4/204.2 MB 65.2 MB/s eta 0:00:01\n",
      "   ----------------------------- --------- 156.5/204.2 MB 65.2 MB/s eta 0:00:01\n",
      "   ------------------------------ -------- 159.6/204.2 MB 65.6 MB/s eta 0:00:01\n",
      "   ------------------------------- ------- 162.9/204.2 MB 65.6 MB/s eta 0:00:01\n",
      "   ------------------------------- ------- 166.1/204.2 MB 65.6 MB/s eta 0:00:01\n",
      "   -------------------------------- ------ 169.3/204.2 MB 65.6 MB/s eta 0:00:01\n",
      "   -------------------------------- ------ 172.5/204.2 MB 65.6 MB/s eta 0:00:01\n",
      "   --------------------------------- ----- 175.7/204.2 MB 65.6 MB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 178.9/204.2 MB 65.2 MB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 182.0/204.2 MB 65.2 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 185.3/204.2 MB 65.6 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 188.5/204.2 MB 65.6 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 191.7/204.2 MB 65.6 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 194.8/204.2 MB 65.6 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 198.1/204.2 MB 65.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  201.2/204.2 MB 65.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  204.2/204.2 MB 65.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  204.2/204.2 MB 65.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  204.2/204.2 MB 65.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  204.2/204.2 MB 65.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  204.2/204.2 MB 65.2 MB/s eta 0:00:01\n",
      "   --------------------------------------- 204.2/204.2 MB 26.2 MB/s eta 0:00:00\n",
      "Using cached sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "Using cached transformers-4.48.3-py3-none-any.whl (9.7 MB)\n",
      "Using cached watchdog-6.0.0-py3-none-win_amd64.whl (79 kB)\n",
      "Downloading scikit_learn-1.6.1-cp311-cp311-win_amd64.whl (11.1 MB)\n",
      "   ---------------------------------------- 0.0/11.1 MB ? eta -:--:--\n",
      "   ------------- -------------------------- 3.7/11.1 MB 118.3 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 7.7/11.1 MB 123.5 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 8.0/11.1 MB 64.3 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 8.4/11.1 MB 53.8 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 8.8/11.1 MB 43.2 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 9.2/11.1 MB 36.7 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 9.6/11.1 MB 32.4 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 10.1/11.1 MB 29.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 10.5/11.1 MB 26.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 10.8/11.1 MB 23.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.1/11.1 MB 21.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.1/11.1 MB 18.7 MB/s eta 0:00:00\n",
      "Downloading scipy-1.15.1-cp311-cp311-win_amd64.whl (43.9 MB)\n",
      "   ---------------------------------------- 0.0/43.9 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 3.7/43.9 MB 118.9 MB/s eta 0:00:01\n",
      "   ------ --------------------------------- 7.4/43.9 MB 94.9 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 11.1/43.9 MB 72.6 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 14.8/43.9 MB 72.6 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 18.5/43.9 MB 72.6 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 21.4/43.9 MB 73.1 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 26.1/43.9 MB 73.1 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 29.9/43.9 MB 73.1 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 33.7/43.9 MB 81.8 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 37.3/43.9 MB 72.6 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 40.7/43.9 MB 72.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  43.9/43.9 MB 72.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  43.9/43.9 MB 72.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 43.9/43.9 MB 54.4 MB/s eta 0:00:00\n",
      "Using cached gitdb-4.0.12-py3-none-any.whl (62 kB)\n",
      "Using cached jsonschema-4.23.0-py3-none-any.whl (88 kB)\n",
      "Using cached narwhals-1.26.0-py3-none-any.whl (306 kB)\n",
      "Using cached safetensors-0.5.2-cp38-abi3-win_amd64.whl (303 kB)\n",
      "Using cached threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Using cached tokenizers-0.21.0-cp39-abi3-win_amd64.whl (2.4 MB)\n",
      "Using cached jsonschema_specifications-2024.10.1-py3-none-any.whl (18 kB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Using cached referencing-0.36.2-py3-none-any.whl (26 kB)\n",
      "Downloading rpds_py-0.22.3-cp311-cp311-win_amd64.whl (231 kB)\n",
      "   ---------------------------------------- 0.0/231.8 kB ? eta -:--:--\n",
      "   --------------------------------------- 231.8/231.8 kB 14.8 MB/s eta 0:00:00\n",
      "Using cached smmap-5.0.2-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: mpmath, watchdog, toml, threadpoolctl, sympy, smmap, scipy, safetensors, rpds-py, narwhals, cachetools, blinker, torch, scikit-learn, referencing, pydeck, gitdb, tokenizers, jsonschema-specifications, gitpython, transformers, jsonschema, sentence-transformers, altair, streamlit, langchain_experimental\n",
      "Successfully installed altair-5.5.0 blinker-1.9.0 cachetools-5.5.1 gitdb-4.0.12 gitpython-3.1.44 jsonschema-4.23.0 jsonschema-specifications-2024.10.1 langchain_experimental-0.3.4 mpmath-1.3.0 narwhals-1.26.0 pydeck-0.9.1 referencing-0.36.2 rpds-py-0.22.3 safetensors-0.5.2 scikit-learn-1.6.1 scipy-1.15.1 sentence-transformers-3.4.1 smmap-5.0.2 streamlit-1.42.0 sympy-1.13.1 threadpoolctl-3.5.0 tokenizers-0.21.0 toml-0.10.2 torch-2.6.0 transformers-4.48.3 watchdog-6.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: textract 1.6.5 has a non-standard dependency specifier extract-msg<=0.29.*. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of textract or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.0.1\n",
      "[notice] To update, run: C:\\Users\\nessa\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install haystack-ai pandas datasets matplotlib python-dotenv qdrant-client langchain-openai langchain_community streamlit sentence-transformers langchain_experimental ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "os.environ[\"OPENAI_API_KEY\"] = api_key  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"rajpurkar/squad_v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_union = pd.concat([pd.DataFrame(dataset['train']), pd.DataFrame(dataset['validation'])], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## separação de informações"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "o dataset tem esse formato\n",
    "- Index(['id', 'title', 'context', 'question', 'answers'], dtype='object')\n",
    "\n",
    "#### Contexto\n",
    "existem dois tipos de separação para contexto\n",
    "- mantendo todas as tuplas separadamente, ou seja, title e seu context correspondente\n",
    "- unindo todos os context de um mesmo title, reduzindo bastante o numero de tuplas e deixando os contextos mais completos\n",
    "\n",
    "#### Perguntas e Resposta (Q&A)\n",
    "- obtendo o titulo, pergunta e resposta de cada tupla e salvando individualmente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "class Contexto(Enum):\n",
    "    GERAIS_100 = 'contexto_gerais_100'\n",
    "    GERAIS_50 = 'contexto_gerais_50'\n",
    "    GERAIS_25 = 'contexto_gerais_25'\n",
    "    GERAIS_15 = 'contexto_gerais_15'\n",
    "    PORTITULO_100 = 'contexto_portitulo_100'\n",
    "    PORTITULO_50 = 'contexto_portitulo_50'\n",
    "    PORTITULO_25 = 'contexto_portitulo_25'\n",
    "    PORTITULO_15 = 'contexto_portitulo_15'\n",
    "    QA_PAIRS = 'qa_pairs'\n",
    "    QA_PAIRS_PORTITULO_100 = 'qa_pairs_portitulo_100'\n",
    "    QA_PAIRS_PORTITULO_50 = 'qa_pairs_portitulo_50'\n",
    "    QA_PAIRS_PORTITULO_25 = 'qa_pairs_portitulo_25'\n",
    "    QA_PAIRS_PORTITULO_15 = 'qa_pairs_portitulo_15'\n",
    "    QA_PAIRS_GERAIS_100 = 'qa_pairs_gerais_100'\n",
    "    QA_PAIRS_GERAIS_50 = 'qa_pairs_gerais_50'\n",
    "    QA_PAIRS_GERAIS_25 = 'qa_pairs_gerais_25'\n",
    "    QA_PAIRS_GERAIS_15 = 'qa_pairs_gerais_15'\n",
    "    TESTE_PORTITULO = 'contexto_portitulo_teste'\n",
    "    TESTE_GERAIS = 'contexto_gerais_teste'\n",
    "    QA_TESTE_PORTITULO = 'qa_teste_portitulo'\n",
    "    QA_TESTE_GERAIS = 'qa_teste_gerais'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### unindo contextos por titulo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consolidate_contexts(group):\n",
    "    # Remover duplicatas dentro de um mesmo título\n",
    "    unique_contexts = set(group)\n",
    "    # Juntar os contextos em um único texto corrido\n",
    "    return \" \".join(unique_contexts)\n",
    "\n",
    "contextos_df_unindo = data_union.copy().groupby(\"title\")[\"context\"].apply(consolidate_contexts).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 100% dos dados (embaralhados)\n",
    "contextos_df_unindo.to_csv(f'''data/{Contexto.PORTITULO_100.value}.csv''', index=False)\n",
    "\n",
    "# 50% dos dados\n",
    "contextos_df_unindo.sample(frac=0.5, random_state=42).to_csv(f'''data/{Contexto.PORTITULO_50.value}.csv''', index=False)\n",
    "\n",
    "# 25% dos dados\n",
    "contextos_df_unindo.sample(frac=0.25, random_state=42).to_csv(f'''data/{Contexto.PORTITULO_25.value}.csv''', index=False)\n",
    "\n",
    "# 15% dos dados\n",
    "contextos_df_unindo.sample(frac=0.15, random_state=42).to_csv(f'''data/{Contexto.PORTITULO_15.value}.csv''', index=False)\n",
    "\n",
    "print(\"Arquivos gerados com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sem unir por titulo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contextos_df_title = data_union.copy()\n",
    "contextos_df_title = contextos_df_title[[\"title\", \"context\"]]\n",
    "\n",
    "# Embaralha os dados antes de amostrar\n",
    "contextos_df_title = contextos_df_title.sample(frac=1, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 100% dos dados\n",
    "contextos_df_title.to_csv(f'''data/{Contexto.GERAIS_100.value}.csv''', index=False)\n",
    "\n",
    "# 50% dos dados\n",
    "contextos_df_title.sample(frac=0.5, random_state=42).to_csv(f'''data/{Contexto.GERAIS_50.value}.csv''', index=False)\n",
    "\n",
    "# 25% dos dados\n",
    "contextos_df_title.sample(frac=0.25, random_state=42).to_csv(f'''data/{Contexto.GERAIS_25.value}.csv''', index=False)\n",
    "\n",
    "# 15% dos dados\n",
    "contextos_df_title.sample(frac=0.15, random_state=42).to_csv(f'''data/{Contexto.GERAIS_15.value}.csv''', index=False)\n",
    "\n",
    "print(\"Arquivos gerados com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### separando qa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_df = data_union[[\"title\", \"question\", \"answers\"]]\n",
    "\n",
    "def extract_answers(row):\n",
    "    return row['answers']['text']\n",
    "\n",
    "qa_df['answers'] = qa_df.apply(extract_answers, axis=1)\n",
    "qa_df = qa_df[qa_df['answers'].str.len() > 0]\n",
    "\n",
    "def process_answers(answer_list):\n",
    "    if not isinstance(answer_list, list):\n",
    "        return answer_list  # Retorna diretamente caso não seja uma lista\n",
    "    \n",
    "    unique_answers = list(set(answer_list))  # Remove duplicatas\n",
    "    \n",
    "    if len(unique_answers) == 1:\n",
    "        return unique_answers[0]  # Retorna como string se houver apenas um item único\n",
    "    \n",
    "    return unique_answers  # Retorna a lista se houver múltiplos valores distintos\n",
    "\n",
    "qa_df['answers'] = qa_df['answers'].apply(process_answers)\n",
    "\n",
    "qa_df.to_csv(f'''data/qa/{Contexto.QA_PAIRS.value}_geral.csv''', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## haystack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- inicializando itens do haystack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack import Pipeline\n",
    "from haystack.document_stores.in_memory import InMemoryDocumentStore\n",
    "from haystack.components.retrievers import InMemoryEmbeddingRetriever\n",
    "from haystack.components.converters import TextFileToDocument\n",
    "from haystack.components.preprocessors import DocumentCleaner, DocumentSplitter\n",
    "from haystack.components.embedders import OpenAIDocumentEmbedder, OpenAITextEmbedder\n",
    "from haystack.components.writers import DocumentWriter\n",
    "from haystack.components.builders import PromptBuilder\n",
    "from haystack.components.generators import OpenAIGenerator\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- montando pipeline de indexacao de documentos\n",
    "- montando pipeline de recuperação e geração"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(source_file: str, qa_passado_df: pd.DataFrame, output_filename: str):\n",
    "    text_file_converter = TextFileToDocument()\n",
    "    cleaner = DocumentCleaner()\n",
    "    splitter = DocumentSplitter()\n",
    "    embedder = OpenAIDocumentEmbedder()\n",
    "    indexing_pipeline = Pipeline()\n",
    "\n",
    "    text_embedder = OpenAITextEmbedder()\n",
    "    template = \"\"\"Given these contexts, answer the question.\n",
    "                    Context:\n",
    "                    {% for doc in documents %}\n",
    "                        {{ doc.content }}\n",
    "                    {% endfor %}\n",
    "                    Question: {{query}}\n",
    "                    Answer:\"\"\"\n",
    "    prompt_builder = PromptBuilder(template=template)\n",
    "    llm = OpenAIGenerator()\n",
    "    rag_pipeline = Pipeline()\n",
    "\n",
    "    print('------------------- montando pipeline de indexação de docs')\n",
    "    document_store = InMemoryDocumentStore()\n",
    "    writer = DocumentWriter(document_store)\n",
    "    retriever = InMemoryEmbeddingRetriever(document_store)\n",
    "    \n",
    "    indexing_pipeline.add_component(\"converter\", text_file_converter)\n",
    "    indexing_pipeline.add_component(\"cleaner\", cleaner)\n",
    "    indexing_pipeline.add_component(\"splitter\", splitter)\n",
    "    indexing_pipeline.add_component(\"embedder\", embedder)\n",
    "    indexing_pipeline.add_component(\"writer\", writer)\n",
    "\n",
    "    indexing_pipeline.connect(\"converter.documents\", \"cleaner.documents\")\n",
    "    indexing_pipeline.connect(\"cleaner.documents\", \"splitter.documents\")\n",
    "    indexing_pipeline.connect(\"splitter.documents\", \"embedder.documents\")\n",
    "    indexing_pipeline.connect(\"embedder.documents\", \"writer.documents\")\n",
    "\n",
    "    print('------------------- pipeline montada, lendo arquivo')\n",
    "    indexing_pipeline.run(data={\"sources\": [f'''data/{source_file}.csv''']})\n",
    "\n",
    "    print(f\"Documentos do arquivo {source_file} indexados com sucesso!\")\n",
    "    print('------------------- montando pipelines de rag (retriver e generator)')\n",
    "\n",
    "    rag_pipeline.add_component(\"text_embedder\", text_embedder)\n",
    "    rag_pipeline.add_component(\"retriever\", retriever)\n",
    "    rag_pipeline.add_component(\"prompt_builder\", prompt_builder)\n",
    "    rag_pipeline.add_component(\"llm\", llm)\n",
    "\n",
    "    rag_pipeline.connect(\"text_embedder.embedding\", \"retriever.query_embedding\")\n",
    "    rag_pipeline.connect(\"retriever.documents\", \"prompt_builder.documents\")\n",
    "    rag_pipeline.connect(\"prompt_builder\", \"llm\")\n",
    "\n",
    "    print('------------------- pipelines de rag montada')\n",
    "\n",
    "    print('------------------- iniciando testes de perguntas e respostas')\n",
    "\n",
    "    results = []\n",
    "    i = 0\n",
    "    total = len(qa_passado_df)\n",
    "    \n",
    "    for index, row in qa_passado_df.iterrows():\n",
    "        query = row[\"question\"]\n",
    "        expected_answer = row[\"answers\"]\n",
    "        \n",
    "        generated_answer = rag_pipeline.run(data={\"prompt_builder\": {\"query\": query}, \"text_embedder\": {\"text\": query}})\n",
    "\n",
    "        results.append({\n",
    "            \"title\": row[\"title\"],\n",
    "            \"question\": query,\n",
    "            \"expected_answer\": expected_answer,\n",
    "            \"generated_answer\": generated_answer[\"llm\"][\"replies\"],\n",
    "        })\n",
    "        i += 1\n",
    "\n",
    "        print(f\"################### Processando ({i}/{total})...\")\n",
    "        time.sleep(0.1)\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv(f'''data/qa/{output_filename}.csv''', index=False)\n",
    "\n",
    "    print(f\"\\n-------------------Finalizado! Resultados salvos em {output_filename}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## resultados haystack por titulo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_df_5_porcento = qa_df.sample(frac=0.05, random_state=42)\n",
    "qa_df_01_porcento = qa_df.sample(frac=0.001, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# qa_df_usado = qa_df.copy()\n",
    "# qa_df_usado.to_csv(f'''data/qa/{Contexto.QA_PAIRS.value}_usado.csv''')\n",
    "qa_df_usado = pd.read_csv(f'''data/qa/{Contexto.QA_PAIRS.value}_usado.csv''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_df_usado.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline(Contexto.PORTITULO_15.value, qa_df_usado, Contexto.QA_PAIRS_PORTITULO_15.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline(Contexto.PORTITULO_25.value, qa_df_usado, Contexto.QA_PAIRS_PORTITULO_25.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline(Contexto.PORTITULO_50.value, qa_df_usado, Contexto.QA_PAIRS_PORTITULO_50.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline(Contexto.PORTITULO_100.value, qa_df_usado, Contexto.QA_PAIRS_PORTITULO_100.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## resultados haystack geral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline(Contexto.GERAIS_15.value, qa_df_usado, Contexto.QA_PAIRS_GERAIS_15.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline(Contexto.GERAIS_25.value, qa_df_usado, Contexto.QA_PAIRS_GERAIS_25.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline(Contexto.GERAIS_50.value, qa_df_usado, Contexto.QA_PAIRS_GERAIS_50.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline(Contexto.GERAIS_100.value, qa_df_usado, Contexto.QA_PAIRS_GERAIS_100.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## comparação de respostas com llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### usando openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import re\n",
    "\n",
    "def use_llm_openai(esperada, obtida):\n",
    "    cliente = OpenAI(api_key=api_key)\n",
    "\n",
    "    prompt = f'''\n",
    "        Você é um assistente que compara textos. \n",
    "        Compare a resposta esperada '{esperada}' com a resposta adquirida '{obtida}' e diga se são semanticamente semelhantes. \n",
    "\n",
    "        Responda em uma escala de 0 a 10, onde 0 significa que os textos são completamente diferentes e 10 significa que são idênticos em sentido e semanticamente.\n",
    "\n",
    "        Tente entender o sentido completo da resposta, não apenas palavras-chave.\n",
    "        Mas também se atente a detalhes como palavras-chave e seus significados.\n",
    "        Por exemplo, orações com palavras diferentes mas com o mesmo significado devem ser consideradas semelhantes.\n",
    "        Em caso de datas, números ou informações específicas, considere a semelhança do contexto em que estão inseridos.\n",
    "\n",
    "        Importante: Uma resposta adquirida pode ser mais longa e detalhada que a resposta esperada. Isso não necessariamente a torna diferente, desde que ela ainda aborde o mesmo tópico e não contradiga a resposta esperada. Portanto, mesmo que a resposta adquirida seja mais longa, se ela ainda estiver alinhada com a resposta esperada, considere-a semelhante.\n",
    "\n",
    "        Responda apenas o número equivalente à semelhança dos textos.\n",
    "\n",
    "        # exemplo de saida\n",
    "        10\n",
    "    '''\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    response = cliente.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        store=True,\n",
    "        messages=messages,\n",
    "        temperature=0\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def remove_specific_characters(input_string):\n",
    "    input_string = re.sub(r'(?<=[a-zA-Z])\\.', '', input_string)\n",
    "    input_string = re.sub(r'[\\[\\]\\'\\\"]', '', input_string)\n",
    "    return input_string\n",
    "\n",
    "def is_similar_using_llm_openai(esperada: str = \"\",obtida: str = \"\"):\n",
    "    if isinstance(esperada, list):\n",
    "        esperada = esperada[0]\n",
    "    if isinstance(obtida, list):\n",
    "        obtida = obtida[0]\n",
    "\n",
    "    esperada = remove_specific_characters(esperada)\n",
    "    obtida = remove_specific_characters(obtida)\n",
    "    response = use_llm_openai(esperada, obtida)\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teste = is_similar_using_llm_openai(\"archdioceses or departments of the Roman Curia\", \"The Roman Curia.\")\n",
    "teste2 = is_similar_using_llm_openai('Central Standard Time',\"['The contexts provided do not mention Saskatoon or its time observance specific to time zones, Daylight Saving Time, or any other relevant information. Therefore, based on the information given, I cannot determine what time Saskatoon observes all year long.']\")\n",
    "teste3 = is_similar_using_llm_openai('VHF omnidirectional range',['VOR stands for VHF Omnidirectional Range.'])\n",
    "teste4 = is_similar_using_llm_openai('32nd',\"['In 2009, Tucson ranked as the 32nd largest city in the United States.']\")\n",
    "teste5 = is_similar_using_llm_openai('1996',\"['Labour published a new draft manifesto in 1996, called \"\"New Labour, New Life For Britain.\"\"']\")\n",
    "print(f\"Teste 1: {teste}, Teste 2: {teste2}, Teste 3: {teste3}, Teste 4 que devia dar 10: {teste4}, Teste 5 que devia dar 10: {teste5}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frase1=\"email, web-hosting, or online storage services\"\n",
    "frase2=\"['Internet hosting services provide the infrastructure and technology needed to make websites accessible on the Internet. This includes server space for storing website files, bandwidth for transmitting data to users, domain name registration, email accounts, and often additional services such as database management, security features, and technical support. Hosting services enable individuals and organizations to have an online presence by hosting their web content and applications.']\"\n",
    "print(is_similar_using_llm_openai(frase1, frase2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_respostas_usando_llm_openai(arquivo_csv: str, qa='qa'):\n",
    "    df = pd.read_csv(f'''data/{qa}/{arquivo_csv}.csv''')\n",
    "    df['is_similar'] = df.apply(lambda row: is_similar_using_llm_openai(row['expected_answer'], row['generated_answer']), axis=1)\n",
    "    df.to_csv(f'''data/{qa}/{arquivo_csv}.csv''', index=False)\n",
    "\n",
    "def compare_respostas_usando_llm_contagem_openai(arquivo_csv: str, qa='qa'):\n",
    "    df = pd.read_csv(f'''data/{qa}/{arquivo_csv}.csv''')\n",
    "\n",
    "    count = 0\n",
    "    total = len(df)\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        df.loc[index, 'is_similar'] = is_similar_using_llm_openai(row['expected_answer'], row['generated_answer'])\n",
    "        \n",
    "        count += 1\n",
    "        \n",
    "        print(f'*********** Progresso: {count}/{total}')\n",
    "\n",
    "    df.to_csv(f'''data/{qa}/{arquivo_csv}.csv''', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_respostas_usando_llm_contagem_openai(Contexto.QA_PAIRS_GERAIS_15.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_respostas_usando_llm_contagem_openai(Contexto.QA_PAIRS_GERAIS_25.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_respostas_usando_llm_contagem_openai(Contexto.QA_PAIRS_GERAIS_50.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_respostas_usando_llm_contagem_openai(Contexto.QA_PAIRS_GERAIS_100.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_respostas_usando_llm_contagem_openai(Contexto.QA_PAIRS_PORTITULO_15.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_respostas_usando_llm_contagem_openai(Contexto.QA_PAIRS_PORTITULO_25.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_respostas_usando_llm_contagem_openai(Contexto.QA_PAIRS_PORTITULO_50.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_respostas_usando_llm_contagem_openai(Contexto.QA_PAIRS_PORTITULO_100.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### usando deepseek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "import re\n",
    "\n",
    "def use_llm_deepseek(esperada, obtida):\n",
    "    prompt = f'''\n",
    "        Você é um assistente que compara respostas.\n",
    "        Seu objetivo é avaliar a similaridade entre uma resposta correta e uma resposta obtida.\n",
    "        \n",
    "        Resposta correta: '{esperada}'.\n",
    "        Resposta obtida: '{obtida}'.\n",
    "        \n",
    "        Analise se a resposta obtida responde corretamente ao que foi perguntado.\n",
    "        \n",
    "        Critérios de avaliação:\n",
    "        - Se a resposta obtida for idêntica ou contém à resposta correta, dê nota 10.\n",
    "        - Se a resposta obtida abordar o mesmo tópico e contexto da correta, mas com palavras diferentes, dê uma nota entre 7 e 9.\n",
    "        - Se a resposta obtida for parcialmente correta ou incompleta, dê uma nota entre 4 e 6.\n",
    "        - Se a resposta obtida não responder corretamente ou for irrelevante, dê uma nota entre 0 e 3.\n",
    "        \n",
    "        Considerações especiais:\n",
    "        - Para respostas que envolvem números, datas ou termos exatos, verifique se eles correspondem exatamente. Pequenas variações podem diminuir a pontuação.\n",
    "        - Se um número ou data essencial estiver incorreto, a nota deve ser baixa (0 a 3).\n",
    "        - Se a resposta obtida definir um conceito sem nomeá-lo corretamente, a nota também deve ser reduzida.\n",
    "        - Se na resposta testiver coisas como \"I don't know\", \"I'm not sure\", \"I cannot\", \"I can't\", \"The context does not\", a nota deve ser 0.\n",
    "        \n",
    "        Responda apenas com um número de 0 a 10.\n",
    "        # Exemplo de saída\n",
    "        10\n",
    "    '''\n",
    "    \n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = ollama.chat(model='deepseek-r1:8b', messages=messages)\n",
    "    response_text = response['message']['content'].strip()\n",
    "    response_text = re.sub(r\"<think>.*?</think>\\n?\", \"\", response_text, flags=re.DOTALL)\n",
    "    response_text = re.sub(r\"[^0-9]\", \"\", response_text)\n",
    "    return response_text\n",
    "\n",
    "def remove_specific_characters(input_string):\n",
    "    input_string = re.sub(r'(?<=[a-zA-Z])\\.', '', input_string)\n",
    "    input_string = re.sub(r'[\\[\\]\\'\\\"]', '', input_string)\n",
    "    return input_string\n",
    "\n",
    "def is_similar_using_llm_deepseek(esperada: str = \"\", obtida: str = \"\"):\n",
    "    if isinstance(esperada, list):\n",
    "        esperada = esperada[0]\n",
    "    if isinstance(obtida, list):\n",
    "        obtida = obtida[0]\n",
    "\n",
    "    esperada = remove_specific_characters(esperada)\n",
    "    obtida = remove_specific_characters(obtida)\n",
    "    response = use_llm_deepseek(esperada, obtida)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teste = is_similar_using_llm_deepseek(\"archdioceses or departments of the Roman Curia\", \"The Roman Curia.\")\n",
    "teste2 = is_similar_using_llm_deepseek('Central Standard Time',\"['The contexts provided do not mention Saskatoon or its time observance specific to time zones, Daylight Saving Time, or any other relevant information. Therefore, based on the information given, I cannot determine what time Saskatoon observes all year long.']\")\n",
    "teste3 = is_similar_using_llm_deepseek('VHF omnidirectional range',['VOR stands for VHF Omnidirectional Range.'])\n",
    "teste4 = is_similar_using_llm_deepseek('32nd',\"['In 2009, Tucson ranked as the 32nd largest city in the United States.']\")\n",
    "teste5 = is_similar_using_llm_deepseek('1996',\"['Labour published a new draft manifesto in 1996, called \"\"New Labour, New Life For Britain.\"\"']\")\n",
    "print(f\"Teste 1: {teste}, Teste 2: {teste2}, Teste 3: {teste3}, Teste 4 que devia dar 10: {teste4}, Teste 5 que devia dar 10: {teste5}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_respostas_usando_llm_deepseek(arquivo_csv: str, qa='qa'):\n",
    "    df = pd.read_csv(f'''data/{qa}/{arquivo_csv}.csv''')\n",
    "    df['is_similar'] = df.apply(lambda row: is_similar_using_llm_deepseek(row['expected_answer'], row['generated_answer']), axis=1)\n",
    "    df.to_csv(f'''data/{qa}/{arquivo_csv}.csv''', index=False)\n",
    "\n",
    "def compare_respostas_usando_llm_contagem_deepseek(arquivo_csv: str, qa='qa'):\n",
    "    df = pd.read_csv(f'''data/{qa}/{arquivo_csv}.csv''')\n",
    "\n",
    "    count = 0\n",
    "    total = len(df)\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        df.loc[index, 'is_similar'] = is_similar_using_llm_deepseek(row['expected_answer'], row['generated_answer'])\n",
    "        \n",
    "        count += 1\n",
    "        \n",
    "        print(f'*********** Progresso: {count}/{total}')\n",
    "\n",
    "    df.to_csv(f'''data/{qa}/{arquivo_csv}.csv''', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_respostas_usando_llm_contagem_deepseek(Contexto.QA_PAIRS_GERAIS_15.value, 'qa_langchain_deepseek')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_respostas_usando_llm_contagem_deepseek(Contexto.QA_PAIRS_GERAIS_25.value, 'qa_langchain_deepseek')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_respostas_usando_llm_contagem_deepseek(Contexto.QA_PAIRS_GERAIS_50.value, 'qa_langchain_deepseek')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_respostas_usando_llm_contagem_deepseek(Contexto.QA_PAIRS_GERAIS_100.value, 'qa_langchain_deepseek')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_respostas_usando_llm_contagem_deepseek(Contexto.QA_PAIRS_PORTITULO_15.value, 'qa_langchain_deepseek')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_respostas_usando_llm_contagem_deepseek(Contexto.QA_PAIRS_PORTITULO_25.value, 'qa_langchain_deepseek')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_respostas_usando_llm_contagem_deepseek(Contexto.QA_PAIRS_PORTITULO_50.value, 'qa_langchain_deepseek')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_respostas_usando_llm_contagem_deepseek(Contexto.QA_PAIRS_PORTITULO_100.value, 'qa_langchain_deepseek')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## adicionando testes a parte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contextos_df_title.sample(frac=0.05, random_state=42).to_csv(f'''data/{Contexto.TESTE_PORTITULO.value}.csv''', index=False)\n",
    "contextos_df_unindo.sample(frac=0.05, random_state=42).to_csv(f'''data/{Contexto.TESTE_GERAIS.value}.csv''', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_df_novo = qa_df.sample(frac=0.01, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline(Contexto.TESTE_PORTITULO.value, qa_df_novo, Contexto.QA_TESTE_PORTITULO.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline(Contexto.TESTE_GERAIS.value, qa_df_novo, Contexto.QA_TESTE_PORTITULO.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_respostas_usando_llm_contagem(Contexto.QA_TESTE_PORTITULO.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_respostas_usando_llm_contagem(Contexto.QA_TESTE_GERAIS.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## avaliando resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base que quero avaliar\n",
    "QA_avaliado = Contexto.QA_PAIRS_GERAIS_100.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obtendo_respostas = pd.read_csv(f'''data/qa/{QA_avaliado}.csv''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar um dicionário para armazenar a contagem por faixa de 0 a 10\n",
    "bins = list(range(11))  # Criando bins de 0 a 10\n",
    "\n",
    "# Contar as ocorrências em cada faixa\n",
    "contagem = obtendo_respostas[\"is_similar\"].value_counts().sort_index()\n",
    "contagem = contagem.reindex(bins, fill_value=0)  # Garantir que todas as faixas apareçam\n",
    "\n",
    "# Calcular porcentagem\n",
    "total = contagem.sum()\n",
    "porcentagem = (contagem / total) * 100\n",
    "\n",
    "\n",
    "# Exibir a tabela de porcentagens\n",
    "print(f'Distribuição de similaridade no dataset {QA_avaliado} (%):')\n",
    "print(porcentagem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cores = [\"blue\", \"red\", \"green\", \"purple\", \"orange\", \"brown\", \"pink\", \"gray\", \"cyan\", \"magenta\"]\n",
    "\n",
    "# Criando um gráfico de barras\n",
    "plt.figure(figsize=(6, 4))\n",
    "obtendo_respostas[\"is_similar\"].value_counts().plot(kind=\"bar\", color=cores)\n",
    "plt.title(f'''Distribuição de similaridade {QA_avaliado}''')\n",
    "plt.xlabel(\"o quão similar é\")\n",
    "plt.ylabel(\"Frequência\")\n",
    "plt.xticks(rotation=0)\n",
    "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "plt.show()\n",
    "\n",
    "# Criando um gráfico de pizza\n",
    "plt.figure(figsize=(6, 6))\n",
    "obtendo_respostas[\"is_similar\"].value_counts().plot(kind=\"pie\", autopct=\"%1.1f%%\", colors=cores, startangle=90, wedgeprops={\"edgecolor\": \"black\"})\n",
    "plt.title(f'''Proporção de similaridade {QA_avaliado}''')\n",
    "plt.ylabel(\"\")  # Removendo label desnecessário\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Dados das distribuições de similaridade\n",
    "datasets = {\n",
    "    \"qa_pairs_portitulo_15\": [47.18, 0.06, 6.73, 15.52, 0.88, 2.74, 0.90, 11.26, 11.54, 2.44, 0.76],\n",
    "    \"qa_pairs_portitulo_25\": [42.39, 0.00, 6.93, 15.67, 0.80, 3.04, 0.96, 12.90, 13.68, 2.84, 0.80],\n",
    "    \"qa_pairs_portitulo_50\": [34.17, 0.04, 6.43, 16.33, 1.04, 4.01, 0.84, 15.73, 16.37, 3.57, 1.46],\n",
    "    \"qa_pairs_portitulo_100\": [16.95, 0.00, 5.55, 18.89, 1.24, 5.89, 0.78, 20.83, 22.38, 5.25, 2.24],\n",
    "    \"qa_pairs_gerais_15\": [29.47, 0.06, 6.81, 18.13, 0.76, 4.35, 0.76, 16.43, 17.47, 3.99, 1.76],\n",
    "    \"qa_pairs_gerais_25\": [25.32, 0.02, 6.73, 17.97, 1.06, 5.09, 0.72, 17.33, 18.81, 5.03, 1.92],\n",
    "}\n",
    "\n",
    "# Configuração dos gráficos\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "x = np.arange(len(datasets[\"qa_pairs_portitulo_15\"]))  # Posições no eixo X\n",
    "width = 0.15  # Largura das barras\n",
    "\n",
    "# Cores para os datasets\n",
    "colors = ['b', 'g', 'r', 'c', 'm', 'y']\n",
    "\n",
    "# Criando barras para cada dataset\n",
    "for i, (label, values) in enumerate(datasets.items()):\n",
    "    ax.bar(x + i * width, values, width, label=label, color=colors[i])\n",
    "\n",
    "# Ajustes no gráfico\n",
    "ax.set_xlabel(\"Níveis de Similaridade\")\n",
    "ax.set_ylabel(\"Percentual (%)\")\n",
    "ax.set_title(\"Comparação das Distribuições de Similaridade\")\n",
    "ax.set_xticks(x + width)\n",
    "ax.set_xticklabels(range(11))\n",
    "ax.legend()\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Exibir gráfico\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.document_loaders import DataFrameLoader\n",
    "from langchain_community.vectorstores import Qdrant\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.schema import (\n",
    "    SystemMessage,\n",
    "    HumanMessage\n",
    ")\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_langchain_openai(base: str, qa_passado_df, output_filename: str):\n",
    "\n",
    "    data_df = pd.read_csv(f'''data/{base}.csv''')\n",
    "\n",
    "    chat = ChatOpenAI(\n",
    "        model=\"gpt-4o\",\n",
    "    )\n",
    "    loader = DataFrameLoader(data_df, page_content_column=\"context\")\n",
    "    embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "    documents = loader.load()\n",
    "\n",
    "    print('------------------- obtendo documentos para pipeline')\n",
    "    qdrant = Qdrant.from_documents(\n",
    "        documents=documents,\n",
    "        embedding=embeddings,\n",
    "        location=\":memory:\",\n",
    "        collection_name=\"rag\"\n",
    "    )\n",
    "\n",
    "    print('------------------- finalizando documentos para pipeline')\n",
    "\n",
    "    print('------------------- iniciando testes de perguntas e respostas')\n",
    "\n",
    "    results = []\n",
    "    i = 0\n",
    "    total = len(qa_passado_df)\n",
    "    \n",
    "    for index, row in qa_passado_df.iterrows():\n",
    "        query = row[\"question\"]\n",
    "        expected_answer = row[\"answers\"]\n",
    "        \n",
    "        results_retriver = qdrant.similarity_search(query, k=3)\n",
    "        source_knowledge = \"\\n\".join([x.page_content for x in results_retriver])\n",
    "        augment_prompt = f\"\"\"Use the context below to answer the question.\n",
    "\n",
    "        Context:\n",
    "        {source_knowledge}\n",
    "        -------------------------\n",
    "        Succinct answers are necessary, always focusing on answering the question. \n",
    "        To get straight to the point, avoid unnecessary information.\n",
    "        Avoid repeat the question in the answer and focus in the objective answer.\n",
    "        -------------------------\n",
    "        Question: {query}\"\"\"\n",
    "\n",
    "\n",
    "        messages = [\n",
    "            SystemMessage(content=\"You are a RAG that uses contexts to answer questions.\"),\n",
    "            HumanMessage(content=augment_prompt)\n",
    "        ]\n",
    "\n",
    "        res = chat.invoke(messages)\n",
    "\n",
    "        results.append({\n",
    "            \"title\": row[\"title\"],\n",
    "            \"question\": query,\n",
    "            \"expected_answer\": expected_answer,\n",
    "            \"generated_answer\": res.content,\n",
    "        })\n",
    "        i += 1\n",
    "\n",
    "        print(f\"################### Processando ({i}/{total})...\")\n",
    "        time.sleep(0.1)\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv(f'''data/qa_langchain_openai/{output_filename}.csv''', index=False)\n",
    "\n",
    "    print(f\"\\n-------------------Finalizado! Resultados salvos em {output_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- dataset utilizado na outra "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_df_usado = pd.read_csv(f'''data/qa/{Contexto.QA_PAIRS.value}_usado.csv''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "aplicando em contextos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_langchain_openai(Contexto.GERAIS_100.value, qa_df_usado, Contexto.QA_PAIRS_GERAIS_100.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_langchain_openai(Contexto.GERAIS_50.value, qa_df_usado, Contexto.QA_PAIRS_GERAIS_50.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_langchain_openai(Contexto.GERAIS_25.value, qa_df_usado, Contexto.QA_PAIRS_GERAIS_25.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_langchain_openai(Contexto.GERAIS_15.value, qa_df_usado, Contexto.QA_PAIRS_GERAIS_15.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_langchain_openai(Contexto.PORTITULO_15.value, qa_df_usado, Contexto.QA_PAIRS_PORTITULO_15.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_langchain_openai(Contexto.PORTITULO_25.value, qa_df_usado, Contexto.QA_PAIRS_PORTITULO_25.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_langchain_openai(Contexto.PORTITULO_50.value, qa_df_usado, Contexto.QA_PAIRS_PORTITULO_50.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_langchain_openai(Contexto.PORTITULO_100.value, qa_df_usado, Contexto.QA_PAIRS_PORTITULO_100.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- avaliacao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_respostas_usando_llm_contagem_openai(Contexto.QA_PAIRS_GERAIS_100.value, 'qa_langchain_openai')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_respostas_usando_llm_contagem_openai(Contexto.QA_PAIRS_GERAIS_50.value, 'qa_langchain_openai')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_respostas_usando_llm_contagem_openai(Contexto.QA_PAIRS_GERAIS_25.value, 'qa_langchain_openai')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_respostas_usando_llm_contagem_openai(Contexto.QA_PAIRS_GERAIS_15.value, 'qa_langchain_openai')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_respostas_usando_llm_contagem_openai(Contexto.QA_PAIRS_PORTITULO_100.value, 'qa_langchain_openai')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_respostas_usando_llm_contagem_openai(Contexto.QA_PAIRS_PORTITULO_50.value, 'qa_langchain_openai')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_respostas_usando_llm_contagem_openai(Contexto.QA_PAIRS_PORTITULO_25.value, 'qa_langchain_openai')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_respostas_usando_llm_contagem_openai(Contexto.QA_PAIRS_PORTITULO_15.value, 'qa_langchain_openai')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## rag deepseek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import DataFrameLoader \n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Qdrant\n",
    "from langchain.schema import (\n",
    "    SystemMessage,\n",
    "    HumanMessage\n",
    ")\n",
    "from langchain_community.llms import Ollama \n",
    "import pandas as pd\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## rode no seu terminal\n",
    "## ollama run deepseek-r1:8b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# qa_df_usado = pd.read_csv(f'''data/qa/{Contexto.QA_PAIRS.value}_usado.csv''')\n",
    "qa_df = pd.read_csv(f'''data/qa/{Contexto.QA_PAIRS.value}_usado.csv''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_df_usado = qa_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_langchain_deepseek(base: str, qa_passado_df, output_filename: str):\n",
    "\n",
    "    data_df = pd.read_csv(f'''data/{base}.csv''')\n",
    "\n",
    "    llm = Ollama(model=\"deepseek-r1:8b\") \n",
    "\n",
    "    loader = DataFrameLoader(data_df, page_content_column=\"context\")\n",
    "    documents = loader.load()\n",
    "    embeddings_model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=embeddings_model_name) \n",
    "\n",
    "    print('------------------- obtendo documentos para pipeline')\n",
    "    qdrant = Qdrant.from_documents(\n",
    "        documents=documents,\n",
    "        embedding=embeddings,\n",
    "        location=\":memory:\",\n",
    "        collection_name=\"rag\"\n",
    "    )\n",
    "\n",
    "    print('------------------- finalizando documentos para pipeline')\n",
    "\n",
    "    print('------------------- iniciando testes de perguntas e respostas')\n",
    "\n",
    "    results = []\n",
    "    i = 0\n",
    "    total = len(qa_passado_df)\n",
    "    \n",
    "    for index, row in qa_passado_df.iterrows():\n",
    "        query = row[\"question\"]\n",
    "        expected_answer = row[\"answers\"]\n",
    "        \n",
    "        results_retriver = qdrant.similarity_search(query, k=3)\n",
    "        source_knowledge = \"\\n\".join([x.page_content for x in results_retriver])\n",
    "        augment_prompt = f\"\"\"Use the context below to answer the question.\n",
    "\n",
    "        Context:\n",
    "        {source_knowledge}\n",
    "        -------------------------\n",
    "        1. Use ONLY the context below.  \n",
    "        2. If in doubt, say \"I don't know\".\n",
    "        -------------------------\n",
    "        Succinct answers are necessary, always focusing on answering the question. \n",
    "        To get straight to the point, avoid unnecessary information.\n",
    "        Avoid repeat the question in the answer and focus in the objective answer.\n",
    "        -------------------------\n",
    "        Question: {query}\"\"\"\n",
    "\n",
    "\n",
    "        messages = [\n",
    "            SystemMessage(content=\"You are a RAG that uses contexts to answer questions.\"),\n",
    "            HumanMessage(content=augment_prompt)\n",
    "        ]\n",
    "\n",
    "        resp = llm.invoke(messages)\n",
    "        response = re.sub(r\"<think>.*?</think>\\n?\", \"\",resp, flags=re.DOTALL)\n",
    "        texto_sem_quebras_de_linha = response.replace(\"\\n\", \" \")\n",
    "\n",
    "        results.append({\n",
    "            \"title\": row[\"title\"],\n",
    "            \"question\": query,\n",
    "            \"expected_answer\": expected_answer,\n",
    "            \"generated_answer\": texto_sem_quebras_de_linha,\n",
    "        })\n",
    "        i += 1\n",
    "\n",
    "        print(f\"################### Processando ({i}/{total})...\")\n",
    "        time.sleep(0.1)\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv(f'''data/qa_langchain_deepseek/{output_filename}.csv''', index=False)\n",
    "\n",
    "    print(f\"\\n-------------------Finalizado! Resultados salvos em {output_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------- obtendo documentos para pipeline\n",
      "------------------- finalizando documentos para pipeline\n",
      "------------------- iniciando testes de perguntas e respostas\n",
      "################### Processando (1/5008)...\n"
     ]
    }
   ],
   "source": [
    "pipeline_langchain_deepseek(Contexto.GERAIS_100.value, qa_df_usado, Contexto.QA_PAIRS_GERAIS_100.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_langchain_deepseek(Contexto.GERAIS_50.value, qa_df_usado, Contexto.QA_PAIRS_GERAIS_50.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_langchain_deepseek(Contexto.GERAIS_25.value, qa_df_usado, Contexto.QA_PAIRS_GERAIS_25.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_langchain_deepseek(Contexto.GERAIS_15.value, qa_df_usado, Contexto.QA_PAIRS_GERAIS_15.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_langchain_deepseek(Contexto.PORTITULO_15.value, qa_df_usado, Contexto.QA_PAIRS_PORTITULO_15.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_langchain_deepseek(Contexto.PORTITULO_25.value, qa_df_usado, Contexto.QA_PAIRS_PORTITULO_25.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_langchain_deepseek(Contexto.PORTITULO_50.value, qa_df_usado, Contexto.QA_PAIRS_PORTITULO_50.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_langchain_deepseek(Contexto.PORTITULO_100.value, qa_df_usado, Contexto.QA_PAIRS_PORTITULO_100.value)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
