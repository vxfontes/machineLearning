{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting haystack-ai\n",
      "  Downloading haystack_ai-2.10.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting pandas\n",
      "  Using cached pandas-2.2.3-cp312-cp312-win_amd64.whl.metadata (19 kB)\n",
      "Collecting datasets\n",
      "  Using cached datasets-3.2.0-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.10.0-cp312-cp312-win_amd64.whl.metadata (11 kB)\n",
      "Collecting python-dotenv\n",
      "  Using cached python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting qdrant-client\n",
      "  Using cached qdrant_client-1.13.2-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting langchain-openai\n",
      "  Downloading langchain_openai-0.3.5-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting langchain_community\n",
      "  Downloading langchain_community-0.3.17-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting streamlit\n",
      "  Downloading streamlit-1.42.0-py2.py3-none-any.whl.metadata (8.9 kB)\n",
      "Collecting sentence-transformers\n",
      "  Downloading sentence_transformers-3.4.1-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting langchain_experimental\n",
      "  Downloading langchain_experimental-0.3.4-py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting haystack-experimental (from haystack-ai)\n",
      "  Using cached haystack_experimental-0.6.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting jinja2 (from haystack-ai)\n",
      "  Downloading jinja2-3.1.5-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting jsonschema (from haystack-ai)\n",
      "  Downloading jsonschema-4.23.0-py3-none-any.whl.metadata (7.9 kB)\n",
      "Collecting lazy-imports (from haystack-ai)\n",
      "  Using cached lazy_imports-0.4.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting more-itertools (from haystack-ai)\n",
      "  Using cached more_itertools-10.6.0-py3-none-any.whl.metadata (37 kB)\n",
      "Collecting networkx (from haystack-ai)\n",
      "  Using cached networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting numpy (from haystack-ai)\n",
      "  Downloading numpy-2.2.2-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
      "     ---------------------------------------- 0.0/60.8 kB ? eta -:--:--\n",
      "     ---------------------------------------- 60.8/60.8 kB 1.6 MB/s eta 0:00:00\n",
      "Collecting openai>=1.56.1 (from haystack-ai)\n",
      "  Downloading openai-1.62.0-py3-none-any.whl.metadata (27 kB)\n",
      "Collecting openapi-llm>=0.4.1 (from haystack-ai)\n",
      "  Downloading openapi_llm-0.4.1-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting posthog<3.12.0 (from haystack-ai)\n",
      "  Downloading posthog-3.11.0-py2.py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting pydantic (from haystack-ai)\n",
      "  Using cached pydantic-2.10.6-py3-none-any.whl.metadata (30 kB)\n",
      "Requirement already satisfied: python-dateutil in c:\\users\\nessa\\onedrive\\desktop\\coding\\machinelearning\\.venv\\lib\\site-packages (from haystack-ai) (2.9.0.post0)\n",
      "Collecting pyyaml (from haystack-ai)\n",
      "  Using cached PyYAML-6.0.2-cp312-cp312-win_amd64.whl.metadata (2.1 kB)\n",
      "Collecting requests (from haystack-ai)\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting tenacity!=8.4.0 (from haystack-ai)\n",
      "  Downloading tenacity-9.0.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting tqdm (from haystack-ai)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "     ---------------------------------------- 0.0/57.7 kB ? eta -:--:--\n",
      "     ---------------------------------------- 57.7/57.7 kB 3.0 MB/s eta 0:00:00\n",
      "Collecting typing-extensions>=4.7 (from haystack-ai)\n",
      "  Using cached typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Downloading pytz-2025.1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Downloading tzdata-2025.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting filelock (from datasets)\n",
      "  Using cached filelock-3.17.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting pyarrow>=15.0.0 (from datasets)\n",
      "  Using cached pyarrow-19.0.0-cp312-cp312-win_amd64.whl.metadata (3.4 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting xxhash (from datasets)\n",
      "  Using cached xxhash-3.5.0-cp312-cp312-win_amd64.whl.metadata (13 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Using cached multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets)\n",
      "  Using cached fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting aiohttp (from datasets)\n",
      "  Downloading aiohttp-3.11.12-cp312-cp312-win_amd64.whl.metadata (8.0 kB)\n",
      "Collecting huggingface-hub>=0.23.0 (from datasets)\n",
      "  Downloading huggingface_hub-0.28.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\nessa\\onedrive\\desktop\\coding\\machinelearning\\.venv\\lib\\site-packages (from datasets) (24.2)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.3.1-cp312-cp312-win_amd64.whl.metadata (5.4 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.56.0-cp312-cp312-win_amd64.whl.metadata (103 kB)\n",
      "     ---------------------------------------- 0.0/104.0 kB ? eta -:--:--\n",
      "     -------------------------------------- 104.0/104.0 kB 6.2 MB/s eta 0:00:00\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.8-cp312-cp312-win_amd64.whl.metadata (6.3 kB)\n",
      "Collecting pillow>=8 (from matplotlib)\n",
      "  Downloading pillow-11.1.0-cp312-cp312-win_amd64.whl.metadata (9.3 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
      "  Using cached pyparsing-3.2.1-py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting grpcio>=1.41.0 (from qdrant-client)\n",
      "  Using cached grpcio-1.70.0-cp312-cp312-win_amd64.whl.metadata (4.0 kB)\n",
      "Collecting grpcio-tools>=1.41.0 (from qdrant-client)\n",
      "  Using cached grpcio_tools-1.70.0-cp312-cp312-win_amd64.whl.metadata (5.5 kB)\n",
      "Collecting httpx>=0.20.0 (from httpx[http2]>=0.20.0->qdrant-client)\n",
      "  Downloading httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting portalocker<3.0.0,>=2.7.0 (from qdrant-client)\n",
      "  Using cached portalocker-2.10.1-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting urllib3<3,>=1.26.14 (from qdrant-client)\n",
      "  Downloading urllib3-2.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting langchain-core<1.0.0,>=0.3.34 (from langchain-openai)\n",
      "  Downloading langchain_core-0.3.35-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting tiktoken<1,>=0.7 (from langchain-openai)\n",
      "  Using cached tiktoken-0.8.0-cp312-cp312-win_amd64.whl.metadata (6.8 kB)\n",
      "Collecting langchain<1.0.0,>=0.3.18 (from langchain_community)\n",
      "  Downloading langchain-0.3.18-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting SQLAlchemy<3,>=1.4 (from langchain_community)\n",
      "  Downloading SQLAlchemy-2.0.38-cp312-cp312-win_amd64.whl.metadata (9.9 kB)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n",
      "  Using cached dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain_community)\n",
      "  Downloading pydantic_settings-2.7.1-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting langsmith<0.4,>=0.1.125 (from langchain_community)\n",
      "  Downloading langsmith-0.3.8-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain_community)\n",
      "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
      "Collecting altair<6,>=4.0 (from streamlit)\n",
      "  Downloading altair-5.5.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting blinker<2,>=1.0.0 (from streamlit)\n",
      "  Downloading blinker-1.9.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting cachetools<6,>=4.0 (from streamlit)\n",
      "  Downloading cachetools-5.5.1-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting click<9,>=7.0 (from streamlit)\n",
      "  Downloading click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting protobuf<6,>=3.20 (from streamlit)\n",
      "  Using cached protobuf-5.29.3-cp310-abi3-win_amd64.whl.metadata (592 bytes)\n",
      "Collecting rich<14,>=10.14.0 (from streamlit)\n",
      "  Downloading rich-13.9.4-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting toml<2,>=0.10.1 (from streamlit)\n",
      "  Downloading toml-0.10.2-py2.py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting watchdog<7,>=2.1.5 (from streamlit)\n",
      "  Downloading watchdog-6.0.0-py3-none-win_amd64.whl.metadata (44 kB)\n",
      "     ---------------------------------------- 0.0/44.3 kB ? eta -:--:--\n",
      "     ---------------------------------------- 44.3/44.3 kB ? eta 0:00:00\n",
      "Collecting gitpython!=3.1.19,<4,>=3.0.7 (from streamlit)\n",
      "  Downloading GitPython-3.1.44-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
      "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: tornado<7,>=6.0.3 in c:\\users\\nessa\\onedrive\\desktop\\coding\\machinelearning\\.venv\\lib\\site-packages (from streamlit) (6.4.2)\n",
      "Collecting transformers<5.0.0,>=4.41.0 (from sentence-transformers)\n",
      "  Downloading transformers-4.48.3-py3-none-any.whl.metadata (44 kB)\n",
      "     ---------------------------------------- 0.0/44.4 kB ? eta -:--:--\n",
      "     ---------------------------------------- 44.4/44.4 kB ? eta 0:00:00\n",
      "Collecting torch>=1.11.0 (from sentence-transformers)\n",
      "  Downloading torch-2.6.0-cp312-cp312-win_amd64.whl.metadata (28 kB)\n",
      "Collecting scikit-learn (from sentence-transformers)\n",
      "  Downloading scikit_learn-1.6.1-cp312-cp312-win_amd64.whl.metadata (15 kB)\n",
      "Collecting scipy (from sentence-transformers)\n",
      "  Downloading scipy-1.15.1-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
      "     ---------------------------------------- 0.0/60.8 kB ? eta -:--:--\n",
      "     ---------------------------------------- 60.8/60.8 kB 1.6 MB/s eta 0:00:00\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets)\n",
      "  Downloading aiohappyeyeballs-2.4.6-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
      "  Downloading aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp->datasets)\n",
      "  Downloading attrs-25.1.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
      "  Using cached frozenlist-1.5.0-cp312-cp312-win_amd64.whl.metadata (14 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
      "  Using cached multidict-6.1.0-cp312-cp312-win_amd64.whl.metadata (5.1 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp->datasets)\n",
      "  Downloading propcache-0.2.1-cp312-cp312-win_amd64.whl.metadata (9.5 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp->datasets)\n",
      "  Downloading yarl-1.18.3-cp312-cp312-win_amd64.whl.metadata (71 kB)\n",
      "     ---------------------------------------- 0.0/71.4 kB ? eta -:--:--\n",
      "     ---------------------------------------- 71.4/71.4 kB 4.1 MB/s eta 0:00:00\n",
      "Collecting narwhals>=1.14.2 (from altair<6,>=4.0->streamlit)\n",
      "  Using cached narwhals-1.26.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\nessa\\onedrive\\desktop\\coding\\machinelearning\\.venv\\lib\\site-packages (from click<9,>=7.0->streamlit) (0.4.6)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
      "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
      "  Using cached typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
      "  Using cached gitdb-4.0.12-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting setuptools (from grpcio-tools>=1.41.0->qdrant-client)\n",
      "  Downloading setuptools-75.8.0-py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting anyio (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client)\n",
      "  Downloading anyio-4.8.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting certifi (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client)\n",
      "  Downloading certifi-2025.1.31-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting httpcore==1.* (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client)\n",
      "  Downloading httpcore-1.0.7-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting idna (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client)\n",
      "  Using cached h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Collecting h2<5,>=3 (from httpx[http2]>=0.20.0->qdrant-client)\n",
      "  Using cached h2-4.2.0-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting jsonschema-specifications>=2023.03.6 (from jsonschema->haystack-ai)\n",
      "  Downloading jsonschema_specifications-2024.10.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting referencing>=0.28.4 (from jsonschema->haystack-ai)\n",
      "  Downloading referencing-0.36.2-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting rpds-py>=0.7.1 (from jsonschema->haystack-ai)\n",
      "  Downloading rpds_py-0.22.3-cp312-cp312-win_amd64.whl.metadata (4.2 kB)\n",
      "Collecting langchain-text-splitters<1.0.0,>=0.3.6 (from langchain<1.0.0,>=0.3.18->langchain_community)\n",
      "  Using cached langchain_text_splitters-0.3.6-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<1.0.0,>=0.3.34->langchain-openai)\n",
      "  Using cached jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.4,>=0.1.125->langchain_community)\n",
      "  Downloading orjson-3.10.15-cp312-cp312-win_amd64.whl.metadata (42 kB)\n",
      "     ---------------------------------------- 0.0/42.9 kB ? eta -:--:--\n",
      "     ---------------------------------------- 42.9/42.9 kB ? eta 0:00:00\n",
      "Collecting requests-toolbelt<2.0.0,>=1.0.0 (from langsmith<0.4,>=0.1.125->langchain_community)\n",
      "  Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Collecting zstandard<0.24.0,>=0.23.0 (from langsmith<0.4,>=0.1.125->langchain_community)\n",
      "  Downloading zstandard-0.23.0-cp312-cp312-win_amd64.whl.metadata (3.0 kB)\n",
      "Collecting distro<2,>=1.7.0 (from openai>=1.56.1->haystack-ai)\n",
      "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting jiter<1,>=0.4.0 (from openai>=1.56.1->haystack-ai)\n",
      "  Using cached jiter-0.8.2-cp312-cp312-win_amd64.whl.metadata (5.3 kB)\n",
      "Collecting sniffio (from openai>=1.56.1->haystack-ai)\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting jsonref (from openapi-llm>=0.4.1->haystack-ai)\n",
      "  Downloading jsonref-1.1.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: pywin32>=226 in c:\\users\\nessa\\onedrive\\desktop\\coding\\machinelearning\\.venv\\lib\\site-packages (from portalocker<3.0.0,>=2.7.0->qdrant-client) (308)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\nessa\\onedrive\\desktop\\coding\\machinelearning\\.venv\\lib\\site-packages (from posthog<3.12.0->haystack-ai) (1.17.0)\n",
      "Collecting monotonic>=1.5 (from posthog<3.12.0->haystack-ai)\n",
      "  Using cached monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting backoff>=1.10.0 (from posthog<3.12.0->haystack-ai)\n",
      "  Using cached backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic->haystack-ai)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.27.2 (from pydantic->haystack-ai)\n",
      "  Using cached pydantic_core-2.27.2-cp312-cp312-win_amd64.whl.metadata (6.7 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->haystack-ai)\n",
      "  Using cached MarkupSafe-3.0.2-cp312-cp312-win_amd64.whl.metadata (4.1 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests->haystack-ai)\n",
      "  Downloading charset_normalizer-3.4.1-cp312-cp312-win_amd64.whl.metadata (36 kB)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich<14,>=10.14.0->streamlit)\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\nessa\\onedrive\\desktop\\coding\\machinelearning\\.venv\\lib\\site-packages (from rich<14,>=10.14.0->streamlit) (2.19.1)\n",
      "Collecting greenlet!=0.4.17 (from SQLAlchemy<3,>=1.4->langchain_community)\n",
      "  Using cached greenlet-3.1.1-cp312-cp312-win_amd64.whl.metadata (3.9 kB)\n",
      "Collecting regex>=2022.1.18 (from tiktoken<1,>=0.7->langchain-openai)\n",
      "  Downloading regex-2024.11.6-cp312-cp312-win_amd64.whl.metadata (41 kB)\n",
      "     ---------------------------------------- 0.0/41.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 41.5/41.5 kB ? eta 0:00:00\n",
      "Collecting sympy==1.13.1 (from torch>=1.11.0->sentence-transformers)\n",
      "  Using cached sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy==1.13.1->torch>=1.11.0->sentence-transformers)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Downloading tokenizers-0.21.0-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Downloading safetensors-0.5.2-cp38-abi3-win_amd64.whl.metadata (3.9 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn->sentence-transformers)\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn->sentence-transformers)\n",
      "  Using cached threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
      "  Using cached smmap-5.0.2-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting hyperframe<7,>=6.1 (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client)\n",
      "  Using cached hyperframe-6.1.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting hpack<5,>=4.1 (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client)\n",
      "  Using cached hpack-4.1.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.34->langchain-openai)\n",
      "  Using cached jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit)\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
      "  Using cached mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Downloading haystack_ai-2.10.0-py3-none-any.whl (440 kB)\n",
      "   ---------------------------------------- 0.0/440.2 kB ? eta -:--:--\n",
      "   --------------------------------------  430.1/440.2 kB 13.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 440.2/440.2 kB 6.8 MB/s eta 0:00:00\n",
      "Using cached pandas-2.2.3-cp312-cp312-win_amd64.whl (11.5 MB)\n",
      "Using cached datasets-3.2.0-py3-none-any.whl (480 kB)\n",
      "Downloading matplotlib-3.10.0-cp312-cp312-win_amd64.whl (8.0 MB)\n",
      "   ---------------------------------------- 0.0/8.0 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 1.1/8.0 MB 23.7 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 3.6/8.0 MB 46.2 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 7.1/8.0 MB 56.5 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 7.2/8.0 MB 45.9 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 7.5/8.0 MB 34.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 7.7/8.0 MB 28.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  8.0/8.0 MB 24.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 8.0/8.0 MB 23.3 MB/s eta 0:00:00\n",
      "Using cached python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Using cached qdrant_client-1.13.2-py3-none-any.whl (306 kB)\n",
      "Downloading langchain_openai-0.3.5-py3-none-any.whl (54 kB)\n",
      "   ---------------------------------------- 0.0/55.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 55.0/55.0 kB ? eta 0:00:00\n",
      "Downloading langchain_community-0.3.17-py3-none-any.whl (2.5 MB)\n",
      "   ---------------------------------------- 0.0/2.5 MB ? eta -:--:--\n",
      "   -------------------------------------- - 2.4/2.5 MB 159.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.5/2.5 MB 32.1 MB/s eta 0:00:00\n",
      "Downloading streamlit-1.42.0-py2.py3-none-any.whl (9.6 MB)\n",
      "   ---------------------------------------- 0.0/9.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.1/9.6 MB 3.3 MB/s eta 0:00:03\n",
      "   - -------------------------------------- 0.3/9.6 MB 3.2 MB/s eta 0:00:03\n",
      "   - -------------------------------------- 0.5/9.6 MB 3.5 MB/s eta 0:00:03\n",
      "   -- ------------------------------------- 0.7/9.6 MB 4.0 MB/s eta 0:00:03\n",
      "   ---- ----------------------------------- 1.0/9.6 MB 4.2 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 1.3/9.6 MB 4.8 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 1.6/9.6 MB 5.0 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 2.1/9.6 MB 5.5 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 2.6/9.6 MB 6.1 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 3.1/9.6 MB 6.5 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 3.6/9.6 MB 6.9 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 4.1/9.6 MB 7.4 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 4.8/9.6 MB 7.9 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 5.5/9.6 MB 8.4 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 6.2/9.6 MB 8.9 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 7.0/9.6 MB 9.3 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 7.8/9.6 MB 9.8 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 8.7/9.6 MB 10.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.6/9.6 MB 10.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 9.6/9.6 MB 10.5 MB/s eta 0:00:00\n",
      "Downloading sentence_transformers-3.4.1-py3-none-any.whl (275 kB)\n",
      "   ---------------------------------------- 0.0/275.9 kB ? eta -:--:--\n",
      "   --------------------------------------- 275.9/275.9 kB 16.6 MB/s eta 0:00:00\n",
      "Downloading langchain_experimental-0.3.4-py3-none-any.whl (209 kB)\n",
      "   ---------------------------------------- 0.0/209.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 209.2/209.2 kB ? eta 0:00:00\n",
      "Downloading aiohttp-3.11.12-cp312-cp312-win_amd64.whl (437 kB)\n",
      "   ---------------------------------------- 0.0/437.9 kB ? eta -:--:--\n",
      "   --------------------------------------- 437.9/437.9 kB 28.5 MB/s eta 0:00:00\n",
      "Using cached altair-5.5.0-py3-none-any.whl (731 kB)\n",
      "Downloading blinker-1.9.0-py3-none-any.whl (8.5 kB)\n",
      "Downloading cachetools-5.5.1-py3-none-any.whl (9.5 kB)\n",
      "Downloading click-8.1.8-py3-none-any.whl (98 kB)\n",
      "   ---------------------------------------- 0.0/98.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 98.2/98.2 kB ? eta 0:00:00\n",
      "Downloading contourpy-1.3.1-cp312-cp312-win_amd64.whl (220 kB)\n",
      "   ---------------------------------------- 0.0/221.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 221.0/221.0 kB ? eta 0:00:00\n",
      "Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Using cached dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Using cached dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Downloading fonttools-4.56.0-cp312-cp312-win_amd64.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   ----------------- ---------------------- 1.0/2.2 MB 30.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 2.1/2.2 MB 22.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.2/2.2 MB 19.8 MB/s eta 0:00:00\n",
      "Using cached fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
      "Downloading GitPython-3.1.44-py3-none-any.whl (207 kB)\n",
      "   ---------------------------------------- 0.0/207.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 207.6/207.6 kB ? eta 0:00:00\n",
      "Using cached grpcio-1.70.0-cp312-cp312-win_amd64.whl (4.3 MB)\n",
      "Using cached grpcio_tools-1.70.0-cp312-cp312-win_amd64.whl (1.1 MB)\n",
      "Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Using cached httpcore-1.0.7-py3-none-any.whl (78 kB)\n",
      "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
      "Downloading huggingface_hub-0.28.1-py3-none-any.whl (464 kB)\n",
      "   ---------------------------------------- 0.0/464.1 kB ? eta -:--:--\n",
      "   --------------------------------------- 464.1/464.1 kB 28.4 MB/s eta 0:00:00\n",
      "Using cached jsonschema-4.23.0-py3-none-any.whl (88 kB)\n",
      "Downloading kiwisolver-1.4.8-cp312-cp312-win_amd64.whl (71 kB)\n",
      "   ---------------------------------------- 0.0/71.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 71.9/71.9 kB ? eta 0:00:00\n",
      "Downloading langchain-0.3.18-py3-none-any.whl (1.0 MB)\n",
      "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
      "   ---------------------------------------  1.0/1.0 MB 32.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.0/1.0 MB 21.3 MB/s eta 0:00:00\n",
      "Downloading langchain_core-0.3.35-py3-none-any.whl (413 kB)\n",
      "   ---------------------------------------- 0.0/413.2 kB ? eta -:--:--\n",
      "   --------------------------------------- 413.2/413.2 kB 26.9 MB/s eta 0:00:00\n",
      "Downloading langsmith-0.3.8-py3-none-any.whl (332 kB)\n",
      "   ---------------------------------------- 0.0/332.8 kB ? eta -:--:--\n",
      "   --------------------------------------- 332.8/332.8 kB 21.5 MB/s eta 0:00:00\n",
      "Using cached multiprocess-0.70.16-py312-none-any.whl (146 kB)\n",
      "Downloading numpy-2.2.2-cp312-cp312-win_amd64.whl (12.6 MB)\n",
      "   ---------------------------------------- 0.0/12.6 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.9/12.6 MB 19.8 MB/s eta 0:00:01\n",
      "   ------ --------------------------------- 2.2/12.6 MB 23.1 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 3.3/12.6 MB 26.5 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 4.6/12.6 MB 26.7 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 5.8/12.6 MB 26.7 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 7.2/12.6 MB 27.1 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 8.6/12.6 MB 27.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 10.0/12.6 MB 27.7 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 11.5/12.6 MB 28.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.6/12.6 MB 28.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.6/12.6 MB 27.3 MB/s eta 0:00:00\n",
      "Downloading openai-1.62.0-py3-none-any.whl (464 kB)\n",
      "   ---------------------------------------- 0.0/464.8 kB ? eta -:--:--\n",
      "   --------------------------------------- 464.8/464.8 kB 28.4 MB/s eta 0:00:00\n",
      "Downloading openapi_llm-0.4.1-py3-none-any.whl (24 kB)\n",
      "Downloading pillow-11.1.0-cp312-cp312-win_amd64.whl (2.6 MB)\n",
      "   ---------------------------------------- 0.0/2.6 MB ? eta -:--:--\n",
      "   ----------------------- ---------------- 1.5/2.6 MB 49.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.6/2.6 MB 33.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.6/2.6 MB 27.8 MB/s eta 0:00:00\n",
      "Downloading portalocker-2.10.1-py3-none-any.whl (18 kB)\n",
      "Downloading posthog-3.11.0-py2.py3-none-any.whl (72 kB)\n",
      "   ---------------------------------------- 0.0/72.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 72.0/72.0 kB ? eta 0:00:00\n",
      "Using cached protobuf-5.29.3-cp310-abi3-win_amd64.whl (434 kB)\n",
      "Using cached pyarrow-19.0.0-cp312-cp312-win_amd64.whl (25.2 MB)\n",
      "Using cached pydantic-2.10.6-py3-none-any.whl (431 kB)\n",
      "Using cached pydantic_core-2.27.2-cp312-cp312-win_amd64.whl (2.0 MB)\n",
      "Downloading pydantic_settings-2.7.1-py3-none-any.whl (29 kB)\n",
      "Downloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
      "   ---------------------------------------- 0.0/6.9 MB ? eta -:--:--\n",
      "   ---------- ----------------------------- 1.7/6.9 MB 53.8 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 3.3/6.9 MB 41.7 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 5.1/6.9 MB 35.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  6.8/6.9 MB 39.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.9/6.9 MB 33.9 MB/s eta 0:00:00\n",
      "Downloading jinja2-3.1.5-py3-none-any.whl (134 kB)\n",
      "   ---------------------------------------- 0.0/134.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 134.6/134.6 kB ? eta 0:00:00\n",
      "Downloading pyparsing-3.2.1-py3-none-any.whl (107 kB)\n",
      "   ---------------------------------------- 0.0/107.7 kB ? eta -:--:--\n",
      "   ---------------------------------------- 107.7/107.7 kB ? eta 0:00:00\n",
      "Downloading pytz-2025.1-py2.py3-none-any.whl (507 kB)\n",
      "   ---------------------------------------- 0.0/507.9 kB ? eta -:--:--\n",
      "   --------------------------------------- 507.9/507.9 kB 31.1 MB/s eta 0:00:00\n",
      "Using cached PyYAML-6.0.2-cp312-cp312-win_amd64.whl (156 kB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Downloading rich-13.9.4-py3-none-any.whl (242 kB)\n",
      "   ---------------------------------------- 0.0/242.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 242.4/242.4 kB ? eta 0:00:00\n",
      "Downloading SQLAlchemy-2.0.38-cp312-cp312-win_amd64.whl (2.1 MB)\n",
      "   ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
      "   ---------------------------------- ----- 1.8/2.1 MB 38.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.1/2.1 MB 33.7 MB/s eta 0:00:00\n",
      "Downloading tenacity-9.0.0-py3-none-any.whl (28 kB)\n",
      "Using cached tiktoken-0.8.0-cp312-cp312-win_amd64.whl (883 kB)\n",
      "Using cached toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Downloading torch-2.6.0-cp312-cp312-win_amd64.whl (204.1 MB)\n",
      "   ---------------------------------------- 0.0/204.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.5/204.1 MB 46.6 MB/s eta 0:00:05\n",
      "    --------------------------------------- 3.4/204.1 MB 43.2 MB/s eta 0:00:05\n",
      "   - -------------------------------------- 5.3/204.1 MB 42.4 MB/s eta 0:00:05\n",
      "   - -------------------------------------- 7.1/204.1 MB 45.7 MB/s eta 0:00:05\n",
      "   - -------------------------------------- 9.2/204.1 MB 41.8 MB/s eta 0:00:05\n",
      "   -- ------------------------------------- 11.2/204.1 MB 40.9 MB/s eta 0:00:05\n",
      "   -- ------------------------------------- 13.1/204.1 MB 40.9 MB/s eta 0:00:05\n",
      "   -- ------------------------------------- 15.2/204.1 MB 40.9 MB/s eta 0:00:05\n",
      "   --- ------------------------------------ 17.3/204.1 MB 40.9 MB/s eta 0:00:05\n",
      "   --- ------------------------------------ 19.5/204.1 MB 43.7 MB/s eta 0:00:05\n",
      "   ---- ----------------------------------- 21.8/204.1 MB 43.5 MB/s eta 0:00:05\n",
      "   ---- ----------------------------------- 23.9/204.1 MB 43.5 MB/s eta 0:00:05\n",
      "   ----- ---------------------------------- 26.4/204.1 MB 46.7 MB/s eta 0:00:04\n",
      "   ----- ---------------------------------- 28.6/204.1 MB 46.9 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 31.1/204.1 MB 50.4 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 33.0/204.1 MB 50.1 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 35.5/204.1 MB 50.4 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 38.0/204.1 MB 50.4 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 40.4/204.1 MB 50.4 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 43.0/204.1 MB 54.7 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 45.6/204.1 MB 54.4 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 48.2/204.1 MB 54.7 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 50.9/204.1 MB 54.7 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 53.6/204.1 MB 54.7 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 56.2/204.1 MB 54.7 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 59.0/204.1 MB 59.5 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 61.3/204.1 MB 54.4 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 64.1/204.1 MB 54.4 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 66.9/204.1 MB 54.4 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 70.0/204.1 MB 59.8 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 72.6/204.1 MB 59.5 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 75.8/204.1 MB 59.5 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 78.6/204.1 MB 59.5 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 81.7/204.1 MB 59.5 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 84.5/204.1 MB 59.5 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 87.8/204.1 MB 59.5 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 90.8/204.1 MB 65.2 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 94.0/204.1 MB 65.6 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 96.8/204.1 MB 65.6 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 99.7/204.1 MB 59.5 MB/s eta 0:00:02\n",
      "   ------------------- ------------------- 102.9/204.1 MB 65.6 MB/s eta 0:00:02\n",
      "   -------------------- ------------------ 105.9/204.1 MB 65.6 MB/s eta 0:00:02\n",
      "   -------------------- ------------------ 108.9/204.1 MB 59.5 MB/s eta 0:00:02\n",
      "   --------------------- ----------------- 112.1/204.1 MB 65.2 MB/s eta 0:00:02\n",
      "   ---------------------- ---------------- 115.3/204.1 MB 65.2 MB/s eta 0:00:02\n",
      "   ---------------------- ---------------- 118.6/204.1 MB 65.6 MB/s eta 0:00:02\n",
      "   ----------------------- --------------- 121.9/204.1 MB 65.6 MB/s eta 0:00:02\n",
      "   ----------------------- --------------- 125.5/204.1 MB 73.1 MB/s eta 0:00:02\n",
      "   ------------------------ -------------- 128.8/204.1 MB 72.6 MB/s eta 0:00:02\n",
      "   ------------------------- ------------- 131.7/204.1 MB 65.6 MB/s eta 0:00:02\n",
      "   ------------------------- ------------- 135.4/204.1 MB 72.6 MB/s eta 0:00:01\n",
      "   -------------------------- ------------ 138.7/204.1 MB 72.6 MB/s eta 0:00:01\n",
      "   --------------------------- ----------- 142.1/204.1 MB 72.6 MB/s eta 0:00:01\n",
      "   --------------------------- ----------- 145.2/204.1 MB 73.1 MB/s eta 0:00:01\n",
      "   ---------------------------- ---------- 148.4/204.1 MB 65.6 MB/s eta 0:00:01\n",
      "   ----------------------------- --------- 152.7/204.1 MB 73.1 MB/s eta 0:00:01\n",
      "   ----------------------------- --------- 156.3/204.1 MB 81.8 MB/s eta 0:00:01\n",
      "   ------------------------------ -------- 159.5/204.1 MB 81.8 MB/s eta 0:00:01\n",
      "   ------------------------------- ------- 163.5/204.1 MB 81.8 MB/s eta 0:00:01\n",
      "   ------------------------------- ------- 167.3/204.1 MB 81.8 MB/s eta 0:00:01\n",
      "   -------------------------------- ------ 171.6/204.1 MB 81.8 MB/s eta 0:00:01\n",
      "   --------------------------------- ----- 175.0/204.1 MB 81.8 MB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 178.1/204.1 MB 81.8 MB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 181.2/204.1 MB 65.6 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 184.5/204.1 MB 65.6 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 187.4/204.1 MB 65.6 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 190.7/204.1 MB 65.6 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 194.2/204.1 MB 65.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 197.3/204.1 MB 65.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  200.5/204.1 MB 65.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  203.5/204.1 MB 65.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  204.1/204.1 MB 73.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  204.1/204.1 MB 73.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  204.1/204.1 MB 73.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  204.1/204.1 MB 73.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  204.1/204.1 MB 73.1 MB/s eta 0:00:01\n",
      "   --------------------------------------- 204.1/204.1 MB 25.2 MB/s eta 0:00:00\n",
      "Using cached sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "   ---------------------------------------- 0.0/78.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 78.5/78.5 kB ? eta 0:00:00\n",
      "Downloading transformers-4.48.3-py3-none-any.whl (9.7 MB)\n",
      "   ---------------------------------------- 0.0/9.7 MB ? eta -:--:--\n",
      "   ---------------- ----------------------- 4.0/9.7 MB 85.9 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 8.3/9.7 MB 89.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 9.7/9.7 MB 77.3 MB/s eta 0:00:00\n",
      "Using cached typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Downloading tzdata-2025.1-py2.py3-none-any.whl (346 kB)\n",
      "   ---------------------------------------- 0.0/346.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 346.8/346.8 kB ? eta 0:00:00\n",
      "Downloading urllib3-2.3.0-py3-none-any.whl (128 kB)\n",
      "   ---------------------------------------- 0.0/128.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 128.4/128.4 kB ? eta 0:00:00\n",
      "Downloading watchdog-6.0.0-py3-none-win_amd64.whl (79 kB)\n",
      "   ---------------------------------------- 0.0/79.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 79.1/79.1 kB ? eta 0:00:00\n",
      "Downloading filelock-3.17.0-py3-none-any.whl (16 kB)\n",
      "Using cached haystack_experimental-0.6.0-py3-none-any.whl (106 kB)\n",
      "Using cached lazy_imports-0.4.0-py3-none-any.whl (12 kB)\n",
      "Using cached more_itertools-10.6.0-py3-none-any.whl (63 kB)\n",
      "Using cached networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "Downloading scikit_learn-1.6.1-cp312-cp312-win_amd64.whl (11.1 MB)\n",
      "   ---------------------------------------- 0.0/11.1 MB ? eta -:--:--\n",
      "   -------------- ------------------------- 3.9/11.1 MB 125.2 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 7.9/11.1 MB 84.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.1/11.1 MB 81.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.1/11.1 MB 73.0 MB/s eta 0:00:00\n",
      "Downloading scipy-1.15.1-cp312-cp312-win_amd64.whl (43.6 MB)\n",
      "   ---------------------------------------- 0.0/43.6 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 3.9/43.6 MB 83.4 MB/s eta 0:00:01\n",
      "   ------- -------------------------------- 8.0/43.6 MB 102.9 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 12.3/43.6 MB 81.8 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 16.6/43.6 MB 93.9 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 21.0/43.6 MB 81.8 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 24.4/43.6 MB 93.0 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 27.5/43.6 MB 81.8 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 30.8/43.6 MB 72.6 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 34.0/43.6 MB 65.6 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 37.2/43.6 MB 65.6 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 40.3/43.6 MB 73.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  43.6/43.6 MB 72.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  43.6/43.6 MB 72.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 43.6/43.6 MB 50.1 MB/s eta 0:00:00\n",
      "Using cached xxhash-3.5.0-cp312-cp312-win_amd64.whl (30 kB)\n",
      "Downloading aiohappyeyeballs-2.4.6-py3-none-any.whl (14 kB)\n",
      "Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading anyio-4.8.0-py3-none-any.whl (96 kB)\n",
      "   ---------------------------------------- 0.0/96.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 96.0/96.0 kB ? eta 0:00:00\n",
      "Downloading attrs-25.1.0-py3-none-any.whl (63 kB)\n",
      "   ---------------------------------------- 0.0/63.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 63.2/63.2 kB ? eta 0:00:00\n",
      "Using cached backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Downloading certifi-2025.1.31-py3-none-any.whl (166 kB)\n",
      "   ---------------------------------------- 0.0/166.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 166.4/166.4 kB ? eta 0:00:00\n",
      "Downloading charset_normalizer-3.4.1-cp312-cp312-win_amd64.whl (102 kB)\n",
      "   ---------------------------------------- 0.0/102.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 102.8/102.8 kB 5.8 MB/s eta 0:00:00\n",
      "Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Using cached frozenlist-1.5.0-cp312-cp312-win_amd64.whl (51 kB)\n",
      "Downloading gitdb-4.0.12-py3-none-any.whl (62 kB)\n",
      "   ---------------------------------------- 0.0/62.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 62.8/62.8 kB 3.5 MB/s eta 0:00:00\n",
      "Using cached greenlet-3.1.1-cp312-cp312-win_amd64.whl (299 kB)\n",
      "Using cached h2-4.2.0-py3-none-any.whl (60 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached jiter-0.8.2-cp312-cp312-win_amd64.whl (204 kB)\n",
      "Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Using cached jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Downloading jsonschema_specifications-2024.10.1-py3-none-any.whl (18 kB)\n",
      "Downloading langchain_text_splitters-0.3.6-py3-none-any.whl (31 kB)\n",
      "Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Using cached MarkupSafe-3.0.2-cp312-cp312-win_amd64.whl (15 kB)\n",
      "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
      "   ---------------------------------------- 0.0/50.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 50.9/50.9 kB 2.5 MB/s eta 0:00:00\n",
      "Using cached monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
      "Using cached multidict-6.1.0-cp312-cp312-win_amd64.whl (28 kB)\n",
      "Downloading narwhals-1.26.0-py3-none-any.whl (306 kB)\n",
      "   ---------------------------------------- 0.0/306.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 306.6/306.6 kB ? eta 0:00:00\n",
      "Downloading orjson-3.10.15-cp312-cp312-win_amd64.whl (133 kB)\n",
      "   ---------------------------------------- 0.0/133.7 kB ? eta -:--:--\n",
      "   ---------------------------------------- 133.7/133.7 kB ? eta 0:00:00\n",
      "Downloading propcache-0.2.1-cp312-cp312-win_amd64.whl (44 kB)\n",
      "   ---------------------------------------- 0.0/44.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 44.1/44.1 kB ? eta 0:00:00\n",
      "Downloading referencing-0.36.2-py3-none-any.whl (26 kB)\n",
      "Downloading regex-2024.11.6-cp312-cp312-win_amd64.whl (273 kB)\n",
      "   ---------------------------------------- 0.0/273.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 273.6/273.6 kB ? eta 0:00:00\n",
      "Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "Downloading rpds_py-0.22.3-cp312-cp312-win_amd64.whl (235 kB)\n",
      "   ---------------------------------------- 0.0/235.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 235.8/235.8 kB ? eta 0:00:00\n",
      "Downloading safetensors-0.5.2-cp38-abi3-win_amd64.whl (303 kB)\n",
      "   ---------------------------------------- 0.0/303.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 303.8/303.8 kB ? eta 0:00:00\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Using cached threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Downloading tokenizers-0.21.0-cp39-abi3-win_amd64.whl (2.4 MB)\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.4/2.4 MB 74.3 MB/s eta 0:00:00\n",
      "Using cached typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Downloading yarl-1.18.3-cp312-cp312-win_amd64.whl (90 kB)\n",
      "   ---------------------------------------- 0.0/90.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 90.4/90.4 kB ? eta 0:00:00\n",
      "Downloading zstandard-0.23.0-cp312-cp312-win_amd64.whl (495 kB)\n",
      "   ---------------------------------------- 0.0/495.6 kB ? eta -:--:--\n",
      "   --------------------------------------- 495.6/495.6 kB 30.3 MB/s eta 0:00:00\n",
      "Downloading jsonref-1.1.0-py3-none-any.whl (9.4 kB)\n",
      "Downloading setuptools-75.8.0-py3-none-any.whl (1.2 MB)\n",
      "   ---------------------------------------- 0.0/1.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.2/1.2 MB 76.2 MB/s eta 0:00:00\n",
      "Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Using cached hpack-4.1.0-py3-none-any.whl (34 kB)\n",
      "Using cached hyperframe-6.1.0-py3-none-any.whl (13 kB)\n",
      "Using cached jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
      "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Using cached mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Downloading smmap-5.0.2-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: pytz, mpmath, monotonic, zstandard, xxhash, watchdog, urllib3, tzdata, typing-extensions, tqdm, toml, threadpoolctl, tenacity, sympy, sniffio, smmap, setuptools, safetensors, rpds-py, regex, pyyaml, python-dotenv, pyparsing, pyarrow, protobuf, propcache, portalocker, pillow, orjson, numpy, networkx, narwhals, mypy-extensions, multidict, more-itertools, mdurl, marshmallow, MarkupSafe, lazy-imports, kiwisolver, jsonref, jsonpointer, joblib, jiter, idna, hyperframe, httpx-sse, hpack, h11, grpcio, greenlet, fsspec, frozenlist, fonttools, filelock, distro, dill, cycler, click, charset-normalizer, certifi, cachetools, blinker, backoff, attrs, annotated-types, aiohappyeyeballs, yarl, typing-inspect, SQLAlchemy, scipy, requests, referencing, pydantic-core, pandas, multiprocess, markdown-it-py, jsonpatch, jinja2, httpcore, h2, grpcio-tools, gitdb, contourpy, anyio, aiosignal, torch, tiktoken, scikit-learn, rich, requests-toolbelt, pydeck, pydantic, posthog, matplotlib, jsonschema-specifications, huggingface-hub, httpx, gitpython, dataclasses-json, aiohttp, tokenizers, pydantic-settings, openapi-llm, openai, langsmith, jsonschema, transformers, qdrant-client, langchain-core, datasets, altair, streamlit, sentence-transformers, langchain-text-splitters, langchain-openai, langchain, langchain_community, langchain_experimental, haystack-experimental, haystack-ai\n",
      "Successfully installed MarkupSafe-3.0.2 SQLAlchemy-2.0.38 aiohappyeyeballs-2.4.6 aiohttp-3.11.12 aiosignal-1.3.2 altair-5.5.0 annotated-types-0.7.0 anyio-4.8.0 attrs-25.1.0 backoff-2.2.1 blinker-1.9.0 cachetools-5.5.1 certifi-2025.1.31 charset-normalizer-3.4.1 click-8.1.8 contourpy-1.3.1 cycler-0.12.1 dataclasses-json-0.6.7 datasets-3.2.0 dill-0.3.8 distro-1.9.0 filelock-3.17.0 fonttools-4.56.0 frozenlist-1.5.0 fsspec-2024.9.0 gitdb-4.0.12 gitpython-3.1.44 greenlet-3.1.1 grpcio-1.70.0 grpcio-tools-1.70.0 h11-0.14.0 h2-4.2.0 haystack-ai-2.10.0 haystack-experimental-0.6.0 hpack-4.1.0 httpcore-1.0.7 httpx-0.28.1 httpx-sse-0.4.0 huggingface-hub-0.28.1 hyperframe-6.1.0 idna-3.10 jinja2-3.1.5 jiter-0.8.2 joblib-1.4.2 jsonpatch-1.33 jsonpointer-3.0.0 jsonref-1.1.0 jsonschema-4.23.0 jsonschema-specifications-2024.10.1 kiwisolver-1.4.8 langchain-0.3.18 langchain-core-0.3.35 langchain-openai-0.3.5 langchain-text-splitters-0.3.6 langchain_community-0.3.17 langchain_experimental-0.3.4 langsmith-0.3.8 lazy-imports-0.4.0 markdown-it-py-3.0.0 marshmallow-3.26.1 matplotlib-3.10.0 mdurl-0.1.2 monotonic-1.6 more-itertools-10.6.0 mpmath-1.3.0 multidict-6.1.0 multiprocess-0.70.16 mypy-extensions-1.0.0 narwhals-1.26.0 networkx-3.4.2 numpy-2.2.2 openai-1.62.0 openapi-llm-0.4.1 orjson-3.10.15 pandas-2.2.3 pillow-11.1.0 portalocker-2.10.1 posthog-3.11.0 propcache-0.2.1 protobuf-5.29.3 pyarrow-19.0.0 pydantic-2.10.6 pydantic-core-2.27.2 pydantic-settings-2.7.1 pydeck-0.9.1 pyparsing-3.2.1 python-dotenv-1.0.1 pytz-2025.1 pyyaml-6.0.2 qdrant-client-1.13.2 referencing-0.36.2 regex-2024.11.6 requests-2.32.3 requests-toolbelt-1.0.0 rich-13.9.4 rpds-py-0.22.3 safetensors-0.5.2 scikit-learn-1.6.1 scipy-1.15.1 sentence-transformers-3.4.1 setuptools-75.8.0 smmap-5.0.2 sniffio-1.3.1 streamlit-1.42.0 sympy-1.13.1 tenacity-9.0.0 threadpoolctl-3.5.0 tiktoken-0.8.0 tokenizers-0.21.0 toml-0.10.2 torch-2.6.0 tqdm-4.67.1 transformers-4.48.3 typing-extensions-4.12.2 typing-inspect-0.9.0 tzdata-2025.1 urllib3-2.3.0 watchdog-6.0.0 xxhash-3.5.0 yarl-1.18.3 zstandard-0.23.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install haystack-ai pandas datasets matplotlib python-dotenv qdrant-client langchain-openai langchain_community streamlit sentence-transformers langchain_experimental ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.10.0-cp312-cp312-win_amd64.whl.metadata (4.5 kB)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in c:\\users\\nessa\\onedrive\\desktop\\coding\\machinelearning\\.venv\\lib\\site-packages (from faiss-cpu) (2.2.2)\n",
      "Requirement already satisfied: packaging in c:\\users\\nessa\\onedrive\\desktop\\coding\\machinelearning\\.venv\\lib\\site-packages (from faiss-cpu) (24.2)\n",
      "Downloading faiss_cpu-1.10.0-cp312-cp312-win_amd64.whl (13.7 MB)\n",
      "   ---------------------------------------- 0.0/13.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/13.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/13.7 MB 495.5 kB/s eta 0:00:28\n",
      "   ---------------------------------------- 0.1/13.7 MB 1.0 MB/s eta 0:00:14\n",
      "   - -------------------------------------- 0.3/13.7 MB 2.0 MB/s eta 0:00:07\n",
      "   -- ------------------------------------- 1.0/13.7 MB 4.6 MB/s eta 0:00:03\n",
      "   ------ --------------------------------- 2.1/13.7 MB 7.9 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 4.8/13.7 MB 15.5 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 10.3/13.7 MB 38.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 12.6/13.7 MB 93.0 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 13.1/13.7 MB 65.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  13.4/13.7 MB 54.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  13.7/13.7 MB 38.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 13.7/13.7 MB 36.4 MB/s eta 0:00:00\n",
      "Installing collected packages: faiss-cpu\n",
      "Successfully installed faiss-cpu-1.10.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# sem placa da nvidia\n",
    "%pip install faiss-cpu\n",
    "\n",
    "# com placa da nvidia\n",
    "# %pip install faiss-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "os.environ[\"OPENAI_API_KEY\"] = api_key  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"rajpurkar/squad_v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_union = pd.concat([pd.DataFrame(dataset['train']), pd.DataFrame(dataset['validation'])], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## separação de informações"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "o dataset tem esse formato\n",
    "- Index(['id', 'title', 'context', 'question', 'answers'], dtype='object')\n",
    "\n",
    "#### Contexto\n",
    "existem dois tipos de separação para contexto\n",
    "- mantendo todas as tuplas separadamente, ou seja, title e seu context correspondente\n",
    "- unindo todos os context de um mesmo title, reduzindo bastante o numero de tuplas e deixando os contextos mais completos\n",
    "\n",
    "#### Perguntas e Resposta (Q&A)\n",
    "- obtendo o titulo, pergunta e resposta de cada tupla e salvando individualmente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "class Contexto(Enum):\n",
    "    GERAIS_100 = 'contexto_gerais_100'\n",
    "    GERAIS_50 = 'contexto_gerais_50'\n",
    "    GERAIS_25 = 'contexto_gerais_25'\n",
    "    GERAIS_15 = 'contexto_gerais_15'\n",
    "    PORTITULO_100 = 'contexto_portitulo_100'\n",
    "    PORTITULO_50 = 'contexto_portitulo_50'\n",
    "    PORTITULO_25 = 'contexto_portitulo_25'\n",
    "    PORTITULO_15 = 'contexto_portitulo_15'\n",
    "    QA_PAIRS = 'qa_pairs'\n",
    "    QA_PAIRS_PORTITULO_100 = 'qa_pairs_portitulo_100'\n",
    "    QA_PAIRS_PORTITULO_50 = 'qa_pairs_portitulo_50'\n",
    "    QA_PAIRS_PORTITULO_25 = 'qa_pairs_portitulo_25'\n",
    "    QA_PAIRS_PORTITULO_15 = 'qa_pairs_portitulo_15'\n",
    "    QA_PAIRS_GERAIS_100 = 'qa_pairs_gerais_100'\n",
    "    QA_PAIRS_GERAIS_50 = 'qa_pairs_gerais_50'\n",
    "    QA_PAIRS_GERAIS_25 = 'qa_pairs_gerais_25'\n",
    "    QA_PAIRS_GERAIS_15 = 'qa_pairs_gerais_15'\n",
    "    TESTE_PORTITULO = 'contexto_portitulo_teste'\n",
    "    TESTE_GERAIS = 'contexto_gerais_teste'\n",
    "    QA_TESTE_PORTITULO = 'qa_teste_portitulo'\n",
    "    QA_TESTE_GERAIS = 'qa_teste_gerais'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### unindo contextos por titulo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consolidate_contexts(group):\n",
    "    # Remover duplicatas dentro de um mesmo título\n",
    "    unique_contexts = set(group)\n",
    "    # Juntar os contextos em um único texto corrido\n",
    "    return \" \".join(unique_contexts)\n",
    "\n",
    "contextos_df_unindo = data_union.copy().groupby(\"title\")[\"context\"].apply(consolidate_contexts).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 100% dos dados (embaralhados)\n",
    "contextos_df_unindo.to_csv(f'''data/{Contexto.PORTITULO_100.value}.csv''', index=False)\n",
    "\n",
    "# 50% dos dados\n",
    "contextos_df_unindo.sample(frac=0.5, random_state=42).to_csv(f'''data/{Contexto.PORTITULO_50.value}.csv''', index=False)\n",
    "\n",
    "# 25% dos dados\n",
    "contextos_df_unindo.sample(frac=0.25, random_state=42).to_csv(f'''data/{Contexto.PORTITULO_25.value}.csv''', index=False)\n",
    "\n",
    "# 15% dos dados\n",
    "contextos_df_unindo.sample(frac=0.15, random_state=42).to_csv(f'''data/{Contexto.PORTITULO_15.value}.csv''', index=False)\n",
    "\n",
    "print(\"Arquivos gerados com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sem unir por titulo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contextos_df_title = data_union.copy()\n",
    "contextos_df_title = contextos_df_title[[\"title\", \"context\"]]\n",
    "\n",
    "# Embaralha os dados antes de amostrar\n",
    "contextos_df_title = contextos_df_title.sample(frac=1, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 100% dos dados\n",
    "contextos_df_title.to_csv(f'''data/{Contexto.GERAIS_100.value}.csv''', index=False)\n",
    "\n",
    "# 50% dos dados\n",
    "contextos_df_title.sample(frac=0.5, random_state=42).to_csv(f'''data/{Contexto.GERAIS_50.value}.csv''', index=False)\n",
    "\n",
    "# 25% dos dados\n",
    "contextos_df_title.sample(frac=0.25, random_state=42).to_csv(f'''data/{Contexto.GERAIS_25.value}.csv''', index=False)\n",
    "\n",
    "# 15% dos dados\n",
    "contextos_df_title.sample(frac=0.15, random_state=42).to_csv(f'''data/{Contexto.GERAIS_15.value}.csv''', index=False)\n",
    "\n",
    "print(\"Arquivos gerados com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### separando qa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_df = data_union[[\"title\", \"question\", \"answers\"]]\n",
    "\n",
    "def extract_answers(row):\n",
    "    return row['answers']['text']\n",
    "\n",
    "qa_df['answers'] = qa_df.apply(extract_answers, axis=1)\n",
    "qa_df = qa_df[qa_df['answers'].str.len() > 0]\n",
    "\n",
    "def process_answers(answer_list):\n",
    "    if not isinstance(answer_list, list):\n",
    "        return answer_list  # Retorna diretamente caso não seja uma lista\n",
    "    \n",
    "    unique_answers = list(set(answer_list))  # Remove duplicatas\n",
    "    \n",
    "    if len(unique_answers) == 1:\n",
    "        return unique_answers[0]  # Retorna como string se houver apenas um item único\n",
    "    \n",
    "    return unique_answers  # Retorna a lista se houver múltiplos valores distintos\n",
    "\n",
    "qa_df['answers'] = qa_df['answers'].apply(process_answers)\n",
    "\n",
    "qa_df.to_csv(f'''data/qa/{Contexto.QA_PAIRS.value}_geral.csv''', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## haystack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- inicializando itens do haystack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack import Pipeline\n",
    "from haystack.document_stores.in_memory import InMemoryDocumentStore\n",
    "from haystack.components.retrievers import InMemoryEmbeddingRetriever\n",
    "from haystack.components.converters import TextFileToDocument\n",
    "from haystack.components.preprocessors import DocumentCleaner, DocumentSplitter\n",
    "from haystack.components.embedders import OpenAIDocumentEmbedder, OpenAITextEmbedder\n",
    "from haystack.components.writers import DocumentWriter\n",
    "from haystack.components.builders import PromptBuilder\n",
    "from haystack.components.generators import OpenAIGenerator\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- montando pipeline de indexacao de documentos\n",
    "- montando pipeline de recuperação e geração"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(source_file: str, qa_passado_df: pd.DataFrame, output_filename: str):\n",
    "    text_file_converter = TextFileToDocument()\n",
    "    cleaner = DocumentCleaner()\n",
    "    splitter = DocumentSplitter()\n",
    "    embedder = OpenAIDocumentEmbedder()\n",
    "    indexing_pipeline = Pipeline()\n",
    "\n",
    "    text_embedder = OpenAITextEmbedder()\n",
    "    template = \"\"\"Given these contexts, answer the question.\n",
    "                    Context:\n",
    "                    {% for doc in documents %}\n",
    "                        {{ doc.content }}\n",
    "                    {% endfor %}\n",
    "                    Question: {{query}}\n",
    "                    Answer:\"\"\"\n",
    "    prompt_builder = PromptBuilder(template=template)\n",
    "    llm = OpenAIGenerator()\n",
    "    rag_pipeline = Pipeline()\n",
    "\n",
    "    print('------------------- montando pipeline de indexação de docs')\n",
    "    document_store = InMemoryDocumentStore()\n",
    "    writer = DocumentWriter(document_store)\n",
    "    retriever = InMemoryEmbeddingRetriever(document_store)\n",
    "    \n",
    "    indexing_pipeline.add_component(\"converter\", text_file_converter)\n",
    "    indexing_pipeline.add_component(\"cleaner\", cleaner)\n",
    "    indexing_pipeline.add_component(\"splitter\", splitter)\n",
    "    indexing_pipeline.add_component(\"embedder\", embedder)\n",
    "    indexing_pipeline.add_component(\"writer\", writer)\n",
    "\n",
    "    indexing_pipeline.connect(\"converter.documents\", \"cleaner.documents\")\n",
    "    indexing_pipeline.connect(\"cleaner.documents\", \"splitter.documents\")\n",
    "    indexing_pipeline.connect(\"splitter.documents\", \"embedder.documents\")\n",
    "    indexing_pipeline.connect(\"embedder.documents\", \"writer.documents\")\n",
    "\n",
    "    print('------------------- pipeline montada, lendo arquivo')\n",
    "    indexing_pipeline.run(data={\"sources\": [f'''data/{source_file}.csv''']})\n",
    "\n",
    "    print(f\"Documentos do arquivo {source_file} indexados com sucesso!\")\n",
    "    print('------------------- montando pipelines de rag (retriver e generator)')\n",
    "\n",
    "    rag_pipeline.add_component(\"text_embedder\", text_embedder)\n",
    "    rag_pipeline.add_component(\"retriever\", retriever)\n",
    "    rag_pipeline.add_component(\"prompt_builder\", prompt_builder)\n",
    "    rag_pipeline.add_component(\"llm\", llm)\n",
    "\n",
    "    rag_pipeline.connect(\"text_embedder.embedding\", \"retriever.query_embedding\")\n",
    "    rag_pipeline.connect(\"retriever.documents\", \"prompt_builder.documents\")\n",
    "    rag_pipeline.connect(\"prompt_builder\", \"llm\")\n",
    "\n",
    "    print('------------------- pipelines de rag montada')\n",
    "\n",
    "    print('------------------- iniciando testes de perguntas e respostas')\n",
    "\n",
    "    results = []\n",
    "    i = 0\n",
    "    total = len(qa_passado_df)\n",
    "    \n",
    "    for index, row in qa_passado_df.iterrows():\n",
    "        query = row[\"question\"]\n",
    "        expected_answer = row[\"answers\"]\n",
    "        \n",
    "        generated_answer = rag_pipeline.run(data={\"prompt_builder\": {\"query\": query}, \"text_embedder\": {\"text\": query}})\n",
    "\n",
    "        results.append({\n",
    "            \"title\": row[\"title\"],\n",
    "            \"question\": query,\n",
    "            \"expected_answer\": expected_answer,\n",
    "            \"generated_answer\": generated_answer[\"llm\"][\"replies\"],\n",
    "        })\n",
    "        i += 1\n",
    "\n",
    "        print(f\"################### Processando ({i}/{total})...\")\n",
    "        time.sleep(0.1)\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv(f'''data/qa/{output_filename}.csv''', index=False)\n",
    "\n",
    "    print(f\"\\n-------------------Finalizado! Resultados salvos em {output_filename}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## resultados haystack por titulo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_df_5_porcento = qa_df.sample(frac=0.05, random_state=42)\n",
    "qa_df_01_porcento = qa_df.sample(frac=0.001, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# qa_df_usado = qa_df.copy()\n",
    "# qa_df_usado.to_csv(f'''data/qa/{Contexto.QA_PAIRS.value}_usado.csv''')\n",
    "qa_df_usado = pd.read_csv(f'''data/qa/{Contexto.QA_PAIRS.value}_usado.csv''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_df_usado.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline(Contexto.PORTITULO_15.value, qa_df_usado, Contexto.QA_PAIRS_PORTITULO_15.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline(Contexto.PORTITULO_25.value, qa_df_usado, Contexto.QA_PAIRS_PORTITULO_25.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline(Contexto.PORTITULO_50.value, qa_df_usado, Contexto.QA_PAIRS_PORTITULO_50.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline(Contexto.PORTITULO_100.value, qa_df_usado, Contexto.QA_PAIRS_PORTITULO_100.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## resultados haystack geral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline(Contexto.GERAIS_15.value, qa_df_usado, Contexto.QA_PAIRS_GERAIS_15.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline(Contexto.GERAIS_25.value, qa_df_usado, Contexto.QA_PAIRS_GERAIS_25.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline(Contexto.GERAIS_50.value, qa_df_usado, Contexto.QA_PAIRS_GERAIS_50.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline(Contexto.GERAIS_100.value, qa_df_usado, Contexto.QA_PAIRS_GERAIS_100.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## comparação de respostas com llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import re\n",
    "\n",
    "def use_llm(esperada, obtida):\n",
    "    cliente = OpenAI(api_key=api_key)\n",
    "\n",
    "    prompt = f'''\n",
    "        Você é um assistente que compara textos. \n",
    "        Compare a resposta esperada '{esperada}' com a resposta adquirida '{obtida}' e diga se são semanticamente semelhantes. \n",
    "\n",
    "        Responda em uma escala de 0 a 10, onde 0 significa que os textos são completamente diferentes e 10 significa que são idênticos em sentido e semanticamente.\n",
    "\n",
    "        Tente entender o sentido completo da resposta, não apenas palavras-chave.\n",
    "        Mas também se atente a detalhes como palavras-chave e seus significados.\n",
    "        Por exemplo, orações com palavras diferentes mas com o mesmo significado devem ser consideradas semelhantes.\n",
    "        Em caso de datas, números ou informações específicas, considere a semelhança do contexto em que estão inseridos.\n",
    "\n",
    "        Importante: Uma resposta adquirida pode ser mais longa e detalhada que a resposta esperada. Isso não necessariamente a torna diferente, desde que ela ainda aborde o mesmo tópico e não contradiga a resposta esperada. Portanto, mesmo que a resposta adquirida seja mais longa, se ela ainda estiver alinhada com a resposta esperada, considere-a semelhante.\n",
    "\n",
    "        Responda apenas o número equivalente à semelhança dos textos.\n",
    "\n",
    "        # exemplo de saida\n",
    "        10\n",
    "    '''\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    response = cliente.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        store=True,\n",
    "        messages=messages,\n",
    "        temperature=0\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def remove_specific_characters(input_string):\n",
    "    input_string = re.sub(r'(?<=[a-zA-Z])\\.', '', input_string)\n",
    "    input_string = re.sub(r'[\\[\\]\\'\\\"]', '', input_string)\n",
    "    return input_string\n",
    "\n",
    "def is_similar_using_llm(esperada: str = \"\",obtida: str = \"\"):\n",
    "    if isinstance(esperada, list):\n",
    "        esperada = esperada[0]\n",
    "    if isinstance(obtida, list):\n",
    "        obtida = obtida[0]\n",
    "\n",
    "    esperada = remove_specific_characters(esperada)\n",
    "    obtida = remove_specific_characters(obtida)\n",
    "    response = use_llm(esperada, obtida)\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teste = is_similar_using_llm(\"archdioceses or departments of the Roman Curia\", \"The Roman Curia.\")\n",
    "teste2 = is_similar_using_llm('Central Standard Time',\"['The contexts provided do not mention Saskatoon or its time observance specific to time zones, Daylight Saving Time, or any other relevant information. Therefore, based on the information given, I cannot determine what time Saskatoon observes all year long.']\")\n",
    "teste3 = is_similar_using_llm('VHF omnidirectional range',['VOR stands for VHF Omnidirectional Range.'])\n",
    "teste4 = is_similar_using_llm('32nd',\"['In 2009, Tucson ranked as the 32nd largest city in the United States.']\")\n",
    "teste5 = is_similar_using_llm('1996',\"['Labour published a new draft manifesto in 1996, called \"\"New Labour, New Life For Britain.\"\"']\")\n",
    "print(f\"Teste 1: {teste}, Teste 2: {teste2}, Teste 3: {teste3}, Teste 4 que devia dar 10: {teste4}, Teste 5 que devia dar 10: {teste5}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frase1=\"email, web-hosting, or online storage services\"\n",
    "frase2=\"['Internet hosting services provide the infrastructure and technology needed to make websites accessible on the Internet. This includes server space for storing website files, bandwidth for transmitting data to users, domain name registration, email accounts, and often additional services such as database management, security features, and technical support. Hosting services enable individuals and organizations to have an online presence by hosting their web content and applications.']\"\n",
    "print(is_similar_using_llm(frase1, frase2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_respostas_usando_llm(arquivo_csv: str, qa='qa'):\n",
    "    df = pd.read_csv(f'''data/{qa}/{arquivo_csv}.csv''')\n",
    "    df['is_similar'] = df.apply(lambda row: is_similar_using_llm(row['expected_answer'], row['generated_answer']), axis=1)\n",
    "    df.to_csv(f'''data/{qa}/{arquivo_csv}.csv''', index=False)\n",
    "\n",
    "def compare_respostas_usando_llm_contagem(arquivo_csv: str, qa='qa'):\n",
    "    df = pd.read_csv(f'''data/{qa}/{arquivo_csv}.csv''')\n",
    "\n",
    "    count = 0\n",
    "    total = len(df)\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        df.loc[index, 'is_similar'] = is_similar_using_llm(row['expected_answer'], row['generated_answer'])\n",
    "        \n",
    "        count += 1\n",
    "        \n",
    "        print(f'*********** Progresso: {count}/{total}')\n",
    "\n",
    "    df.to_csv(f'''data/{qa}/{arquivo_csv}.csv''', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_respostas_usando_llm_contagem(Contexto.QA_PAIRS_GERAIS_15.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_respostas_usando_llm_contagem(Contexto.QA_PAIRS_GERAIS_25.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_respostas_usando_llm_contagem(Contexto.QA_PAIRS_GERAIS_50.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_respostas_usando_llm_contagem(Contexto.QA_PAIRS_GERAIS_100.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_respostas_usando_llm_contagem(Contexto.QA_PAIRS_PORTITULO_15.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_respostas_usando_llm_contagem(Contexto.QA_PAIRS_PORTITULO_25.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_respostas_usando_llm_contagem(Contexto.QA_PAIRS_PORTITULO_50.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_respostas_usando_llm_contagem(Contexto.QA_PAIRS_PORTITULO_100.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## adicionando testes a parte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contextos_df_title.sample(frac=0.05, random_state=42).to_csv(f'''data/{Contexto.TESTE_PORTITULO.value}.csv''', index=False)\n",
    "contextos_df_unindo.sample(frac=0.05, random_state=42).to_csv(f'''data/{Contexto.TESTE_GERAIS.value}.csv''', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_df_novo = qa_df.sample(frac=0.01, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline(Contexto.TESTE_PORTITULO.value, qa_df_novo, Contexto.QA_TESTE_PORTITULO.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline(Contexto.TESTE_GERAIS.value, qa_df_novo, Contexto.QA_TESTE_PORTITULO.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_respostas_usando_llm_contagem(Contexto.QA_TESTE_PORTITULO.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_respostas_usando_llm_contagem(Contexto.QA_TESTE_GERAIS.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## avaliando resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base que quero avaliar\n",
    "QA_avaliado = Contexto.QA_PAIRS_GERAIS_100.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obtendo_respostas = pd.read_csv(f'''data/qa/{QA_avaliado}.csv''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar um dicionário para armazenar a contagem por faixa de 0 a 10\n",
    "bins = list(range(11))  # Criando bins de 0 a 10\n",
    "\n",
    "# Contar as ocorrências em cada faixa\n",
    "contagem = obtendo_respostas[\"is_similar\"].value_counts().sort_index()\n",
    "contagem = contagem.reindex(bins, fill_value=0)  # Garantir que todas as faixas apareçam\n",
    "\n",
    "# Calcular porcentagem\n",
    "total = contagem.sum()\n",
    "porcentagem = (contagem / total) * 100\n",
    "\n",
    "\n",
    "# Exibir a tabela de porcentagens\n",
    "print(f'Distribuição de similaridade no dataset {QA_avaliado} (%):')\n",
    "print(porcentagem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cores = [\"blue\", \"red\", \"green\", \"purple\", \"orange\", \"brown\", \"pink\", \"gray\", \"cyan\", \"magenta\"]\n",
    "\n",
    "# Criando um gráfico de barras\n",
    "plt.figure(figsize=(6, 4))\n",
    "obtendo_respostas[\"is_similar\"].value_counts().plot(kind=\"bar\", color=cores)\n",
    "plt.title(f'''Distribuição de similaridade {QA_avaliado}''')\n",
    "plt.xlabel(\"o quão similar é\")\n",
    "plt.ylabel(\"Frequência\")\n",
    "plt.xticks(rotation=0)\n",
    "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "plt.show()\n",
    "\n",
    "# Criando um gráfico de pizza\n",
    "plt.figure(figsize=(6, 6))\n",
    "obtendo_respostas[\"is_similar\"].value_counts().plot(kind=\"pie\", autopct=\"%1.1f%%\", colors=cores, startangle=90, wedgeprops={\"edgecolor\": \"black\"})\n",
    "plt.title(f'''Proporção de similaridade {QA_avaliado}''')\n",
    "plt.ylabel(\"\")  # Removendo label desnecessário\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Dados das distribuições de similaridade\n",
    "datasets = {\n",
    "    \"qa_pairs_portitulo_15\": [47.18, 0.06, 6.73, 15.52, 0.88, 2.74, 0.90, 11.26, 11.54, 2.44, 0.76],\n",
    "    \"qa_pairs_portitulo_25\": [42.39, 0.00, 6.93, 15.67, 0.80, 3.04, 0.96, 12.90, 13.68, 2.84, 0.80],\n",
    "    \"qa_pairs_portitulo_50\": [34.17, 0.04, 6.43, 16.33, 1.04, 4.01, 0.84, 15.73, 16.37, 3.57, 1.46],\n",
    "    \"qa_pairs_portitulo_100\": [16.95, 0.00, 5.55, 18.89, 1.24, 5.89, 0.78, 20.83, 22.38, 5.25, 2.24],\n",
    "    \"qa_pairs_gerais_15\": [29.47, 0.06, 6.81, 18.13, 0.76, 4.35, 0.76, 16.43, 17.47, 3.99, 1.76],\n",
    "    \"qa_pairs_gerais_25\": [25.32, 0.02, 6.73, 17.97, 1.06, 5.09, 0.72, 17.33, 18.81, 5.03, 1.92],\n",
    "}\n",
    "\n",
    "# Configuração dos gráficos\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "x = np.arange(len(datasets[\"qa_pairs_portitulo_15\"]))  # Posições no eixo X\n",
    "width = 0.15  # Largura das barras\n",
    "\n",
    "# Cores para os datasets\n",
    "colors = ['b', 'g', 'r', 'c', 'm', 'y']\n",
    "\n",
    "# Criando barras para cada dataset\n",
    "for i, (label, values) in enumerate(datasets.items()):\n",
    "    ax.bar(x + i * width, values, width, label=label, color=colors[i])\n",
    "\n",
    "# Ajustes no gráfico\n",
    "ax.set_xlabel(\"Níveis de Similaridade\")\n",
    "ax.set_ylabel(\"Percentual (%)\")\n",
    "ax.set_title(\"Comparação das Distribuições de Similaridade\")\n",
    "ax.set_xticks(x + width)\n",
    "ax.set_xticklabels(range(11))\n",
    "ax.legend()\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Exibir gráfico\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.document_loaders import DataFrameLoader\n",
    "from langchain_community.vectorstores import Qdrant\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.schema import (\n",
    "    SystemMessage,\n",
    "    HumanMessage\n",
    ")\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_langchain_openai(base: str, qa_passado_df, output_filename: str):\n",
    "\n",
    "    data_df = pd.read_csv(f'''data/{base}.csv''')\n",
    "\n",
    "    chat = ChatOpenAI(\n",
    "        model=\"gpt-4o\",\n",
    "    )\n",
    "    loader = DataFrameLoader(data_df, page_content_column=\"context\")\n",
    "    embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "    documents = loader.load()\n",
    "\n",
    "    print('------------------- obtendo documentos para pipeline')\n",
    "    qdrant = Qdrant.from_documents(\n",
    "        documents=documents,\n",
    "        embedding=embeddings,\n",
    "        location=\":memory:\",\n",
    "        collection_name=\"rag\"\n",
    "    )\n",
    "\n",
    "    print('------------------- finalizando documentos para pipeline')\n",
    "\n",
    "    print('------------------- iniciando testes de perguntas e respostas')\n",
    "\n",
    "    results = []\n",
    "    i = 0\n",
    "    total = len(qa_passado_df)\n",
    "    \n",
    "    for index, row in qa_passado_df.iterrows():\n",
    "        query = row[\"question\"]\n",
    "        expected_answer = row[\"answers\"]\n",
    "        \n",
    "        results_retriver = qdrant.similarity_search(query, k=3)\n",
    "        source_knowledge = \"\\n\".join([x.page_content for x in results_retriver])\n",
    "        augment_prompt = f\"\"\"Use the context below to answer the question.\n",
    "\n",
    "        Context:\n",
    "        {source_knowledge}\n",
    "        -------------------------\n",
    "        Succinct answers are necessary, always focusing on answering the question. \n",
    "        To get straight to the point, avoid unnecessary information.\n",
    "        Avoid repeat the question in the answer and focus in the objective answer.\n",
    "        -------------------------\n",
    "        Question: {query}\"\"\"\n",
    "\n",
    "\n",
    "        messages = [\n",
    "            SystemMessage(content=\"You are a RAG that uses contexts to answer questions.\"),\n",
    "            HumanMessage(content=augment_prompt)\n",
    "        ]\n",
    "\n",
    "        res = chat.invoke(messages)\n",
    "\n",
    "        results.append({\n",
    "            \"title\": row[\"title\"],\n",
    "            \"question\": query,\n",
    "            \"expected_answer\": expected_answer,\n",
    "            \"generated_answer\": res.content,\n",
    "        })\n",
    "        i += 1\n",
    "\n",
    "        print(f\"################### Processando ({i}/{total})...\")\n",
    "        time.sleep(0.1)\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv(f'''data/qa_langchain_openai/{output_filename}.csv''', index=False)\n",
    "\n",
    "    print(f\"\\n-------------------Finalizado! Resultados salvos em {output_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- dataset utilizado na outra "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_df_usado = pd.read_csv(f'''data/qa/{Contexto.QA_PAIRS.value}_usado.csv''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "aplicando em contextos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_langchain_openai(Contexto.GERAIS_100.value, qa_df_usado, Contexto.QA_PAIRS_GERAIS_100.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_langchain_openai(Contexto.GERAIS_50.value, qa_df_usado, Contexto.QA_PAIRS_GERAIS_50.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_langchain_openai(Contexto.GERAIS_25.value, qa_df_usado, Contexto.QA_PAIRS_GERAIS_25.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_langchain_openai(Contexto.GERAIS_15.value, qa_df_usado, Contexto.QA_PAIRS_GERAIS_15.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_langchain_openai(Contexto.PORTITULO_15.value, qa_df_usado, Contexto.QA_PAIRS_PORTITULO_15.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_langchain_openai(Contexto.PORTITULO_25.value, qa_df_usado, Contexto.QA_PAIRS_PORTITULO_25.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_langchain_openai(Contexto.PORTITULO_50.value, qa_df_usado, Contexto.QA_PAIRS_PORTITULO_50.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_langchain_openai(Contexto.PORTITULO_100.value, qa_df_usado, Contexto.QA_PAIRS_PORTITULO_100.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- avaliacao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_respostas_usando_llm_contagem(Contexto.QA_PAIRS_GERAIS_100.value, 'qa_langchain_openai')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_respostas_usando_llm_contagem(Contexto.QA_PAIRS_GERAIS_50.value, 'qa_langchain_openai')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_respostas_usando_llm_contagem(Contexto.QA_PAIRS_GERAIS_25.value, 'qa_langchain_openai')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_respostas_usando_llm_contagem(Contexto.QA_PAIRS_GERAIS_15.value, 'qa_langchain_openai')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_respostas_usando_llm_contagem(Contexto.QA_PAIRS_PORTITULO_100.value, 'qa_langchain_openai')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_respostas_usando_llm_contagem(Contexto.QA_PAIRS_PORTITULO_50.value, 'qa_langchain_openai')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_respostas_usando_llm_contagem(Contexto.QA_PAIRS_PORTITULO_25.value, 'qa_langchain_openai')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_respostas_usando_llm_contagem(Contexto.QA_PAIRS_PORTITULO_15.value, 'qa_langchain_openai')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## rag deepseek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import DataFrameLoader \n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.llms import Ollama \n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## rode no seu terminal\n",
    "## ollama run deepseek-r1:8b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# qa_df_usado = pd.read_csv(f'''data/qa/{Contexto.QA_PAIRS.value}_usado.csv''')\n",
    "qa_df = pd.read_csv(f'''data/qa/{Contexto.QA_PAIRS.value}_usado.csv''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_df_usado = qa_df.sample(frac=0.005, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_langchain_deepseek(base: str, qa_passado_df, output_filename: str):\n",
    "\n",
    "    data_df = pd.read_csv(f'''data/{base}.csv''')\n",
    "\n",
    "    llm = Ollama(model=\"deepseek-r1:8b\") \n",
    "\n",
    "    loader = DataFrameLoader(data_df, page_content_column=\"context\")\n",
    "    documents = loader.load()\n",
    "    embeddings_model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=embeddings_model_name) \n",
    "\n",
    "    print('------------------- obtendo documentos para pipeline')\n",
    "    qdrant = Qdrant.from_documents(\n",
    "        documents=documents,\n",
    "        embedding=embeddings,\n",
    "        location=\":memory:\",\n",
    "        collection_name=\"rag\"\n",
    "    )\n",
    "\n",
    "    print('------------------- finalizando documentos para pipeline')\n",
    "\n",
    "    print('------------------- iniciando testes de perguntas e respostas')\n",
    "\n",
    "    results = []\n",
    "    i = 0\n",
    "    total = len(qa_passado_df)\n",
    "    \n",
    "    for index, row in qa_passado_df.iterrows():\n",
    "        query = row[\"question\"]\n",
    "        expected_answer = row[\"answers\"]\n",
    "        \n",
    "        results_retriver = qdrant.similarity_search(query, k=3)\n",
    "        source_knowledge = \"\\n\".join([x.page_content for x in results_retriver])\n",
    "        augment_prompt = f\"\"\"Use the context below to answer the question.\n",
    "\n",
    "        Context:\n",
    "        {source_knowledge}\n",
    "        -------------------------\n",
    "        1. Use ONLY the context below.  \n",
    "        2. If in doubt, say \"I don't know\".\n",
    "        -------------------------\n",
    "        Succinct answers are necessary, always focusing on answering the question. \n",
    "        To get straight to the point, avoid unnecessary information.\n",
    "        Avoid repeat the question in the answer and focus in the objective answer.\n",
    "        -------------------------\n",
    "        Question: {query}\"\"\"\n",
    "\n",
    "\n",
    "        messages = [\n",
    "            SystemMessage(content=\"You are a RAG that uses contexts to answer questions.\"),\n",
    "            HumanMessage(content=augment_prompt)\n",
    "        ]\n",
    "\n",
    "        resp = llm.invoke(messages)\n",
    "        response = re.sub(r\"<think>.*?</think>\\n?\", \"\",resp, flags=re.DOTALL)\n",
    "\n",
    "        results.append({\n",
    "            \"title\": row[\"title\"],\n",
    "            \"question\": query,\n",
    "            \"expected_answer\": expected_answer,\n",
    "            \"generated_answer\": response,\n",
    "        })\n",
    "        i += 1\n",
    "\n",
    "        print(f\"################### Processando ({i}/{total})...\")\n",
    "        time.sleep(0.1)\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv(f'''data/qa_langchain_deepseek/{output_filename}.csv''', index=False)\n",
    "\n",
    "    print(f\"\\n-------------------Finalizado! Resultados salvos em {output_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------- obtendo documentos para pipeline\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nessa\\OneDrive\\Desktop\\coding\\machineLearning\\.venv\\Lib\\site-packages\\langchain_community\\vectorstores\\qdrant.py:192: UserWarning: Local mode is not recommended for collections with more than 20,000 points. Current collection contains 20032 points. Consider using Qdrant in Docker or Qdrant Cloud for better performance with large datasets.\n",
      "  self.client.upsert(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------- finalizando documentos para pipeline\n",
      "------------------- iniciando testes de perguntas e respostas\n",
      "################### Processando (1/25)...\n",
      "################### Processando (2/25)...\n",
      "################### Processando (3/25)...\n",
      "################### Processando (4/25)...\n",
      "################### Processando (5/25)...\n",
      "################### Processando (6/25)...\n",
      "################### Processando (7/25)...\n",
      "################### Processando (8/25)...\n",
      "################### Processando (9/25)...\n",
      "################### Processando (10/25)...\n"
     ]
    }
   ],
   "source": [
    "pipeline_langchain_deepseek(Contexto.PORTITULO_15.value, qa_df_usado, Contexto.QA_TESTE_PORTITULO.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "import re\n",
    "\n",
    "def use_llm(esperada, obtida):\n",
    "    prompt = f'''\n",
    "        Você é um assistente que compara textos. \n",
    "        Compare a resposta esperada '{esperada}' com a resposta adquirida '{obtida}' e diga se são semanticamente semelhantes. \n",
    "\n",
    "        Responda em uma escala de 0 a 10, onde 0 significa que os textos são completamente diferentes e 10 significa que são idênticos em sentido e semanticamente.\n",
    "\n",
    "        Tente entender o sentido completo da resposta, não apenas palavras-chave.\n",
    "        Mas também se atente a detalhes como palavras-chave e seus significados.\n",
    "        Por exemplo, orações com palavras diferentes mas com o mesmo significado devem ser consideradas semelhantes.\n",
    "        Em caso de datas, números ou informações específicas, considere a semelhança do contexto em que estão inseridos.\n",
    "\n",
    "        Importante: Uma resposta adquirida pode ser mais longa e detalhada que a resposta esperada. Isso não necessariamente a torna diferente, desde que ela ainda aborde o mesmo tópico e não contradiga a resposta esperada. Portanto, mesmo que a resposta adquirida seja mais longa, se ela ainda estiver alinhada com a resposta esperada, considere-a semelhante.\n",
    "\n",
    "        Responda apenas o número equivalente à semelhança dos textos.\n",
    "\n",
    "        # exemplo de saída\n",
    "        10\n",
    "    '''\n",
    "    \n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = ollama.chat(model='deepseek-r1:8b', messages=messages)\n",
    "    return response['message']['content'].strip()\n",
    "\n",
    "def remove_specific_characters(input_string):\n",
    "    input_string = re.sub(r'(?<=[a-zA-Z])\\.', '', input_string)\n",
    "    input_string = re.sub(r'[\\[\\]\\'\\\"]', '', input_string)\n",
    "    return input_string\n",
    "\n",
    "def is_similar_using_llm(esperada: str = \"\", obtida: str = \"\"):\n",
    "    if isinstance(esperada, list):\n",
    "        esperada = esperada[0]\n",
    "    if isinstance(obtida, list):\n",
    "        obtida = obtida[0]\n",
    "\n",
    "    esperada = remove_specific_characters(esperada)\n",
    "    obtida = remove_specific_characters(obtida)\n",
    "    response = use_llm(esperada, obtida)\n",
    "    return response\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
