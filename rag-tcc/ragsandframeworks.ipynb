{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: haystack-ai in /Users/vx/Desktop/coding/machineLearning/.venv/lib/python3.13/site-packages (2.7.0)\n",
      "Requirement already satisfied: trafilatura in /Users/vx/Desktop/coding/machineLearning/.venv/lib/python3.13/site-packages (2.0.0)\n",
      "Requirement already satisfied: datasets in /Users/vx/Desktop/coding/machineLearning/.venv/lib/python3.13/site-packages (3.2.0)\n",
      "Requirement already satisfied: txtai in /Users/vx/Desktop/coding/machineLearning/.venv/lib/python3.13/site-packages (8.2.0)\n",
      "Requirement already satisfied: pandas in /Users/vx/Desktop/coding/machineLearning/.venv/lib/python3.13/site-packages (2.2.3)\n",
      "Requirement already satisfied: haystack-experimental in /Users/vx/Desktop/coding/machineLearning/.venv/lib/python3.13/site-packages (from haystack-ai) (0.5.0)\n",
      "Requirement already satisfied: jinja2 in /Users/vx/Desktop/coding/machineLearning/.venv/lib/python3.13/site-packages (from haystack-ai) (3.1.5)\n",
      "Requirement already satisfied: lazy-imports in /Users/vx/Desktop/coding/machineLearning/.venv/lib/python3.13/site-packages (from haystack-ai) (0.4.0)\n",
      "Requirement already satisfied: more-itertools in /Users/vx/Desktop/coding/machineLearning/.venv/lib/python3.13/site-packages (from haystack-ai) (10.6.0)\n",
      "Requirement already satisfied: networkx in /Users/vx/Desktop/coding/machineLearning/.venv/lib/python3.13/site-packages (from haystack-ai) (3.4.2)\n",
      "Requirement already satisfied: numpy in /Users/vx/Desktop/coding/machineLearning/.venv/lib/python3.13/site-packages (from haystack-ai) (2.2.2)\n",
      "Requirement already satisfied: openai>=1.1.0 in /Users/vx/Desktop/coding/machineLearning/.venv/lib/python3.13/site-packages (from haystack-ai) (1.61.0)\n",
      "Requirement already satisfied: posthog in /Users/vx/Desktop/coding/machineLearning/.venv/lib/python3.13/site-packages (from haystack-ai) (3.11.0)\n",
      "Requirement already satisfied: python-dateutil in /Users/vx/Desktop/coding/machineLearning/.venv/lib/python3.13/site-packages (from haystack-ai) (2.9.0.post0)\n",
      "Requirement already satisfied: pyyaml in /Users/vx/Desktop/coding/machineLearning/.venv/lib/python3.13/site-packages (from haystack-ai) (6.0.2)\n",
      "Requirement already satisfied: requests in /Users/vx/Desktop/coding/machineLearning/.venv/lib/python3.13/site-packages (from haystack-ai) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0 in /Users/vx/Desktop/coding/machineLearning/.venv/lib/python3.13/site-packages (from haystack-ai) (9.0.0)\n",
      "Requirement already satisfied: tqdm in /Users/vx/Desktop/coding/machineLearning/.venv/lib/python3.13/site-packages (from haystack-ai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /Users/vx/Desktop/coding/machineLearning/.venv/lib/python3.13/site-packages (from haystack-ai) (4.12.2)\n",
      "Requirement already satisfied: certifi in /Users/vx/Desktop/coding/machineLearning/.venv/lib/python3.13/site-packages (from trafilatura) (2025.1.31)\n",
      "Requirement already satisfied: charset_normalizer>=3.4.0 in /Users/vx/Desktop/coding/machineLearning/.venv/lib/python3.13/site-packages (from trafilatura) (3.4.1)\n",
      "Requirement already satisfied: courlan>=1.3.2 in /Users/vx/Desktop/coding/machineLearning/.venv/lib/python3.13/site-packages (from trafilatura) (1.3.2)\n",
      "Requirement already satisfied: htmldate>=1.9.2 in /Users/vx/Desktop/coding/machineLearning/.venv/lib/python3.13/site-packages (from trafilatura) (1.9.3)\n",
      "Requirement already satisfied: justext>=3.0.1 in /Users/vx/Desktop/coding/machineLearning/.venv/lib/python3.13/site-packages (from trafilatura) (3.0.1)\n",
      "Requirement already satisfied: lxml>=5.3.0 in /Users/vx/Desktop/coding/machineLearning/.venv/lib/python3.13/site-packages (from trafilatura) (5.3.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in /Users/vx/Desktop/coding/machineLearning/.venv/lib/python3.13/site-packages (from trafilatura) (2.3.0)\n",
      "Requirement already satisfied: filelock in /Users/vx/Desktop/coding/machineLearning/.venv/lib/python3.13/site-packages (from datasets) (3.17.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /Users/vx/Desktop/coding/machineLearning/.venv/lib/python3.13/site-packages (from datasets) (19.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /Users/vx/Desktop/coding/machineLearning/.venv/lib/python3.13/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: xxhash in /Users/vx/Desktop/coding/machineLearning/.venv/lib/python3.13/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /Users/vx/Desktop/coding/machineLearning/.venv/lib/python3.13/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /Users/vx/Desktop/coding/machineLearning/.venv/lib/python3.13/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in /Users/vx/Desktop/coding/machineLearning/.venv/lib/python3.13/site-packages (from datasets) (3.11.11)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in /Users/vx/Desktop/coding/machineLearning/.venv/lib/python3.13/site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: packaging in /Users/vx/Desktop/coding/machineLearning/.venv/lib/python3.13/site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: faiss-cpu>=1.7.1.post2 in /Users/vx/Desktop/coding/machineLearning/.venv/lib/python3.13/site-packages (from txtai) (1.10.0)\n",
      "Requirement already satisfied: msgpack>=1.0.7 in /Users/vx/Desktop/coding/machineLearning/.venv/lib/python3.13/site-packages (from txtai) (1.1.0)\n",
      "Requirement already satisfied: torch>=1.12.1 in /Users/vx/Desktop/coding/machineLearning/.venv/lib/python3.13/site-packages (from txtai) (2.6.0)\n",
      "Requirement already satisfied: transformers>=4.45.0 in /Users/vx/Desktop/coding/machineLearning/.venv/lib/python3.13/site-packages (from txtai) (4.48.2)\n",
      "Requirement already satisfied: regex>=2022.8.17 in /Users/vx/Desktop/coding/machineLearning/.venv/lib/python3.13/site-packages (from txtai) (2024.11.6)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/vx/Desktop/coding/machineLearning/.venv/lib/python3.13/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/vx/Desktop/coding/machineLearning/.venv/lib/python3.13/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: babel>=2.16.0 in /Users/vx/Desktop/coding/machineLearning/.venv/lib/python3.13/site-packages (from courlan>=1.3.2->trafilatura) (2.17.0)\n",
      "Requirement already satisfied: tld>=0.13 in /Users/vx/Desktop/coding/machineLearning/.venv/lib/python3.13/site-packages (from courlan>=1.3.2->trafilatura) (0.13)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/vx/Desktop/coding/machineLearning/.venv/lib/python3.13/site-packages (from aiohttp->datasets) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/vx/Desktop/coding/machineLearning/.venv/lib/python3.13/site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/vx/Desktop/coding/machineLearning/.venv/lib/python3.13/site-packages (from aiohttp->datasets) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/vx/Desktop/coding/machineLearning/.venv/lib/python3.13/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/vx/Desktop/coding/machineLearning/.venv/lib/python3.13/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/vx/Desktop/coding/machineLearning/.venv/lib/python3.13/site-packages (from aiohttp->datasets) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/vx/Desktop/coding/machineLearning/.venv/lib/python3.13/site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: dateparser>=1.1.2 in /Users/vx/Desktop/coding/machineLearning/.venv/lib/python3.13/site-packages (from htmldate>=1.9.2->trafilatura) (1.2.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/vx/Desktop/coding/machineLearning/.venv/lib/python3.13/site-packages (from openai>=1.1.0->haystack-ai) (4.8.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/vx/Desktop/coding/machineLearning/.venv/lib/python3.13/site-packages (from openai>=1.1.0->haystack-ai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/vx/Desktop/coding/machineLearning/.venv/lib/python3.13/site-packages (from openai>=1.1.0->haystack-ai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/vx/Desktop/coding/machineLearning/.venv/lib/python3.13/site-packages (from openai>=1.1.0->haystack-ai) (0.8.2)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Users/vx/Desktop/coding/machineLearning/.venv/lib/python3.13/site-packages (from openai>=1.1.0->haystack-ai) (2.10.6)\n",
      "Requirement already satisfied: sniffio in /Users/vx/Desktop/coding/machineLearning/.venv/lib/python3.13/site-packages (from openai>=1.1.0->haystack-ai) (1.3.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/vx/Desktop/coding/machineLearning/.venv/lib/python3.13/site-packages (from python-dateutil->haystack-ai) (1.17.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/vx/Desktop/coding/machineLearning/.venv/lib/python3.13/site-packages (from requests->haystack-ai) (3.10)\n",
      "Requirement already satisfied: setuptools in /Users/vx/Desktop/coding/machineLearning/.venv/lib/python3.13/site-packages (from torch>=1.12.1->txtai) (75.8.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /Users/vx/Desktop/coding/machineLearning/.venv/lib/python3.13/site-packages (from torch>=1.12.1->txtai) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/vx/Desktop/coding/machineLearning/.venv/lib/python3.13/site-packages (from sympy==1.13.1->torch>=1.12.1->txtai) (1.3.0)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /Users/vx/Desktop/coding/machineLearning/.venv/lib/python3.13/site-packages (from transformers>=4.45.0->txtai) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/vx/Desktop/coding/machineLearning/.venv/lib/python3.13/site-packages (from transformers>=4.45.0->txtai) (0.5.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/vx/Desktop/coding/machineLearning/.venv/lib/python3.13/site-packages (from jinja2->haystack-ai) (3.0.2)\n",
      "Requirement already satisfied: monotonic>=1.5 in /Users/vx/Desktop/coding/machineLearning/.venv/lib/python3.13/site-packages (from posthog->haystack-ai) (1.6)\n",
      "Requirement already satisfied: backoff>=1.10.0 in /Users/vx/Desktop/coding/machineLearning/.venv/lib/python3.13/site-packages (from posthog->haystack-ai) (2.2.1)\n",
      "Requirement already satisfied: tzlocal in /Users/vx/Desktop/coding/machineLearning/.venv/lib/python3.13/site-packages (from dateparser>=1.1.2->htmldate>=1.9.2->trafilatura) (5.2)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/vx/Desktop/coding/machineLearning/.venv/lib/python3.13/site-packages (from httpx<1,>=0.23.0->openai>=1.1.0->haystack-ai) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/vx/Desktop/coding/machineLearning/.venv/lib/python3.13/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.1.0->haystack-ai) (0.14.0)\n",
      "Requirement already satisfied: lxml-html-clean in /Users/vx/Desktop/coding/machineLearning/.venv/lib/python3.13/site-packages (from lxml[html_clean]>=4.4.2->justext>=3.0.1->trafilatura) (0.4.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/vx/Desktop/coding/machineLearning/.venv/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->openai>=1.1.0->haystack-ai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /Users/vx/Desktop/coding/machineLearning/.venv/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->openai>=1.1.0->haystack-ai) (2.27.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install haystack-ai trafilatura datasets txtai pandas datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "os.environ[\"OPENAI_API_KEY\"] = api_key  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dataset usado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vx/Desktop/coding/machineLearning/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"rajpurkar/squad_v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 130319\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 11873\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_df = pd.DataFrame(dataset['train'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>answers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>56be85543aeaaa14008c9063</td>\n",
       "      <td>Beyoncé</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "      <td>When did Beyonce start becoming popular?</td>\n",
       "      <td>{'text': ['in the late 1990s'], 'answer_start'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>56be85543aeaaa14008c9065</td>\n",
       "      <td>Beyoncé</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "      <td>What areas did Beyonce compete in when she was...</td>\n",
       "      <td>{'text': ['singing and dancing'], 'answer_star...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>56be85543aeaaa14008c9066</td>\n",
       "      <td>Beyoncé</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "      <td>When did Beyonce leave Destiny's Child and bec...</td>\n",
       "      <td>{'text': ['2003'], 'answer_start': [526]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56bf6b0f3aeaaa14008c9601</td>\n",
       "      <td>Beyoncé</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "      <td>In what city and state did Beyonce  grow up?</td>\n",
       "      <td>{'text': ['Houston, Texas'], 'answer_start': [...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>56bf6b0f3aeaaa14008c9602</td>\n",
       "      <td>Beyoncé</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "      <td>In which decade did Beyonce become famous?</td>\n",
       "      <td>{'text': ['late 1990s'], 'answer_start': [276]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130314</th>\n",
       "      <td>5a7e070b70df9f001a875439</td>\n",
       "      <td>Matter</td>\n",
       "      <td>The term \"matter\" is used throughout physics i...</td>\n",
       "      <td>Physics has broadly agreed on the definition o...</td>\n",
       "      <td>{'text': [], 'answer_start': []}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130315</th>\n",
       "      <td>5a7e070b70df9f001a87543a</td>\n",
       "      <td>Matter</td>\n",
       "      <td>The term \"matter\" is used throughout physics i...</td>\n",
       "      <td>Who coined the term partonic matter?</td>\n",
       "      <td>{'text': [], 'answer_start': []}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130316</th>\n",
       "      <td>5a7e070b70df9f001a87543b</td>\n",
       "      <td>Matter</td>\n",
       "      <td>The term \"matter\" is used throughout physics i...</td>\n",
       "      <td>What is another name for anti-matter?</td>\n",
       "      <td>{'text': [], 'answer_start': []}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130317</th>\n",
       "      <td>5a7e070b70df9f001a87543c</td>\n",
       "      <td>Matter</td>\n",
       "      <td>The term \"matter\" is used throughout physics i...</td>\n",
       "      <td>Matter usually does not need to be used in con...</td>\n",
       "      <td>{'text': [], 'answer_start': []}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130318</th>\n",
       "      <td>5a7e070b70df9f001a87543d</td>\n",
       "      <td>Matter</td>\n",
       "      <td>The term \"matter\" is used throughout physics i...</td>\n",
       "      <td>What field of study has a variety of unusual c...</td>\n",
       "      <td>{'text': [], 'answer_start': []}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>130319 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              id    title  \\\n",
       "0       56be85543aeaaa14008c9063  Beyoncé   \n",
       "1       56be85543aeaaa14008c9065  Beyoncé   \n",
       "2       56be85543aeaaa14008c9066  Beyoncé   \n",
       "3       56bf6b0f3aeaaa14008c9601  Beyoncé   \n",
       "4       56bf6b0f3aeaaa14008c9602  Beyoncé   \n",
       "...                          ...      ...   \n",
       "130314  5a7e070b70df9f001a875439   Matter   \n",
       "130315  5a7e070b70df9f001a87543a   Matter   \n",
       "130316  5a7e070b70df9f001a87543b   Matter   \n",
       "130317  5a7e070b70df9f001a87543c   Matter   \n",
       "130318  5a7e070b70df9f001a87543d   Matter   \n",
       "\n",
       "                                                  context  \\\n",
       "0       Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n",
       "1       Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n",
       "2       Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n",
       "3       Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n",
       "4       Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n",
       "...                                                   ...   \n",
       "130314  The term \"matter\" is used throughout physics i...   \n",
       "130315  The term \"matter\" is used throughout physics i...   \n",
       "130316  The term \"matter\" is used throughout physics i...   \n",
       "130317  The term \"matter\" is used throughout physics i...   \n",
       "130318  The term \"matter\" is used throughout physics i...   \n",
       "\n",
       "                                                 question  \\\n",
       "0                When did Beyonce start becoming popular?   \n",
       "1       What areas did Beyonce compete in when she was...   \n",
       "2       When did Beyonce leave Destiny's Child and bec...   \n",
       "3           In what city and state did Beyonce  grow up?    \n",
       "4              In which decade did Beyonce become famous?   \n",
       "...                                                   ...   \n",
       "130314  Physics has broadly agreed on the definition o...   \n",
       "130315               Who coined the term partonic matter?   \n",
       "130316              What is another name for anti-matter?   \n",
       "130317  Matter usually does not need to be used in con...   \n",
       "130318  What field of study has a variety of unusual c...   \n",
       "\n",
       "                                                  answers  \n",
       "0       {'text': ['in the late 1990s'], 'answer_start'...  \n",
       "1       {'text': ['singing and dancing'], 'answer_star...  \n",
       "2               {'text': ['2003'], 'answer_start': [526]}  \n",
       "3       {'text': ['Houston, Texas'], 'answer_start': [...  \n",
       "4         {'text': ['late 1990s'], 'answer_start': [276]}  \n",
       "...                                                   ...  \n",
       "130314                   {'text': [], 'answer_start': []}  \n",
       "130315                   {'text': [], 'answer_start': []}  \n",
       "130316                   {'text': [], 'answer_start': []}  \n",
       "130317                   {'text': [], 'answer_start': []}  \n",
       "130318                   {'text': [], 'answer_start': []}  \n",
       "\n",
       "[130319 rows x 5 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consolidate_contexts(group):\n",
    "    # Remover duplicatas dentro de um mesmo título\n",
    "    unique_contexts = set(group)\n",
    "    # Juntar os contextos em um único texto corrido\n",
    "    return \" \".join(unique_contexts)\n",
    "\n",
    "contextos_df = data_df.groupby(\"title\")[\"context\"].apply(consolidate_contexts).reset_index()\n",
    "contextos_df.rename(columns={\"context\": \"contextos\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "contextos_df.to_csv(\"contextos_por_titulo.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### haystack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- rag com arquivo simples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from haystack import Pipeline, PredefinedPipeline\n",
    "from dotenv import load_dotenv\n",
    "import urllib.request\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "os.environ[\"OPENAI_API_KEY\"] = api_key\n",
    "\n",
    "# urllib.request.urlretrieve(\"https://archive.org/stream/leonardodavinci00brocrich/leonardodavinci00brocrich_djvu.txt\",\n",
    "#                            \"davinci.txt\")  \n",
    "\n",
    "indexing_pipeline =  Pipeline.from_template(PredefinedPipeline.INDEXING)\n",
    "indexing_pipeline.run(data={\"sources\": ['contextos_por_titulo.csv']})\n",
    "# indexing_pipeline.run(data={\"sources\": [\"davinci.txt\"]})\n",
    "\n",
    "rag_pipeline =  Pipeline.from_template(PredefinedPipeline.RAG)\n",
    "\n",
    "query = \"Which film featured Destiny's Child's first major single?\"\n",
    "result = rag_pipeline.run(data={\"prompt_builder\": {\"query\":query}, \"text_embedder\": {\"text\": query}})\n",
    "print(result[\"llm\"][\"replies\"][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- mesmo rag so que definindo manualmente as pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "\n",
    "from haystack import Pipeline\n",
    "from haystack.document_stores.in_memory import InMemoryDocumentStore\n",
    "from haystack.components.retrievers import InMemoryEmbeddingRetriever\n",
    "from haystack.components.converters import TextFileToDocument\n",
    "from haystack.components.preprocessors import DocumentCleaner, DocumentSplitter\n",
    "from haystack.components.embedders import OpenAIDocumentEmbedder, OpenAITextEmbedder\n",
    "from haystack.components.writers import DocumentWriter\n",
    "from haystack.components.builders import PromptBuilder\n",
    "from haystack.components.generators import OpenAIGenerator\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "os.environ[\"OPENAI_API_KEY\"] = api_key\n",
    "urllib.request.urlretrieve(\"https://archive.org/stream/leonardodavinci00brocrich/leonardodavinci00brocrich_djvu.txt\",\n",
    "                           \"davinci.txt\")    \n",
    "\n",
    "document_store = InMemoryDocumentStore()\n",
    "\n",
    "text_file_converter = TextFileToDocument()\n",
    "cleaner = DocumentCleaner()\n",
    "splitter = DocumentSplitter()\n",
    "embedder = OpenAIDocumentEmbedder()\n",
    "writer = DocumentWriter(document_store)\n",
    "\n",
    "indexing_pipeline = Pipeline()\n",
    "indexing_pipeline.add_component(\"converter\", text_file_converter)\n",
    "indexing_pipeline.add_component(\"cleaner\", cleaner)\n",
    "indexing_pipeline.add_component(\"splitter\", splitter)\n",
    "indexing_pipeline.add_component(\"embedder\", embedder)\n",
    "indexing_pipeline.add_component(\"writer\", writer)\n",
    "\n",
    "indexing_pipeline.connect(\"converter.documents\", \"cleaner.documents\")\n",
    "indexing_pipeline.connect(\"cleaner.documents\", \"splitter.documents\")\n",
    "indexing_pipeline.connect(\"splitter.documents\", \"embedder.documents\")\n",
    "indexing_pipeline.connect(\"embedder.documents\", \"writer.documents\")\n",
    "indexing_pipeline.run(data={\"sources\": [\"davinci.txt\"]})\n",
    "\n",
    "text_embedder = OpenAITextEmbedder()\n",
    "retriever = InMemoryEmbeddingRetriever(document_store)\n",
    "template = \"\"\"Given these documents, answer the question.\n",
    "                Documents:\n",
    "                {% for doc in documents %}\n",
    "                    {{ doc.content }}\n",
    "                {% endfor %}\n",
    "                Question: {{query}}\n",
    "                Answer:\"\"\"\n",
    "prompt_builder = PromptBuilder(template=template)\n",
    "llm = OpenAIGenerator()\n",
    "\n",
    "rag_pipeline = Pipeline()\n",
    "rag_pipeline.add_component(\"text_embedder\", text_embedder)\n",
    "rag_pipeline.add_component(\"retriever\", retriever)\n",
    "rag_pipeline.add_component(\"prompt_builder\", prompt_builder)\n",
    "rag_pipeline.add_component(\"llm\", llm)\n",
    "\n",
    "rag_pipeline.connect(\"text_embedder.embedding\", \"retriever.query_embedding\")\n",
    "rag_pipeline.connect(\"retriever.documents\", \"prompt_builder.documents\")\n",
    "rag_pipeline.connect(\"prompt_builder\", \"llm\")\n",
    "\n",
    "query = \"Com quantos anos o leonardo foi de arrasta pra cima?\"\n",
    "result = rag_pipeline.run(data={\"prompt_builder\": {\"query\":query}, \"text_embedder\": {\"text\": query}})\n",
    "print(result[\"llm\"][\"replies\"][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### haystack usando meu dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack import Pipeline, PredefinedPipeline\n",
    "\n",
    "indexing_pipeline =  Pipeline.from_template(PredefinedPipeline.INDEXING)\n",
    "indexing_pipeline.run(data={\"sources\": ['contextos_por_titulo.csv']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_pipeline =  Pipeline.from_template(PredefinedPipeline.RAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "query = \"What sect of Buddhism is the only remaining one based in Sanskrit?\"\n",
    "result = rag_pipeline.run(data={\"prompt_builder\": {\"query\":query}, \"text_embedder\": {\"text\": query}})\n",
    "print(result[\"llm\"][\"replies\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from haystack import Pipeline\n",
    "from haystack.document_stores.in_memory import InMemoryDocumentStore\n",
    "from haystack.components.retrievers import InMemoryEmbeddingRetriever\n",
    "from haystack.components.converters import TextFileToDocument\n",
    "from haystack.components.preprocessors import DocumentCleaner, DocumentSplitter\n",
    "from haystack.components.embedders import OpenAIDocumentEmbedder, OpenAITextEmbedder\n",
    "from haystack.components.writers import DocumentWriter\n",
    "from haystack.components.builders import PromptBuilder\n",
    "from haystack.components.generators import OpenAIGenerator\n",
    "\n",
    "document_store = InMemoryDocumentStore()\n",
    "\n",
    "text_file_converter = TextFileToDocument()\n",
    "cleaner = DocumentCleaner()\n",
    "splitter = DocumentSplitter()\n",
    "embedder = OpenAIDocumentEmbedder()\n",
    "writer = DocumentWriter(document_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexing_pipeline = Pipeline()\n",
    "indexing_pipeline.add_component(\"converter\", text_file_converter)\n",
    "indexing_pipeline.add_component(\"cleaner\", cleaner)\n",
    "indexing_pipeline.add_component(\"splitter\", splitter)\n",
    "indexing_pipeline.add_component(\"embedder\", embedder)\n",
    "indexing_pipeline.add_component(\"writer\", writer)\n",
    "\n",
    "indexing_pipeline.connect(\"converter.documents\", \"cleaner.documents\")\n",
    "indexing_pipeline.connect(\"cleaner.documents\", \"splitter.documents\")\n",
    "indexing_pipeline.connect(\"splitter.documents\", \"embedder.documents\")\n",
    "indexing_pipeline.connect(\"embedder.documents\", \"writer.documents\")\n",
    "indexing_pipeline.run(data={\"sources\": [\"contextos_por_titulo.csv\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "text_embedder = OpenAITextEmbedder()\n",
    "retriever = InMemoryEmbeddingRetriever(document_store)\n",
    "template = \"\"\"Given these documents, answer the question.\n",
    "                Documents:\n",
    "                {% for doc in documents %}\n",
    "                    {{ doc.content }}\n",
    "                {% endfor %}\n",
    "                Question: {{query}}\n",
    "                Answer:\"\"\"\n",
    "prompt_builder = PromptBuilder(template=template)\n",
    "llm = OpenAIGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_pipeline = Pipeline()\n",
    "rag_pipeline.add_component(\"text_embedder\", text_embedder)\n",
    "rag_pipeline.add_component(\"retriever\", retriever)\n",
    "rag_pipeline.add_component(\"prompt_builder\", prompt_builder)\n",
    "rag_pipeline.add_component(\"llm\", llm)\n",
    "\n",
    "rag_pipeline.connect(\"text_embedder.embedding\", \"retriever.query_embedding\")\n",
    "rag_pipeline.connect(\"retriever.documents\", \"prompt_builder.documents\")\n",
    "rag_pipeline.connect(\"prompt_builder\", \"llm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Which film featured Destiny's Child's first major single?\"\n",
    "result = rag_pipeline.run(data={\"prompt_builder\": {\"query\":query}, \"text_embedder\": {\"text\": query}})\n",
    "print(result[\"llm\"][\"replies\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### txtai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /Users/vx/Desktop/coding/machineLearning/.venv/lib/python3.13/site-packages (2.6.0)\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.21.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (6.1 kB)\n",
      "Collecting torchaudio\n",
      "  Downloading torchaudio-2.6.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: txtai in /Users/vx/Desktop/coding/machineLearning/.venv/lib/python3.13/site-packages (8.2.0)\n",
      "Collecting autoawq\n",
      "  Using cached autoawq-0.2.8.tar.gz (71 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[23 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m Traceback (most recent call last):\n",
      "  \u001b[31m   \u001b[0m   File \u001b[35m\"/Users/vx/Desktop/coding/machineLearning/.venv/lib/python3.13/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\"\u001b[0m, line \u001b[35m389\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     \u001b[31mmain\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     \u001b[31m~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  \u001b[31m   \u001b[0m   File \u001b[35m\"/Users/vx/Desktop/coding/machineLearning/.venv/lib/python3.13/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\"\u001b[0m, line \u001b[35m373\u001b[0m, in \u001b[35mmain\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     json_out[\"return_val\"] = \u001b[31mhook\u001b[0m\u001b[1;31m(**hook_input[\"kwargs\"])\u001b[0m\n",
      "  \u001b[31m   \u001b[0m                              \u001b[31m~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  \u001b[31m   \u001b[0m   File \u001b[35m\"/Users/vx/Desktop/coding/machineLearning/.venv/lib/python3.13/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\"\u001b[0m, line \u001b[35m143\u001b[0m, in \u001b[35mget_requires_for_build_wheel\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     return hook(config_settings)\n",
      "  \u001b[31m   \u001b[0m   File \u001b[35m\"/private/var/folders/m5/gxpxpsfx2y5g_0qfn36b5jcw0000gn/T/pip-build-env-zvr1sorq/overlay/lib/python3.13/site-packages/setuptools/build_meta.py\"\u001b[0m, line \u001b[35m334\u001b[0m, in \u001b[35mget_requires_for_build_wheel\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     return \u001b[31mself._get_build_requires\u001b[0m\u001b[1;31m(config_settings, requirements=[])\u001b[0m\n",
      "  \u001b[31m   \u001b[0m            \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  \u001b[31m   \u001b[0m   File \u001b[35m\"/private/var/folders/m5/gxpxpsfx2y5g_0qfn36b5jcw0000gn/T/pip-build-env-zvr1sorq/overlay/lib/python3.13/site-packages/setuptools/build_meta.py\"\u001b[0m, line \u001b[35m304\u001b[0m, in \u001b[35m_get_build_requires\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     \u001b[31mself.run_setup\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     \u001b[31m~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  \u001b[31m   \u001b[0m   File \u001b[35m\"/private/var/folders/m5/gxpxpsfx2y5g_0qfn36b5jcw0000gn/T/pip-build-env-zvr1sorq/overlay/lib/python3.13/site-packages/setuptools/build_meta.py\"\u001b[0m, line \u001b[35m522\u001b[0m, in \u001b[35mrun_setup\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     \u001b[31msuper().run_setup\u001b[0m\u001b[1;31m(setup_script=setup_script)\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     \u001b[31m~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  \u001b[31m   \u001b[0m   File \u001b[35m\"/private/var/folders/m5/gxpxpsfx2y5g_0qfn36b5jcw0000gn/T/pip-build-env-zvr1sorq/overlay/lib/python3.13/site-packages/setuptools/build_meta.py\"\u001b[0m, line \u001b[35m320\u001b[0m, in \u001b[35mrun_setup\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     \u001b[31mexec\u001b[0m\u001b[1;31m(code, locals())\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     \u001b[31m~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  \u001b[31m   \u001b[0m   File \u001b[35m\"<string>\"\u001b[0m, line \u001b[35m2\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "  \u001b[31m   \u001b[0m \u001b[1;35mModuleNotFoundError\u001b[0m: \u001b[35mNo module named 'torch'\u001b[0m\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[?25h\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
      "\u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "\u001b[31m╰─>\u001b[0m See above for output.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch torchvision torchaudio txtai autoawq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from txtai import Embeddings, RAG, LLM\n",
    "from txtai.pipeline import Extractor\n",
    "\n",
    "embeddings = Embeddings(path=\"sentence-transformers/nli-mpnet-base-v2\", content=True, autoid=\"uuid5\")\n",
    "embeddings.index(contextos_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps\n"
     ]
    }
   ],
   "source": [
    "extractor = Extractor(embeddings, \"google/flan-t5-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Who killed the Shakyas?\"\n",
    "context = lambda question: [{\"query\": question, \"question\": f\"Answer the following question using the context below. \\n Question: {question} \\n Context:\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_query = extractor(context(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps\n"
     ]
    }
   ],
   "source": [
    "rag = RAG(embeddings, \"google/flan-t5-large\", template=\"\"\"\n",
    "    Answer the following question using the provided context.\n",
    "\n",
    "    Question:\n",
    "    {question}\n",
    "\n",
    "    Context:\n",
    "    {context}\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': 'el hombre'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag(\"Who killed the Shakyas?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
