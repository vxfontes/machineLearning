{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting haystack-ai\n",
      "  Using cached haystack_ai-2.8.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: trafilatura in c:\\users\\nessa\\onedrive\\desktop\\coding\\machinelearning\\.venv\\lib\\site-packages (2.0.0)\n",
      "Requirement already satisfied: datasets in c:\\users\\nessa\\onedrive\\desktop\\coding\\machinelearning\\.venv\\lib\\site-packages (3.2.0)\n",
      "Requirement already satisfied: txtai in c:\\users\\nessa\\onedrive\\desktop\\coding\\machinelearning\\.venv\\lib\\site-packages (8.1.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\nessa\\onedrive\\desktop\\coding\\machinelearning\\.venv\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: haystack-experimental in c:\\users\\nessa\\onedrive\\desktop\\coding\\machinelearning\\.venv\\lib\\site-packages (from haystack-ai) (0.4.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\nessa\\onedrive\\desktop\\coding\\machinelearning\\.venv\\lib\\site-packages (from haystack-ai) (3.1.4)\n",
      "Requirement already satisfied: lazy-imports in c:\\users\\nessa\\onedrive\\desktop\\coding\\machinelearning\\.venv\\lib\\site-packages (from haystack-ai) (0.4.0)\n",
      "Requirement already satisfied: more-itertools in c:\\users\\nessa\\onedrive\\desktop\\coding\\machinelearning\\.venv\\lib\\site-packages (from haystack-ai) (10.5.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\nessa\\onedrive\\desktop\\coding\\machinelearning\\.venv\\lib\\site-packages (from haystack-ai) (3.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\nessa\\onedrive\\desktop\\coding\\machinelearning\\.venv\\lib\\site-packages (from haystack-ai) (1.26.4)\n",
      "Requirement already satisfied: openai>=1.1.0 in c:\\users\\nessa\\onedrive\\desktop\\coding\\machinelearning\\.venv\\lib\\site-packages (from haystack-ai) (1.51.0)\n",
      "Requirement already satisfied: posthog in c:\\users\\nessa\\onedrive\\desktop\\coding\\machinelearning\\.venv\\lib\\site-packages (from haystack-ai) (3.5.0)\n",
      "Requirement already satisfied: python-dateutil in c:\\users\\nessa\\onedrive\\desktop\\coding\\machinelearning\\.venv\\lib\\site-packages (from haystack-ai) (2.9.0.post0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\nessa\\onedrive\\desktop\\coding\\machinelearning\\.venv\\lib\\site-packages (from haystack-ai) (6.0.1)\n",
      "Requirement already satisfied: requests in c:\\users\\nessa\\onedrive\\desktop\\coding\\machinelearning\\.venv\\lib\\site-packages (from haystack-ai) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0 in c:\\users\\nessa\\onedrive\\desktop\\coding\\machinelearning\\.venv\\lib\\site-packages (from haystack-ai) (8.2.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\nessa\\onedrive\\desktop\\coding\\machinelearning\\.venv\\lib\\site-packages (from haystack-ai) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\nessa\\onedrive\\desktop\\coding\\machinelearning\\.venv\\lib\\site-packages (from haystack-ai) (4.11.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\nessa\\onedrive\\desktop\\coding\\machinelearning\\.venv\\lib\\site-packages (from trafilatura) (2024.2.2)\n",
      "Requirement already satisfied: charset_normalizer>=3.4.0 in c:\\users\\nessa\\onedrive\\desktop\\coding\\machinelearning\\.venv\\lib\\site-packages (from trafilatura) (3.4.0)\n",
      "Requirement already satisfied: courlan>=1.3.2 in c:\\users\\nessa\\onedrive\\desktop\\coding\\machinelearning\\.venv\\lib\\site-packages (from trafilatura) (1.3.2)\n",
      "Requirement already satisfied: htmldate>=1.9.2 in c:\\users\\nessa\\onedrive\\desktop\\coding\\machinelearning\\.venv\\lib\\site-packages (from trafilatura) (1.9.2)\n",
      "Requirement already satisfied: justext>=3.0.1 in c:\\users\\nessa\\onedrive\\desktop\\coding\\machinelearning\\.venv\\lib\\site-packages (from trafilatura) (3.0.1)\n",
      "Requirement already satisfied: lxml>=5.3.0 in c:\\users\\nessa\\onedrive\\desktop\\coding\\machinelearning\\.venv\\lib\\site-packages (from trafilatura) (5.3.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in c:\\users\\nessa\\onedrive\\desktop\\coding\\machinelearning\\.venv\\lib\\site-packages (from trafilatura) (1.26.18)\n",
      "Requirement already satisfied: filelock in c:\\users\\nessa\\onedrive\\desktop\\coding\\machinelearning\\.venv\\lib\\site-packages (from datasets) (3.14.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\nessa\\onedrive\\desktop\\coding\\machinelearning\\.venv\\lib\\site-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\nessa\\onedrive\\desktop\\coding\\machinelearning\\.venv\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: xxhash in c:\\users\\nessa\\onedrive\\desktop\\coding\\machinelearning\\.venv\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\nessa\\onedrive\\desktop\\coding\\machinelearning\\.venv\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in c:\\users\\nessa\\onedrive\\desktop\\coding\\machinelearning\\.venv\\lib\\site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.3.1)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\nessa\\onedrive\\desktop\\coding\\machinelearning\\.venv\\lib\\site-packages (from datasets) (3.9.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in c:\\users\\nessa\\onedrive\\desktop\\coding\\machinelearning\\.venv\\lib\\site-packages (from datasets) (0.27.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\nessa\\onedrive\\desktop\\coding\\machinelearning\\.venv\\lib\\site-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: faiss-cpu>=1.7.1.post2 in c:\\users\\nessa\\onedrive\\desktop\\coding\\machinelearning\\.venv\\lib\\site-packages (from txtai) (1.9.0.post1)\n",
      "Requirement already satisfied: msgpack>=1.0.7 in c:\\users\\nessa\\onedrive\\desktop\\coding\\machinelearning\\.venv\\lib\\site-packages (from txtai) (1.1.0)\n",
      "Requirement already satisfied: torch>=1.12.1 in c:\\users\\nessa\\onedrive\\desktop\\coding\\machinelearning\\.venv\\lib\\site-packages (from txtai) (2.3.0)\n",
      "Requirement already satisfied: transformers>=4.45.0 in c:\\users\\nessa\\onedrive\\desktop\\coding\\machinelearning\\.venv\\lib\\site-packages (from txtai) (4.47.1)\n",
      "Requirement already satisfied: regex>=2022.8.17 in c:\\users\\nessa\\onedrive\\desktop\\coding\\machinelearning\\.venv\\lib\\site-packages (from txtai) (2024.5.10)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\nessa\\onedrive\\desktop\\coding\\machinelearning\\.venv\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\nessa\\onedrive\\desktop\\coding\\machinelearning\\.venv\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: babel>=2.16.0 in c:\\users\\nessa\\onedrive\\desktop\\coding\\machinelearning\\.venv\\lib\\site-packages (from courlan>=1.3.2->trafilatura) (2.16.0)\n",
      "Requirement already satisfied: tld>=0.13 in c:\\users\\nessa\\onedrive\\desktop\\coding\\machinelearning\\.venv\\lib\\site-packages (from courlan>=1.3.2->trafilatura) (0.13)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\nessa\\onedrive\\desktop\\coding\\machinelearning\\.venv\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\nessa\\onedrive\\desktop\\coding\\machinelearning\\.venv\\lib\\site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\nessa\\onedrive\\desktop\\coding\\machinelearning\\.venv\\lib\\site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\nessa\\onedrive\\desktop\\coding\\machinelearning\\.venv\\lib\\site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\nessa\\onedrive\\desktop\\coding\\machinelearning\\.venv\\lib\\site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: dateparser>=1.1.2 in c:\\users\\nessa\\onedrive\\desktop\\coding\\machinelearning\\.venv\\lib\\site-packages (from htmldate>=1.9.2->trafilatura) (1.2.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\nessa\\onedrive\\desktop\\coding\\machinelearning\\.venv\\lib\\site-packages (from openai>=1.1.0->haystack-ai) (4.3.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\nessa\\onedrive\\desktop\\coding\\machinelearning\\.venv\\lib\\site-packages (from openai>=1.1.0->haystack-ai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\nessa\\onedrive\\desktop\\coding\\machinelearning\\.venv\\lib\\site-packages (from openai>=1.1.0->haystack-ai) (0.27.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\nessa\\onedrive\\desktop\\coding\\machinelearning\\.venv\\lib\\site-packages (from openai>=1.1.0->haystack-ai) (0.6.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\nessa\\onedrive\\desktop\\coding\\machinelearning\\.venv\\lib\\site-packages (from openai>=1.1.0->haystack-ai) (2.7.1)\n",
      "Requirement already satisfied: sniffio in c:\\users\\nessa\\onedrive\\desktop\\coding\\machinelearning\\.venv\\lib\\site-packages (from openai>=1.1.0->haystack-ai) (1.3.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\nessa\\onedrive\\desktop\\coding\\machinelearning\\.venv\\lib\\site-packages (from python-dateutil->haystack-ai) (1.16.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\nessa\\onedrive\\desktop\\coding\\machinelearning\\.venv\\lib\\site-packages (from requests->haystack-ai) (3.7)\n",
      "Requirement already satisfied: sympy in c:\\users\\nessa\\onedrive\\desktop\\coding\\machinelearning\\.venv\\lib\\site-packages (from torch>=1.12.1->txtai) (1.12)\n",
      "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\nessa\\onedrive\\desktop\\coding\\machinelearning\\.venv\\lib\\site-packages (from torch>=1.12.1->txtai) (2021.4.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\nessa\\onedrive\\desktop\\coding\\machinelearning\\.venv\\lib\\site-packages (from tqdm->haystack-ai) (0.4.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\nessa\\onedrive\\desktop\\coding\\machinelearning\\.venv\\lib\\site-packages (from transformers>=4.45.0->txtai) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\nessa\\onedrive\\desktop\\coding\\machinelearning\\.venv\\lib\\site-packages (from transformers>=4.45.0->txtai) (0.4.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\nessa\\onedrive\\desktop\\coding\\machinelearning\\.venv\\lib\\site-packages (from jinja2->haystack-ai) (2.1.5)\n",
      "Requirement already satisfied: monotonic>=1.5 in c:\\users\\nessa\\onedrive\\desktop\\coding\\machinelearning\\.venv\\lib\\site-packages (from posthog->haystack-ai) (1.6)\n",
      "Requirement already satisfied: backoff>=1.10.0 in c:\\users\\nessa\\onedrive\\desktop\\coding\\machinelearning\\.venv\\lib\\site-packages (from posthog->haystack-ai) (2.2.1)\n",
      "Requirement already satisfied: tzlocal in c:\\users\\nessa\\onedrive\\desktop\\coding\\machinelearning\\.venv\\lib\\site-packages (from dateparser>=1.1.2->htmldate>=1.9.2->trafilatura) (5.2)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\nessa\\onedrive\\desktop\\coding\\machinelearning\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->openai>=1.1.0->haystack-ai) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\nessa\\onedrive\\desktop\\coding\\machinelearning\\.venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.1.0->haystack-ai) (0.14.0)\n",
      "Requirement already satisfied: lxml-html-clean in c:\\users\\nessa\\onedrive\\desktop\\coding\\machinelearning\\.venv\\lib\\site-packages (from lxml[html_clean]>=4.4.2->justext>=3.0.1->trafilatura) (0.4.1)\n",
      "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\nessa\\onedrive\\desktop\\coding\\machinelearning\\.venv\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=1.12.1->txtai) (2021.4.0)\n",
      "Requirement already satisfied: tbb==2021.* in c:\\users\\nessa\\onedrive\\desktop\\coding\\machinelearning\\.venv\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=1.12.1->txtai) (2021.12.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\nessa\\onedrive\\desktop\\coding\\machinelearning\\.venv\\lib\\site-packages (from pydantic<3,>=1.9.0->openai>=1.1.0->haystack-ai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.2 in c:\\users\\nessa\\onedrive\\desktop\\coding\\machinelearning\\.venv\\lib\\site-packages (from pydantic<3,>=1.9.0->openai>=1.1.0->haystack-ai) (2.18.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\nessa\\onedrive\\desktop\\coding\\machinelearning\\.venv\\lib\\site-packages (from sympy->torch>=1.12.1->txtai) (1.3.0)\n",
      "Using cached haystack_ai-2.8.0-py3-none-any.whl (391 kB)\n",
      "Installing collected packages: haystack-ai\n",
      "Successfully installed haystack-ai-2.8.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install haystack-ai trafilatura datasets txtai pandas datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "os.environ[\"OPENAI_API_KEY\"] = api_key  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dataset usado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"rajpurkar/squad_v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 130319\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 11873\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_df = pd.DataFrame(dataset['train'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>answers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>56be85543aeaaa14008c9063</td>\n",
       "      <td>Beyonc√©</td>\n",
       "      <td>Beyonc√© Giselle Knowles-Carter (/biÀêÀàj…ínse…™/ b...</td>\n",
       "      <td>When did Beyonce start becoming popular?</td>\n",
       "      <td>{'text': ['in the late 1990s'], 'answer_start'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>56be85543aeaaa14008c9065</td>\n",
       "      <td>Beyonc√©</td>\n",
       "      <td>Beyonc√© Giselle Knowles-Carter (/biÀêÀàj…ínse…™/ b...</td>\n",
       "      <td>What areas did Beyonce compete in when she was...</td>\n",
       "      <td>{'text': ['singing and dancing'], 'answer_star...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>56be85543aeaaa14008c9066</td>\n",
       "      <td>Beyonc√©</td>\n",
       "      <td>Beyonc√© Giselle Knowles-Carter (/biÀêÀàj…ínse…™/ b...</td>\n",
       "      <td>When did Beyonce leave Destiny's Child and bec...</td>\n",
       "      <td>{'text': ['2003'], 'answer_start': [526]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56bf6b0f3aeaaa14008c9601</td>\n",
       "      <td>Beyonc√©</td>\n",
       "      <td>Beyonc√© Giselle Knowles-Carter (/biÀêÀàj…ínse…™/ b...</td>\n",
       "      <td>In what city and state did Beyonce  grow up?</td>\n",
       "      <td>{'text': ['Houston, Texas'], 'answer_start': [...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>56bf6b0f3aeaaa14008c9602</td>\n",
       "      <td>Beyonc√©</td>\n",
       "      <td>Beyonc√© Giselle Knowles-Carter (/biÀêÀàj…ínse…™/ b...</td>\n",
       "      <td>In which decade did Beyonce become famous?</td>\n",
       "      <td>{'text': ['late 1990s'], 'answer_start': [276]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130314</th>\n",
       "      <td>5a7e070b70df9f001a875439</td>\n",
       "      <td>Matter</td>\n",
       "      <td>The term \"matter\" is used throughout physics i...</td>\n",
       "      <td>Physics has broadly agreed on the definition o...</td>\n",
       "      <td>{'text': [], 'answer_start': []}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130315</th>\n",
       "      <td>5a7e070b70df9f001a87543a</td>\n",
       "      <td>Matter</td>\n",
       "      <td>The term \"matter\" is used throughout physics i...</td>\n",
       "      <td>Who coined the term partonic matter?</td>\n",
       "      <td>{'text': [], 'answer_start': []}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130316</th>\n",
       "      <td>5a7e070b70df9f001a87543b</td>\n",
       "      <td>Matter</td>\n",
       "      <td>The term \"matter\" is used throughout physics i...</td>\n",
       "      <td>What is another name for anti-matter?</td>\n",
       "      <td>{'text': [], 'answer_start': []}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130317</th>\n",
       "      <td>5a7e070b70df9f001a87543c</td>\n",
       "      <td>Matter</td>\n",
       "      <td>The term \"matter\" is used throughout physics i...</td>\n",
       "      <td>Matter usually does not need to be used in con...</td>\n",
       "      <td>{'text': [], 'answer_start': []}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130318</th>\n",
       "      <td>5a7e070b70df9f001a87543d</td>\n",
       "      <td>Matter</td>\n",
       "      <td>The term \"matter\" is used throughout physics i...</td>\n",
       "      <td>What field of study has a variety of unusual c...</td>\n",
       "      <td>{'text': [], 'answer_start': []}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>130319 rows √ó 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              id    title  \\\n",
       "0       56be85543aeaaa14008c9063  Beyonc√©   \n",
       "1       56be85543aeaaa14008c9065  Beyonc√©   \n",
       "2       56be85543aeaaa14008c9066  Beyonc√©   \n",
       "3       56bf6b0f3aeaaa14008c9601  Beyonc√©   \n",
       "4       56bf6b0f3aeaaa14008c9602  Beyonc√©   \n",
       "...                          ...      ...   \n",
       "130314  5a7e070b70df9f001a875439   Matter   \n",
       "130315  5a7e070b70df9f001a87543a   Matter   \n",
       "130316  5a7e070b70df9f001a87543b   Matter   \n",
       "130317  5a7e070b70df9f001a87543c   Matter   \n",
       "130318  5a7e070b70df9f001a87543d   Matter   \n",
       "\n",
       "                                                  context  \\\n",
       "0       Beyonc√© Giselle Knowles-Carter (/biÀêÀàj…ínse…™/ b...   \n",
       "1       Beyonc√© Giselle Knowles-Carter (/biÀêÀàj…ínse…™/ b...   \n",
       "2       Beyonc√© Giselle Knowles-Carter (/biÀêÀàj…ínse…™/ b...   \n",
       "3       Beyonc√© Giselle Knowles-Carter (/biÀêÀàj…ínse…™/ b...   \n",
       "4       Beyonc√© Giselle Knowles-Carter (/biÀêÀàj…ínse…™/ b...   \n",
       "...                                                   ...   \n",
       "130314  The term \"matter\" is used throughout physics i...   \n",
       "130315  The term \"matter\" is used throughout physics i...   \n",
       "130316  The term \"matter\" is used throughout physics i...   \n",
       "130317  The term \"matter\" is used throughout physics i...   \n",
       "130318  The term \"matter\" is used throughout physics i...   \n",
       "\n",
       "                                                 question  \\\n",
       "0                When did Beyonce start becoming popular?   \n",
       "1       What areas did Beyonce compete in when she was...   \n",
       "2       When did Beyonce leave Destiny's Child and bec...   \n",
       "3           In what city and state did Beyonce  grow up?    \n",
       "4              In which decade did Beyonce become famous?   \n",
       "...                                                   ...   \n",
       "130314  Physics has broadly agreed on the definition o...   \n",
       "130315               Who coined the term partonic matter?   \n",
       "130316              What is another name for anti-matter?   \n",
       "130317  Matter usually does not need to be used in con...   \n",
       "130318  What field of study has a variety of unusual c...   \n",
       "\n",
       "                                                  answers  \n",
       "0       {'text': ['in the late 1990s'], 'answer_start'...  \n",
       "1       {'text': ['singing and dancing'], 'answer_star...  \n",
       "2               {'text': ['2003'], 'answer_start': [526]}  \n",
       "3       {'text': ['Houston, Texas'], 'answer_start': [...  \n",
       "4         {'text': ['late 1990s'], 'answer_start': [276]}  \n",
       "...                                                   ...  \n",
       "130314                   {'text': [], 'answer_start': []}  \n",
       "130315                   {'text': [], 'answer_start': []}  \n",
       "130316                   {'text': [], 'answer_start': []}  \n",
       "130317                   {'text': [], 'answer_start': []}  \n",
       "130318                   {'text': [], 'answer_start': []}  \n",
       "\n",
       "[130319 rows x 5 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consolidate_contexts(group):\n",
    "    # Remover duplicatas dentro de um mesmo t√≠tulo\n",
    "    unique_contexts = set(group)\n",
    "    # Juntar os contextos em um √∫nico texto corrido\n",
    "    return \" \".join(unique_contexts)\n",
    "\n",
    "contextos_df = data_df.groupby(\"title\")[\"context\"].apply(consolidate_contexts).reset_index()\n",
    "contextos_df.rename(columns={\"context\": \"contextos\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "contextos_df.to_csv(\"contextos_por_titulo.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### haystack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- rag com arquivo simples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from haystack import Pipeline, PredefinedPipeline\n",
    "from dotenv import load_dotenv\n",
    "import urllib.request\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "os.environ[\"OPENAI_API_KEY\"] = api_key\n",
    "\n",
    "# urllib.request.urlretrieve(\"https://archive.org/stream/leonardodavinci00brocrich/leonardodavinci00brocrich_djvu.txt\",\n",
    "#                            \"davinci.txt\")  \n",
    "\n",
    "indexing_pipeline =  Pipeline.from_template(PredefinedPipeline.INDEXING)\n",
    "indexing_pipeline.run(data={\"sources\": ['contextos_por_titulo.csv']})\n",
    "# indexing_pipeline.run(data={\"sources\": [\"davinci.txt\"]})\n",
    "\n",
    "rag_pipeline =  Pipeline.from_template(PredefinedPipeline.RAG)\n",
    "\n",
    "query = \"Which film featured Destiny's Child's first major single?\"\n",
    "result = rag_pipeline.run(data={\"prompt_builder\": {\"query\":query}, \"text_embedder\": {\"text\": query}})\n",
    "print(result[\"llm\"][\"replies\"][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- mesmo rag so que definindo manualmente as pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "\n",
    "from haystack import Pipeline\n",
    "from haystack.document_stores.in_memory import InMemoryDocumentStore\n",
    "from haystack.components.retrievers import InMemoryEmbeddingRetriever\n",
    "from haystack.components.converters import TextFileToDocument\n",
    "from haystack.components.preprocessors import DocumentCleaner, DocumentSplitter\n",
    "from haystack.components.embedders import OpenAIDocumentEmbedder, OpenAITextEmbedder\n",
    "from haystack.components.writers import DocumentWriter\n",
    "from haystack.components.builders import PromptBuilder\n",
    "from haystack.components.generators import OpenAIGenerator\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "os.environ[\"OPENAI_API_KEY\"] = api_key\n",
    "urllib.request.urlretrieve(\"https://archive.org/stream/leonardodavinci00brocrich/leonardodavinci00brocrich_djvu.txt\",\n",
    "                           \"davinci.txt\")    \n",
    "\n",
    "document_store = InMemoryDocumentStore()\n",
    "\n",
    "text_file_converter = TextFileToDocument()\n",
    "cleaner = DocumentCleaner()\n",
    "splitter = DocumentSplitter()\n",
    "embedder = OpenAIDocumentEmbedder()\n",
    "writer = DocumentWriter(document_store)\n",
    "\n",
    "indexing_pipeline = Pipeline()\n",
    "indexing_pipeline.add_component(\"converter\", text_file_converter)\n",
    "indexing_pipeline.add_component(\"cleaner\", cleaner)\n",
    "indexing_pipeline.add_component(\"splitter\", splitter)\n",
    "indexing_pipeline.add_component(\"embedder\", embedder)\n",
    "indexing_pipeline.add_component(\"writer\", writer)\n",
    "\n",
    "indexing_pipeline.connect(\"converter.documents\", \"cleaner.documents\")\n",
    "indexing_pipeline.connect(\"cleaner.documents\", \"splitter.documents\")\n",
    "indexing_pipeline.connect(\"splitter.documents\", \"embedder.documents\")\n",
    "indexing_pipeline.connect(\"embedder.documents\", \"writer.documents\")\n",
    "indexing_pipeline.run(data={\"sources\": [\"davinci.txt\"]})\n",
    "\n",
    "text_embedder = OpenAITextEmbedder()\n",
    "retriever = InMemoryEmbeddingRetriever(document_store)\n",
    "template = \"\"\"Given these documents, answer the question.\n",
    "                Documents:\n",
    "                {% for doc in documents %}\n",
    "                    {{ doc.content }}\n",
    "                {% endfor %}\n",
    "                Question: {{query}}\n",
    "                Answer:\"\"\"\n",
    "prompt_builder = PromptBuilder(template=template)\n",
    "llm = OpenAIGenerator()\n",
    "\n",
    "rag_pipeline = Pipeline()\n",
    "rag_pipeline.add_component(\"text_embedder\", text_embedder)\n",
    "rag_pipeline.add_component(\"retriever\", retriever)\n",
    "rag_pipeline.add_component(\"prompt_builder\", prompt_builder)\n",
    "rag_pipeline.add_component(\"llm\", llm)\n",
    "\n",
    "rag_pipeline.connect(\"text_embedder.embedding\", \"retriever.query_embedding\")\n",
    "rag_pipeline.connect(\"retriever.documents\", \"prompt_builder.documents\")\n",
    "rag_pipeline.connect(\"prompt_builder\", \"llm\")\n",
    "\n",
    "query = \"Com quantos anos o leonardo foi de arrasta pra cima?\"\n",
    "result = rag_pipeline.run(data={\"prompt_builder\": {\"query\":query}, \"text_embedder\": {\"text\": query}})\n",
    "print(result[\"llm\"][\"replies\"][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### haystack usando meu dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating embeddings: 347it [07:19,  1.27s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'embedder': {'meta': {'model': 'text-embedding-ada-002',\n",
       "   'usage': {'prompt_tokens': 2963067, 'total_tokens': 2963067}}},\n",
       " 'writer': {'documents_written': 11094}}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from haystack import Pipeline, PredefinedPipeline\n",
    "\n",
    "indexing_pipeline =  Pipeline.from_template(PredefinedPipeline.INDEXING)\n",
    "indexing_pipeline.run(data={\"sources\": ['contextos_por_titulo.csv']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_pipeline =  Pipeline.from_template(PredefinedPipeline.RAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sect of Buddhism that is the only remaining one based in Sanskrit is Tibetan Buddhism.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "query = \"What sect of Buddhism is the only remaining one based in Sanskrit?\"\n",
    "result = rag_pipeline.run(data={\"prompt_builder\": {\"query\":query}, \"text_embedder\": {\"text\": query}})\n",
    "print(result[\"llm\"][\"replies\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nessa\\OneDrive\\Desktop\\coding\\machineLearning\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from haystack import Pipeline\n",
    "from haystack.document_stores.in_memory import InMemoryDocumentStore\n",
    "from haystack.components.retrievers import InMemoryEmbeddingRetriever\n",
    "from haystack.components.converters import TextFileToDocument\n",
    "from haystack.components.preprocessors import DocumentCleaner, DocumentSplitter\n",
    "from haystack.components.embedders import OpenAIDocumentEmbedder, OpenAITextEmbedder\n",
    "from haystack.components.writers import DocumentWriter\n",
    "from haystack.components.builders import PromptBuilder\n",
    "from haystack.components.generators import OpenAIGenerator\n",
    "\n",
    "document_store = InMemoryDocumentStore()\n",
    "\n",
    "text_file_converter = TextFileToDocument()\n",
    "cleaner = DocumentCleaner()\n",
    "splitter = DocumentSplitter()\n",
    "embedder = OpenAIDocumentEmbedder()\n",
    "writer = DocumentWriter(document_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating embeddings: 347it [07:23,  1.28s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'embedder': {'meta': {'model': 'text-embedding-ada-002',\n",
       "   'usage': {'prompt_tokens': 2963116, 'total_tokens': 2963116}}},\n",
       " 'writer': {'documents_written': 11094}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexing_pipeline = Pipeline()\n",
    "indexing_pipeline.add_component(\"converter\", text_file_converter)\n",
    "indexing_pipeline.add_component(\"cleaner\", cleaner)\n",
    "indexing_pipeline.add_component(\"splitter\", splitter)\n",
    "indexing_pipeline.add_component(\"embedder\", embedder)\n",
    "indexing_pipeline.add_component(\"writer\", writer)\n",
    "\n",
    "indexing_pipeline.connect(\"converter.documents\", \"cleaner.documents\")\n",
    "indexing_pipeline.connect(\"cleaner.documents\", \"splitter.documents\")\n",
    "indexing_pipeline.connect(\"splitter.documents\", \"embedder.documents\")\n",
    "indexing_pipeline.connect(\"embedder.documents\", \"writer.documents\")\n",
    "indexing_pipeline.run(data={\"sources\": [\"contextos_por_titulo.csv\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "text_embedder = OpenAITextEmbedder()\n",
    "retriever = InMemoryEmbeddingRetriever(document_store)\n",
    "template = \"\"\"Given these documents, answer the question.\n",
    "                Documents:\n",
    "                {% for doc in documents %}\n",
    "                    {{ doc.content }}\n",
    "                {% endfor %}\n",
    "                Question: {{query}}\n",
    "                Answer:\"\"\"\n",
    "prompt_builder = PromptBuilder(template=template)\n",
    "llm = OpenAIGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<haystack.core.pipeline.pipeline.Pipeline object at 0x000002114A2A4650>\n",
       "üöÖ Components\n",
       "  - text_embedder: OpenAITextEmbedder\n",
       "  - retriever: InMemoryEmbeddingRetriever\n",
       "  - prompt_builder: PromptBuilder\n",
       "  - llm: OpenAIGenerator\n",
       "üõ§Ô∏è Connections\n",
       "  - text_embedder.embedding -> retriever.query_embedding (List[float])\n",
       "  - retriever.documents -> prompt_builder.documents (List[Document])\n",
       "  - prompt_builder.prompt -> llm.prompt (str)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_pipeline = Pipeline()\n",
    "rag_pipeline.add_component(\"text_embedder\", text_embedder)\n",
    "rag_pipeline.add_component(\"retriever\", retriever)\n",
    "rag_pipeline.add_component(\"prompt_builder\", prompt_builder)\n",
    "rag_pipeline.add_component(\"llm\", llm)\n",
    "\n",
    "rag_pipeline.connect(\"text_embedder.embedding\", \"retriever.query_embedding\")\n",
    "rag_pipeline.connect(\"retriever.documents\", \"prompt_builder.documents\")\n",
    "rag_pipeline.connect(\"prompt_builder\", \"llm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The film that featured Destiny's Child's first major single \"Killing Time\" is the soundtrack to the 1997 film, Men in Black.\n"
     ]
    }
   ],
   "source": [
    "query = \"Which film featured Destiny's Child's first major single?\"\n",
    "result = rag_pipeline.run(data={\"prompt_builder\": {\"query\":query}, \"text_embedder\": {\"text\": query}})\n",
    "print(result[\"llm\"][\"replies\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### txtai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: txtai in c:\\users\\nessa\\onedrive\\desktop\\coding\\machinelearning\\.venv\\lib\\site-packages (8.1.0)\n",
      "Requirement already satisfied: autoawq in c:\\users\\nessa\\onedrive\\desktop\\coding\\machinelearning\\.venv\\lib\\site-packages (0.2.6)\n",
      "Requirement already satisfied: faiss-cpu>=1.7.1.post2 in c:\\users\\nessa\\onedrive\\desktop\\coding\\machinelearning\\.venv\\lib\\site-packages (from txtai) (1.9.0.post1)\n",
      "Requirement already satisfied: msgpack>=1.0.7 in c:\\users\\nessa\\onedrive\\desktop\\coding\\machinelearning\\.venv\\lib\\site-packages (from txtai) (1.1.0)\n",
      "Requirement already satisfied: torch>=1.12.1 in c:\\users\\nessa\\onedrive\\desktop\\coding\\machinelearning\\.venv\\lib\\site-packages (from txtai) (2.3.1)\n",
      "Requirement already satisfied: transformers>=4.45.0 in c:\\users\\nessa\\onedrive\\desktop\\coding\\machinelearning\\.venv\\lib\\site-packages (from txtai) (4.47.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.0 in c:\\users\\nessa\\onedrive\\desktop\\coding\\machinelearning\\.venv\\lib\\site-packages (from txtai) (0.27.0)\n",
      "Requirement already satisfied: numpy>=1.18.4 in c:\\users\\nessa\\onedrive\\desktop\\coding\\machinelearning\\.venv\\lib\\site-packages (from txtai) (1.26.4)\n",
      "Requirement already satisfied: pyyaml>=5.3 in c:\\users\\nessa\\onedrive\\desktop\\coding\\machinelearning\\.venv\\lib\\site-packages (from txtai) (6.0.1)\n",
      "Requirement already satisfied: regex>=2022.8.17 in c:\\users\\nessa\\onedrive\\desktop\\coding\\machinelearning\\.venv\\lib\\site-packages (from txtai) (2024.5.10)\n",
      "Requirement already satisfied: tokenizers>=0.12.1 in c:\\users\\nessa\\onedrive\\desktop\\coding\\machinelearning\\.venv\\lib\\site-packages (from autoawq) (0.21.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\nessa\\onedrive\\desktop\\coding\\machinelearning\\.venv\\lib\\site-packages (from autoawq) (4.11.0)\n",
      "Requirement already satisfied: accelerate in c:\\users\\nessa\\onedrive\\desktop\\coding\\machinelearning\\.venv\\lib\\site-packages (from autoawq) (1.2.1)\n",
      "Requirement already satisfied: datasets in c:\\users\\nessa\\onedrive\\desktop\\coding\\machinelearning\\.venv\\lib\\site-packages (from autoawq) (3.2.0)\n",
      "Requirement already satisfied: zstandard in c:\\users\\nessa\\onedrive\\desktop\\coding\\machinelearning\\.venv\\lib\\site-packages (from autoawq) (0.23.0)\n",
      "Requirement already satisfied: autoawq-kernels in c:\\users\\nessa\\onedrive\\desktop\\coding\\machinelearning\\.venv\\lib\\site-packages (from autoawq) (0.0.7)\n",
      "Requirement already satisfied: filelock in c:\\users\\nessa\\onedrive\\desktop\\coding\\machinelearning\\.venv\\lib\\site-packages (from torch>=1.12.1->txtai) (3.14.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\nessa\\onedrive\\desktop\\coding\\machinelearning\\.venv\\lib\\site-packages (from torch>=1.12.1->txtai) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\nessa\\onedrive\\desktop\\coding\\machinelearning\\.venv\\lib\\site-packages (from torch>=1.12.1->txtai) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\nessa\\onedrive\\desktop\\coding\\machinelearning\\.venv\\lib\\site-packages (from torch>=1.12.1->txtai) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\nessa\\onedrive\\desktop\\coding\\machinelearning\\.venv\\lib\\site-packages (from torch>=1.12.1->txtai) (2024.3.1)\n",
      "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\nessa\\onedrive\\desktop\\coding\\machinelearning\\.venv\\lib\\site-packages (from torch>=1.12.1->txtai) (2021.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\nessa\\onedrive\\desktop\\coding\\machinelearning\\.venv\\lib\\site-packages (from faiss-cpu>=1.7.1.post2->txtai) (23.2)\n",
      "Requirement already satisfied: requests in c:\\users\\nessa\\onedrive\\desktop\\coding\\machinelearning\\.venv\\lib\\site-packages (from huggingface-hub>=0.19.0->txtai) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\nessa\\onedrive\\desktop\\coding\\machinelearning\\.venv\\lib\\site-packages (from huggingface-hub>=0.19.0->txtai) (4.66.4)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\nessa\\onedrive\\desktop\\coding\\machinelearning\\.venv\\lib\\site-packages (from transformers>=4.45.0->txtai) (0.4.3)\n",
      "Requirement already satisfied: psutil in c:\\users\\nessa\\onedrive\\desktop\\coding\\machinelearning\\.venv\\lib\\site-packages (from accelerate->autoawq) (5.9.8)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\nessa\\onedrive\\desktop\\coding\\machinelearning\\.venv\\lib\\site-packages (from datasets->autoawq) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\nessa\\onedrive\\desktop\\coding\\machinelearning\\.venv\\lib\\site-packages (from datasets->autoawq) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\nessa\\onedrive\\desktop\\coding\\machinelearning\\.venv\\lib\\site-packages (from datasets->autoawq) (2.2.2)\n",
      "Requirement already satisfied: xxhash in c:\\users\\nessa\\onedrive\\desktop\\coding\\machinelearning\\.venv\\lib\\site-packages (from datasets->autoawq) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\nessa\\onedrive\\desktop\\coding\\machinelearning\\.venv\\lib\\site-packages (from datasets->autoawq) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\nessa\\onedrive\\desktop\\coding\\machinelearning\\.venv\\lib\\site-packages (from datasets->autoawq) (3.9.5)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\nessa\\onedrive\\desktop\\coding\\machinelearning\\.venv\\lib\\site-packages (from aiohttp->datasets->autoawq) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\nessa\\onedrive\\desktop\\coding\\machinelearning\\.venv\\lib\\site-packages (from aiohttp->datasets->autoawq) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\nessa\\onedrive\\desktop\\coding\\machinelearning\\.venv\\lib\\site-packages (from aiohttp->datasets->autoawq) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\nessa\\onedrive\\desktop\\coding\\machinelearning\\.venv\\lib\\site-packages (from aiohttp->datasets->autoawq) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\nessa\\onedrive\\desktop\\coding\\machinelearning\\.venv\\lib\\site-packages (from aiohttp->datasets->autoawq) (1.9.4)\n",
      "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\nessa\\onedrive\\desktop\\coding\\machinelearning\\.venv\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=1.12.1->txtai) (2021.4.0)\n",
      "Requirement already satisfied: tbb==2021.* in c:\\users\\nessa\\onedrive\\desktop\\coding\\machinelearning\\.venv\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=1.12.1->txtai) (2021.12.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\nessa\\onedrive\\desktop\\coding\\machinelearning\\.venv\\lib\\site-packages (from requests->huggingface-hub>=0.19.0->txtai) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\nessa\\onedrive\\desktop\\coding\\machinelearning\\.venv\\lib\\site-packages (from requests->huggingface-hub>=0.19.0->txtai) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\nessa\\onedrive\\desktop\\coding\\machinelearning\\.venv\\lib\\site-packages (from requests->huggingface-hub>=0.19.0->txtai) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nessa\\onedrive\\desktop\\coding\\machinelearning\\.venv\\lib\\site-packages (from requests->huggingface-hub>=0.19.0->txtai) (2024.2.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\nessa\\onedrive\\desktop\\coding\\machinelearning\\.venv\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub>=0.19.0->txtai) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\nessa\\onedrive\\desktop\\coding\\machinelearning\\.venv\\lib\\site-packages (from jinja2->torch>=1.12.1->txtai) (2.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\nessa\\onedrive\\desktop\\coding\\machinelearning\\.venv\\lib\\site-packages (from pandas->datasets->autoawq) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\nessa\\onedrive\\desktop\\coding\\machinelearning\\.venv\\lib\\site-packages (from pandas->datasets->autoawq) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\nessa\\onedrive\\desktop\\coding\\machinelearning\\.venv\\lib\\site-packages (from pandas->datasets->autoawq) (2024.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\nessa\\onedrive\\desktop\\coding\\machinelearning\\.venv\\lib\\site-packages (from sympy->torch>=1.12.1->txtai) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\nessa\\onedrive\\desktop\\coding\\machinelearning\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets->autoawq) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install txtai autoawq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from txtai import Embeddings, RAG, LLM\n",
    "from txtai.pipeline import Extractor\n",
    "\n",
    "embeddings = Embeddings(path=\"sentence-transformers/nli-mpnet-base-v2\", content=True, autoid=\"uuid5\")\n",
    "embeddings.index(contextos_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "extractor = Extractor(embeddings, \"google/flan-t5-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Who killed the Shakyas?\"\n",
    "context = lambda question: [{\"query\": question, \"question\": f\"Answer the following question using the context below. \\n Question: {question} \\n Context:\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'answer': 'Contextos'}]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extractor(context(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "rag = RAG(embeddings, \"google/flan-t5-large\", template=\"\"\"\n",
    "    Answer the following question using the provided context.\n",
    "\n",
    "    Question:\n",
    "    {question}\n",
    "\n",
    "    Context:\n",
    "    {context}\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': 'el hombre'}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "rag(\"Who killed the Shakyas?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
